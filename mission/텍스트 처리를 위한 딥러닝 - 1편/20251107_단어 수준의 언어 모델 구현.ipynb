{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMrOOpAChP5OIySdlOjEp8k"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import numpy as np\n","import random"],"metadata":{"id":"SHQghmLswvL6","executionInfo":{"status":"ok","timestamp":1762490152821,"user_tz":-540,"elapsed":6,"user":{"displayName":"김광무","userId":"03808645168826839149"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["\n","# --- 1. 학습용 데이터 (Corpus) 준비 ---\n","\n","# 간단하고 반복적인 텍스트가 학습에 용이합니다.\n","corpus_text = (\n","    \"Mary had a little lamb, its fleece was white as snow. \"\n","    \"And everywhere that Mary went, the lamb was sure to go. \"\n","    \"It followed her to school one day, which was against the rule. \"\n","    \"It made the children laugh and play, to see a lamb at school.\"\n",")"],"metadata":{"id":"G6LTwrrBl_PZ","executionInfo":{"status":"ok","timestamp":1762490054597,"user_tz":-540,"elapsed":21,"user":{"displayName":"김광무","userId":"03808645168826839149"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["\n","# 1.1. 전처리 (1단계와 유사하나, 불용어/어간추출 안 함)\n","# 언어 모델은 'a', 'the', 'was' 같은 단어도 예측해야 하므로 제거하지 않습니다.\n","tokens = corpus_text.lower().split()\n","\n","# 1.2. 어휘 사전 (Vocabulary) 구축\n","# 각 고유 단어에 정수 인덱스(ID)를 부여합니다.\n","vocab = sorted(list(set(tokens)))\n","word_to_idx = {word: i for i, word in enumerate(vocab)}\n","idx_to_word = {i: word for i, word in enumerate(vocab)}\n","\n","VOCAB_SIZE = len(vocab)\n","print(f\"총 어휘 수 (VOCAB_SIZE): {VOCAB_SIZE}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GGzesCYVmBhV","executionInfo":{"status":"ok","timestamp":1762490064392,"user_tz":-540,"elapsed":43,"user":{"displayName":"김광무","userId":"03808645168826839149"}},"outputId":"fa271859-f4d9-47ba-b2b2-41d0c080ffdb"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["총 어휘 수 (VOCAB_SIZE): 36\n"]}]},{"cell_type":"code","source":["\n","\n","# 1.3. 학습 데이터셋 (Sliding Window) 생성\n","# (예: \"Mary had a\" -> \"little\")\n","CONTEXT_SIZE = 3 # 예측을 위해 참고할 이전 단어 수\n","sequences = []\n","\n","# \"슬라이딩 윈도우\" 방식으로 (X, y) 페어를 만듭니다.\n","# [Image of sliding window for NLP text data]\n","for i in range(len(tokens) - CONTEXT_SIZE):\n","  context = tokens[i : i + CONTEXT_SIZE]\n","  target = tokens[i + CONTEXT_SIZE]\n","\n","  # 단어를 인덱스로 변환\n","  context_indices = [word_to_idx[w] for w in context]\n","  target_index = word_to_idx[target]\n","\n","  sequences.append((context_indices, target_index))\n","\n","print(f\"총 학습 시퀀스 수: {len(sequences)}\")\n","print(f\"첫 번째 시퀀스 (X, y): {sequences[0]}\")\n","print(f\"  -> ({[idx_to_word[i] for i in sequences[0][0]]}, {idx_to_word[sequences[0][1]]})\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XM9lYgXYmD6q","executionInfo":{"status":"ok","timestamp":1762490172787,"user_tz":-540,"elapsed":5,"user":{"displayName":"김광무","userId":"03808645168826839149"}},"outputId":"6160e279-f3a9-4350-83f8-b82fe86c3fb0"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["총 학습 시퀀스 수: 44\n","첫 번째 시퀀스 (X, y): ([20, 11, 0], 18)\n","  -> (['mary', 'had', 'a'], little)\n"]}]},{"cell_type":"code","source":["# --- 2. 헬퍼 함수 (데이터 -> 텐서) ---\n","def sequence_to_tensor(context_indices, target_index):\n","  # X (Context) 텐서\n","  context_tensor = torch.tensor(context_indices, dtype=torch.long)\n","  # y (Target) 텐서\n","  target_tensor = torch.tensor([target_index], dtype=torch.long)\n","  return context_tensor, target_tensor\n","\n"],"metadata":{"id":"Fq7CjaL6mF7B","executionInfo":{"status":"ok","timestamp":1762490173284,"user_tz":-540,"elapsed":3,"user":{"displayName":"김광무","userId":"03808645168826839149"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["# --- 3. LSTM 언어 모델 정의 (수정) ---\n","\n","class WordLSTM_LM(nn.Module):\n","  def __init__(self, vocab_size, embedding_dim, hidden_dim):\n","    super(WordLSTM_LM, self).__init__()\n","\n","    # [수정 1] hidden_dim을 self 변수로 저장합니다.\n","    self.hidden_dim = hidden_dim\n","\n","    # 4단계 개념: (중요) 임베딩 레이어\n","    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","\n","    # 3단계 개념: LSTM 레이어\n","    self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n","\n","    # 3단계 개념: 출력 레이어\n","    self.linear = nn.Linear(hidden_dim, vocab_size)\n","\n","    # LogSoftmax (NLLLoss와 짝)\n","    self.log_softmax = nn.LogSoftmax(dim=1)\n","\n","  def forward(self, context_tensor, hidden, cell):\n","    # context_tensor: [CONTEXT_SIZE]\n","\n","    # 1. 임베딩: [CONTEXT_SIZE] -> [CONTEXT_SIZE, EMBEDDING_DIM]\n","    embeds = self.embedding(context_tensor).view(len(context_tensor), 1, -1)\n","\n","    # 2. LSTM:\n","    lstm_out, (hidden, cell) = self.lstm(embeds, (hidden, cell))\n","\n","    # 3. Linear: 시퀀스의 '마지막' 출력만 사용\n","    output = self.linear(lstm_out[-1])\n","\n","    # 4. Softmax\n","    log_probs = self.log_softmax(output)\n","\n","    return log_probs, hidden, cell\n","\n","  def init_hidden_cell(self, batch_size=1):\n","    # [수정 2] hidden_dim 대신 self.hidden_dim을 사용합니다.\n","    return (torch.zeros(1, batch_size, self.hidden_dim),\n","            torch.zeros(1, batch_size, self.hidden_dim))"],"metadata":{"id":"a4AVx7xEmIH0","executionInfo":{"status":"ok","timestamp":1762490226437,"user_tz":-540,"elapsed":8,"user":{"displayName":"김광무","userId":"03808645168826839149"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["# --- 4. 모델 학습 ---\n","\n","# 4.1. 하이퍼파라미터 설정\n","EMBEDDING_DIM = 50  # 4단계의 vector_size와 동일한 개념\n","HIDDEN_DIM = 64     # 3단계의 hidden_size와 동일한 개념\n","N_EPOCHS = 200\n","learning_rate = 0.01"],"metadata":{"id":"TcWA0Fd_mLXb","executionInfo":{"status":"ok","timestamp":1762490174980,"user_tz":-540,"elapsed":3,"user":{"displayName":"김광무","userId":"03808645168826839149"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["\n","model = WordLSTM_LM(VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_DIM)\n","criterion = nn.NLLLoss() # LogSoftmax와 짝\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","print(\"\\n--- 모델 학습 시작 ---\")\n","\n","for epoch in range(1, N_EPOCHS + 1):\n","  epoch_loss = 0\n","\n","  # (데이터셋이 작으므로 매번 셔플하며 학습)\n","  random.shuffle(sequences)\n","\n","  for context_indices, target_index in sequences:\n","    # 1. 데이터 준비\n","    context_tensor, target_tensor = sequence_to_tensor(context_indices, target_index)\n","\n","    # 2. 그래디언트/은닉 상태 초기화\n","    model.zero_grad()\n","    hidden, cell = model.init_hidden_cell()\n","\n","    # 3. 순전파 (Forward)\n","    log_probs, hidden, cell = model(context_tensor, hidden, cell)\n","\n","    # 4. 손실 계산 및 역전파\n","    loss = criterion(log_probs, target_tensor)\n","    loss.backward()\n","    optimizer.step()\n","\n","    epoch_loss += loss.item()\n","\n","  if epoch % 20 == 0:\n","    print(f\"Epoch {epoch:3d} / {N_EPOCHS} | Avg Loss: {epoch_loss / len(sequences):.4f}\")\n","\n","print(\"--- 모델 학습 완료 ---\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CVKcOZzTmNRk","executionInfo":{"status":"ok","timestamp":1762490251106,"user_tz":-540,"elapsed":19819,"user":{"displayName":"김광무","userId":"03808645168826839149"}},"outputId":"840e1d47-3f97-4941-e744-283faa9ff7a9"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- 모델 학습 시작 ---\n","Epoch  20 / 200 | Avg Loss: 0.0032\n","Epoch  40 / 200 | Avg Loss: 0.0009\n","Epoch  60 / 200 | Avg Loss: 0.0004\n","Epoch  80 / 200 | Avg Loss: 0.0002\n","Epoch 100 / 200 | Avg Loss: 0.0001\n","Epoch 120 / 200 | Avg Loss: 0.0001\n","Epoch 140 / 200 | Avg Loss: 0.0000\n","Epoch 160 / 200 | Avg Loss: 0.0000\n","Epoch 180 / 200 | Avg Loss: 0.0000\n","Epoch 200 / 200 | Avg Loss: 0.0000\n","--- 모델 학습 완료 ---\n"]}]},{"cell_type":"code","source":["\n","# --- 5. 모델 평가 (다음 단어 예측) ---\n","\n","def predict_next_word(seed_text):\n","  print(f\"\\n--- 예측 테스트 ---\")\n","  print(f\"입력: '{seed_text}'\")\n","\n","  # (중요) 평가 모드 + 그래디언트 계산 중지\n","  model.eval()\n","  with torch.no_grad():\n","    # 1. 입력 텍스트 전처리\n","    seed_tokens = seed_text.lower().split()\n","    if len(seed_tokens) < CONTEXT_SIZE:\n","      print(f\"Error: 입력 텍스트는 최소 {CONTEXT_SIZE}단어여야 합니다.\")\n","      return\n","\n","    # 마지막 CONTEXT_SIZE 만큼의 단어만 사용\n","    context_tokens = seed_tokens[-CONTEXT_SIZE:]\n","    context_indices = [word_to_idx[w] for w in context_tokens]\n","    context_tensor = torch.tensor(context_indices, dtype=torch.long)\n","\n","    # 2. 모델 예측\n","    hidden, cell = model.init_hidden_cell()\n","    log_probs, _, _ = model(context_tensor, hidden, cell)\n","\n","    # 3. 결과 해석\n","    # 가장 확률이 높은(log_prob가 가장 큰) 단어의 인덱스\n","    predicted_idx = torch.argmax(log_probs).item()\n","    predicted_word = idx_to_word[predicted_idx]\n","\n","    print(f\"예측된 다음 단어: '{predicted_word}'\")"],"metadata":{"id":"AwfwgSD1mQL2","executionInfo":{"status":"ok","timestamp":1762490256630,"user_tz":-540,"elapsed":5,"user":{"displayName":"김광무","userId":"03808645168826839149"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["\n","\n","# 테스트 실행\n","predict_next_word(\"Mary had a\")\n","predict_next_word(\"lamb was sure\")\n","predict_next_word(\"children laugh and\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LI9bBOfwl2xi","executionInfo":{"status":"ok","timestamp":1762490257827,"user_tz":-540,"elapsed":23,"user":{"displayName":"김광무","userId":"03808645168826839149"}},"outputId":"3e9e0921-ac7e-43b3-bae9-46f19628f32f"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- 예측 테스트 ---\n","입력: 'Mary had a'\n","예측된 다음 단어: 'little'\n","\n","--- 예측 테스트 ---\n","입력: 'lamb was sure'\n","예측된 다음 단어: 'to'\n","\n","--- 예측 테스트 ---\n","입력: 'children laugh and'\n","예측된 다음 단어: 'play,'\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"3Yt6UJLJmxCr"},"execution_count":null,"outputs":[]}]}