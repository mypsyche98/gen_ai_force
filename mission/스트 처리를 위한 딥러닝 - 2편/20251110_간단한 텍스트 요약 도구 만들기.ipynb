{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOM/4J/mjQlY61lQQtvy+1M"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install networkx scikit-learn"],"metadata":{"id":"SHQghmLswvL6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1762493738893,"user_tz":-540,"elapsed":6892,"user":{"displayName":"김광무","userId":"03808645168826839149"}},"outputId":"88d60726-0e0e-4423-ceff-b9df35d41584"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (3.5)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n","Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (2.0.2)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n"]}]},{"cell_type":"code","source":["import nltk\n","from nltk.tokenize import sent_tokenize, word_tokenize\n","from nltk.corpus import stopwords\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","import networkx as nx  # (pip install networkx)\n","import numpy as np\n"],"metadata":{"id":"qfSdMfRNz_q2","executionInfo":{"status":"ok","timestamp":1762493773763,"user_tz":-540,"elapsed":10880,"user":{"displayName":"김광무","userId":"03808645168826839149"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["nltk.download('punkt')\n","nltk.download('punkt_tab')\n","nltk.download('stopwords')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DmyswC7b0LAg","executionInfo":{"status":"ok","timestamp":1762493781968,"user_tz":-540,"elapsed":918,"user":{"displayName":"김광무","userId":"03808645168826839149"}},"outputId":"f1ed2881-62aa-4399-d591-cf52b682f661"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["stop_words = set(stopwords.words('english'))"],"metadata":{"id":"OWUJLdAp0M54","executionInfo":{"status":"ok","timestamp":1762493782187,"user_tz":-540,"elapsed":11,"user":{"displayName":"김광무","userId":"03808645168826839149"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# --- 2. 텍스트 전처리 헬퍼 함수 (2-2 재사용) ---\n","def preprocess_for_vectorizing(text):\n","  \"\"\"TF-IDF 벡터화를 위한 전처리 함수\"\"\"\n","  tokens = word_tokenize(text.lower())\n","  filtered_tokens = [\n","    word for word in tokens\n","    if word.isalpha() and word not in stop_words\n","  ]\n","  # 벡터라이저는 토큰 리스트가 아닌 '공백으로 연결된 문자열'을 받음\n","  return \" \".join(filtered_tokens)"],"metadata":{"id":"Pz3IK2t40PRC","executionInfo":{"status":"ok","timestamp":1762493790518,"user_tz":-540,"elapsed":20,"user":{"displayName":"김광무","userId":"03808645168826839149"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# --- 3. 텍스트 요약기 함수 (TextRank) ---\n","\n","def summarize_text_textrank(text, top_n=3):\n","  \"\"\"\n","  TextRank(PageRank) 알고리즘을 사용하여 텍스트를 요약합니다.\n","  \"\"\"\n","\n","  # 1. 텍스트 -> 문장 리스트로 분리 (원본 문장 유지)\n","  original_sentences = sent_tokenize(text)\n","\n","  if len(original_sentences) <= top_n:\n","    return text # 요약할 필요가 없음\n","\n","  # 2. 각 문장을 TF-IDF 벡터화를 위해 전처리\n","  processed_sentences = [preprocess_for_vectorizing(s) for s in original_sentences]\n","\n","  # 3. TF-IDF 벡터화 (2-2의 GloVe 대신 사용, 더 간편함)\n","  vectorizer = TfidfVectorizer()\n","  try:\n","    tfidf_matrix = vectorizer.fit_transform(processed_sentences)\n","  except ValueError:\n","    # 모든 문장이 불용어로만 구성된 경우\n","    return \"\"\n","\n","  # 4. 문장 간 코사인 유사도 행렬 생성 (2-2 응용)\n","  #\n","  # (N x N 크기의 행렬, N=문장 수)\n","  # 이 행렬이 바로 문장 간의 '그래프(Graph)'입니다.\n","  similarity_matrix = cosine_similarity(tfidf_matrix)\n","\n","  # 5. 유사도 행렬을 NetworkX 그래프로 변환\n","  G = nx.from_numpy_array(similarity_matrix)\n","\n","  # 6. PageRank 알고리즘 적용 (문장 중요도 점수 계산)\n","  #\n","  # \"링크가 많은 페이지\" -> \"유사도가 높은 문장\"\n","  scores = nx.pagerank(G) # {0: 0.05, 1: 0.12, ...}\n","\n","  # 7. 점수(score) 기준으로 상위 N개 문장 인덱스 추출\n","  ranked_sentences = sorted(\n","    ((scores[i], s, i) for i, s in enumerate(original_sentences)),\n","    reverse=True\n","  )\n","  # (score, \"원본 문장\", original_index) 튜플의 리스트\n","\n","  top_sentence_tuples = ranked_sentences[:top_n]\n","\n","  # 8. (중요) 요약문 생성: 상위 N개 문장을 '원본 순서대로' 재정렬\n","  # (점수 순으로 나열하면 요약문의 흐름이 엉망이 됨)\n","  top_sentence_tuples_sorted_by_index = sorted(\n","    top_sentence_tuples,\n","    key=lambda x: x[2] # original_index(x[2]) 기준 정렬\n","  )\n","\n","  # 9. 최종 요약문 반환\n","  summary = \" \".join([s for score, s, i in top_sentence_tuples_sorted_by_index])\n","  return summary"],"metadata":{"id":"_m5dzMoN0ReC","executionInfo":{"status":"ok","timestamp":1762493823488,"user_tz":-540,"elapsed":40,"user":{"displayName":"김광무","userId":"03808645168826839149"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["\n","\n","# --- 4. \"응용 프로그램\" 테스트 ---\n","\n","# 요약 테스트를 위한 긴 예제 텍스트\n","example_long_text = \"\"\"\n","Natural Language Processing (NLP) is a dynamic subfield of artificial intelligence (AI)\n","that focuses on the interaction between computers and human language.\n","The ultimate goal of NLP is to enable computers to process, understand, and generate\n","human language in a way that is both meaningful and valuable.\n","NLP has many real-world applications, including machine translation,\n","sentiment analysis, and virtual assistants like Siri and Alexa.\n","Core tasks in NLP involve tokenization, parsing, and stop-word removal,\n","which are foundational steps. More advanced techniques rely on machine learning\n","and deep learning models. For example, Recurrent Neural Networks (RNNs) were\n","popular for sequence tasks, but Transformers have now become the industry standard.\n","These models are incredibly large but provide state-of-the-art results.\n","The field continues to evolve rapidly, driving innovation across many sectors.\n","\"\"\"\n","\n","print(\"--- 원본 텍스트 ---\")\n","print(example_long_text)\n","print(\"-\" * 30)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"06UVrDQT0ZLT","executionInfo":{"status":"ok","timestamp":1762493831160,"user_tz":-540,"elapsed":22,"user":{"displayName":"김광무","userId":"03808645168826839149"}},"outputId":"b9a93575-4129-4ff1-96c9-6f1454ba5af8"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["--- 원본 텍스트 ---\n","\n","Natural Language Processing (NLP) is a dynamic subfield of artificial intelligence (AI) \n","that focuses on the interaction between computers and human language. \n","The ultimate goal of NLP is to enable computers to process, understand, and generate \n","human language in a way that is both meaningful and valuable. \n","NLP has many real-world applications, including machine translation, \n","sentiment analysis, and virtual assistants like Siri and Alexa. \n","Core tasks in NLP involve tokenization, parsing, and stop-word removal, \n","which are foundational steps. More advanced techniques rely on machine learning \n","and deep learning models. For example, Recurrent Neural Networks (RNNs) were \n","popular for sequence tasks, but Transformers have now become the industry standard. \n","These models are incredibly large but provide state-of-the-art results. \n","The field continues to evolve rapidly, driving innovation across many sectors.\n","\n","------------------------------\n"]}]},{"cell_type":"code","source":["\n","# 2문장으로 요약 요청\n","summary = summarize_text_textrank(example_long_text, top_n=2)\n","\n","print(f\"--- 2문장 요약 결과 ---\")\n","print(summary)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mSFhBGy40HeI","executionInfo":{"status":"ok","timestamp":1762493836169,"user_tz":-540,"elapsed":70,"user":{"displayName":"김광무","userId":"03808645168826839149"}},"outputId":"56c62cb9-3651-46e5-a39d-9ee9438484cb"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["--- 2문장 요약 결과 ---\n","The ultimate goal of NLP is to enable computers to process, understand, and generate \n","human language in a way that is both meaningful and valuable. NLP has many real-world applications, including machine translation, \n","sentiment analysis, and virtual assistants like Siri and Alexa.\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"F6jfE0Ht0aqU"},"execution_count":null,"outputs":[]}]}