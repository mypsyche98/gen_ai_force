{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMc1t/BEHTxX1nv+Vsc8xNS"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install -q scikit-learn numpy python-levenshtein gensim"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a-de9mnbys3-","executionInfo":{"status":"ok","timestamp":1762493445396,"user_tz":-540,"elapsed":12610,"user":{"displayName":"김광무","userId":"03808645168826839149"}},"outputId":"995025fa-c2b8-4dcb-f5d9-083dccaecfa7"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.3/153.3 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m57.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m73.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","source":["import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","import numpy as np\n","import Levenshtein  # (pip install python-levenshtein)\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","import gensim.downloader as api\n","import warnings\n","\n","# 경고 무시\n","warnings.filterwarnings('ignore')"],"metadata":{"id":"zUhfTOWDys1P","executionInfo":{"status":"ok","timestamp":1762493492944,"user_tz":-540,"elapsed":11929,"user":{"displayName":"김광무","userId":"03808645168826839149"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["nltk.download('punkt')\n","nltk.download('punkt_tab')\n","nltk.download('stopwords')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TaTZ5VO4ysyY","executionInfo":{"status":"ok","timestamp":1762493502609,"user_tz":-540,"elapsed":1697,"user":{"displayName":"김광무","userId":"03808645168826839149"}},"outputId":"28668ae1-11a9-4c61-ac61-fdc6de0302ea"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["\n","# 2-1편의 도구를 재사용\n","stop_words_set = set(stopwords.words('english'))\n","\n","# --- 2. 범용 전처리 함수 ---\n","def preprocess_to_tokens(text, stop_words):\n","  \"\"\"전처리: 토큰화, 소문자화, 불용어/구두점 제거 후 '토큰 리스트' 반환\"\"\"\n","  tokens = word_tokenize(text.lower())\n","  filtered_tokens = [\n","    word for word in tokens\n","    if word.isalpha() and word not in stop_words\n","  ]\n","  return filtered_tokens"],"metadata":{"id":"_cwKp5sozLsR","executionInfo":{"status":"ok","timestamp":1762493514044,"user_tz":-540,"elapsed":9,"user":{"displayName":"김광무","userId":"03808645168826839149"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["\n","\n","# --- 3. 유사도 계산기 구현 ---\n","\n","## 3-1. 집합 기반 (Set-based) 유사도\n","# (단어의 공유 여부에만 관심)\n","\n","def jaccard_similarity(text1, text2, stop_words):\n","  \"\"\"자카드 유사도: (교집합 크기) / (합집합 크기)\"\"\"\n","  set1 = set(preprocess_to_tokens(text1, stop_words))\n","  set2 = set(preprocess_to_tokens(text2, stop_words))\n","\n","  intersection = set1.intersection(set2)\n","  union = set1.union(set2)\n","\n","  if not union:\n","    return 0.0\n","  return len(intersection) / len(union)\n","\n","def dice_coefficient(text1, text2, stop_words):\n","  \"\"\"다이스 계수: 2 * (교집합 크기) / (set1 크기 + set2 크기)\"\"\"\n","  set1 = set(preprocess_to_tokens(text1, stop_words))\n","  set2 = set(preprocess_to_tokens(text2, stop_words))\n","\n","  intersection = set1.intersection(set2)\n","\n","  if not set1 and not set2:\n","    return 0.0\n","  return (2 * len(intersection)) / (len(set1) + len(set2))\n","\n","## 3-2. 편집 거리 기반 (Edit-based) 유사도\n","# (문자열 자체의 모양에만 관심, 오타/수정에 사용)\n","\n","def levenshtein_similarity(text1, text2):\n","  \"\"\"레벤슈타인 유사도: (1 - 편집 거리 / 최대 길이)\"\"\"\n","  # 전처리 없이 원본 텍스트(소문자)를 비교\n","  s1 = text1.lower()\n","  s2 = text2.lower()\n","\n","  # 거리(Distance) 계산 (0이면 동일)\n","  distance = Levenshtein.distance(s1, s2)\n","\n","  # 유사도(Similarity)로 변환 (1이면 동일)\n","  max_len = max(len(s1), len(s2))\n","  if max_len == 0:\n","    return 1.0\n","\n","  return 1.0 - (distance / max_len)\n","\n","\n","## 3-3. 벡터 기반 (Vector-based) 유사도\n","# (단어의 의미/중요도에 관심)\n","\n","# 가. TF-IDF + 코사인 유사도\n","tfidf_vectorizer = TfidfVectorizer()\n","\n","def tfidf_cosine_similarity(text1, text2, stop_words):\n","  \"\"\"TF-IDF 벡터 간의 코사인 유사도\"\"\"\n","  # TF-IDF는 문맥(corpus)이 있어야 하지만,\n","  # 여기서는 두 문장만으로 간단히 벡터화합니다.\n","\n","  # 전처리된 '문장' (토큰 리스트가 아님)이 필요\n","  text1_processed = \" \".join(preprocess_to_tokens(text1, stop_words))\n","  text2_processed = \" \".join(preprocess_to_tokens(text2, stop_words))\n","\n","  try:\n","    tfidf_matrix = tfidf_vectorizer.fit_transform([text1_processed, text2_processed])\n","    return cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]\n","  except ValueError:\n","    # (예: 두 문장 모두 불용어로만 구성되어 어휘가 0개일 때)\n","    return 0.0\n","\n","\n","# 나. (1편 응용) GloVe 임베딩 + 코사인 유사도\n","def load_glove_model():\n","  \"\"\"사전 학습된 GloVe 모델 로드\"\"\"\n","  try:\n","    # gensim.downloader를 api로 임포트 (정상 작동)\n","    glove_model = api.load(\"glove-wiki-gigaword-50\")\n","    print(\"\\n(GloVe-wiki-gigaword-50 모델 로드 완료)\")\n","    return glove_model\n","  except Exception as e:\n","    print(f\"\\n모델 로드 실패: {e}\")\n","    print(\"인터넷 연결을 확인하거나 'glove-wiki-gigaword-50' 모델을 수동으로 다운로드하세요.\")\n","    return None\n","\n","# (이전 코드에서 이미 로드 시도했으므로, 다시 로드)\n","glove_model = load_glove_model()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j4qk8AUAzR_A","executionInfo":{"status":"ok","timestamp":1762493588894,"user_tz":-540,"elapsed":48639,"user":{"displayName":"김광무","userId":"03808645168826839149"}},"outputId":"cd2bddb8-b072-4c05-c00f-051f3474dc87"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["[==================================================] 100.0% 66.0/66.0MB downloaded\n","\n","(GloVe-wiki-gigaword-50 모델 로드 완료)\n"]}]},{"cell_type":"code","source":["def get_sentence_vector(text, model, stop_words):\n","  \"\"\"문장을 단어 벡터의 '평균'으로 변환 (Sentence Embedding)\"\"\"\n","  tokens = preprocess_to_tokens(text, stop_words)\n","  vectors = []\n","\n","  for word in tokens:\n","    if word in model: # 모델 어휘 사전에 단어가 있는지 확인\n","      vectors.append(model[word])\n","\n","  if not vectors:\n","    return np.zeros(model.vector_size)\n","\n","  #\n","  # 모든 유효한 단어 벡터의 평균을 내어 '문장 벡터' 생성\n","  return np.mean(vectors, axis=0)\n","\n","\n","def glove_cosine_similarity(text1, text2, model, stop_words):\n","  \"\"\"GloVe 임베딩 벡터 간의 코사인 유사도\"\"\"\n","  if model is None:\n","    return 0.0\n","\n","  v1 = get_sentence_vector(text1, model, stop_words)\n","  v2 = get_sentence_vector(text2, model, stop_words)\n","\n","  # sklearn은 2D 배열 입력을 요구하므로 reshape\n","  v1 = v1.reshape(1, -1)\n","  v2 = v2.reshape(1, -1)\n","\n","  #\n","  return cosine_similarity(v1, v2)[0][0]\n"],"metadata":{"id":"YKuKiJwkzVyL","executionInfo":{"status":"ok","timestamp":1762493593717,"user_tz":-540,"elapsed":4,"user":{"displayName":"김광무","userId":"03808645168826839149"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["\n","# --- 4. \"응용 프로그램\" 테스트 ---\n","\n","# 테스트할 문장 쌍\n","s1 = \"The king rules the country\"\n","s2 = \"The queen governs the nation\"\n","s3 = \"I love this great movie\"\n","s4 = \"I hate this terrible film\"\n","s5 = \"How old are you?\"\n","s6 = \"What is your age?\"\n","\n","print(\"\\n--- [비교 1: 의미는 비슷하나 단어가 다름] ---\")\n","print(f\"  S1: {s1}\")\n","print(f\"  S2: {s2}\")\n","print(f\"  > Jaccard (단어 공유):     {jaccard_similarity(s1, s2, stop_words_set):.4f}\")\n","print(f\"  > TF-IDF+Cosine (중요도):  {tfidf_cosine_similarity(s1, s2, stop_words_set):.4f}\")\n","print(f\"  > GloVe+Cosine (의미):    {glove_cosine_similarity(s1, s2, glove_model, stop_words_set):.4f}\")\n","print(f\"  > Levenshtein (문자열):   {levenshtein_similarity(s1, s2):.4f}\")\n","\n","\n","print(\"\\n--- [비교 2: 구조는 비슷하나 의미가 반대] ---\")\n","print(f\"  S3: {s3}\")\n","print(f\"  S4: {s4}\")\n","print(f\"  > Jaccard (단어 공유):     {jaccard_similarity(s3, s4, stop_words_set):.4f}\")\n","print(f\"  > TF-IDF+Cosine (중요도):  {tfidf_cosine_similarity(s3, s4, stop_words_set):.4f}\")\n","print(f\"  > GloVe+Cosine (의미):    {glove_cosine_similarity(s3, s4, glove_model, stop_words_set):.4f}\")\n","print(f\"  > Levenshtein (문자열):   {levenshtein_similarity(s3, s4):.4f}\")\n","\n","\n","print(\"\\n--- [비교 3: 의미는 같으나 표현이 다름] ---\")\n","print(f\"  S5: {s5}\")\n","print(f\"  S6: {s6}\")\n","print(f\"  > Jaccard (단어 공유):     {jaccard_similarity(s5, s6, stop_words_set):.4f}\")\n","print(f\"  > TF-IDF+Cosine (중요도):  {tfidf_cosine_similarity(s5, s6, stop_words_set):.4f}\")\n","print(f\"  > GloVe+Cosine (의미):    {glove_cosine_similarity(s5, s6, glove_model, stop_words_set):.4f}\")\n","print(f\"  > Levenshtein (문자열):   {levenshtein_similarity(s5, s6):.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D_6ZUOOBydQ0","executionInfo":{"status":"ok","timestamp":1762493596208,"user_tz":-540,"elapsed":74,"user":{"displayName":"김광무","userId":"03808645168826839149"}},"outputId":"51e33b15-24c2-42a6-beb3-31d1954064fa"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- [비교 1: 의미는 비슷하나 단어가 다름] ---\n","  S1: The king rules the country\n","  S2: The queen governs the nation\n","  > Jaccard (단어 공유):     0.0000\n","  > TF-IDF+Cosine (중요도):  0.0000\n","  > GloVe+Cosine (의미):    0.8530\n","  > Levenshtein (문자열):   0.3929\n","\n","--- [비교 2: 구조는 비슷하나 의미가 반대] ---\n","  S3: I love this great movie\n","  S4: I hate this terrible film\n","  > Jaccard (단어 공유):     0.0000\n","  > TF-IDF+Cosine (중요도):  0.0000\n","  > GloVe+Cosine (의미):    0.8566\n","  > Levenshtein (문자열):   0.4400\n","\n","--- [비교 3: 의미는 같으나 표현이 다름] ---\n","  S5: How old are you?\n","  S6: What is your age?\n","  > Jaccard (단어 공유):     0.0000\n","  > TF-IDF+Cosine (중요도):  0.0000\n","  > GloVe+Cosine (의미):    0.6041\n","  > Levenshtein (문자열):   0.2353\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"UFFS3mcyzgE8"},"execution_count":null,"outputs":[]}]}