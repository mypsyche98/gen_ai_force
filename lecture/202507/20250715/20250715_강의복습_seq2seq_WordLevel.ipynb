{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 6029,
     "status": "ok",
     "timestamp": 1752558941680,
     "user": {
      "displayName": "김광무",
      "userId": "03808645168826839149"
     },
     "user_tz": -540
    },
    "id": "oJWRnk7DRWsm"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import unicodedata\n",
    "import urllib3\n",
    "from tensorflow.keras.layers import Embedding, GRU, Dense\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2598,
     "status": "ok",
     "timestamp": 1752558944281,
     "user": {
      "displayName": "김광무",
      "userId": "03808645168826839149"
     },
     "user_tz": -540
    },
    "id": "QjpVnDGwpvqZ",
    "outputId": "683ba6ca-5ff2-4e25-fb5f-f53208305f7b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZIP file downloaded to fra-eng.zip\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "def download_zip(url, output_path):\n",
    "    response = requests.get(url, headers=headers, stream=True)\n",
    "    if response.status_code == 200:\n",
    "        with open(output_path, 'wb') as f:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "        print(f\"ZIP file downloaded to {output_path}\")\n",
    "    else:\n",
    "        print(f\"Failed to download. HTTP Response Code: {response.status_code}\")\n",
    "\n",
    "url = \"http://www.manythings.org/anki/fra-eng.zip\"\n",
    "output_path = \"fra-eng.zip\"\n",
    "download_zip(url, output_path)\n",
    "\n",
    "path = os.getcwd()\n",
    "zipfilename = os.path.join(path, output_path)\n",
    "\n",
    "with zipfile.ZipFile(zipfilename, 'r') as zip_ref:\n",
    "    zip_ref.extractall(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 992,
     "status": "ok",
     "timestamp": 1752558963758,
     "user": {
      "displayName": "김광무",
      "userId": "03808645168826839149"
     },
     "user_tz": -540
    },
    "id": "cldmr2jlpx70",
    "outputId": "7fd56c54-74b1-406f-bcc4-02ecf2bf30a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 개수 : 237838\n"
     ]
    }
   ],
   "source": [
    "lines = pd.read_csv('fra.txt', names=['src', 'tar', 'lic'], sep='\\t')\n",
    "del lines['lic']\n",
    "print('전체 샘플의 개수 :',len(lines))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "executionInfo": {
     "elapsed": 51,
     "status": "ok",
     "timestamp": 1752558972661,
     "user": {
      "displayName": "김광무",
      "userId": "03808645168826839149"
     },
     "user_tz": -540
    },
    "id": "KA779WfMp4ft",
    "outputId": "143fe400-923c-419b-c684-2b4f40909342"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>tar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26102</th>\n",
       "      <td>Don't look for us.</td>\n",
       "      <td>Ne nous cherchez pas !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19565</th>\n",
       "      <td>Breathe normally.</td>\n",
       "      <td>Respire normalement.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17319</th>\n",
       "      <td>She deserved it.</td>\n",
       "      <td>Elle l'a mérité.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6132</th>\n",
       "      <td>Make a guess.</td>\n",
       "      <td>Hasarde une hypothèse !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25366</th>\n",
       "      <td>You're prisoners.</td>\n",
       "      <td>Vous êtes prisonniers.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21854</th>\n",
       "      <td>I'm nearly ready.</td>\n",
       "      <td>Je suis pratiquement prêt.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1247</th>\n",
       "      <td>I'm brave.</td>\n",
       "      <td>Je suis courageux.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32312</th>\n",
       "      <td>Are we ready to go?</td>\n",
       "      <td>Sommes-nous prêtes à y aller ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24754</th>\n",
       "      <td>When does it end?</td>\n",
       "      <td>Quand cela finit-il ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25486</th>\n",
       "      <td>You're very wise.</td>\n",
       "      <td>Vous êtes fort avisée.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       src                             tar\n",
       "26102   Don't look for us.          Ne nous cherchez pas !\n",
       "19565    Breathe normally.            Respire normalement.\n",
       "17319     She deserved it.                Elle l'a mérité.\n",
       "6132         Make a guess.         Hasarde une hypothèse !\n",
       "25366    You're prisoners.          Vous êtes prisonniers.\n",
       "21854    I'm nearly ready.      Je suis pratiquement prêt.\n",
       "1247            I'm brave.              Je suis courageux.\n",
       "32312  Are we ready to go?  Sommes-nous prêtes à y aller ?\n",
       "24754    When does it end?           Quand cela finit-il ?\n",
       "25486    You're very wise.          Vous êtes fort avisée."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_samples = 33000\n",
    "\n",
    "lines = lines.loc[:, 'src':'tar']\n",
    "lines = lines[0:num_samples] # 6만개만 저장\n",
    "lines.sample(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "executionInfo": {
     "elapsed": 134,
     "status": "ok",
     "timestamp": 1752558985361,
     "user": {
      "displayName": "김광무",
      "userId": "03808645168826839149"
     },
     "user_tz": -540
    },
    "id": "SifH0QBZp65h",
    "outputId": "86543455-bf3b-40c7-ed61-7fe11f01df7c"
   },
   "outputs": [],
   "source": [
    "def to_ascii(s):\n",
    "  # 프랑스어 악센트(accent) 삭제\n",
    "  # 예시 : 'déjà diné' -> deja dine\n",
    "  return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "                   if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def preprocess_sentence(sent):\n",
    "  # 악센트 제거 함수 호출\n",
    "  sent = to_ascii(sent.lower())\n",
    "\n",
    "  # 단어와 구두점 사이에 공백 추가.\n",
    "  # ex) \"I am a student.\" => \"I am a student .\"\n",
    "  sent = re.sub(r\"([?.!,¿])\", r\" \\1\", sent)\n",
    "\n",
    "  # (a-z, A-Z, \".\", \"?\", \"!\", \",\") 이들을 제외하고는 전부 공백으로 변환.\n",
    "  sent = re.sub(r\"[^a-zA-Z!.?]+\", r\" \", sent)\n",
    "\n",
    "  # 다수 개의 공백을 하나의 공백으로 치환\n",
    "  sent = re.sub(r\"\\s+\", \" \", sent)\n",
    "  return sent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 245,
     "status": "ok",
     "timestamp": 1752559002854,
     "user": {
      "displayName": "김광무",
      "userId": "03808645168826839149"
     },
     "user_tz": -540
    },
    "id": "kAca3Wccp9-s"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전처리 전 영어 문장 : Have you had dinner?\n",
      "전처리 후 영어 문장 : have you had dinner ?\n",
      "전처리 전 프랑스어 문장 : Avez-vous déjà diné?\n",
      "전처리 후 프랑스어 문장 : avez vous deja dine ?\n"
     ]
    }
   ],
   "source": [
    "# 전처리 테스트\n",
    "en_sent = u\"Have you had dinner?\"\n",
    "fr_sent = u\"Avez-vous déjà diné?\"\n",
    "\n",
    "print('전처리 전 영어 문장 :', en_sent)\n",
    "print('전처리 후 영어 문장 :',preprocess_sentence(en_sent))\n",
    "print('전처리 전 프랑스어 문장 :', fr_sent)\n",
    "print('전처리 후 프랑스어 문장 :', preprocess_sentence(fr_sent))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1752559005555,
     "user": {
      "displayName": "김광무",
      "userId": "03808645168826839149"
     },
     "user_tz": -540
    },
    "id": "WAReaU1wqCOO",
    "outputId": "6ef7104e-04d2-4e6f-e42d-73dc317e4b03"
   },
   "outputs": [],
   "source": [
    "def load_preprocessed_data():\n",
    "  encoder_input, decoder_input, decoder_target = [], [], []\n",
    "\n",
    "  with open(\"fra.txt\", \"r\") as lines:\n",
    "    for i, line in enumerate(lines):\n",
    "      # source 데이터와 target 데이터 분리\n",
    "      src_line, tar_line, _ = line.strip().split('\\t')\n",
    "\n",
    "      # source 데이터 전처리\n",
    "      src_line = [w for w in preprocess_sentence(src_line).split()]\n",
    "\n",
    "      # target 데이터 전처리\n",
    "      tar_line = preprocess_sentence(tar_line)\n",
    "      tar_line_in = [w for w in (\"<sos> \" + tar_line).split()]\n",
    "      tar_line_out = [w for w in (tar_line + \" <eos>\").split()]\n",
    "\n",
    "      encoder_input.append(src_line)\n",
    "      decoder_input.append(tar_line_in)\n",
    "      decoder_target.append(tar_line_out)\n",
    "\n",
    "      if i == num_samples - 1:\n",
    "        break\n",
    "\n",
    "  return encoder_input, decoder_input, decoder_target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1752559022837,
     "user": {
      "displayName": "김광무",
      "userId": "03808645168826839149"
     },
     "user_tz": -540
    },
    "id": "v7JcNcrkqC8a",
    "outputId": "9d568fd1-b5b7-4881-beac-851179e41417"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "인코더의 입력 : [['go', '.'], ['go', '.'], ['go', '.'], ['go', '.'], ['hi', '.']]\n",
      "디코더의 입력 : [['<sos>', 'va', '!'], ['<sos>', 'marche', '.'], ['<sos>', 'en', 'route', '!'], ['<sos>', 'bouge', '!'], ['<sos>', 'salut', '!']]\n",
      "디코더의 레이블 : [['va', '!', '<eos>'], ['marche', '.', '<eos>'], ['en', 'route', '!', '<eos>'], ['bouge', '!', '<eos>'], ['salut', '!', '<eos>']]\n"
     ]
    }
   ],
   "source": [
    "sents_en_in, sents_fra_in, sents_fra_out = load_preprocessed_data()\n",
    "print('인코더의 입력 :',sents_en_in[:5])\n",
    "print('디코더의 입력 :',sents_fra_in[:5])\n",
    "print('디코더의 레이블 :',sents_fra_out[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1752559033986,
     "user": {
      "displayName": "김광무",
      "userId": "03808645168826839149"
     },
     "user_tz": -540
    },
    "id": "VXQxnxeWqHKW",
    "outputId": "842bb4ca-4cec-4703-dcf0-b2c7f4c139c6"
   },
   "outputs": [],
   "source": [
    "tokenizer_en = Tokenizer(filters=\"\", lower=False)\n",
    "tokenizer_en.fit_on_texts(sents_en_in)\n",
    "encoder_input = tokenizer_en.texts_to_sequences(sents_en_in)\n",
    "encoder_input = pad_sequences(encoder_input, padding=\"post\")\n",
    "\n",
    "tokenizer_fra = Tokenizer(filters=\"\", lower=False)\n",
    "tokenizer_fra.fit_on_texts(sents_fra_in)\n",
    "tokenizer_fra.fit_on_texts(sents_fra_out)\n",
    "\n",
    "decoder_input = tokenizer_fra.texts_to_sequences(sents_fra_in)\n",
    "decoder_input = pad_sequences(decoder_input, padding=\"post\")\n",
    "\n",
    "decoder_target = tokenizer_fra.texts_to_sequences(sents_fra_out)\n",
    "decoder_target = pad_sequences(decoder_target, padding=\"post\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 376,
     "status": "ok",
     "timestamp": 1752559045668,
     "user": {
      "displayName": "김광무",
      "userId": "03808645168826839149"
     },
     "user_tz": -540
    },
    "id": "pdN3djKPqJ4m",
    "outputId": "5992839f-d807-4bfa-ffa8-f21e7cfe75e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "인코더의 입력의 크기(shape) : (33000, 7)\n",
      "디코더의 입력의 크기(shape) : (33000, 16)\n",
      "디코더의 레이블의 크기(shape) : (33000, 16)\n"
     ]
    }
   ],
   "source": [
    "print('인코더의 입력의 크기(shape) :',encoder_input.shape)\n",
    "print('디코더의 입력의 크기(shape) :',decoder_input.shape)\n",
    "print('디코더의 레이블의 크기(shape) :',decoder_target.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 481,
     "status": "ok",
     "timestamp": 1752559118810,
     "user": {
      "displayName": "김광무",
      "userId": "03808645168826839149"
     },
     "user_tz": -540
    },
    "id": "upAZF7sTqeJB",
    "outputId": "00f1e365-d912-47d3-e7e5-c6f8304e7c39"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 단어 집합의 크기 : 4497, 프랑스어 단어 집합의 크기 : 7894\n"
     ]
    }
   ],
   "source": [
    "src_vocab_size = len(tokenizer_en.word_index) + 1\n",
    "tar_vocab_size = len(tokenizer_fra.word_index) + 1\n",
    "print(\"영어 단어 집합의 크기 : {:d}, 프랑스어 단어 집합의 크기 : {:d}\".format(src_vocab_size, tar_vocab_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 336,
     "status": "ok",
     "timestamp": 1752559054062,
     "user": {
      "displayName": "김광무",
      "userId": "03808645168826839149"
     },
     "user_tz": -540
    },
    "id": "_hCSMM3NqMpJ",
    "outputId": "44715f1d-118b-4a9d-d70b-ada074dfa6af"
   },
   "outputs": [],
   "source": [
    "src_to_index = tokenizer_en.word_index\n",
    "index_to_src = tokenizer_en.index_word\n",
    "tar_to_index = tokenizer_fra.word_index\n",
    "index_to_tar = tokenizer_fra.index_word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1752559069341,
     "user": {
      "displayName": "김광무",
      "userId": "03808645168826839149"
     },
     "user_tz": -540
    },
    "id": "kyNh_OOoqOsn",
    "outputId": "68aa8582-7c4b-4781-9591-02db64298447"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "랜덤 시퀀스 : [27705 23836 28894 ... 27235  6218 21169]\n"
     ]
    }
   ],
   "source": [
    "indices = np.arange(encoder_input.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "print('랜덤 시퀀스 :',indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 441,
     "status": "ok",
     "timestamp": 1752559122425,
     "user": {
      "displayName": "김광무",
      "userId": "03808645168826839149"
     },
     "user_tz": -540
    },
    "id": "6U278lwpqSgv"
   },
   "outputs": [],
   "source": [
    "encoder_input = encoder_input[indices]\n",
    "decoder_input = decoder_input[indices]\n",
    "decoder_target = decoder_target[indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 4978,
     "status": "ok",
     "timestamp": 1752559134691,
     "user": {
      "displayName": "김광무",
      "userId": "03808645168826839149"
     },
     "user_tz": -540
    },
    "id": "9D31MB3ZqUDE"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  2, 157,  54,   1,   0,   0,   0], dtype=int32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input[30997]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 142,
     "status": "ok",
     "timestamp": 1752559142464,
     "user": {
      "displayName": "김광무",
      "userId": "03808645168826839149"
     },
     "user_tz": -540
    },
    "id": "ZiVLmX_YqhQU"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  2,  12,  16, 145,  25, 102,   1,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0], dtype=int32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_input[30997]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 80,
     "status": "ok",
     "timestamp": 1752559186670,
     "user": {
      "displayName": "김광무",
      "userId": "03808645168826839149"
     },
     "user_tz": -540
    },
    "id": "fVAZvQIPqkVm"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 12,  16, 145,  25, 102,   1,   3,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0], dtype=int32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_target[30997]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "fuh2bHQmqvJC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검증 데이터의 개수 : 3300\n"
     ]
    }
   ],
   "source": [
    "n_of_val = int(33000*0.1)\n",
    "print('검증 데이터의 개수 :',n_of_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "fYYZazdPqxMK"
   },
   "outputs": [],
   "source": [
    "model.save('20250715_강의복습_seq2seq_CharacterLevel.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_train = encoder_input[:-n_of_val]\n",
    "decoder_input_train = decoder_input[:-n_of_val]\n",
    "decoder_target_train = decoder_target[:-n_of_val]\n",
    "\n",
    "encoder_input_test = encoder_input[-n_of_val:]\n",
    "decoder_input_test = decoder_input[-n_of_val:]\n",
    "decoder_target_test = decoder_target[-n_of_val:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 source 데이터의 크기 : (29700, 7)\n",
      "훈련 target 데이터의 크기 : (29700, 16)\n",
      "훈련 target 레이블의 크기 : (29700, 16)\n",
      "테스트 source 데이터의 크기 : (3300, 7)\n",
      "테스트 target 데이터의 크기 : (3300, 16)\n",
      "테스트 target 레이블의 크기 : (3300, 16)\n"
     ]
    }
   ],
   "source": [
    "print('훈련 source 데이터의 크기 :',encoder_input_train.shape)\n",
    "print('훈련 target 데이터의 크기 :',decoder_input_train.shape)\n",
    "print('훈련 target 레이블의 크기 :',decoder_target_train.shape)\n",
    "print('테스트 source 데이터의 크기 :',encoder_input_test.shape)\n",
    "print('테스트 target 데이터의 크기 :',decoder_input_test.shape)\n",
    "print('테스트 target 레이블의 크기 :',decoder_target_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Masking\n",
    "from tensorflow.keras.models import Model\n",
    "embedding_dim = 64\n",
    "hidden_units = 64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코더\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "enc_emb = Embedding(src_vocab_size, embedding_dim)(encoder_inputs) # 임베딩 층\n",
    "enc_masking = Masking(mask_value=0.0)(enc_emb) # 패딩 0은 연산에서 제외\n",
    "encoder_lstm = LSTM(hidden_units, return_state=True) # 상태값 리턴을 위해 return_state는 True\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(enc_masking) # 은닉 상태와 셀 상태를 리턴\n",
    "encoder_states = [state_h, state_c] # 인코더의 은닉 상태와 셀 상태를 저장\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디코더\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "dec_emb_layer = Embedding(tar_vocab_size, hidden_units) # 임베딩 층\n",
    "dec_emb = dec_emb_layer(decoder_inputs) # 패딩 0은 연산에서 제외\n",
    "dec_masking = Masking(mask_value=0.0)(dec_emb)\n",
    "\n",
    "# 상태값 리턴을 위해 return_state는 True, 모든 시점에 대해서 단어를 예측하기 위해 return_sequences는 True\n",
    "decoder_lstm = LSTM(hidden_units, return_sequences=True, return_state=True) \n",
    "\n",
    "# 인코더의 은닉 상태를 초기 은닉 상태(initial_state)로 사용\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_masking,\n",
    "                                     initial_state=encoder_states)\n",
    "\n",
    "# 모든 시점의 결과에 대해서 소프트맥스 함수를 사용한 출력층을 통해 단어 예측\n",
    "decoder_dense = Dense(tar_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# 모델의 입력과 출력을 정의.\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "233/233 [==============================] - 16s 60ms/step - loss: 3.3577 - acc: 0.6159 - val_loss: 2.0140 - val_acc: 0.6220\n",
      "Epoch 2/50\n",
      "233/233 [==============================] - 13s 56ms/step - loss: 1.8454 - acc: 0.6753 - val_loss: 1.7167 - val_acc: 0.7419\n",
      "Epoch 3/50\n",
      "233/233 [==============================] - 14s 61ms/step - loss: 1.6285 - acc: 0.7466 - val_loss: 1.5485 - val_acc: 0.7584\n",
      "Epoch 4/50\n",
      "233/233 [==============================] - 14s 58ms/step - loss: 1.4817 - acc: 0.7625 - val_loss: 1.4364 - val_acc: 0.7664\n",
      "Epoch 5/50\n",
      "233/233 [==============================] - 14s 58ms/step - loss: 1.3847 - acc: 0.7730 - val_loss: 1.3622 - val_acc: 0.7814\n",
      "Epoch 6/50\n",
      "233/233 [==============================] - 15s 64ms/step - loss: 1.3103 - acc: 0.7863 - val_loss: 1.3004 - val_acc: 0.7931\n",
      "Epoch 7/50\n",
      "233/233 [==============================] - 14s 60ms/step - loss: 1.2463 - acc: 0.7962 - val_loss: 1.2482 - val_acc: 0.7993\n",
      "Epoch 8/50\n",
      "233/233 [==============================] - 16s 70ms/step - loss: 1.1851 - acc: 0.8039 - val_loss: 1.1925 - val_acc: 0.8075\n",
      "Epoch 9/50\n",
      "233/233 [==============================] - 14s 61ms/step - loss: 1.1224 - acc: 0.8151 - val_loss: 1.1412 - val_acc: 0.8170\n",
      "Epoch 10/50\n",
      "233/233 [==============================] - 16s 67ms/step - loss: 1.0650 - acc: 0.8239 - val_loss: 1.0951 - val_acc: 0.8247\n",
      "Epoch 11/50\n",
      "233/233 [==============================] - 15s 64ms/step - loss: 1.0139 - acc: 0.8304 - val_loss: 1.0575 - val_acc: 0.8291\n",
      "Epoch 12/50\n",
      "233/233 [==============================] - 16s 67ms/step - loss: 0.9681 - acc: 0.8354 - val_loss: 1.0260 - val_acc: 0.8327\n",
      "Epoch 13/50\n",
      "233/233 [==============================] - 14s 60ms/step - loss: 0.9286 - acc: 0.8397 - val_loss: 0.9980 - val_acc: 0.8366\n",
      "Epoch 14/50\n",
      "233/233 [==============================] - 14s 59ms/step - loss: 0.8914 - acc: 0.8433 - val_loss: 0.9718 - val_acc: 0.8394\n",
      "Epoch 15/50\n",
      "233/233 [==============================] - 17s 72ms/step - loss: 0.8578 - acc: 0.8468 - val_loss: 0.9511 - val_acc: 0.8420\n",
      "Epoch 16/50\n",
      "233/233 [==============================] - 14s 60ms/step - loss: 0.8268 - acc: 0.8501 - val_loss: 0.9331 - val_acc: 0.8443\n",
      "Epoch 17/50\n",
      "233/233 [==============================] - 13s 57ms/step - loss: 0.7982 - acc: 0.8530 - val_loss: 0.9154 - val_acc: 0.8462\n",
      "Epoch 18/50\n",
      "233/233 [==============================] - 14s 62ms/step - loss: 0.7717 - acc: 0.8557 - val_loss: 0.9010 - val_acc: 0.8477\n",
      "Epoch 19/50\n",
      "233/233 [==============================] - 14s 60ms/step - loss: 0.7465 - acc: 0.8581 - val_loss: 0.8880 - val_acc: 0.8495\n",
      "Epoch 20/50\n",
      "233/233 [==============================] - 14s 60ms/step - loss: 0.7237 - acc: 0.8608 - val_loss: 0.8735 - val_acc: 0.8511\n",
      "Epoch 21/50\n",
      "233/233 [==============================] - 15s 64ms/step - loss: 0.7005 - acc: 0.8634 - val_loss: 0.8606 - val_acc: 0.8523\n",
      "Epoch 22/50\n",
      "233/233 [==============================] - 13s 57ms/step - loss: 0.6787 - acc: 0.8660 - val_loss: 0.8508 - val_acc: 0.8531\n",
      "Epoch 23/50\n",
      "233/233 [==============================] - 14s 59ms/step - loss: 0.6588 - acc: 0.8681 - val_loss: 0.8396 - val_acc: 0.8543\n",
      "Epoch 24/50\n",
      "233/233 [==============================] - 13s 54ms/step - loss: 0.6391 - acc: 0.8706 - val_loss: 0.8303 - val_acc: 0.8558\n",
      "Epoch 25/50\n",
      "233/233 [==============================] - 12s 53ms/step - loss: 0.6202 - acc: 0.8729 - val_loss: 0.8234 - val_acc: 0.8551\n",
      "Epoch 26/50\n",
      "233/233 [==============================] - 13s 54ms/step - loss: 0.6019 - acc: 0.8751 - val_loss: 0.8127 - val_acc: 0.8573\n",
      "Epoch 27/50\n",
      "233/233 [==============================] - 13s 55ms/step - loss: 0.5843 - acc: 0.8773 - val_loss: 0.8058 - val_acc: 0.8590\n",
      "Epoch 28/50\n",
      "233/233 [==============================] - 13s 55ms/step - loss: 0.5685 - acc: 0.8794 - val_loss: 0.7994 - val_acc: 0.8587\n",
      "Epoch 29/50\n",
      "233/233 [==============================] - 13s 56ms/step - loss: 0.5518 - acc: 0.8818 - val_loss: 0.7944 - val_acc: 0.8600\n",
      "Epoch 30/50\n",
      "233/233 [==============================] - 13s 55ms/step - loss: 0.5364 - acc: 0.8838 - val_loss: 0.7878 - val_acc: 0.8608\n",
      "Epoch 31/50\n",
      "233/233 [==============================] - 13s 56ms/step - loss: 0.5212 - acc: 0.8862 - val_loss: 0.7816 - val_acc: 0.8622\n",
      "Epoch 32/50\n",
      "233/233 [==============================] - 14s 58ms/step - loss: 0.5069 - acc: 0.8881 - val_loss: 0.7746 - val_acc: 0.8635\n",
      "Epoch 33/50\n",
      "233/233 [==============================] - 13s 54ms/step - loss: 0.4931 - acc: 0.8904 - val_loss: 0.7736 - val_acc: 0.8636\n",
      "Epoch 34/50\n",
      "233/233 [==============================] - 13s 54ms/step - loss: 0.4803 - acc: 0.8922 - val_loss: 0.7676 - val_acc: 0.8638\n",
      "Epoch 35/50\n",
      "233/233 [==============================] - 13s 55ms/step - loss: 0.4674 - acc: 0.8942 - val_loss: 0.7642 - val_acc: 0.8648\n",
      "Epoch 36/50\n",
      "233/233 [==============================] - 13s 55ms/step - loss: 0.4545 - acc: 0.8964 - val_loss: 0.7593 - val_acc: 0.8656\n",
      "Epoch 37/50\n",
      "233/233 [==============================] - 13s 56ms/step - loss: 0.4424 - acc: 0.8984 - val_loss: 0.7561 - val_acc: 0.8657\n",
      "Epoch 38/50\n",
      "233/233 [==============================] - 13s 55ms/step - loss: 0.4302 - acc: 0.9009 - val_loss: 0.7537 - val_acc: 0.8659\n",
      "Epoch 39/50\n",
      "233/233 [==============================] - 13s 55ms/step - loss: 0.4194 - acc: 0.9025 - val_loss: 0.7493 - val_acc: 0.8668\n",
      "Epoch 40/50\n",
      "233/233 [==============================] - 13s 54ms/step - loss: 0.4086 - acc: 0.9043 - val_loss: 0.7489 - val_acc: 0.8673\n",
      "Epoch 41/50\n",
      "233/233 [==============================] - 13s 56ms/step - loss: 0.3983 - acc: 0.9061 - val_loss: 0.7453 - val_acc: 0.8682\n",
      "Epoch 42/50\n",
      "233/233 [==============================] - 14s 60ms/step - loss: 0.3883 - acc: 0.9082 - val_loss: 0.7406 - val_acc: 0.8689\n",
      "Epoch 43/50\n",
      "233/233 [==============================] - 14s 59ms/step - loss: 0.3790 - acc: 0.9098 - val_loss: 0.7384 - val_acc: 0.8693\n",
      "Epoch 44/50\n",
      "233/233 [==============================] - 14s 59ms/step - loss: 0.3689 - acc: 0.9118 - val_loss: 0.7367 - val_acc: 0.8696\n",
      "Epoch 45/50\n",
      "233/233 [==============================] - 14s 59ms/step - loss: 0.3606 - acc: 0.9133 - val_loss: 0.7361 - val_acc: 0.8698\n",
      "Epoch 46/50\n",
      "233/233 [==============================] - 14s 62ms/step - loss: 0.3518 - acc: 0.9152 - val_loss: 0.7311 - val_acc: 0.8711\n",
      "Epoch 47/50\n",
      "233/233 [==============================] - 13s 56ms/step - loss: 0.3427 - acc: 0.9170 - val_loss: 0.7329 - val_acc: 0.8711\n",
      "Epoch 48/50\n",
      "233/233 [==============================] - 14s 60ms/step - loss: 0.3360 - acc: 0.9183 - val_loss: 0.7319 - val_acc: 0.8712\n",
      "Epoch 49/50\n",
      "233/233 [==============================] - 15s 62ms/step - loss: 0.3280 - acc: 0.9203 - val_loss: 0.7298 - val_acc: 0.8722\n",
      "Epoch 50/50\n",
      "233/233 [==============================] - 14s 62ms/step - loss: 0.3206 - acc: 0.9214 - val_loss: 0.7285 - val_acc: 0.8726\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x3204394c0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=[encoder_input_train, decoder_input_train], y=decoder_target_train, \\\n",
    "          validation_data=([encoder_input_test, decoder_input_test], decoder_target_test),\n",
    "          batch_size=128, epochs=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('20250715_강의복습_seq2seq_WordLevel.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코더\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "# 디코더 설계 시작\n",
    "# 이전 시점의 상태를 보관할 텐서\n",
    "decoder_state_input_h = Input(shape=(hidden_units,))\n",
    "decoder_state_input_c = Input(shape=(hidden_units,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "# 훈련 때 사용했던 임베딩 층을 재사용\n",
    "dec_emb2 = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "# 다음 단어 예측을 위해 이전 시점의 상태를 현 시점의 초기 상태로 사용\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_states_inputs)\n",
    "decoder_states2 = [state_h2, state_c2]\n",
    "\n",
    "# 모든 시점에 대해서 단어 예측\n",
    "decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
    "\n",
    "# 수정된 디코더\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs2] + decoder_states2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "  # 입력으로부터 인코더의 마지막 시점의 상태(은닉 상태, 셀 상태)를 얻음\n",
    "  states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "  # <SOS>에 해당하는 정수 생성\n",
    "  target_seq = np.zeros((1,1))\n",
    "  target_seq[0, 0] = tar_to_index['<sos>']\n",
    "\n",
    "  stop_condition = False\n",
    "  decoded_sentence = ''\n",
    "\n",
    "  # stop_condition이 True가 될 때까지 루프 반복\n",
    "  # 구현의 간소화를 위해서 이 함수는 배치 크기를 1로 가정합니다.\n",
    "  while not stop_condition:\n",
    "    # 이점 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n",
    "    output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "    # 예측 결과를 단어로 변환\n",
    "    sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "    sampled_char = index_to_tar[sampled_token_index]\n",
    "\n",
    "    # 현재 시점의 예측 단어를 예측 문장에 추가\n",
    "    decoded_sentence += ' '+sampled_char\n",
    "\n",
    "    # <eos>에 도달하거나 정해진 길이를 넘으면 중단.\n",
    "    if (sampled_char == '<eos>' or\n",
    "        len(decoded_sentence) > 50):\n",
    "        stop_condition = True\n",
    "\n",
    "    # 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장\n",
    "    target_seq = np.zeros((1,1))\n",
    "    target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "    # 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\n",
    "    states_value = [h, c]\n",
    "\n",
    "  return decoded_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
    "def seq_to_src(input_seq):\n",
    "  sentence = ''\n",
    "  for encoded_word in input_seq:\n",
    "    if(encoded_word != 0):\n",
    "      sentence = sentence + index_to_src[encoded_word] + ' '\n",
    "  return sentence\n",
    "\n",
    "# 번역문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
    "def seq_to_tar(input_seq):\n",
    "  sentence = ''\n",
    "  for encoded_word in input_seq:\n",
    "    if(encoded_word != 0 and encoded_word != tar_to_index['<sos>'] and encoded_word != tar_to_index['<eos>']):\n",
    "      sentence = sentence + index_to_tar[encoded_word] + ' '\n",
    "  return sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 383ms/step\n",
      "1/1 [==============================] - 0s 114ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 7ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "입력문장 : is it yours ? \n",
      "정답문장 : est ce le votre ? \n",
      "번역문장 : est ce la verite ? \n",
      "--------------------------------------------------\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "입력문장 : please forgive us . \n",
      "정답문장 : veuillez nous pardonner . \n",
      "번역문장 : veuillez vous aider ! \n",
      "--------------------------------------------------\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 7ms/step\n",
      "입력문장 : it s our job . \n",
      "정답문장 : c est notre boulot . \n",
      "번역문장 : c est notre boulot . \n",
      "--------------------------------------------------\n",
      "1/1 [==============================] - 0s 7ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 7ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "입력문장 : we ll visit you . \n",
      "정답문장 : nous te rendrons visite . \n",
      "번역문장 : nous viendrons vous rendre visite . \n",
      "--------------------------------------------------\n",
      "1/1 [==============================] - 0s 7ms/step\n",
      "1/1 [==============================] - 0s 7ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 7ms/step\n",
      "1/1 [==============================] - 0s 7ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "입력문장 : tom is smarter . \n",
      "정답문장 : tom est plus intelligent . \n",
      "번역문장 : tom est plus tard . \n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for seq_index in [3, 50, 100, 300, 1001]:\n",
    "  input_seq = encoder_input_train[seq_index: seq_index + 1]\n",
    "  decoded_sentence = decode_sequence(input_seq)\n",
    "\n",
    "  print(\"입력문장 :\",seq_to_src(encoder_input_train[seq_index]))\n",
    "  print(\"정답문장 :\",seq_to_tar(decoder_input_train[seq_index]))\n",
    "  print(\"번역문장 :\",decoded_sentence[1:-5])\n",
    "  print(\"-\"*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "입력문장 : close the blinds . \n",
      "정답문장 : fermez les stores . \n",
      "번역문장 : ferme les stores . \n",
      "--------------------------------------------------\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 7ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "입력문장 : we got lost . \n",
      "정답문장 : on s est perdus . \n",
      "번역문장 : nous avons ete perdu . \n",
      "--------------------------------------------------\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 7ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "입력문장 : everyone waited . \n",
      "정답문장 : tout le monde a attendu . \n",
      "번역문장 : tout le monde attendit . \n",
      "--------------------------------------------------\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 7ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 7ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 7ms/step\n",
      "입력문장 : dogs can swim . \n",
      "정답문장 : les chiens savent nager . \n",
      "번역문장 : les chiens peuvent nager . \n",
      "--------------------------------------------------\n",
      "1/1 [==============================] - 0s 7ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 7ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 7ms/step\n",
      "1/1 [==============================] - 0s 7ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "입력문장 : who s that boy ? \n",
      "정답문장 : qui est ce garcon ? \n",
      "번역문장 : qui est ce garcon ? \n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for seq_index in [3, 50, 100, 300, 1001]:\n",
    "  input_seq = encoder_input_test[seq_index: seq_index + 1]\n",
    "  decoded_sentence = decode_sequence(input_seq)\n",
    "\n",
    "  print(\"입력문장 :\",seq_to_src(encoder_input_test[seq_index]))\n",
    "  print(\"정답문장 :\",seq_to_tar(decoder_input_test[seq_index]))\n",
    "  print(\"번역문장 :\",decoded_sentence[1:-5])\n",
    "  print(\"-\"*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNALQX+emubcl+mY84mIOle",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
