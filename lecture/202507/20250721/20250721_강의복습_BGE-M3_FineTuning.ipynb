{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 113836,
     "status": "ok",
     "timestamp": 1754201119192,
     "user": {
      "displayName": "김광무",
      "userId": "03808645168826839149"
     },
     "user_tz": -540
    },
    "id": "DHjzPvyRefj3",
    "outputId": "4a639e5f-7731-4e28-e621-e45ec9255811"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting FlagEmbedding\n",
      "  Downloading FlagEmbedding-1.3.5.tar.gz (163 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting peft\n",
      "  Downloading peft-0.17.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.11.0.post1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.0 kB)\n",
      "Collecting LM_Cocktail\n",
      "  Downloading LM_Cocktail-0.0.4.tar.gz (9.0 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from FlagEmbedding) (2.8.0.dev20250319+cu128)\n",
      "Collecting transformers>=4.44.2 (from FlagEmbedding)\n",
      "  Downloading transformers-4.54.1-py3-none-any.whl.metadata (41 kB)\n",
      "Collecting datasets>=2.19.0 (from FlagEmbedding)\n",
      "  Downloading datasets-4.0.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting accelerate>=0.20.1 (from FlagEmbedding)\n",
      "  Downloading accelerate-1.9.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting sentence_transformers (from FlagEmbedding)\n",
      "  Downloading sentence_transformers-5.0.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting ir-datasets (from FlagEmbedding)\n",
      "  Downloading ir_datasets-0.5.11-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting sentencepiece (from FlagEmbedding)\n",
      "  Downloading sentencepiece-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting protobuf (from FlagEmbedding)\n",
      "  Downloading protobuf-6.31.1-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from peft) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from peft) (24.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from peft) (6.0.2)\n",
      "Collecting tqdm (from peft)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting safetensors (from peft)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting huggingface_hub>=0.25.0 (from peft)\n",
      "  Downloading huggingface_hub-0.34.3-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.19.0->FlagEmbedding) (3.16.1)\n",
      "Collecting pyarrow>=15.0.0 (from datasets>=2.19.0->FlagEmbedding)\n",
      "  Downloading pyarrow-21.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets>=2.19.0->FlagEmbedding)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets>=2.19.0->FlagEmbedding)\n",
      "  Downloading pandas-2.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (91 kB)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.19.0->FlagEmbedding) (2.32.3)\n",
      "Collecting xxhash (from datasets>=2.19.0->FlagEmbedding)\n",
      "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets>=2.19.0->FlagEmbedding)\n",
      "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.19.0->FlagEmbedding) (2024.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft) (4.12.2)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface_hub>=0.25.0->peft)\n",
      "  Downloading hf_xet-1.1.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (879 bytes)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->FlagEmbedding) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->FlagEmbedding) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->FlagEmbedding) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.61 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->FlagEmbedding) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.57 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->FlagEmbedding) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.57 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->FlagEmbedding) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.8.0.87 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->FlagEmbedding) (9.8.0.87)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.3.14 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->FlagEmbedding) (12.8.3.14)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.41 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->FlagEmbedding) (11.3.3.41)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.55 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->FlagEmbedding) (10.3.9.55)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.2.55 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->FlagEmbedding) (11.7.2.55)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.7.53 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->FlagEmbedding) (12.5.7.53)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->FlagEmbedding) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.25.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->FlagEmbedding) (2.25.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.55 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->FlagEmbedding) (12.8.55)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.61 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->FlagEmbedding) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.0.11 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->FlagEmbedding) (1.13.0.11)\n",
      "Requirement already satisfied: pytorch-triton==3.3.0+git96316ce5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->FlagEmbedding) (3.3.0+git96316ce5)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-triton==3.3.0+git96316ce5->torch>=1.6.0->FlagEmbedding) (77.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers>=4.44.2->FlagEmbedding)\n",
      "  Downloading regex-2025.7.34-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers>=4.44.2->FlagEmbedding)\n",
      "  Downloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.11/dist-packages (from ir-datasets->FlagEmbedding) (4.13.3)\n",
      "Collecting inscriptis>=2.2.0 (from ir-datasets->FlagEmbedding)\n",
      "  Downloading inscriptis-2.6.0-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting lxml>=4.5.2 (from ir-datasets->FlagEmbedding)\n",
      "  Downloading lxml-6.0.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting trec-car-tools>=2.5.4 (from ir-datasets->FlagEmbedding)\n",
      "  Downloading trec_car_tools-2.6-py3-none-any.whl.metadata (640 bytes)\n",
      "Collecting lz4>=3.1.10 (from ir-datasets->FlagEmbedding)\n",
      "  Downloading lz4-4.4.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting warc3-wet>=0.2.3 (from ir-datasets->FlagEmbedding)\n",
      "  Downloading warc3_wet-0.2.5-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting warc3-wet-clueweb09>=0.2.5 (from ir-datasets->FlagEmbedding)\n",
      "  Downloading warc3-wet-clueweb09-0.2.5.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting zlib-state>=0.1.3 (from ir-datasets->FlagEmbedding)\n",
      "  Downloading zlib_state-0.1.9-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting ijson>=3.1.3 (from ir-datasets->FlagEmbedding)\n",
      "  Downloading ijson-3.4.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\n",
      "Collecting unlzw3>=0.2.1 (from ir-datasets->FlagEmbedding)\n",
      "  Downloading unlzw3-0.2.3-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting scikit-learn (from sentence_transformers->FlagEmbedding)\n",
      "  Downloading scikit_learn-1.7.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
      "Collecting scipy (from sentence_transformers->FlagEmbedding)\n",
      "  Downloading scipy-1.16.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence_transformers->FlagEmbedding) (11.0.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.4.1->ir-datasets->FlagEmbedding) (2.6)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.19.0->FlagEmbedding)\n",
      "  Downloading aiohttp-3.12.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.19.0->FlagEmbedding) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.19.0->FlagEmbedding) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.19.0->FlagEmbedding) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.19.0->FlagEmbedding) (2025.1.31)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch>=1.6.0->FlagEmbedding) (1.3.0)\n",
      "Collecting cbor>=1.0.0 (from trec-car-tools>=2.5.4->ir-datasets->FlagEmbedding)\n",
      "  Downloading cbor-1.0.0.tar.gz (20 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.6.0->FlagEmbedding) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.19.0->FlagEmbedding) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets>=2.19.0->FlagEmbedding)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets>=2.19.0->FlagEmbedding)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn->sentence_transformers->FlagEmbedding)\n",
      "  Downloading joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->sentence_transformers->FlagEmbedding)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.19.0->FlagEmbedding)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.19.0->FlagEmbedding)\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.19.0->FlagEmbedding) (25.3.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.19.0->FlagEmbedding)\n",
      "  Downloading frozenlist-1.7.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.19.0->FlagEmbedding)\n",
      "  Downloading multidict-6.6.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.19.0->FlagEmbedding)\n",
      "  Downloading propcache-0.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.19.0->FlagEmbedding)\n",
      "  Downloading yarl-1.20.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (73 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.19.0->FlagEmbedding) (1.16.0)\n",
      "Downloading peft-0.17.0-py3-none-any.whl (503 kB)\n",
      "Downloading faiss_cpu-1.11.0.post1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (31.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m266.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-1.9.0-py3-none-any.whl (367 kB)\n",
      "Downloading datasets-4.0.0-py3-none-any.whl (494 kB)\n",
      "Downloading huggingface_hub-0.34.3-py3-none-any.whl (558 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m558.8/558.8 kB\u001b[0m \u001b[31m145.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading transformers-4.54.1-py3-none-any.whl (11.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m546.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ir_datasets-0.5.11-py3-none-any.whl (866 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m866.1/866.1 kB\u001b[0m \u001b[31m191.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-6.31.1-cp39-abi3-manylinux2014_x86_64.whl (321 kB)\n",
      "Downloading sentence_transformers-5.0.0-py3-none-any.whl (470 kB)\n",
      "Downloading sentencepiece-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m314.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading hf_xet-1.1.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m411.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ijson-3.4.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (134 kB)\n",
      "Downloading inscriptis-2.6.0-py3-none-any.whl (45 kB)\n",
      "Downloading lxml-6.0.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (5.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m451.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading lz4-4.4.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m326.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "Downloading pyarrow-21.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (42.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m562.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2025.7.34-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (798 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m798.9/798.9 kB\u001b[0m \u001b[31m187.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m438.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading trec_car_tools-2.6-py3-none-any.whl (8.4 kB)\n",
      "Downloading unlzw3-0.2.3-py3-none-any.whl (6.7 kB)\n",
      "Downloading warc3_wet-0.2.5-py3-none-any.whl (18 kB)\n",
      "Downloading zlib_state-0.1.9-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21 kB)\n",
      "Downloading pandas-2.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m450.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.7.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m300.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.16.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.4/35.4 MB\u001b[0m \u001b[31m246.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Downloading aiohttp-3.12.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m360.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Downloading frozenlist-1.7.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (235 kB)\n",
      "Downloading multidict-6.6.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (246 kB)\n",
      "Downloading propcache-0.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
      "Downloading yarl-1.20.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (348 kB)\n",
      "Building wheels for collected packages: FlagEmbedding, LM_Cocktail, warc3-wet-clueweb09, cbor\n",
      "  Building wheel for FlagEmbedding (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for FlagEmbedding: filename=flagembedding-1.3.5-py3-none-any.whl size=233820 sha256=f577c78f2b756e73dca0bfef84c699116b55be021ec0f0c54937d258e4f78a5f\n",
      "  Stored in directory: /root/.cache/pip/wheels/fc/1c/66/c9c846a8f8cbd9574db8d76b0a61410a087bc07d53682a54f4\n",
      "  Building wheel for LM_Cocktail (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for LM_Cocktail: filename=lm_cocktail-0.0.4-py3-none-any.whl size=9662 sha256=b1c90c31573bb3809d11bc33e6597a2b65b8f2625770da25ac7b79808139ecaf\n",
      "  Stored in directory: /root/.cache/pip/wheels/96/58/d0/036177e687d42e35cc67d11191f45c7211f2693fe089bff6f8\n",
      "  Building wheel for warc3-wet-clueweb09 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for warc3-wet-clueweb09: filename=warc3_wet_clueweb09-0.2.5-py3-none-any.whl size=18998 sha256=93a954bfce6b3c09bbc9c0b6fb5380555a3907a2c171f3019bbe9ca37c2886e0\n",
      "  Stored in directory: /root/.cache/pip/wheels/63/f9/dc/2dd16d3330e327236e4d407941975c42d5159d200cdb7922d8\n",
      "  Building wheel for cbor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for cbor: filename=cbor-1.0.0-cp311-cp311-linux_x86_64.whl size=53977 sha256=016c8ce6a984a81a414493a374e2c592ced9114b75cf117e557e88e81c2bf341\n",
      "  Stored in directory: /root/.cache/pip/wheels/21/6b/45/0c34253b1af07d1d9dc524f6d44d74a6b191c43152e6aaf641\n",
      "Successfully built FlagEmbedding LM_Cocktail warc3-wet-clueweb09 cbor\n",
      "Installing collected packages: warc3-wet-clueweb09, warc3-wet, sentencepiece, pytz, cbor, zlib-state, xxhash, unlzw3, tzdata, trec-car-tools, tqdm, threadpoolctl, scipy, safetensors, regex, pyarrow, protobuf, propcache, multidict, lz4, lxml, joblib, ijson, hf-xet, frozenlist, faiss-cpu, dill, aiohappyeyeballs, yarl, scikit-learn, pandas, multiprocess, inscriptis, huggingface_hub, aiosignal, tokenizers, ir-datasets, aiohttp, transformers, accelerate, sentence_transformers, peft, datasets, LM_Cocktail, FlagEmbedding\n",
      "Successfully installed FlagEmbedding-1.3.5 LM_Cocktail-0.0.4 accelerate-1.9.0 aiohappyeyeballs-2.6.1 aiohttp-3.12.15 aiosignal-1.4.0 cbor-1.0.0 datasets-4.0.0 dill-0.3.8 faiss-cpu-1.11.0.post1 frozenlist-1.7.0 hf-xet-1.1.5 huggingface_hub-0.34.3 ijson-3.4.0 inscriptis-2.6.0 ir-datasets-0.5.11 joblib-1.5.1 lxml-6.0.0 lz4-4.4.4 multidict-6.6.3 multiprocess-0.70.16 pandas-2.3.1 peft-0.17.0 propcache-0.3.2 protobuf-6.31.1 pyarrow-21.0.0 pytz-2025.2 regex-2025.7.34 safetensors-0.5.3 scikit-learn-1.7.1 scipy-1.16.1 sentence_transformers-5.0.0 sentencepiece-0.2.0 threadpoolctl-3.6.0 tokenizers-0.21.4 tqdm-4.67.1 transformers-4.54.1 trec-car-tools-2.6 tzdata-2025.2 unlzw3-0.2.3 warc3-wet-0.2.5 warc3-wet-clueweb09-0.2.5 xxhash-3.5.0 yarl-1.20.1 zlib-state-0.1.9\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -U FlagEmbedding peft faiss-cpu LM_Cocktail\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1754201142157,
     "user": {
      "displayName": "김광무",
      "userId": "03808645168826839149"
     },
     "user_tz": -540
    },
    "id": "oJWRnk7DRWsm",
    "outputId": "902546d7-5d6c-46d7-b31b-29bf28a0a063"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터가 'toy_finetune_data.jsonl' 파일로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# 번역된 데이터\n",
    "translated_data = [\n",
    "    {\"query\": \"다섯 명의 여성이 해변을 따라 플립플롭을 신고 걸어간다.\", \"pos\": [\"플립플롭을 신은 몇몇 여성들이 해변을 따라 걸어가고 있다\"], \"neg\": [\"4명의 여성이 해변에 앉아 있다.\", \"1996년에 개혁이 있었다.\", \"그녀는 자신의 기록을 정정하기 위해 법정에 가지 않을 것이다.\", \"그 남자는 하와이에 대해 이야기하고 있다.\", \"한 여성이 밖에 서 있다.\", \"전투는 끝났다.\", \"한 무리의 사람들이 배구를 하고 있다.\"]},\n",
    "    {\"query\": \"한 여성이 높은 절벽 위에서 한 발로 서서 강을 내려다보고 있다.\", \"pos\": [\"한 여성이 절벽 위에 서 있다.\"], \"neg\": [\"한 여성이 의자에 앉아 있다.\", \"조지 부시는 공화당원들에게 최고 고문들의 조언에 반하여 이 어리석은 생각을 고려조차 하지 않겠다고 말했다.\", \"그 가족은 무너지고 있었다.\", \"아무도 회의에 나타나지 않았다\", \"한 소년이 밖에서 모래를 가지고 놀고 있다.\", \"전보를 받자마자 끝났다.\", \"한 아이가 자기 방에서 책을 읽고 있다.\"]},\n",
    "    {\"query\": \"두 여성이 악기를 연주하고 있다; 한 명은 클라리넷, 다른 한 명은 바이올린을 연주한다.\", \"pos\": [\"몇 사람이 곡을 연주하고 있다.\"], \"neg\": [\"두 여성이 기타와 드럼을 연주하고 있다.\", \"한 남자가 산을 스키를 타고 내려가고 있다.\", \"살인자가 생각했던 때에 치명적인 용량이 투여되지 않았다.\", \"자전거를 타고 있는 사람\", \"그 소녀는 아치길에 기대어 서 있다.\", \"한 무리의 여성들이 소파 오페라를 보고 있다.\", \"사람들은 나이가 들어도 절대 잊지 않는다.\"]},\n",
    "    {\"query\": \"파란색 탱크톱을 입은 소녀가 앉아서 세 마리의 개를 지켜보고 있다.\", \"pos\": [\"한 소녀가 파란색을 입고 있다.\"], \"neg\": [\"한 소녀가 세 마리의 고양이와 함께 있다.\", \"사람들이 장례 행렬을 지켜보고 있다.\", \"그 아이는 검은색을 입고 있다.\", \"공립학교에서 우리에게 재정은 문제이다.\", \"수영장에 있는 아이들.\", \"폭행당하는 것은 진정시키는 일이다.\", \"나는 18살에 심각한 문제에 직면했다.\"]},\n",
    "    {\"query\": \"노란 개가 숲길을 따라 달리고 있다.\", \"pos\": [\"개가 달리고 있다\"], \"neg\": [\"고양이가 달리고 있다\", \"스틸은 그녀의 원래 이야기를 지키지 않았다.\", \"이 규칙은 사람들이 자녀 양육비를 내는 것을 막는다.\", \"조끼를 입은 남자가 차 안에 앉아 있다.\", \"검은 옷을 입고 흰색 반다나와 선글라스를 낀 사람이 버스 정류장에서 기다리고 있다.\", \"글로브나 메일 중 어느 쪽도 캐나다의 현재 도로 체계 상태에 대해 언급하지 않았다.\", \"스프링 크릭 시설은 오래되고 구식이다.\"]},\n",
    "    {\"query\": \"각 단계에서의 필수 활동과 그 활동들과 관련된 중요한 요소들을 설명한다.\", \"pos\": [\"필수 활동에 대한 중요 요소들이 설명되어 있다.\"], \"neg\": [\"중요한 활동들을 설명하지만 그 활동들과 관련된 중요한 요소들에 대한 규정은 없다.\", \"사람들이 항의하기 위해 모여 있다.\", \"주 정부는 당신이 그렇게 하기를 선호할 것이다.\", \"한 소녀가 한 소년 옆에 앉아 있다.\", \"두 남성이 공연하고 있다.\", \"아무도 뛰고 있지 않다\", \"콘라드는 머리를 맞도록 음모를 꾸미고 있었다.\"]},\n",
    "    {\"query\": \"한 남자가 레스토랑에서 연설을 하고 있다.\", \"pos\": [\"한 사람이 연설을 하고 있다.\"], \"neg\": [\"그 남자는 테이블에 앉아 음식을 먹고 있다.\", \"이것은 확실히 승인이 아니다.\", \"그들은 은퇴 때문에 집을 팔았지, 대출 때문이 아니다.\", \"미주리 주의 인장은 완벽하다.\", \"누군가가 손을 들고 있다.\", \"한 운동선수가 1500미터 수영 경기에 참가하고 있다.\", \"두 남자가 마술 쇼를 보고 있다.\"]},\n",
    "    {\"query\": \"인디언들이 코트를 입고 음식과 음료를 가지고 모임을 갖고 있다.\", \"pos\": [\"인디언 그룹이 음식과 음료를 가지고 모임을 갖고 있다\"], \"neg\": [\"인디언 그룹이 장례식을 하고 있다\", \"이것은 팔마의 큰 투우장에서 겨울 오후에만 공연된다.\", \"올바른 정보는 법률 서비스 관행과 사법 체계를 강화할 수 있다.\", \"한편, 본토는 인구가 없었다.\", \"두 아이가 자고 있다.\", \"어부가 원숭이를 잡으려고 하고 있다\", \"사람들이 기차 안에 있다\"]},\n",
    "    {\"query\": \"보라색 머리를 한 여성이 밖에서 자전거를 타고 있다.\", \"pos\": [\"한 여성이 자전거를 타고 있다.\"], \"neg\": [\"한 여성이 공원에서 조깅을 하고 있다.\", \"그 거리는 하얀색으로 칠해진 집들로 가득했다.\", \"한 그룹이 안에서 영화를 보고 있다.\", \"소풍에서 남자들이 스테이크를 자르고 있다\", \"여러 명의 요리사들이 앉아서 음식에 대해 이야기하고 있다.\", \"위원회는 중요한 대안들이 고려되지 않았다고 지적한다.\", \"우리는 장작이 다 떨어져서 불을 위해 소나무 바늘을 사용해야 했다.\"]},\n",
    "    {\"query\": \"한 남자가 도시 거리에서 인력거로 두 여성을 끌고 있다.\", \"pos\": [\"한 남자가 도시에 있다.\"], \"neg\": [\"한 남자가 비행기 조종사이다.\", \"그것은 지루하고 평범하다.\", \"아침 햇살이 밝게 비치고 따뜻했다.\", \"두 사람이 부두에서 뛰어내렸다.\", \"사람들이 우주선 발사를 보고 있다.\", \"테레사 수녀는 쉬운 선택이다.\", \"원하는 속도로 갈 수 있는 것은 가치가 있다.\"]}\n",
    "]\n",
    "\n",
    "# JSONL 파일로 저장\n",
    "with open('toy_finetune_data.jsonl', 'w', encoding='utf-8') as f:\n",
    "    for item in translated_data:\n",
    "        json.dump(item, f, ensure_ascii=False)\n",
    "        f.write('\\n')\n",
    "\n",
    "print(\"데이터가 'toy_finetune_data.jsonl' 파일로 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24242,
     "status": "ok",
     "timestamp": 1754038524796,
     "user": {
      "displayName": "김광무",
      "userId": "03808645168826839149"
     },
     "user_tz": -540
    },
    "id": "a4cXbsQM13KB",
    "outputId": "b054aa11-58e9-4787-8614-a20ae046de15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/FlagEmbedding\n"
     ]
    }
   ],
   "source": [
    "import FlagEmbedding\n",
    "import os\n",
    "\n",
    "# FlagEmbedding 패키지의 설치 경로 확인\n",
    "print(os.path.dirname(FlagEmbedding.__file__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 114,
     "status": "ok",
     "timestamp": 1754039047474,
     "user": {
      "displayName": "김광무",
      "userId": "03808645168826839149"
     },
     "user_tz": -540
    },
    "id": "UF5wzjxH35fA",
    "outputId": "a566c34d-c966-4d35-d7fc-d1d902c306dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 24\n",
      "drwxr-xr-x  6 root root 4096 Aug  1 08:54 abc\n",
      "drwxr-xr-x 11 root root 4096 Aug  1 08:54 evaluation\n",
      "drwxr-xr-x  5 root root 4096 Aug  1 08:54 finetune\n",
      "drwxr-xr-x  5 root root 4096 Aug  1 08:54 inference\n",
      "-rw-r--r--  1 root root   54 Aug  1 08:54 __init__.py\n",
      "drwxr-xr-x  2 root root 4096 Aug  1 08:54 __pycache__\n"
     ]
    }
   ],
   "source": [
    "!ls -l /usr/local/lib/python3.11/dist-packages/FlagEmbedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jy0uhxRn5WZO"
   },
   "source": [
    "https://github.com/FlagOpen/FlagEmbedding/blob/master/scripts/hn_mine.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uv32NS6q9HdL"
   },
   "source": [
    "FlagEmbedding.baai_general_embedding.finetune.hn_mine 는 더이상 지원되지 않고 별도의 파일로 제공됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 149859,
     "status": "ok",
     "timestamp": 1754201307815,
     "user": {
      "displayName": "김광무",
      "userId": "03808645168826839149"
     },
     "user_tz": -540
    },
    "id": "0S98Zd_ZdQ70",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "99b9f6a8-f6c7-4559-e70d-4bb02af7548e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-03 06:06:16.457110: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1754201176.694077    1145 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1754201176.754188    1145 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-08-03 06:06:17.230258: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "tokenizer_config.json: 100% 444/444 [00:00<00:00, 1.80MB/s]\n",
      "sentencepiece.bpe.model: 100% 5.07M/5.07M [00:00<00:00, 70.8MB/s]\n",
      "tokenizer.json: 100% 17.1M/17.1M [00:00<00:00, 181MB/s]\n",
      "special_tokens_map.json: 100% 964/964 [00:00<00:00, 4.35MB/s]\n",
      "Fetching 30 files:   0% 0/30 [00:00<?, ?it/s]\n",
      "README.md: 15.8kB [00:00, 23.2MB/s]\n",
      "\n",
      "config_sentence_transformers.json: 100% 123/123 [00:00<00:00, 624kB/s]\n",
      "\n",
      "\n",
      "config.json:   0% 0.00/191 [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "config.json: 100% 191/191 [00:00<00:00, 37.6kB/s]\n",
      ".DS_Store: 100% 6.15k/6.15k [00:00<00:00, 833kB/s]\n",
      "\n",
      "bm25.jpg:   0% 0.00/132k [00:00<?, ?B/s]\u001b[A\n",
      "\n",
      "config.json:   0% 0.00/687 [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "config.json: 100% 687/687 [00:00<00:00, 1.29MB/s]\n",
      ".gitattributes: 1.63kB [00:00, 2.35MB/s]\n",
      "Fetching 30 files:   3% 1/30 [00:00<00:05,  5.69it/s]\n",
      "\n",
      "bm25.jpg: 100% 132k/132k [00:00<00:00, 7.13MB/s]\n",
      "colbert_linear.pt: 100% 2.10M/2.10M [00:00<00:00, 29.3MB/s]\n",
      "\n",
      "long.jpg:   0% 0.00/485k [00:00<?, ?B/s]\u001b[A\n",
      "\n",
      "miracl.jpg:   0% 0.00/576k [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "mkqa.jpg:   0% 0.00/608k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "others.webp:   0% 0.00/21.0k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "others.webp: 100% 21.0k/21.0k [00:00<00:00, 11.6MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "modules.json: 100% 349/349 [00:00<00:00, 1.30MB/s]\n",
      "long.jpg: 100% 485k/485k [00:00<00:00, 12.6MB/s]\n",
      "\n",
      "miracl.jpg: 100% 576k/576k [00:00<00:00, 15.5MB/s]\n",
      "long.jpg: 100% 127k/127k [00:00<00:00, 5.17MB/s]\n",
      "mkqa.jpg: 100% 608k/608k [00:00<00:00, 16.2MB/s]\n",
      "nqa.jpg: 100% 158k/158k [00:00<00:00, 5.65MB/s]\n",
      "\n",
      "Constant_7_attr__value: 100% 65.6k/65.6k [00:00<00:00, 4.90MB/s]\n",
      "\n",
      "config.json: 100% 698/698 [00:00<00:00, 3.70MB/s]\n",
      "Fetching 30 files:  57% 17/30 [00:00<00:00, 44.33it/s]\n",
      "model.onnx:   0% 0.00/725k [00:00<?, ?B/s]\u001b[A\n",
      "\n",
      "model.onnx_data:   0% 0.00/2.27G [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model.onnx: 100% 725k/725k [00:00<00:00, 17.9MB/s]\n",
      "\n",
      "pytorch_model.bin:   0% 0.00/2.27G [00:00<?, ?B/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "sparse_linear.pt: 100% 3.52k/3.52k [00:00<00:00, 16.5MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "sentence_bert_config.json: 100% 54.0/54.0 [00:00<00:00, 304kB/s]\n",
      "\n",
      "\n",
      "model.onnx_data:   0% 10.5M/2.27G [00:00<00:27, 81.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "tokenizer_config.json: 0.00B [00:00, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "tokenizer_config.json: 1.17kB [00:00, 681kB/s]\n",
      "\n",
      "\n",
      "\n",
      "tokenizer.json: 100% 17.1M/17.1M [00:00<00:00, 65.3MB/s]\n",
      "\n",
      "\n",
      "model.onnx_data:   1% 31.5M/2.27G [00:00<00:19, 114MB/s] \u001b[A\u001b[A\n",
      "pytorch_model.bin:   1% 31.5M/2.27G [00:00<00:18, 124MB/s] \u001b[A\n",
      "pytorch_model.bin:   2% 52.4M/2.27G [00:00<00:16, 138MB/s]\u001b[A\n",
      "\n",
      "model.onnx_data:   2% 52.4M/2.27G [00:00<00:20, 110MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:   3% 73.4M/2.27G [00:00<00:16, 135MB/s]\u001b[A\n",
      "\n",
      "model.onnx_data:   3% 73.4M/2.27G [00:00<00:18, 116MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:   4% 94.4M/2.27G [00:00<00:16, 134MB/s]\u001b[A\n",
      "\n",
      "model.onnx_data:   4% 94.4M/2.27G [00:00<00:17, 127MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:   5% 115M/2.27G [00:00<00:16, 135MB/s] \u001b[A\n",
      "\n",
      "model.onnx_data:   5% 115M/2.27G [00:00<00:16, 128MB/s] \u001b[A\u001b[A\n",
      "pytorch_model.bin:   6% 136M/2.27G [00:01<00:15, 139MB/s]\u001b[A\n",
      "\n",
      "model.onnx_data:   6% 136M/2.27G [00:01<00:17, 124MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:   7% 157M/2.27G [00:01<00:14, 150MB/s]\u001b[A\n",
      "\n",
      "model.onnx_data:   7% 157M/2.27G [00:01<00:15, 136MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:   8% 178M/2.27G [00:01<00:14, 147MB/s]\u001b[A\n",
      "\n",
      "model.onnx_data:   8% 178M/2.27G [00:01<00:15, 135MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:   9% 199M/2.27G [00:01<00:13, 153MB/s]\u001b[A\n",
      "\n",
      "model.onnx_data:   9% 199M/2.27G [00:01<00:14, 139MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:  10% 220M/2.27G [00:01<00:14, 144MB/s]\u001b[A\n",
      "\n",
      "model.onnx_data:  10% 220M/2.27G [00:01<00:16, 124MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:  11% 241M/2.27G [00:01<00:15, 132MB/s]\u001b[A\n",
      "\n",
      "model.onnx_data:  11% 241M/2.27G [00:01<00:15, 131MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:  12% 262M/2.27G [00:01<00:14, 138MB/s]\u001b[A\n",
      "\n",
      "model.onnx_data:  12% 273M/2.27G [00:02<00:12, 154MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:  12% 283M/2.27G [00:02<00:14, 139MB/s]\u001b[A\n",
      "\n",
      "model.onnx_data:  13% 294M/2.27G [00:02<00:14, 132MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:  13% 304M/2.27G [00:02<00:16, 123MB/s]\u001b[A\n",
      "\n",
      "model.onnx_data:  14% 315M/2.27G [00:02<00:15, 126MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:  14% 325M/2.27G [00:02<00:16, 119MB/s]\u001b[A\n",
      "\n",
      "model.onnx_data:  15% 336M/2.27G [00:02<00:14, 131MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:  15% 346M/2.27G [00:02<00:15, 123MB/s]\u001b[A\n",
      "\n",
      "model.onnx_data:  16% 357M/2.27G [00:02<00:15, 122MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:  16% 367M/2.27G [00:02<00:16, 118MB/s]\u001b[A\n",
      "\n",
      "model.onnx_data:  17% 377M/2.27G [00:03<00:16, 114MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:  17% 388M/2.27G [00:03<00:16, 112MB/s]\u001b[A\n",
      "\n",
      "model.onnx_data:  18% 398M/2.27G [00:03<00:15, 121MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:  18% 409M/2.27G [00:03<00:15, 119MB/s]\u001b[A\n",
      "pytorch_model.bin:  19% 430M/2.27G [00:03<00:14, 127MB/s]\u001b[A\n",
      "\n",
      "model.onnx_data:  19% 419M/2.27G [00:03<00:15, 119MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:  20% 451M/2.27G [00:03<00:13, 135MB/s]\u001b[A\n",
      "\n",
      "model.onnx_data:  19% 440M/2.27G [00:03<00:15, 122MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:  21% 472M/2.27G [00:03<00:12, 146MB/s]\u001b[A\n",
      "\n",
      "model.onnx_data:  20% 461M/2.27G [00:03<00:14, 128MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:  22% 493M/2.27G [00:03<00:12, 140MB/s]\u001b[A\n",
      "\n",
      "model.onnx_data:  21% 482M/2.27G [00:03<00:13, 130MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:  23% 514M/2.27G [00:03<00:13, 133MB/s]\u001b[A\n",
      "\n",
      "model.onnx_data:  22% 503M/2.27G [00:03<00:13, 131MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:  24% 535M/2.27G [00:04<00:12, 135MB/s]\u001b[A\n",
      "\n",
      "model.onnx_data:  23% 524M/2.27G [00:04<00:13, 134MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.onnx_data:  24% 545M/2.27G [00:04<00:11, 146MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:  24% 556M/2.27G [00:04<00:13, 131MB/s]\u001b[A\n",
      "\n",
      "model.onnx_data:  25% 566M/2.27G [00:04<00:11, 149MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:  25% 577M/2.27G [00:04<00:12, 135MB/s]\u001b[A\n",
      "\n",
      "model.onnx_data:  26% 587M/2.27G [00:04<00:11, 148MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:  26% 598M/2.27G [00:04<00:11, 140MB/s]\u001b[A\n",
      "\n",
      "model.onnx_data:  27% 608M/2.27G [00:04<00:12, 131MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:  27% 619M/2.27G [00:04<00:12, 129MB/s]\u001b[A\n",
      "\n",
      "model.onnx_data:  28% 629M/2.27G [00:04<00:11, 142MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:  28% 640M/2.27G [00:04<00:13, 123MB/s]\u001b[A\n",
      "\n",
      "model.onnx_data:  29% 650M/2.27G [00:04<00:11, 140MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:  29% 661M/2.27G [00:04<00:12, 133MB/s]\u001b[A\n",
      "\n",
      "model.onnx_data:  30% 671M/2.27G [00:05<00:11, 136MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:  30% 682M/2.27G [00:05<00:11, 137MB/s]\u001b[A\n",
      "\n",
      "model.onnx_data:  31% 692M/2.27G [00:05<00:10, 148MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:  31% 703M/2.27G [00:05<00:11, 139MB/s]\u001b[A\n",
      "\n",
      "model.onnx_data:  31% 713M/2.27G [00:05<00:10, 143MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:  32% 724M/2.27G [00:05<00:11, 135MB/s]\u001b[A\n",
      "\n",
      "model.onnx_data:  32% 734M/2.27G [00:05<00:11, 133MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:  33% 744M/2.27G [00:05<00:11, 136MB/s]\u001b[A\n",
      "\n",
      "model.onnx_data:  33% 755M/2.27G [00:05<00:10, 146MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:  34% 765M/2.27G [00:05<00:11, 130MB/s]\u001b[A\n",
      "\n",
      "model.onnx_data:  34% 776M/2.27G [00:05<00:11, 134MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:  35% 786M/2.27G [00:05<00:11, 129MB/s]\u001b[A\n",
      "\n",
      "model.onnx_data:  35% 797M/2.27G [00:06<00:10, 139MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:  36% 807M/2.27G [00:06<00:10, 140MB/s]\u001b[A\n",
      "\n",
      "model.onnx_data:  36% 818M/2.27G [00:06<00:10, 139MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:  36% 828M/2.27G [00:06<00:09, 154MB/s]\u001b[A\n",
      "pytorch_model.bin:  37% 849M/2.27G [00:06<00:09, 146MB/s]\u001b[A\n",
      "\n",
      "model.onnx_data:  37% 839M/2.27G [00:06<00:13, 102MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:  38% 870M/2.27G [00:06<00:11, 124MB/s]\u001b[A\n",
      "\n",
      "model.onnx_data:  38% 860M/2.27G [00:06<00:14, 99.6MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:  39% 891M/2.27G [00:06<00:12, 113MB/s]\u001b[A\n",
      "\n",
      "model.onnx_data:  39% 881M/2.27G [00:07<00:15, 91.0MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:  40% 912M/2.27G [00:07<00:13, 98.9MB/s]\u001b[A\n",
      "\n",
      "model.onnx_data:  39% 891M/2.27G [00:09<01:16, 18.1MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:  41% 933M/2.27G [00:09<01:03, 21.2MB/s]\u001b[A\n",
      "\n",
      "model.onnx_data:  40% 912M/2.27G [00:09<00:54, 24.8MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:  42% 954M/2.27G [00:09<00:45, 28.8MB/s]\u001b[A\n",
      "pytorch_model.bin:  43% 975M/2.27G [00:10<00:33, 38.8MB/s]\u001b[A\n",
      "pytorch_model.bin:  44% 996M/2.27G [00:10<00:24, 51.1MB/s]\u001b[A\n",
      "\n",
      "model.onnx_data:  41% 923M/2.27G [00:10<00:50, 26.7MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:  45% 1.02G/2.27G [00:10<00:19, 65.5MB/s]\u001b[A\n",
      "\n",
      "model.onnx_data:  41% 933M/2.27G [00:10<00:46, 28.7MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:  46% 1.04G/2.27G [00:10<00:18, 66.4MB/s]\u001b[A\n",
      "\n",
      "model.onnx_data:  42% 944M/2.27G [00:10<00:39, 33.4MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:  47% 1.06G/2.27G [00:10<00:15, 79.1MB/s]\u001b[A\n",
      "\n",
      "model.onnx_data:  43% 965M/2.27G [00:10<00:26, 48.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.onnx_data:  43% 986M/2.27G [00:10<00:19, 65.7MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:  48% 1.08G/2.27G [00:10<00:13, 87.6MB/s]\u001b[A\n",
      "\n",
      "model.onnx_data:  44% 1.01G/2.27G [00:11<00:14, 84.5MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:  48% 1.10G/2.27G [00:11<00:12, 96.5MB/s]\u001b[A\n",
      "\n",
      "model.onnx_data:  45% 1.03G/2.27G [00:11<00:13, 93.4MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:  49% 1.12G/2.27G [00:11<00:11, 104MB/s] \u001b[A\n",
      "pytorch_model.bin:  51% 1.15G/2.27G [00:11<00:08, 131MB/s]\u001b[A\n",
      "\n",
      "model.onnx_data:  46% 1.05G/2.27G [00:11<00:14, 82.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.onnx_data:  47% 1.07G/2.27G [00:11<00:12, 94.5MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:  52% 1.17G/2.27G [00:11<00:09, 112MB/s]\u001b[A\n",
      "\n",
      "model.onnx_data:  48% 1.09G/2.27G [00:11<00:10, 110MB/s] \u001b[A\u001b[A\n",
      "pytorch_model.bin:  53% 1.20G/2.27G [00:11<00:08, 121MB/s]\u001b[A\n",
      "\n",
      "model.onnx_data:  49% 1.11G/2.27G [00:11<00:09, 122MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:  54% 1.22G/2.27G [00:11<00:08, 120MB/s]\u001b[A\n",
      "\n",
      "model.onnx_data:  50% 1.13G/2.27G [00:12<00:09, 121MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:  54% 1.24G/2.27G [00:12<00:08, 124MB/s]\u001b[A\n",
      "\n",
      "model.onnx_data:  51% 1.15G/2.27G [00:12<00:08, 128MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:  55% 1.26G/2.27G [00:12<00:08, 122MB/s]\u001b[A\n",
      "\n",
      "model.onnx_data:  52% 1.17G/2.27G [00:12<00:08, 126MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:  56% 1.28G/2.27G [00:12<00:07, 128MB/s]\u001b[A\n",
      "\n",
      "model.onnx_data:  53% 1.20G/2.27G [00:12<00:08, 127MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:  57% 1.30G/2.27G [00:12<00:06, 141MB/s]\u001b[A\n",
      "\n",
      "model.onnx_data:  54% 1.22G/2.27G [00:12<00:08, 122MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:  58% 1.32G/2.27G [00:12<00:07, 134MB/s]\u001b[A\n",
      "pytorch_model.bin:  59% 1.34G/2.27G [00:12<00:06, 143MB/s]\u001b[A\n",
      "\n",
      "model.onnx_data:  55% 1.24G/2.27G [00:12<00:08, 124MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:  60% 1.36G/2.27G [00:12<00:05, 157MB/s]\u001b[A\n",
      "\n",
      "model.onnx_data:  56% 1.26G/2.27G [00:13<00:08, 123MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:  61% 1.38G/2.27G [00:13<00:05, 158MB/s]\u001b[A\n",
      "pytorch_model.bin:  62% 1.41G/2.27G [00:13<00:05, 167MB/s]\u001b[A\n",
      "\n",
      "model.onnx_data:  57% 1.29G/2.27G [00:13<00:06, 141MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:  63% 1.43G/2.27G [00:13<00:05, 160MB/s]\u001b[A\n",
      "\n",
      "model.onnx_data:  58% 1.31G/2.27G [00:13<00:06, 141MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:  64% 1.45G/2.27G [00:13<00:05, 163MB/s]\u001b[A\n",
      "\n",
      "model.onnx_data:  59% 1.33G/2.27G [00:13<00:07, 122MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:  65% 1.47G/2.27G [00:13<00:05, 135MB/s]\u001b[A\n",
      "\n",
      "model.onnx_data:  60% 1.35G/2.27G [00:13<00:07, 123MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:  66% 1.49G/2.27G [00:13<00:05, 132MB/s]\u001b[A\n",
      "\n",
      "model.onnx_data:  61% 1.37G/2.27G [00:14<00:07, 117MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:  66% 1.51G/2.27G [00:14<00:05, 129MB/s]\u001b[A\n",
      "\n",
      "model.onnx_data:  62% 1.39G/2.27G [00:14<00:07, 120MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:  67% 1.53G/2.27G [00:14<00:05, 129MB/s]\u001b[A\n",
      "\n",
      "model.onnx_data:  62% 1.42G/2.27G [00:14<00:06, 130MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:  68% 1.55G/2.27G [00:14<00:05, 137MB/s]\u001b[A\n",
      "\n",
      "model.onnx_data:  63% 1.44G/2.27G [00:14<00:07, 118MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:  69% 1.57G/2.27G [00:14<00:05, 129MB/s]\u001b[A\n",
      "\n",
      "model.onnx_data:  64% 1.46G/2.27G [00:14<00:06, 131MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:  70% 1.59G/2.27G [00:14<00:05, 127MB/s]\u001b[A\n",
      "\n",
      "model.onnx_data:  65% 1.48G/2.27G [00:14<00:05, 141MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:  71% 1.61G/2.27G [00:14<00:05, 118MB/s]\u001b[A\n",
      "\n",
      "model.onnx_data:  67% 1.51G/2.27G [00:15<00:05, 130MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:  72% 1.64G/2.27G [00:15<00:05, 112MB/s]\u001b[A\n",
      "\n",
      "model.onnx_data:  68% 1.53G/2.27G [00:15<00:05, 141MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:  73% 1.66G/2.27G [00:15<00:05, 111MB/s]\u001b[A\n",
      "\n",
      "model.onnx_data:  68% 1.55G/2.27G [00:15<00:05, 123MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:  74% 1.68G/2.27G [00:15<00:04, 122MB/s]\u001b[A\n",
      "\n",
      "model.onnx_data:  70% 1.58G/2.27G [00:15<00:05, 130MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:  75% 1.70G/2.27G [00:15<00:04, 121MB/s]\u001b[A\n",
      "\n",
      "model.onnx_data:  71% 1.60G/2.27G [00:15<00:04, 138MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:  76% 1.72G/2.27G [00:15<00:04, 124MB/s]\u001b[A\n",
      "\n",
      "model.onnx_data:  72% 1.63G/2.27G [00:15<00:04, 145MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:  77% 1.74G/2.27G [00:15<00:04, 117MB/s]\u001b[A\n",
      "\n",
      "model.onnx_data:  73% 1.65G/2.27G [00:16<00:04, 128MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:  78% 1.76G/2.27G [00:16<00:04, 110MB/s]\u001b[A\n",
      "\n",
      "Fetching 30 files:  57% 17/30 [00:16<00:00, 44.33it/s]\n",
      "pytorch_model.bin:  78% 1.78G/2.27G [00:16<00:03, 124MB/s]\u001b[A\n",
      "\n",
      "model.onnx_data:  74% 1.69G/2.27G [00:16<00:04, 140MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:  79% 1.80G/2.27G [00:16<00:04, 113MB/s]\u001b[A\n",
      "\n",
      "model.onnx_data:  75% 1.71G/2.27G [00:16<00:04, 126MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.onnx_data:  76% 1.73G/2.27G [00:16<00:03, 140MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:  80% 1.82G/2.27G [00:16<00:03, 119MB/s]\u001b[A\n",
      "\n",
      "model.onnx_data:  77% 1.75G/2.27G [00:16<00:04, 127MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:  81% 1.85G/2.27G [00:16<00:04, 97.0MB/s]\u001b[A\n",
      "\n",
      "model.onnx_data:  78% 1.77G/2.27G [00:17<00:04, 107MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:  82% 1.87G/2.27G [00:17<00:03, 106MB/s] \u001b[A\n",
      "\n",
      "model.onnx_data:  79% 1.79G/2.27G [00:17<00:03, 119MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:  83% 1.89G/2.27G [00:17<00:03, 113MB/s]\u001b[A\n",
      "\n",
      "model.onnx_data:  80% 1.81G/2.27G [00:17<00:04, 110MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:  84% 1.91G/2.27G [00:17<00:03, 106MB/s]\u001b[A\n",
      "\n",
      "model.onnx_data:  81% 1.84G/2.27G [00:17<00:03, 119MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:  85% 1.93G/2.27G [00:17<00:02, 120MB/s]\u001b[A\n",
      "\n",
      "model.onnx_data:  82% 1.86G/2.27G [00:17<00:03, 120MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:  86% 1.95G/2.27G [00:17<00:02, 112MB/s]\u001b[A\n",
      "\n",
      "model.onnx_data:  83% 1.88G/2.27G [00:17<00:03, 119MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:  87% 1.97G/2.27G [00:18<00:02, 117MB/s]\u001b[A\n",
      "\n",
      "model.onnx_data:  84% 1.90G/2.27G [00:18<00:02, 131MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.onnx_data:  85% 1.92G/2.27G [00:18<00:02, 146MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:  88% 2.00G/2.27G [00:18<00:01, 137MB/s]\u001b[A\n",
      "\n",
      "model.onnx_data:  86% 1.94G/2.27G [00:19<00:08, 39.8MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:  89% 2.02G/2.27G [00:19<00:06, 39.8MB/s]\u001b[A\n",
      "\n",
      "model.onnx_data:  87% 1.96G/2.27G [00:19<00:06, 48.9MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:  90% 2.03G/2.27G [00:19<00:05, 43.9MB/s]\u001b[A\n",
      "pytorch_model.bin:  90% 2.04G/2.27G [00:19<00:04, 49.1MB/s]\u001b[A\n",
      "\n",
      "model.onnx_data:  87% 1.98G/2.27G [00:20<00:04, 59.3MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:  91% 2.07G/2.27G [00:20<00:03, 62.2MB/s]\u001b[A\n",
      "\n",
      "model.onnx_data:  88% 2.00G/2.27G [00:20<00:03, 72.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.onnx_data:  89% 2.02G/2.27G [00:20<00:03, 76.1MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:  92% 2.09G/2.27G [00:20<00:02, 66.7MB/s]\u001b[A\n",
      "pytorch_model.bin:  92% 2.10G/2.27G [00:20<00:02, 71.6MB/s]\u001b[A\n",
      "\n",
      "model.onnx_data:  90% 2.04G/2.27G [00:20<00:02, 87.3MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:  93% 2.12G/2.27G [00:20<00:01, 82.4MB/s]\u001b[A\n",
      "\n",
      "model.onnx_data:  91% 2.07G/2.27G [00:20<00:02, 97.5MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:  94% 2.14G/2.27G [00:20<00:01, 98.5MB/s]\u001b[A\n",
      "pytorch_model.bin:  95% 2.16G/2.27G [00:20<00:00, 113MB/s] \u001b[A\n",
      "pytorch_model.bin:  96% 2.18G/2.27G [00:21<00:00, 119MB/s]\u001b[A\n",
      "\n",
      "model.onnx_data:  92% 2.09G/2.27G [00:21<00:02, 68.5MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:  97% 2.21G/2.27G [00:21<00:00, 143MB/s]\u001b[A\n",
      "\n",
      "model.onnx_data:  93% 2.10G/2.27G [00:21<00:02, 58.7MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin:  98% 2.23G/2.27G [00:21<00:00, 105MB/s]\u001b[A\n",
      "pytorch_model.bin:  99% 2.25G/2.27G [00:21<00:00, 116MB/s]\u001b[A\n",
      "\n",
      "model.onnx_data:  93% 2.11G/2.27G [00:21<00:02, 53.8MB/s]\u001b[A\u001b[A\n",
      "pytorch_model.bin: 100% 2.27G/2.27G [00:21<00:00, 104MB/s]\n",
      "\n",
      "\n",
      "model.onnx_data:  93% 2.12G/2.27G [00:22<00:02, 51.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.onnx_data:  94% 2.13G/2.27G [00:22<00:02, 47.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.onnx_data:  94% 2.14G/2.27G [00:22<00:02, 53.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.onnx_data:  95% 2.15G/2.27G [00:26<00:12, 9.24MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.onnx_data:  96% 2.18G/2.27G [00:26<00:04, 18.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.onnx_data:  97% 2.20G/2.27G [00:26<00:02, 27.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.onnx_data:  99% 2.23G/2.27G [00:26<00:00, 44.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.onnx_data: 100% 2.27G/2.27G [00:26<00:00, 85.1MB/s]\n",
      "Fetching 30 files: 100% 30/30 [00:27<00:00,  1.11it/s]\n",
      "inferencing embedding for corpus (number=80)--------------\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `XLMRobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "inferencing embedding for queries (number=10)--------------\n",
      "create index and search------------------\n"
     ]
    }
   ],
   "source": [
    "!python -m hn_mine \\\n",
    "--embedder_name_or_path BAAI/bge-m3 \\\n",
    "--input_file toy_finetune_data.jsonl \\\n",
    "--output_file toy_finetune_data_minedHN.jsonl \\\n",
    "--range_for_sampling 2-200 \\\n",
    "--negative_number 15 \\\n",
    "#--use_gpu_for_searching\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bhd1ryOu95Cn"
   },
   "source": [
    "\n",
    "*   FlagEmbedding.baai_general_embedding.finetune.run 의 경로가 아래로 변경됨\n",
    "*   --normlized True가 오타여서 수정을 했으나 여전히 인식하지 못하여 https://github.com/FlagOpen/FlagEmbedding/blob/master/Tutorials/7_Fine-tuning/7.1.2_Fine-tune.ipynb 를 참고 하여 --normalize_embeddings 로 변경함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KOzXnVtkdddG",
    "outputId": "9d1864fe-3745-45f7-e70a-bda3968e9215"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/03/2025 06:58:10 - WARNING - FlagEmbedding.abc.finetune.embedder.AbsRunner -   Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: True\n",
      "08/03/2025 06:58:10 - INFO - FlagEmbedding.abc.finetune.embedder.AbsRunner -   Training/evaluation parameters AbsEmbedderTrainingArguments(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "average_tokens_across_devices=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=True,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=False,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_steps=None,\n",
      "eval_strategy=IntervalStrategy.NO,\n",
      "eval_use_gather_object=False,\n",
      "fix_position_embedding=False,\n",
      "fp16=True,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=None,\n",
      "hub_revision=None,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_for_metrics=[],\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "kd_loss_type=kl_div,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "liger_kernel_config=None,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./20250721_강의복습_BGE-M3_FineTuning/runs/Aug03_06-58-10_b2a690d02cfa,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=10,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "negatives_cross_device=True,\n",
      "no_cuda=False,\n",
      "normalize_embeddings=True,\n",
      "num_train_epochs=5.0,\n",
      "optim=OptimizerNames.ADAMW_TORCH_FUSED,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=./20250721_강의복습_BGE-M3_FineTuning,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=1,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./20250721_강의복습_BGE-M3_FineTuning,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=1000,\n",
      "save_strategy=SaveStrategy.STEPS,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sentence_pooling_method=cls,\n",
      "skip_memory_metrics=True,\n",
      "sub_batch_size=None,\n",
      "temperature=0.02,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_liger_kernel=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "08/03/2025 06:58:10 - INFO - FlagEmbedding.abc.finetune.embedder.AbsRunner -   Model parameters AbsEmbedderModelArguments(model_name_or_path='BAAI/bge-m3', config_name=None, tokenizer_name=None, cache_dir=None, trust_remote_code=False, token=None)\n",
      "08/03/2025 06:58:10 - INFO - FlagEmbedding.abc.finetune.embedder.AbsRunner -   Data parameters AbsEmbedderDataArguments(train_data=['./toy_finetune_data.jsonl'], cache_path=None, train_group_size=2, query_max_len=64, passage_max_len=256, pad_to_multiple_of=None, max_example_num_per_dataset=100000000, query_instruction_for_retrieval='', query_instruction_format='{}{}', knowledge_distillation=False, passage_instruction_for_retrieval=None, passage_instruction_format='{}{}', shuffle_ratio=0.0, same_dataset_within_batch=False, small_threshold=0, drop_threshold=0)\n",
      "tokenizer_config.json: 100%|███████████████████| 444/444 [00:00<00:00, 5.53MB/s]\n",
      "sentencepiece.bpe.model: 100%|██████████████| 5.07M/5.07M [00:00<00:00, 162MB/s]\n",
      "tokenizer.json: 100%|███████████████████████| 17.1M/17.1M [00:00<00:00, 244MB/s]\n",
      "special_tokens_map.json: 100%|█████████████████| 964/964 [00:00<00:00, 6.43MB/s]\n",
      "config.json: 100%|█████████████████████████████| 687/687 [00:00<00:00, 6.59MB/s]\n",
      "pytorch_model.bin: 100%|████████████████████| 2.27G/2.27G [00:04<00:00, 498MB/s]\n",
      "model.safetensors:  12%|██▍                  | 262M/2.27G [00:00<00:06, 324MB/s]08/03/2025 06:58:19 - INFO - FlagEmbedding.finetune.embedder.encoder_only.base.runner -   Config: XLMRobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 8194,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.54.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "08/03/2025 06:58:19 - INFO - FlagEmbedding.abc.finetune.embedder.AbsDataset -   loading data from ./toy_finetune_data.jsonl ...\n",
      "model.safetensors:  15%|███▏                 | 346M/2.27G [00:01<00:05, 350MB/s]\n",
      "Generating train split: 10 examples [00:00, 1017.54 examples/s]1<00:05, 319MB/s]\n",
      "/usr/local/lib/python3.11/dist-packages/FlagEmbedding/finetune/embedder/encoder_only/base/runner.py:75: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `EncoderOnlyEmbedderTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = EncoderOnlyEmbedderTrainer(\n",
      "model.safetensors:  29%|██████               | 650M/2.27G [00:02<00:06, 238MB/s]\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[AYou're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:2695: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "model.safetensors:  32%|██████▊              | 734M/2.27G [00:02<00:05, 288MB/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `XLMRobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "model.safetensors:  43%|█████████            | 986M/2.27G [00:03<00:04, 311MB/s]\n",
      "model.safetensors:  45%|█████████           | 1.03G/2.27G [00:03<00:03, 320MB/s]\u001b[A\n",
      "model.safetensors:  47%|█████████▍          | 1.07G/2.27G [00:03<00:03, 334MB/s]\u001b[A\n",
      "model.safetensors:  51%|██████████▏         | 1.15G/2.27G [00:03<00:03, 345MB/s]\u001b[A\n",
      "model.safetensors:  54%|██████████▉         | 1.24G/2.27G [00:03<00:02, 347MB/s]\u001b[A\n",
      "model.safetensors:  60%|████████████        | 1.36G/2.27G [00:04<00:02, 358MB/s]\u001b[A\n",
      "model.safetensors:  64%|████████████▋       | 1.45G/2.27G [00:04<00:02, 359MB/s]\u001b[A\n",
      "model.safetensors:  67%|█████████████▍      | 1.53G/2.27G [00:04<00:02, 360MB/s]\u001b[A\n",
      "model.safetensors:  71%|██████████████▏     | 1.61G/2.27G [00:05<00:01, 335MB/s]\u001b[A\n",
      "model.safetensors:  73%|██████████████▌     | 1.66G/2.27G [00:05<00:01, 332MB/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'loss': 0.1224, 'grad_norm': 8.12671657968167e-07, 'learning_rate': 8.2e-06, 'epoch': 1.0}\n",
      "model.safetensors:  75%|██████████████▉     | 1.70G/2.27G [00:05<00:01, 316MB/s]\n",
      "model.safetensors:  77%|███████████████▎    | 1.74G/2.27G [00:05<00:01, 299MB/s]\u001b[A\n",
      "model.safetensors:  78%|███████████████▌    | 1.77G/2.27G [00:05<00:01, 292MB/s]\u001b[A\n",
      "model.safetensors:  81%|████████████████▏   | 1.84G/2.27G [00:05<00:01, 270MB/s]\u001b[A\n",
      "model.safetensors:  84%|████████████████▊   | 1.91G/2.27G [00:06<00:01, 300MB/s]\u001b[A\n",
      "model.safetensors:  85%|█████████████████   | 1.94G/2.27G [00:06<00:01, 292MB/s]\u001b[A\n",
      "model.safetensors:  88%|█████████████████▋  | 2.00G/2.27G [00:06<00:01, 267MB/s]\u001b[A\n",
      "model.safetensors:  90%|█████████████████▉  | 2.03G/2.27G [00:06<00:00, 276MB/s]\u001b[A\n",
      "model.safetensors:  93%|██████████████████▌ | 2.11G/2.27G [00:06<00:00, 271MB/s]\u001b[A\n",
      "model.safetensors:  96%|███████████████████▏| 2.18G/2.27G [00:07<00:00, 295MB/s]\u001b[A\n",
      "model.safetensors:  99%|███████████████████▊| 2.24G/2.27G [00:07<00:00, 289MB/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'loss': 0.1538, 'grad_norm': 4.6550932893296704e-05, 'learning_rate': 6.200000000000001e-06, 'epoch': 2.0}\n",
      "model.safetensors:  99%|███████████████████▊| 2.24G/2.27G [00:07<00:00, 289MB/s]\n",
      "model.safetensors: 100%|████████████████████| 2.27G/2.27G [00:07<00:00, 307MB/s]\u001b[A\n",
      "\n",
      " 42%|██████████████████                         | 21/50 [00:05<00:06,  4.55it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:05<00:05,  5.03it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:05<00:04,  5.54it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:05<00:04,  5.97it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:05<00:03,  6.36it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:05<00:03,  6.67it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:06<00:03,  6.77it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:06<00:03,  6.99it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:06<00:03,  6.94it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:06<00:02,  7.04it/s]\u001b[A\n",
      "\u001b[A{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 4.2000000000000004e-06, 'epoch': 3.0}\n",
      "\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:06<00:02,  7.04it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:06<00:02,  6.98it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:06<00:02,  6.90it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:06<00:02,  6.80it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:07<00:02,  6.60it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:07<00:02,  6.54it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:07<00:02,  6.72it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:07<00:01,  6.91it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:07<00:01,  7.02it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:07<00:01,  7.26it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:07<00:01,  7.29it/s]\u001b[A\n",
      "\u001b[A{'loss': 0.0, 'grad_norm': 2.6402011243931156e-08, 'learning_rate': 2.2e-06, 'epoch': 4.0}\n",
      "\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:07<00:01,  7.29it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:08<00:01,  7.42it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:08<00:01,  7.49it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:08<00:00,  7.45it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:08<00:00,  7.38it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:08<00:00,  7.55it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:08<00:00,  7.64it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:08<00:00,  7.64it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:09<00:00,  7.67it/s]\u001b[A\n",
      " 98%|██████████████████████████████████████████▏| 49/50 [00:09<00:00,  7.71it/s]\u001b[A\n",
      "100%|███████████████████████████████████████████| 50/50 [00:09<00:00,  7.77it/s]\u001b[A\n",
      "\u001b[A{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 2.0000000000000002e-07, 'epoch': 5.0}\n",
      "\n",
      "100%|███████████████████████████████████████████| 50/50 [00:09<00:00,  7.77it/s]\u001b[A08/03/2025 06:58:30 - INFO - FlagEmbedding.finetune.embedder.encoder_only.base.trainer -   Saving model checkpoint to ./20250721_강의복습_BGE-M3_FineTuning/checkpoint-50\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "\n",
      "\u001b[A{'train_runtime': 27.8004, 'train_samples_per_second': 1.799, 'train_steps_per_second': 1.799, 'train_loss': 0.05524165630340576, 'epoch': 5.0}\n",
      "\n",
      "100%|███████████████████████████████████████████| 50/50 [00:27<00:00,  1.80it/s]\u001b[A\n",
      "08/03/2025 06:58:49 - INFO - FlagEmbedding.finetune.embedder.encoder_only.base.trainer -   Saving model checkpoint to ./20250721_강의복습_BGE-M3_FineTuning\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "[rank0]:[W803 06:58:56.125586649 ProcessGroupNCCL.cpp:1497] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    }
   ],
   "source": [
    "!torchrun --nproc_per_node 1 \\\n",
    "-m FlagEmbedding.finetune.embedder.encoder_only.base \\\n",
    "--output_dir ./20250721_강의복습_BGE-M3_FineTuning \\\n",
    "--model_name_or_path BAAI/bge-m3 \\\n",
    "--train_data ./toy_finetune_data.jsonl \\\n",
    "--learning_rate 1e-5 \\\n",
    "--fp16 \\\n",
    "--num_train_epochs 5 \\\n",
    "--per_device_train_batch_size 1 \\\n",
    "--dataloader_drop_last True \\\n",
    "--normalize_embeddings True \\\n",
    "--temperature 0.02 \\\n",
    "--query_max_len 64 \\\n",
    "--passage_max_len 256 \\\n",
    "--train_group_size 2 \\\n",
    "--negatives_cross_device \\\n",
    "--logging_steps 10 \\\n",
    "--save_steps 1000 \\\n",
    "--query_instruction_for_retrieval \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "x7AWHF95dgJJ"
   },
   "outputs": [],
   "source": [
    "from FlagEmbedding import FlagModel\n",
    "\n",
    "sentences_1 = [\"다섯 명의 여성이 해변을 따라 플립플롭을 신고 걸어간다.\"]\n",
    "sentences_2 = [\"플립플롭을 신은 몇몇 여성들이 해변을 따라 걸어가고 있다\", \"꽁꽁 얼어붙은 한강 위로 고양이가 걸어가고 있다\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "OlvbamJJdhdx"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `XLMRobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기존 모델: [[0.9336 0.4077]]\n",
      "파인 튜닝된 모델: [[0.955  0.3943]]\n"
     ]
    }
   ],
   "source": [
    "# use_fp16의 값을 True로 사용하면 약간의 성능 저하는 있지만 속도를 더 빠르게 할 수 있습니다.\n",
    "model = FlagModel('BAAI/bge-m3', use_fp16=True)\n",
    "fine_tuned_model = FlagModel('./20250721_강의복습_BGE-M3_FineTuning', use_fp16=True)\n",
    "\n",
    "# 기존 모델로 각각 임베딩\n",
    "embeddings_1 = model.encode(sentences_1)\n",
    "embeddings_2 = model.encode(sentences_2)\n",
    "\n",
    "# 기존 모델로부터 나온 임베딩으로 유사도 계산\n",
    "similarity_from_base_model = embeddings_1 @ embeddings_2.T\n",
    "\n",
    "# 파인 튜닝 모델로 각각 임베딩\n",
    "embeddings_1 = fine_tuned_model.encode(sentences_1)\n",
    "embeddings_2 = fine_tuned_model.encode(sentences_2)\n",
    "\n",
    "# 파인 튜닝 모델로부터 나온 임베딩으로 유사도 계산\n",
    "similarity_from_fine_tuned_model = embeddings_1 @ embeddings_2.T\n",
    "\n",
    "print('기존 모델:', similarity_from_base_model)\n",
    "print('파인 튜닝된 모델:', similarity_from_fine_tuned_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "TIaasxnpdlhA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading BAAI/bge-m3 -----------------\n",
      "loading ./20250721_강의복습_BGE-M3_FineTuning -----------------\n",
      "***weight for each model***: \n",
      "BAAI/bge-m3 0.5\n",
      "./20250721_강의복습_BGE-M3_FineTuning 0.5\n",
      "Saving the new model to ./20250721_강의복습_BGE-M3_FineTuning_mixed\n",
      "Transform the model to the format of 'sentence_transformers' (pooling_method='cls', normalized=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `XLMRobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from LM_Cocktail import mix_models, mix_models_with_data\n",
    "\n",
    "# 기존 모델과 파인 튜닝 모달을 융합하여 mixed_model_1 디렉토리에 융합 모델을 저장.\n",
    "model = mix_models(\n",
    "    model_names_or_paths=[\"BAAI/bge-m3\", \"./20250721_강의복습_BGE-M3_FineTuning\"],\n",
    "    model_type='encoder',\n",
    "    weights=[0.5, 0.5],  # 가중치를 조절하세요.\n",
    "    output_path='./20250721_강의복습_BGE-M3_FineTuning_mixed')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Guqmfu7qdwn9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "융합된 모델: [[0.947 0.394]]\n"
     ]
    }
   ],
   "source": [
    "sentences_1 = [\"다섯 명의 여성이 해변을 따라 플립플롭을 신고 걸어간다.\"]\n",
    "sentences_2 = [\"플립플롭을 신은 몇몇 여성들이 해변을 따라 걸어가고 있다\", \"꽁꽁 얼어붙은 한강 위로 고양이가 걸어가고 있다\"]\n",
    "\n",
    "# Setting use_fp16 to True speeds up computation with a slight performance degradation\n",
    "model = FlagModel('./20250721_강의복습_BGE-M3_FineTuning_mixed', use_fp16=True)\n",
    "\n",
    "embeddings_1 = model.encode(sentences_1)\n",
    "embeddings_2 = model.encode(sentences_2)\n",
    "similarity = embeddings_1 @ embeddings_2.T\n",
    "\n",
    "print('융합된 모델:', similarity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNkVJy0GHjiCS5cBFWorONW",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
