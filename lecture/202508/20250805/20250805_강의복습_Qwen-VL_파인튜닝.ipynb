{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bae18ba8-f950-4e87-8f3c-6e5f0ab02b99"
   },
   "outputs": [],
   "source": [
    "# Install Pytorch & other libraries\n",
    "%pip install -q tensorboard wandb\n",
    "\n",
    "# Install Hugging Face libraries\n",
    "%pip install -q --upgrade \\\n",
    "  \"transformers==4.45.1\" \\\n",
    "  \"datasets==3.0.1\" \\\n",
    "  \"accelerate==0.34.2\" \\\n",
    "  \"evaluate==0.4.3\" \\\n",
    "  \"bitsandbytes==0.44.0\" \\\n",
    "  \"trl==0.11.1\" \\\n",
    "  \"peft==0.13.0\" \\\n",
    "  \"qwen-vl-utils\" \n",
    "\n",
    "%pip install \"Pillow>=9.4.0\"\n",
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping transformers as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping datasets as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping accelerate as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping evaluate as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping bitandbytes as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping trl as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping peft as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping qwen-vl-utils as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping wandb as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip uninstall -y transformers datasets accelerate evaluate bitandbytes trl peft qwen-vl-utils wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Cannot install transformers==4.45.1 and trl==0.20.0 because these package versions have conflicting dependencies.\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[31mERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install Hugging Face libraries\n",
    "%pip install -q \\\n",
    "  \"transformers==4.45.1\" \\\n",
    "  \"datasets==3.6.0\" \\\n",
    "  \"accelerate==1.8.1\" \\\n",
    "  \"evaluate==0.4.3\" \\\n",
    "  \"bitsandbytes==0.46.0\" \\\n",
    "  \"trl==0.20.0\" \\\n",
    "  \"peft==0.15.2\" \\\n",
    "  \"qwen-vl-utils==0.0.11\" \\\n",
    "  \"wandb==0.20.1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install  -U -q git+https://github.com/huggingface/transformers.git git+https://github.com/huggingface/trl.git datasets bitsandbytes peft qwen-vl-utils wandb accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q torch==2.4.1+cu121 torchvision==0.19.1+cu121 torchaudio==2.4.1+cu121 --extra-index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.7.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy>=1.22.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.1.2)\n",
      "Collecting scipy>=1.8.0 (from scikit-learn)\n",
      "  Downloading scipy-1.16.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (61 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.7.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m88.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "Downloading scipy-1.16.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.4/35.4 MB\u001b[0m \u001b[31m150.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.5.1 scikit-learn-1.7.1 scipy-1.16.1 threadpoolctl-3.6.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "02234de3-f5a3-4a94-93e0-c18846a01089"
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import json\n",
    "from PIL import Image\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from transformers import AutoModelForVision2Seq, AutoProcessor\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "from transformers import Qwen2VLProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from peft import LoraConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "83c22022-1417-4c2a-ad0e-6aa7612222dc"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/dummy/dummy/runs/i7kw7byo?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x782200744c50>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.init(mode=\"disabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "361c7a06-fce8-4514-8c8e-7dbbe6652f50"
   },
   "outputs": [],
   "source": [
    "# 시스템(assistant)에게 주어진 역할\n",
    "system_message = \"당신은 이미지와 제품명(name)으로부터 패션/스타일 정보를 추론하는 분류 모델입니다.\"\n",
    "\n",
    "# 실제로 사용자 입력 -> 모델이 답해야 하는 프롬프트\n",
    "prompt = \"\"\"입력 정보:\n",
    "- name: {name}\n",
    "- image: [image]\n",
    "\n",
    "위 정보를 바탕으로, 아래 7가지 key에 대한 값을 JSON 형태로 추론해 주세요:\n",
    "1) gender\n",
    "2) masterCategory\n",
    "3) subCategory\n",
    "4) season\n",
    "5) usage\n",
    "6) baseColour\n",
    "7) articleType\n",
    "\n",
    "출력 시 **아래 JSON 예시 형태**를 반드시 지키세요:\n",
    "{{\n",
    "  \"gender\": \"예시값\",\n",
    "  \"masterCategory\": \"예시값\",\n",
    "  \"subCategory\": \"예시값\",\n",
    "  \"season\": \"예시값\",\n",
    "  \"usage\": \"예시값\",\n",
    "  \"baseColour\": \"예시값\",\n",
    "  \"articleType\": \"예시값\"\n",
    "}}\n",
    "\n",
    "# 예시\n",
    "{{\n",
    "  \"gender\": \"Men\",\n",
    "  \"masterCategory\": \"Accessories\",\n",
    "  \"subCategory\": \"Eyewear\",\n",
    "  \"season\": \"Winter\",\n",
    "  \"usage\": \"Casual\",\n",
    "  \"baseColour\": \"Blue\",\n",
    "  \"articleType\": \"Sunglasses\"\n",
    "}}\n",
    "\n",
    "# 주의\n",
    "- 7개 항목 이외의 정보(텍스트, 문장 등)는 절대 포함하지 마세요.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "27e7eb2f-24f0-4e4b-b47e-d5e1bf680104"
   },
   "outputs": [],
   "source": [
    "def combine_cols_to_label(example):\n",
    "    # 실제 컬럼명에 맞게 수정\n",
    "    label_dict = {\n",
    "        \"gender\": example[\"gender\"],\n",
    "        \"masterCategory\": example[\"masterCategory\"],\n",
    "        \"subCategory\": example[\"subCategory\"],\n",
    "        \"season\": example[\"season\"],\n",
    "        \"usage\": example[\"usage\"],\n",
    "        \"baseColour\": example[\"baseColour\"],\n",
    "        \"articleType\": example[\"articleType\"],\n",
    "    }\n",
    "    example[\"label\"] = json.dumps(label_dict, ensure_ascii=False)\n",
    "    return example\n",
    "\n",
    "def format_data(sample):\n",
    "   # Image.Image를 PngImageFile로 변환\n",
    "   buffer = io.BytesIO()\n",
    "   sample[\"image\"].save(buffer, format='PNG')\n",
    "   buffer.seek(0)\n",
    "   image = Image.open(buffer)\n",
    "\n",
    "   return {\n",
    "       \"messages\": [\n",
    "           {\n",
    "               \"role\": \"system\",\n",
    "               \"content\": [\n",
    "                   {\n",
    "                       \"type\": \"text\",\n",
    "                       \"text\": system_message\n",
    "                   }\n",
    "               ],\n",
    "           },\n",
    "           {\n",
    "               \"role\": \"user\",\n",
    "               \"content\": [\n",
    "                   {\n",
    "                       \"type\": \"text\",\n",
    "                       \"text\": prompt.format(name=sample[\"productDisplayName\"]),\n",
    "                   },\n",
    "                   {\n",
    "                       \"type\": \"image\",\n",
    "                       \"image\": image,\n",
    "                   }\n",
    "               ],\n",
    "           },\n",
    "           {\n",
    "               \"role\": \"assistant\",\n",
    "               \"content\": [\n",
    "                   {\n",
    "                       \"type\": \"text\",\n",
    "                       \"text\": sample[\"label\"],\n",
    "                   }\n",
    "               ],\n",
    "           },\n",
    "       ],\n",
    "   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "1cc9b30f-2665-4d47-a290-f9f94c47ad81"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "069f9ffc6850445ab3d66bef01532e08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/867 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1787f17175cd451fb791f55317ae6ac2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-00000-of-00002-6cff4c59f91661c3.parquet:   0%|          | 0.00/136M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c867f22be7a24480ac981b8ff0ed5772",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-00001-of-00002-bb459e5ac5f01e71.parquet:   0%|          | 0.00/135M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0429cc1a810a440baaee65eebb26acf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/44072 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a7cacdbe4e74a6d9202e076e7caa429",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/44072 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"ashraq/fashion-product-images-small\", split=\"train\")\n",
    "dataset_add_label = dataset.map(combine_cols_to_label)\n",
    "dataset_add_label = dataset_add_label.shuffle(seed=4242)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "b1e348d3-38a3-4223-b0dc-4587dd4cacfd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 15516,\n",
       " 'gender': 'Men',\n",
       " 'masterCategory': 'Footwear',\n",
       " 'subCategory': 'Flip Flops',\n",
       " 'articleType': 'Flip Flops',\n",
       " 'baseColour': 'Navy Blue',\n",
       " 'season': 'Fall',\n",
       " 'year': 2011.0,\n",
       " 'usage': 'Casual',\n",
       " 'productDisplayName': 'Rockport Men Altrezlp Navy Blue Flip Flops',\n",
       " 'image': <PIL.Image.Image image mode=RGB size=60x80>,\n",
       " 'label': '{\"gender\": \"Men\", \"masterCategory\": \"Footwear\", \"subCategory\": \"Flip Flops\", \"season\": \"Fall\", \"usage\": \"Casual\", \"baseColour\": \"Navy Blue\", \"articleType\": \"Flip Flops\"}'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_add_label[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "c80992b0-3352-482f-866f-57f60fb2b583"
   },
   "outputs": [],
   "source": [
    "formatted_dataset = [format_data(row) for row in dataset_add_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "db74a091-8bbf-4dfa-a169-b7a84f611976"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [{'role': 'system',\n",
       "   'content': [{'type': 'text',\n",
       "     'text': '당신은 이미지와 제품명(name)으로부터 패션/스타일 정보를 추론하는 분류 모델입니다.'}]},\n",
       "  {'role': 'user',\n",
       "   'content': [{'type': 'text',\n",
       "     'text': '입력 정보:\\n- name: Rockport Men Altrezlp Navy Blue Flip Flops\\n- image: [image]\\n\\n위 정보를 바탕으로, 아래 7가지 key에 대한 값을 JSON 형태로 추론해 주세요:\\n1) gender\\n2) masterCategory\\n3) subCategory\\n4) season\\n5) usage\\n6) baseColour\\n7) articleType\\n\\n출력 시 **아래 JSON 예시 형태**를 반드시 지키세요:\\n{\\n  \"gender\": \"예시값\",\\n  \"masterCategory\": \"예시값\",\\n  \"subCategory\": \"예시값\",\\n  \"season\": \"예시값\",\\n  \"usage\": \"예시값\",\\n  \"baseColour\": \"예시값\",\\n  \"articleType\": \"예시값\"\\n}\\n\\n# 예시\\n{\\n  \"gender\": \"Men\",\\n  \"masterCategory\": \"Accessories\",\\n  \"subCategory\": \"Eyewear\",\\n  \"season\": \"Winter\",\\n  \"usage\": \"Casual\",\\n  \"baseColour\": \"Blue\",\\n  \"articleType\": \"Sunglasses\"\\n}\\n\\n# 주의\\n- 7개 항목 이외의 정보(텍스트, 문장 등)는 절대 포함하지 마세요.\\n'},\n",
       "    {'type': 'image',\n",
       "     'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=60x80>}]},\n",
       "  {'role': 'assistant',\n",
       "   'content': [{'type': 'text',\n",
       "     'text': '{\"gender\": \"Men\", \"masterCategory\": \"Footwear\", \"subCategory\": \"Flip Flops\", \"season\": \"Fall\", \"usage\": \"Casual\", \"baseColour\": \"Navy Blue\", \"articleType\": \"Flip Flops\"}'}]}]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "daa7b51c-9b4d-47b5-8811-23aa1d3e41a0"
   },
   "outputs": [],
   "source": [
    "# test_size=0.9로 설정하여 전체 데이터의 90%를 테스트 세트로 분리\n",
    "train_dataset, test_dataset = train_test_split(formatted_dataset,\n",
    "                                             test_size=0.9,\n",
    "                                             random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "f870de25-a37a-4e5b-a1b4-fef198d72315"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 데이터의 개수: 4407\n",
      "테스트 데이터의 개수: 39665\n"
     ]
    }
   ],
   "source": [
    "print('학습 데이터의 개수:', len(train_dataset))\n",
    "print('테스트 데이터의 개수:', len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "b6b0fbdb-5db4-41a8-9ee3-883d8f5dfed0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/models/auto/modeling_auto.py:2199: FutureWarning: The class `AutoModelForVision2Seq` is deprecated and will be removed in v5.0. Please use `AutoModelForImageTextToText` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6fc8d7db47a4c48a591258f8165bde9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aeb0204ddf7d4b6db9913e514a308ca4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e0861b8bb6b4c55ae4d76e5cca0d5f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d19673e85fe45e0a330efa845b5bf61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00005.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4cefe77105e423caf1d8a6c1c7b0484",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00005.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b2422fe43494547804978a1df498d9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00005.safetensors:   0%|          | 0.00/3.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c0f9e7f5df441319bf9b491eb806911",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00005.safetensors:   0%|          | 0.00/1.09G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72fd35aefdbc4d1cb4a272c26931bab5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00005.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc5eccd62cba413db9a496744fee519c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cc7a3d275424b2baa252d8004bd3a9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/244 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "022c3b41faa148f18ae917987cb277b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/347 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c068a538e1984339b32c9ce946314a93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b87a133321d040289c496ec4c2bec243",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50d7248e91a74646ad640a979a855c97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64666a12c42f4bb4a3b369ffa24ff03d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9350c77d8e440cdb1a5060934b7afad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 허깅페이스 모델 ID\n",
    "model_id = \"Qwen/Qwen2-VL-7B-Instruct\"\n",
    "\n",
    "# 모델과 프로세서 로드\n",
    "model = AutoModelForVision2Seq.from_pretrained(\n",
    "   model_id,\n",
    "   device_map=\"auto\",                            # GPU 메모리에 자동 할당\n",
    "   torch_dtype=torch.bfloat16,                   # bfloat16 정밀도 사용\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(model_id)  # 텍스트/이미지 전처리기 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "322ca1ea-30a7-4250-800f-cc6592c0d028"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "당신은 이미지와 제품명(name)으로부터 패션/스타일 정보를 추론하는 분류 모델입니다.<|im_end|>\n",
      "<|im_start|>user\n",
      "입력 정보:\n",
      "- name: Mr.Men Men's Charcoal White T-shirt\n",
      "- image: [image]\n",
      "\n",
      "위 정보를 바탕으로, 아래 7가지 key에 대한 값을 JSON 형태로 추론해 주세요:\n",
      "1) gender\n",
      "2) masterCategory\n",
      "3) subCategory\n",
      "4) season\n",
      "5) usage\n",
      "6) baseColour\n",
      "7) articleType\n",
      "\n",
      "출력 시 **아래 JSON 예시 형태**를 반드시 지키세요:\n",
      "{\n",
      "  \"gender\": \"예시값\",\n",
      "  \"masterCategory\": \"예시값\",\n",
      "  \"subCategory\": \"예시값\",\n",
      "  \"season\": \"예시값\",\n",
      "  \"usage\": \"예시값\",\n",
      "  \"baseColour\": \"예시값\",\n",
      "  \"articleType\": \"예시값\"\n",
      "}\n",
      "\n",
      "# 예시\n",
      "{\n",
      "  \"gender\": \"Men\",\n",
      "  \"masterCategory\": \"Accessories\",\n",
      "  \"subCategory\": \"Eyewear\",\n",
      "  \"season\": \"Winter\",\n",
      "  \"usage\": \"Casual\",\n",
      "  \"baseColour\": \"Blue\",\n",
      "  \"articleType\": \"Sunglasses\"\n",
      "}\n",
      "\n",
      "# 주의\n",
      "- 7개 항목 이외의 정보(텍스트, 문장 등)는 절대 포함하지 마세요.\n",
      "<|vision_start|><|image_pad|><|vision_end|><|im_end|>\n",
      "<|im_start|>assistant\n",
      "{\"gender\": \"Men\", \"masterCategory\": \"Apparel\", \"subCategory\": \"Topwear\", \"season\": \"Fall\", \"usage\": \"Casual\", \"baseColour\": \"Grey\", \"articleType\": \"Tshirts\"}<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Preparation for inference\n",
    "text = processor.apply_chat_template(\n",
    "    train_dataset[0][\"messages\"], tokenize=False, add_generation_prompt=False\n",
    ")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "e182a4db-d71e-48f1-9de6-fe88d602af1e"
   },
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "       # 모델 가중치에 LoRA 업데이트를 적용하는 정도를 조절하는 스케일링 계수\n",
    "       lora_alpha=128,\n",
    "       # 과적합을 방지하기 위한 드롭아웃 비율 설정\n",
    "       lora_dropout=0.05,\n",
    "       # LoRA의 순위(rank) - 저차원 행렬의 차원을 결정\n",
    "       r=256,\n",
    "       # 편향(bias) 업데이트 여부 - 'none'은 편향을 업데이트하지 않음\n",
    "       bias=\"none\",\n",
    "       # LoRA를 적용할 대상 모듈들 - 트랜스포머 모델의 주요 투영 레이어들\n",
    "       target_modules=[\n",
    "           \"q_proj\",    # Query 투영 레이어\n",
    "           \"up_proj\",   # FFN 상향 투영 레이어\n",
    "           \"o_proj\",    # Output 투영 레이어\n",
    "           \"k_proj\",    # Key 투영 레이어\n",
    "           \"down_proj\", # FFN 하향 투영 레이어\n",
    "           \"gate_proj\", # FFN 게이트 투영 레이어\n",
    "           \"v_proj\"     # Value 투영 레이어\n",
    "       ],\n",
    "       # 작업 유형 지정 - 인과적 언어 모델링(다음 토큰 예측)\n",
    "       task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "7dc0fb20-9c67-4a76-8882-9f81724e602c"
   },
   "outputs": [],
   "source": [
    "args = SFTConfig(\n",
    "    output_dir=\"Qwen2-VL-7B-Instruct_LoRA_custom_20250805\",           # 저장될 디렉토리와 저장소 ID\n",
    "    num_train_epochs=2,                      # 학습할 총 에포크 수\n",
    "    per_device_train_batch_size=16,           # GPU당 배치 크기\n",
    "    gradient_accumulation_steps=8,           # 그래디언트 누적 스텝 수\n",
    "    gradient_checkpointing=True,             # 메모리 절약을 위한 체크포인팅\n",
    "    optim=\"adamw_torch_fused\",               # 최적화기\n",
    "    logging_steps=10,                        # 로그 기록 주기\n",
    "    save_strategy=\"steps\",                   # 저장 전략\n",
    "    save_steps=50,                           # 저장 주기\n",
    "    bf16=True,                              # bfloat16 사용\n",
    "    learning_rate=1e-4,                     # 학습률\n",
    "    max_grad_norm=0.3,                      # 그래디언트 클리핑\n",
    "    warmup_ratio=0.03,                      # 워밍업 비율\n",
    "    lr_scheduler_type=\"constant\",           # 고정 학습률\n",
    "    push_to_hub=False,                      # 허브 업로드 안 함\n",
    "    remove_unused_columns=False,\n",
    "    dataset_kwargs={\"skip_prepare_dataset\": True},\n",
    "    report_to=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "8196baaa-f31e-4514-891d-ce5fc302d815"
   },
   "outputs": [],
   "source": [
    "def collate_fn(examples):\n",
    "   \"\"\"\n",
    "   텍스트와 이미지가 포함된 대화 데이터를 모델 학습에 적합한 형태로 변환하는 함수\n",
    "\n",
    "   Args:\n",
    "       examples: 각각 \"messages\" 키를 가진 딕셔너리들의 리스트\n",
    "                messages는 role(system/user/assistant)과 content를 포함하는 대화 형태\n",
    "\n",
    "   Returns:\n",
    "       batch: 모델 학습에 사용할 수 있는 토큰화된 텍스트, 이미지, 라벨이 포함된 배치\n",
    "   \"\"\"\n",
    "\n",
    "   # 1단계: 텍스트 전처리 - 채팅 템플릿 적용\n",
    "   # 각 예제의 messages를 모델 고유의 채팅 형식으로 변환\n",
    "   # 모델마다 다른 특수 토큰과 형식을 사용 (예: <|im_start|>, <|system|> 등)\n",
    "   texts = [processor.apply_chat_template(example[\"messages\"], tokenize=False) for example in examples]\n",
    "\n",
    "   # 2단계: 이미지 데이터 추출 및 전처리\n",
    "   # messages에서 이미지 정보를 추출하여 모델이 처리할 수 있는 형태로 변환\n",
    "   # process_vision_info()는 messages에서 이미지를 찾아 적절한 형태로 변환해주는 함수\n",
    "   image_inputs = [process_vision_info(example[\"messages\"])[0] for example in examples]\n",
    "\n",
    "   # 3단계: 텍스트 토크나이징 + 이미지 인코딩\n",
    "   # 텍스트를 토큰으로 변환하고 이미지를 인코딩하여 하나의 배치로 묶음\n",
    "   # return_tensors=\"pt\": PyTorch 텐서 형태로 반환\n",
    "   # padding=True: 배치 내 모든 시퀀스를 같은 길이로 맞춤 (짧은 것은 패딩 토큰으로 채움)\n",
    "   batch = processor(text=texts, images=image_inputs, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "   # 4단계: 라벨 생성 (손실 계산용)\n",
    "   # input_ids를 복사하여 라벨로 사용 (다음 토큰 예측 학습을 위함)\n",
    "   labels = batch[\"input_ids\"].clone()\n",
    "\n",
    "   # 5단계: 패딩 토큰 손실 계산에서 제외\n",
    "   # 패딩된 부분은 실제 데이터가 아니므로 손실 계산에서 제외\n",
    "   # -100으로 설정하면 CrossEntropyLoss에서 자동으로 무시됨\n",
    "   labels[labels == processor.tokenizer.pad_token_id] = -100\n",
    "\n",
    "   # 6단계: 이미지 토큰 손실 계산에서 제외\n",
    "   # 이미지 토큰은 예측 대상이 아니므로 손실 계산에서 제외\n",
    "   if isinstance(processor, Qwen2VLProcessor):\n",
    "       # Qwen2VL 모델에서 사용하는 특수 이미지 토큰들의 ID\n",
    "       # 151652: 이미지 시작 토큰, 151653: 이미지 종료 토큰, 151655: 이미지 패치 토큰\n",
    "       image_tokens = [151652, 151653, 151655]\n",
    "   else:\n",
    "       # 다른 비전-언어 모델의 이미지 토큰 ID 추출\n",
    "       image_tokens = [processor.tokenizer.convert_tokens_to_ids(processor.image_token)]\n",
    "\n",
    "   # 이미지 토큰들을 손실 계산에서 제외 (-100으로 설정)\n",
    "   for image_token_id in image_tokens:\n",
    "       labels[labels == image_token_id] = -100\n",
    "\n",
    "   # 7단계: 최종 배치에 라벨 추가\n",
    "   # 모델 학습 시 손실 계산에 사용될 라벨을 배치에 추가\n",
    "   batch[\"labels\"] = labels\n",
    "\n",
    "   return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "2c3a62ef-06b4-41d8-97ae-5aa2f2a3c36a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단일 예시 데이터:\n",
      "{'messages': [{'role': 'system', 'content': [{'type': 'text', 'text': '당신은 이미지와 제품명(name)으로부터 패션/스타일 정보를 추론하는 분류 모델입니다.'}]}, {'role': 'user', 'content': [{'type': 'text', 'text': '입력 정보:\\n- name: Mr.Men Men\\'s Charcoal White T-shirt\\n- image: [image]\\n\\n위 정보를 바탕으로, 아래 7가지 key에 대한 값을 JSON 형태로 추론해 주세요:\\n1) gender\\n2) masterCategory\\n3) subCategory\\n4) season\\n5) usage\\n6) baseColour\\n7) articleType\\n\\n출력 시 **아래 JSON 예시 형태**를 반드시 지키세요:\\n{\\n  \"gender\": \"예시값\",\\n  \"masterCategory\": \"예시값\",\\n  \"subCategory\": \"예시값\",\\n  \"season\": \"예시값\",\\n  \"usage\": \"예시값\",\\n  \"baseColour\": \"예시값\",\\n  \"articleType\": \"예시값\"\\n}\\n\\n# 예시\\n{\\n  \"gender\": \"Men\",\\n  \"masterCategory\": \"Accessories\",\\n  \"subCategory\": \"Eyewear\",\\n  \"season\": \"Winter\",\\n  \"usage\": \"Casual\",\\n  \"baseColour\": \"Blue\",\\n  \"articleType\": \"Sunglasses\"\\n}\\n\\n# 주의\\n- 7개 항목 이외의 정보(텍스트, 문장 등)는 절대 포함하지 마세요.\\n'}, {'type': 'image', 'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=60x80 at 0x7821AB903F50>}]}, {'role': 'assistant', 'content': [{'type': 'text', 'text': '{\"gender\": \"Men\", \"masterCategory\": \"Apparel\", \"subCategory\": \"Topwear\", \"season\": \"Fall\", \"usage\": \"Casual\", \"baseColour\": \"Grey\", \"articleType\": \"Tshirts\"}'}]}]}\n",
      "\n",
      "처리된 배치 데이터:\n",
      "입력 ID 형태: torch.Size([1, 365])\n",
      "어텐션 마스크 형태: torch.Size([1, 365])\n",
      "이미지 픽셀 형태: torch.Size([24, 1176])\n",
      "레이블 형태: torch.Size([1, 365])\n"
     ]
    }
   ],
   "source": [
    "# 단일 예시 확인\n",
    "example = train_dataset[0]  # 데이터셋의 첫 번째 아이템\n",
    "print(\"단일 예시 데이터:\")\n",
    "print(example)\n",
    "\n",
    "# collate_fn 테스트 (배치 크기 1로)\n",
    "batch = collate_fn([example])\n",
    "print(\"\\n처리된 배치 데이터:\")\n",
    "print(\"입력 ID 형태:\", batch[\"input_ids\"].shape)\n",
    "print(\"어텐션 마스크 형태:\", batch[\"attention_mask\"].shape)\n",
    "print(\"이미지 픽셀 형태:\", batch[\"pixel_values\"].shape)\n",
    "print(\"레이블 형태:\", batch[\"labels\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "23e5a42f-b871-4d7b-b2db-a61b85d1a5f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력에 대한 정수 인코딩 결과:\n",
      "tensor([151644,   8948,    198,  64795,  82528,  33704,  90667,  21329,  80573,\n",
      "        138017,  79632,   3153,      8,  42039, 126558,  45104,    101,  92031,\n",
      "            14, 141274,  32077,  60039,  18411,  57835, 126605,  42905, 128618,\n",
      "         97929,  54070, 142713,  78952,     13, 151645,    198, 151644,    872,\n",
      "           198,  43866,  28754,  60039,    510,     12,    829,     25,   4392,\n",
      "          1321,    268,  11012,    594,   4864,  40465,   5807,    350,  33668,\n",
      "           198,     12,   2168,     25,    508,   1805,   2533,  80901,  60039,\n",
      "         18411,  81718, 144059,  42039,     11, 136646,    220,     22,  19969,\n",
      "         21329,   1376,  19391, 128605,  93668,   4718, 141966,  17380,  57835,\n",
      "        126605,  33883,  55673,  50302,    510,     16,      8,   9825,    198,\n",
      "            17,      8,   7341,   6746,    198,     18,      8,   1186,   6746,\n",
      "           198,     19,      8,   3200,    198,     20,      8,  10431,    198,\n",
      "            21,      8,   2331,  33281,    198,     22,      8,   4549,    929,\n",
      "           271,  53496,  57133,  44518,   3070,  52959,  53442,   4718,  95617,\n",
      "         29326, 141966,    334,  18411, 141762,  66790, 126896,  50302,    510,\n",
      "           515,    220,    330,  12968,    788,    330, 127027,  29326,  82801,\n",
      "           756,    220,    330,  13629,   6746,    788,    330, 127027,  29326,\n",
      "         82801,    756,    220,    330,   1966,   6746,    788,    330, 127027,\n",
      "         29326,  82801,    756,    220,    330,  16798,    788,    330, 127027,\n",
      "         29326,  82801,    756,    220,    330,  17698,    788,    330, 127027,\n",
      "         29326,  82801,    756,    220,    330,   3152,  33281,    788,    330,\n",
      "        127027,  29326,  82801,    756,    220,    330,   7058,    929,    788,\n",
      "           330, 127027,  29326,  82801,    698,    630,      2,  95617,  29326,\n",
      "           198,    515,    220,    330,  12968,    788,    330,  28719,    756,\n",
      "           220,    330,  13629,   6746,    788,    330,   6054,   2433,    756,\n",
      "           220,    330,   1966,   6746,    788,    330,  97454,  98128,    756,\n",
      "           220,    330,  16798,    788,    330,  66188,    756,    220,    330,\n",
      "         17698,    788,    330,  49242,    928,    756,    220,    330,   3152,\n",
      "         33281,    788,    330,  10331,    756,    220,    330,   7058,    929,\n",
      "           788,    330,     50,   2185,  33868,    698,    630,      2,  55673,\n",
      "         20401,    198,     12,    220,     22,  59761, 142654,  87608,  23084,\n",
      "        128792,  20401,  60039,      7, 144153,  53189,     11,  53435,  40853,\n",
      "         77002,      8,  16560,  18585,    230,  66845, 133970,  87425,  95577,\n",
      "         50302,    624, 151652, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151653, 151645,    198, 151644,  77091,    198,   4913,  12968,    788,\n",
      "           330,  28719,    497,    330,  13629,   6746,    788,    330,   2164,\n",
      "         30431,    497,    330,   1966,   6746,    788,    330,   5366,  22744,\n",
      "           497,    330,  16798,    788,    330,  49772,    497,    330,  17698,\n",
      "           788,    330,  49242,    928,    497,    330,   3152,  33281,    788,\n",
      "           330,  59165,    497,    330,   7058,    929,    788,    330,     51,\n",
      "           927,  19666,   9207, 151645,    198])\n"
     ]
    }
   ],
   "source": [
    "print('입력에 대한 정수 인코딩 결과:')\n",
    "print(batch[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "566ab87a-7417-4c5b-b3b5-2733565bfe15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "레이블에 대한 정수 인코딩 결과:\n",
      "tensor([151644,   8948,    198,  64795,  82528,  33704,  90667,  21329,  80573,\n",
      "        138017,  79632,   3153,      8,  42039, 126558,  45104,    101,  92031,\n",
      "            14, 141274,  32077,  60039,  18411,  57835, 126605,  42905, 128618,\n",
      "         97929,  54070, 142713,  78952,     13, 151645,    198, 151644,    872,\n",
      "           198,  43866,  28754,  60039,    510,     12,    829,     25,   4392,\n",
      "          1321,    268,  11012,    594,   4864,  40465,   5807,    350,  33668,\n",
      "           198,     12,   2168,     25,    508,   1805,   2533,  80901,  60039,\n",
      "         18411,  81718, 144059,  42039,     11, 136646,    220,     22,  19969,\n",
      "         21329,   1376,  19391, 128605,  93668,   4718, 141966,  17380,  57835,\n",
      "        126605,  33883,  55673,  50302,    510,     16,      8,   9825,    198,\n",
      "            17,      8,   7341,   6746,    198,     18,      8,   1186,   6746,\n",
      "           198,     19,      8,   3200,    198,     20,      8,  10431,    198,\n",
      "            21,      8,   2331,  33281,    198,     22,      8,   4549,    929,\n",
      "           271,  53496,  57133,  44518,   3070,  52959,  53442,   4718,  95617,\n",
      "         29326, 141966,    334,  18411, 141762,  66790, 126896,  50302,    510,\n",
      "           515,    220,    330,  12968,    788,    330, 127027,  29326,  82801,\n",
      "           756,    220,    330,  13629,   6746,    788,    330, 127027,  29326,\n",
      "         82801,    756,    220,    330,   1966,   6746,    788,    330, 127027,\n",
      "         29326,  82801,    756,    220,    330,  16798,    788,    330, 127027,\n",
      "         29326,  82801,    756,    220,    330,  17698,    788,    330, 127027,\n",
      "         29326,  82801,    756,    220,    330,   3152,  33281,    788,    330,\n",
      "        127027,  29326,  82801,    756,    220,    330,   7058,    929,    788,\n",
      "           330, 127027,  29326,  82801,    698,    630,      2,  95617,  29326,\n",
      "           198,    515,    220,    330,  12968,    788,    330,  28719,    756,\n",
      "           220,    330,  13629,   6746,    788,    330,   6054,   2433,    756,\n",
      "           220,    330,   1966,   6746,    788,    330,  97454,  98128,    756,\n",
      "           220,    330,  16798,    788,    330,  66188,    756,    220,    330,\n",
      "         17698,    788,    330,  49242,    928,    756,    220,    330,   3152,\n",
      "         33281,    788,    330,  10331,    756,    220,    330,   7058,    929,\n",
      "           788,    330,     50,   2185,  33868,    698,    630,      2,  55673,\n",
      "         20401,    198,     12,    220,     22,  59761, 142654,  87608,  23084,\n",
      "        128792,  20401,  60039,      7, 144153,  53189,     11,  53435,  40853,\n",
      "         77002,      8,  16560,  18585,    230,  66845, 133970,  87425,  95577,\n",
      "         50302,    624,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100, 151645,    198, 151644,  77091,    198,   4913,  12968,    788,\n",
      "           330,  28719,    497,    330,  13629,   6746,    788,    330,   2164,\n",
      "         30431,    497,    330,   1966,   6746,    788,    330,   5366,  22744,\n",
      "           497,    330,  16798,    788,    330,  49772,    497,    330,  17698,\n",
      "           788,    330,  49242,    928,    497,    330,   3152,  33281,    788,\n",
      "           330,  59165,    497,    330,   7058,    929,    788,    330,     51,\n",
      "           927,  19666,   9207, 151645,    198])\n"
     ]
    }
   ],
   "source": [
    "print('레이블에 대한 정수 인코딩 결과:')\n",
    "print(batch[\"labels\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "b671c6c0-ca49-467d-b427-df6f9c0cd8d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "디코딩된 텍스트:\n",
      "<|im_start|>system\n",
      "당신은 이미지와 제품명(name)으로부터 패션/스타일 정보를 추론하는 분류 모델입니다.<|im_end|>\n",
      "<|im_start|>user\n",
      "입력 정보:\n",
      "- name: Mr.Men Men's Charcoal White T-shirt\n",
      "- image: [image]\n",
      "\n",
      "위 정보를 바탕으로, 아래 7가지 key에 대한 값을 JSON 형태로 추론해 주세요:\n",
      "1) gender\n",
      "2) masterCategory\n",
      "3) subCategory\n",
      "4) season\n",
      "5) usage\n",
      "6) baseColour\n",
      "7) articleType\n",
      "\n",
      "출력 시 **아래 JSON 예시 형태**를 반드시 지키세요:\n",
      "{\n",
      "  \"gender\": \"예시값\",\n",
      "  \"masterCategory\": \"예시값\",\n",
      "  \"subCategory\": \"예시값\",\n",
      "  \"season\": \"예시값\",\n",
      "  \"usage\": \"예시값\",\n",
      "  \"baseColour\": \"예시값\",\n",
      "  \"articleType\": \"예시값\"\n",
      "}\n",
      "\n",
      "# 예시\n",
      "{\n",
      "  \"gender\": \"Men\",\n",
      "  \"masterCategory\": \"Accessories\",\n",
      "  \"subCategory\": \"Eyewear\",\n",
      "  \"season\": \"Winter\",\n",
      "  \"usage\": \"Casual\",\n",
      "  \"baseColour\": \"Blue\",\n",
      "  \"articleType\": \"Sunglasses\"\n",
      "}\n",
      "\n",
      "# 주의\n",
      "- 7개 항목 이외의 정보(텍스트, 문장 등)는 절대 포함하지 마세요.\n",
      "<|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|im_end|>\n",
      "<|im_start|>assistant\n",
      "{\"gender\": \"Men\", \"masterCategory\": \"Apparel\", \"subCategory\": \"Topwear\", \"season\": \"Fall\", \"usage\": \"Casual\", \"baseColour\": \"Grey\", \"articleType\": \"Tshirts\"}<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 토큰 디코딩 예시 (입력 텍스트가 어떻게 변환되었는지 확인)\n",
    "decoded_text = processor.tokenizer.decode(batch[\"input_ids\"][0])\n",
    "print(\"\\n디코딩된 텍스트:\")\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip uninstall -y bitsandbytes\n",
    "%pip uninstall -r requirements.txt\n",
    "%pip install bitsandbytes==0.37.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip show bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "63b5e71b-4db7-4343-b80d-5ffeb839a6f0"
   },
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    data_collator=collate_fn,\n",
    "    peft_config=peft_config,\n",
    "    processing_class=processor.tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "18243c0b-f767-47fd-a84c-d9923145686f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n",
      "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='70' max='70' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [70/70 27:26, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.965700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.713700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.592000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.522000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.506800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.496800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.461900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "# 학습 시작\n",
    "trainer.train()   # 모델이 자동으로 허브와 output_dir에 저장됨\n",
    "\n",
    "# 모델 저장\n",
    "trainer.save_model()   # 최종 모델을 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMx+TgynUmm1kZNN5Z8au+h",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
