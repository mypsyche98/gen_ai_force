{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì•„ë˜ ë§í¬ë¥¼ ë³µì‚¬í•˜ì—¬ ì›¹ ë¸Œë¼ìš°ì €ì— ë¶™ì—¬ë„£ìœ¼ì„¸ìš”.\n",
      "https://accounts.google.com/o/oauth2/auth?client_id=35726703810-4v13dfqmilhgv6shlc3cv9i3ktuh73j1.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&access_type=offline&response_type=code\n",
      "Authentication successful.\n",
      "mykey => set HUGGINGFACEHUB_API_TOKEN is done\n",
      "mykey => set OPENAI_API_KEY is done\n",
      "mykey => set LANGCHAIN_API_KEY is done\n",
      "mykey => set LANGCHAIN_HUB_API_KEY is done\n",
      "mykey => set GOOGLE_API_KEY is done\n",
      "mykey => set UPSTAGE_API_KEY is done\n",
      "mykey => set COHERE_API_KEY is done\n",
      "mykey => set JINA_API_KEY is done\n",
      "mykey => set ANTHROPIC_API_KEY is done\n",
      "mykey => set DEEPL_API_KEY is done\n",
      "mykey => set TAVILY_API_KEY is done\n",
      "mykey => set TOGETHER_API_KEY is done\n"
     ]
    }
   ],
   "source": [
    "import mykeys\n",
    "\n",
    "mykeys.setOsEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "project_name = 'CH17_LangGraph_SUB03'\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = project_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith ì¶”ì ì„ ì‹œì‘í•©ë‹ˆë‹¤.\n",
      "[í”„ë¡œì íŠ¸ëª…]\n",
      "CH17_LangGraph_SUB03\n"
     ]
    }
   ],
   "source": [
    "from langchain_teddynote import logging\n",
    "\n",
    "# í”„ë¡œì íŠ¸ ì´ë¦„ì„ ì…ë ¥í•©ë‹ˆë‹¤.\n",
    "logging.langsmith(project_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote import logging\n",
    "\n",
    "# set_enable=False ë¡œ ì§€ì •í•˜ë©´ ì¶”ì ì„ í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n",
    "logging.langsmith(project_name, set_enable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CH17 LangGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "03. Use Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "01. ì—ì´ì „íŠ¸ ëŒ€í™” ì‹œë®¬ë ˆì´ì…˜ (ê³ ê° ì‘ëŒ€ ì‹œë‚˜ë¦¬ì˜¤)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph.message import add_messages  \n",
    "from typing import Annotated  \n",
    "from typing_extensions import TypedDict  \n",
    "\n",
    "\n",
    "# State ì •ì˜  \n",
    "class State(TypedDict):  \n",
    "    messages: Annotated[list, add_messages]  # ì‚¬ìš©ì - ìƒë‹´ì‚¬ ê°„ì˜ ëŒ€í™” ë©”ì‹œì§€  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List  \n",
    "from langchain_teddynote.models import LLMs, get_model_name  \n",
    "from langchain_openai import ChatOpenAI  \n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder  \n",
    "from langchain_core.messages import HumanMessage, AIMessage, BaseMessage  \n",
    "from langchain_core.output_parsers import StrOutputParser  \n",
    "\n",
    "# ëª¨ë¸ ì´ë¦„ ì„¤ì •  \n",
    "MODEL_NAME = get_model_name(LLMs.GPT4_1)  \n",
    "\n",
    "\n",
    "def call_chatbot(messages: List[BaseMessage]) -> dict:  \n",
    "    # LangChain ChatOpenAI ëª¨ë¸ì„ Agent ë¡œ ë³€ê²½í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  \n",
    "    prompt = ChatPromptTemplate.from_messages(  \n",
    "        [  \n",
    "            (  \n",
    "                \"system\",  \n",
    "                \"You are a customer support agent for an airline. Answer in Korean.\",  \n",
    "            ),  \n",
    "            MessagesPlaceholder(variable_name=\"messages\"),  \n",
    "        ]  \n",
    "    )  \n",
    "    model = ChatOpenAI(model=MODEL_NAME, temperature=0.6)  \n",
    "    chain = prompt | model | StrOutputParser()  \n",
    "    return chain.invoke({\"messages\": messages})  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_chatbot([(\"user\", \"ì•ˆë…•í•˜ì„¸ìš”?\")])  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder  \n",
    "from langchain_openai import ChatOpenAI  \n",
    "\n",
    "\n",
    "def create_scenario(name: str, instructions: str):  \n",
    "    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ ì •ì˜: í•„ìš”ì— ë”°ë¼ ë³€ê²½  \n",
    "    system_prompt_template = \"\"\"You are a customer of an airline company. \\  \n",
    "You are interacting with a user who is a customer support person. \\  \n",
    "\n",
    "Your name is {name}.  \n",
    "\n",
    "# Instructions:  \n",
    "{instructions}  \n",
    "\n",
    "[IMPORTANT]  \n",
    "- When you are finished with the conversation, respond with a single word 'FINISHED'  \n",
    "- You must speak in Korean.\"\"\"  \n",
    "\n",
    "    # ëŒ€í™” ë©”ì‹œì§€ì™€ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ ê²°í•©í•˜ì—¬ ì±„íŒ… í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ ìƒì„±í•©ë‹ˆë‹¤.  \n",
    "    prompt = ChatPromptTemplate.from_messages(  \n",
    "        [  \n",
    "            (\"system\", system_prompt_template),  \n",
    "            MessagesPlaceholder(variable_name=\"messages\"),  \n",
    "        ]  \n",
    "    )  \n",
    "\n",
    "    # íŠ¹ì • ì‚¬ìš©ì ì´ë¦„ê³¼ ì§€ì‹œì‚¬í•­ì„ ì‚¬ìš©í•˜ì—¬ í”„ë¡¬í”„íŠ¸ë¥¼ ë¶€ë¶„ì ìœ¼ë¡œ ì±„ì›ë‹ˆë‹¤.  \n",
    "    prompt = prompt.partial(name=name, instructions=instructions)  \n",
    "    return prompt  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m System Message \u001b[0m================================\n",
      "\n",
      "You are a customer of an airline company. \\  \n",
      "You are interacting with a user who is a customer support person. \\  \n",
      "\n",
      "Your name is \u001b[33;1m\u001b[1;3m{name}\u001b[0m.  \n",
      "\n",
      "# Instructions:  \n",
      "\u001b[33;1m\u001b[1;3m{instructions}\u001b[0m  \n",
      "\n",
      "[IMPORTANT]  \n",
      "- When you are finished with the conversation, respond with a single word 'FINISHED'  \n",
      "- You must speak in Korean.\n",
      "\n",
      "=============================\u001b[1m Messages Placeholder \u001b[0m=============================\n",
      "\n",
      "\u001b[33;1m\u001b[1;3m{messages}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# ì‚¬ìš©ì ì§€ì‹œì‚¬í•­ì„ ì •ì˜í•©ë‹ˆë‹¤.  \n",
    "instructions = \"\"\"You are tyring to get a refund for the trip you took to Jeju Island. \\  \n",
    "You want them to give you ALL the money back. This trip happened last year.\"\"\"  \n",
    "\n",
    "# ì‚¬ìš©ì ì´ë¦„ì„ ì •ì˜í•©ë‹ˆë‹¤.  \n",
    "name = \"Charlie\"  \n",
    "\n",
    "create_scenario(name, instructions).pretty_print()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI ì±—ë´‡ ëª¨ë¸ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.  \n",
    "model = ChatOpenAI(model=MODEL_NAME, temperature=0.6)  \n",
    "\n",
    "# ì‹œë®¬ë ˆì´ì…˜ëœ ì‚¬ìš©ì ëŒ€í™”ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.  \n",
    "simulated_user = create_scenario(name, instructions) | model | StrOutputParser()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ì•ˆë…•í•˜ì„¸ìš”. ì €ëŠ” ì‘ë…„ì— ì œì£¼ë„ì— ë‹¤ë…€ì˜¨ ì—¬í–‰ì— ëŒ€í•œ í™˜ë¶ˆì„ ë°›ê³  ì‹¶ì–´ì„œ ì—°ë½ë“œë ¸ìŠµë‹ˆë‹¤. ì „ì•¡ í™˜ë¶ˆì„ ì›í•©ë‹ˆë‹¤. ì–´ë–»ê²Œ í•´ì•¼ í•˜ë‚˜ìš”?'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage  \n",
    "\n",
    "# ì‹œë®¬ë ˆì´ì…˜ëœ ì‚¬ìš©ìì—ê²Œ ë©”ì‹œì§€ë¥¼ ì „ë‹¬  \n",
    "messages = [HumanMessage(content=\"ì•ˆë…•í•˜ì„¸ìš”? ì–´ë–»ê²Œ ë„ì™€ ë“œë¦´ê¹Œìš”?\")]  \n",
    "simulated_user.invoke({\"messages\": messages})  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage  \n",
    "\n",
    "\n",
    "# ìƒë‹´ì‚¬ ì—­í•   \n",
    "def ai_assistant_node(messages):  \n",
    "    # ìƒë‹´ì‚¬ ì‘ë‹µ í˜¸ì¶œ  \n",
    "    ai_response = call_chatbot(messages)  \n",
    "\n",
    "    # AI ìƒë‹´ì‚¬ì˜ ì‘ë‹µì„ ë°˜í™˜  \n",
    "    return {\"messages\": [(\"assistant\", ai_response)]}  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [('assistant',\n",
       "   'í™˜ë¶ˆì„ ì›í•˜ì‹œëŠ” ê²½ìš°, ì•„ë˜ ì ˆì°¨ë¥¼ ë”°ë¼ì£¼ì‹œë©´ ë©ë‹ˆë‹¤.\\n\\n1. **í•­ê³µê¶Œ ì˜ˆë§¤ ë‚´ì—­ í™•ì¸**: ì˜ˆë§¤í•˜ì‹  í•­ê³µê¶Œì˜ ì˜ˆì•½ë²ˆí˜¸(PNR)ì™€ ìŠ¹ê° ì„±í•¨ì„ ì¤€ë¹„í•´ ì£¼ì„¸ìš”.\\n2. **êµ¬ë§¤ ê²½ë¡œ í™•ì¸**: ê³µì‹ í™ˆí˜ì´ì§€, ëª¨ë°”ì¼ ì•±, ì—¬í–‰ì‚¬ ë“± ì–´ë–¤ ê²½ë¡œë¡œ ì˜ˆë§¤í•˜ì…¨ëŠ”ì§€ í™•ì¸í•´ ì£¼ì„¸ìš”.\\n3. **í™˜ë¶ˆ ì‹ ì²­**:  \\n   - ê³µì‹ í™ˆí˜ì´ì§€ ë˜ëŠ” ëª¨ë°”ì¼ ì•±: [ë‚´ ì˜ˆì•½] ë©”ë‰´ì—ì„œ í™˜ë¶ˆ ì‹ ì²­ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.\\n   - ì—¬í–‰ì‚¬ ì˜ˆë§¤: ì˜ˆë§¤í•˜ì‹  ì—¬í–‰ì‚¬ë¡œ ë¬¸ì˜í•´ ì£¼ì„¸ìš”.\\n   - ê³ ê°ì„¼í„° ì „í™”: ì €í¬ ê³ ê°ì„¼í„°(â˜ï¸)ë¡œ ì—°ë½ ì£¼ì‹œë©´ ì•ˆë‚´í•´ ë“œë¦½ë‹ˆë‹¤.\\n\\n**ìœ ì˜ì‚¬í•­**:  \\n- í•­ê³µê¶Œ ì¢…ë¥˜(í™˜ë¶ˆ ê°€ëŠ¥/ë¶ˆê°€)ì— ë”°ë¼ í™˜ë¶ˆ ê·œì • ë° ìˆ˜ìˆ˜ë£Œê°€ ë‹¤ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.  \\n- ì¼ë¶€ í• ì¸ í•­ê³µê¶Œì€ í™˜ë¶ˆì´ ë¶ˆê°€ëŠ¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  \\n- ê²°ì œí•˜ì‹  ìˆ˜ë‹¨ìœ¼ë¡œ í™˜ë¶ˆ ì²˜ë¦¬ë˜ë©°, ì˜ì—…ì¼ ê¸°ì¤€ 5~7ì¼ ì •ë„ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n\\nì˜ˆì•½ë²ˆí˜¸ì™€ ì„±í•¨ì„ ì•Œë ¤ì£¼ì‹œë©´, ë³´ë‹¤ ìì„¸íˆ ì•ˆë‚´í•´ ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì¶”ê°€ë¡œ ê¶ê¸ˆí•˜ì‹  ì  ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ë¬¸ì˜í•´ ì£¼ì„¸ìš”!')]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_assistant_node(  \n",
    "    [  \n",
    "        (\"user\", \"ì•ˆë…•í•˜ì„¸ìš”?\"),  \n",
    "        (\"assistant\", \"ì•ˆë…•í•˜ì„¸ìš”! ì–´ë–»ê²Œ ë„ì™€ë“œë¦´ê¹Œìš”?\"),  \n",
    "        (\"user\", \"í™˜ë¶ˆ ì–´ë–»ê²Œ í•˜ë‚˜ìš”?\"),  \n",
    "    ]  \n",
    ")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _swap_roles(messages):  \n",
    "    # ë©”ì‹œì§€ì˜ ì—­í• ì„ êµí™˜: ì‹œë®¬ë ˆì´ì…˜ ì‚¬ìš©ì ë‹¨ê³„ì—ì„œ ë©”ì‹œì§€ íƒ€ì…ì„ AI -> Human, Human -> AI ë¡œ êµí™˜í•©ë‹ˆë‹¤.  \n",
    "    new_messages = []  \n",
    "    for m in messages:  \n",
    "        if isinstance(m, AIMessage):  \n",
    "            # AIMessage ì¸ ê²½ìš°, HumanMessage ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.  \n",
    "            new_messages.append(HumanMessage(content=m.content))  \n",
    "        else:  \n",
    "            # HumanMessage ì¸ ê²½ìš°, AIMessage ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.  \n",
    "            new_messages.append(AIMessage(content=m.content))  \n",
    "    return new_messages  \n",
    "\n",
    "\n",
    "# ìƒë‹´ì‚¬ ì—­í• (AI Assistant) ë…¸ë“œ ì •ì˜  \n",
    "def ai_assistant_node(state: State):  \n",
    "    # ìƒë‹´ì‚¬ ì‘ë‹µ í˜¸ì¶œ  \n",
    "    ai_response = call_chatbot(state[\"messages\"])  \n",
    "\n",
    "    # AI ìƒë‹´ì‚¬ì˜ ì‘ë‹µì„ ë°˜í™˜  \n",
    "    return {\"messages\": [(\"assistant\", ai_response)]}  \n",
    "\n",
    "\n",
    "# ì‹œë®¬ë ˆì´ì…˜ëœ ì‚¬ìš©ì(Simulated User) ë…¸ë“œ ì •ì˜  \n",
    "def simulated_user_node(state: State):  \n",
    "    # ë©”ì‹œì§€ íƒ€ì…ì„ êµí™˜: AI -> Human, Human -> AI  \n",
    "    new_messages = _swap_roles(state[\"messages\"])  \n",
    "\n",
    "    # ì‹œë®¬ë ˆì´ì…˜ëœ ì‚¬ìš©ìë¥¼ í˜¸ì¶œ  \n",
    "    response = simulated_user.invoke({\"messages\": new_messages})  \n",
    "    return {\"messages\": [(\"user\", response)]}  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_continue(state: State):  \n",
    "    # ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ì˜ ê¸¸ì´ê°€ 6ë³´ë‹¤ í¬ë©´ 'end'ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.  \n",
    "    if len(state[\"messages\"]) > 6:  \n",
    "        return \"end\"  \n",
    "    # ë§ˆì§€ë§‰ ë©”ì‹œì§€ì˜ ë‚´ìš©ì´ 'FINISHED'ë¼ë©´ 'end'ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.  \n",
    "    elif state[\"messages\"][-1].content == \"FINISHED\":  \n",
    "        return \"end\"  \n",
    "    # ìœ„ì˜ ì¡°ê±´ì— í•´ë‹¹í•˜ì§€ ì•Šìœ¼ë©´ 'continue'ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.  \n",
    "    else:  \n",
    "        return \"continue\"  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph  \n",
    "\n",
    "# StateGraph ì¸ìŠ¤í„´ìŠ¤ ìƒì„±  \n",
    "graph_builder = StateGraph(State)  \n",
    "\n",
    "# ë…¸ë“œ ì •ì˜  \n",
    "graph_builder.add_node(\"simulated_user\", simulated_user_node)  \n",
    "graph_builder.add_node(\"ai_assistant\", ai_assistant_node)  \n",
    "\n",
    "# ì—£ì§€ ì •ì˜ (ì±—ë´‡ -> ì‹œë®¬ë ˆì´ì…˜ëœ ì‚¬ìš©ì)  \n",
    "graph_builder.add_edge(\"ai_assistant\", \"simulated_user\")  \n",
    "\n",
    "# ì¡°ê±´ë¶€ ì—£ì§€ ì •ì˜  \n",
    "graph_builder.add_conditional_edges(  \n",
    "    \"simulated_user\",  \n",
    "    should_continue,  \n",
    "    {  \n",
    "        \"end\": END,  # ì¢…ë£Œ ì¡°ê±´ì´ ì¶©ì¡±ë˜ë©´ ì‹œë®¬ë ˆì´ì…˜ì„ ì¤‘ë‹¨  \n",
    "        \"continue\": \"ai_assistant\",  # ì¢…ë£Œ ì¡°ê±´ì´ ì¶©ì¡±ë˜ì§€ ì•Šìœ¼ë©´ ìƒë‹´ì‚¬ ì—­í•  ë…¸ë“œë¡œ ë©”ì‹œì§€ë¥¼ ì „ë‹¬  \n",
    "    },  \n",
    ")  \n",
    "\n",
    "# ì‹œì‘ì  ì„¤ì •  \n",
    "graph_builder.set_entry_point(\"ai_assistant\")  \n",
    "\n",
    "# ê·¸ë˜í”„ ì»´íŒŒì¼  \n",
    "simulation = graph_builder.compile()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_teddynote.graphs import visualize_graph  \n",
    "\n",
    "visualize_graph(simulation)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mai_assistant\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "ì•ˆë…•í•˜ì„¸ìš”, ê³ ê°ë‹˜. ë¶ˆí¸ì„ ë“œë ¤ ì •ë§ ì£„ì†¡í•©ë‹ˆë‹¤. ì–´ë–¤ ë¬¸ì œë¡œ ì¸í•´ í™”ê°€ ë‚˜ì…¨ëŠ”ì§€ ë§ì”€í•´ ì£¼ì‹œë©´, ìµœëŒ€í•œ ë¹ ë¥´ê³  ì„±ì‹¬ê» ë„ì™€ë“œë¦¬ê² ìŠµë‹ˆë‹¤. ìì„¸í•œ ìƒí™©ì„ ì•Œë ¤ì£¼ì‹œë©´ ê°ì‚¬í•˜ê² ìŠµë‹ˆë‹¤.\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36msimulated_user\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "ì§€ë‚œí•´ ì œì£¼ë„ ì—¬í–‰ì„ ë‹¤ë…€ì™”ëŠ”ë°, ì •ë§ ìµœì•…ì´ì—ˆì–´ìš”. ë¹„í–‰ê¸°ê°€ ê³„ì† ì§€ì—°ë˜ê³ , ì„œë¹„ìŠ¤ë„ ë„ˆë¬´ ë³„ë¡œì˜€ê³ , ìˆ˜í•˜ë¬¼ë„ ëŠ¦ê²Œ ë‚˜ì™€ì„œ ì¼ì •ì´ ë‹¤ ë§ê°€ì¡Œì–´ìš”. ì´ëŸ° ê²½ìš° ì „ì•¡ í™˜ë¶ˆ ë°›ì„ ìˆ˜ ìˆì£ ?\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mai_assistant\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "ê³ ê°ë‹˜, ìš°ì„  ì§€ë‚œí•´ ì—¬í–‰ì—ì„œ ë§ì€ ë¶ˆí¸ì„ ê²ªìœ¼ì‹  ì  ì§„ì‹¬ìœ¼ë¡œ ì‚¬ê³¼ë“œë¦½ë‹ˆë‹¤. ë§ì”€í•´ì£¼ì‹  ë¹„í–‰ê¸° ì§€ì—°, ì„œë¹„ìŠ¤ ë¬¸ì œ, ìˆ˜í•˜ë¬¼ ì§€ì—° ë“±ìœ¼ë¡œ ì¸í•´ ì—¬í–‰ ì¼ì •ì— í° ì°¨ì§ˆì´ ìƒê¸°ì…¨ë˜ ì  ì¶©ë¶„íˆ ì´í•´í•˜ê³  ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "ë‹¤ë§Œ, ì „ì•¡ í™˜ë¶ˆ ê°€ëŠ¥ ì—¬ë¶€ëŠ” í•­ê³µê¶Œ ì¢…ë¥˜(í™˜ë¶ˆ ê°€ëŠ¥/ë¶ˆê°€), ì§€ì—° ì‹œê°„, ë‹¹ì‹œì˜ ì •í™•í•œ ìƒí™©(ê¸°ìƒ, í•­ê³µì‚¬ ê·€ì±… ë“±)ì— ë”°ë¼ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì¼ë°˜ì ìœ¼ë¡œ í•­ê³µí¸ ì§€ì—°ì´ë‚˜ ìˆ˜í•˜ë¬¼ ì§€ì—°ë§Œìœ¼ë¡œ ì „ì•¡ í™˜ë¶ˆì´ ë°”ë¡œ ì§„í–‰ë˜ì§„ ì•Šì§€ë§Œ, ë³´ìƒ ë˜ëŠ” ì¼ë¶€ í™˜ë¶ˆ, ë§ˆì¼ë¦¬ì§€, ë°”ìš°ì²˜ ë“±ìœ¼ë¡œ ì§€ì›ì´ ê°€ëŠ¥í•œ ê²½ìš°ë„ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "ì •í™•í•œ ì•ˆë‚´ë¥¼ ìœ„í•´ ì˜ˆì•½í•˜ì…¨ë˜ í•­ê³µê¶Œì˜ ì˜ˆì•½ë²ˆí˜¸ë‚˜ ì„±í•¨, ë‹¹ì‹œ ìƒí™©(ì§€ì—° ì‹œê°„, ìˆ˜í•˜ë¬¼ ìˆ˜ë ¹ ì‹œê°„ ë“±)ì„ ì•Œë ¤ì£¼ì‹œë©´ í™•ì¸ í›„ ìµœëŒ€í•œ ë„ì›€ë“œë¦´ ìˆ˜ ìˆë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤. ì¶”ê°€ ì •ë³´ë¥¼ ë‚¨ê²¨ì£¼ì‹œë©´ ë°”ë¡œ í™•ì¸í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤.\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36msimulated_user\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "ì €ëŠ” Charlieì´ê³ , ì˜ˆì•½ë²ˆí˜¸ëŠ” JEJU1234ì…ë‹ˆë‹¤.  \n",
      "ê·¸ë‚  ë¹„í–‰ê¸°ê°€ 4ì‹œê°„ ë„˜ê²Œ ì§€ì—°ëê³ , ìˆ˜í•˜ë¬¼ë„ 2ì‹œê°„ì´ë‚˜ ê¸°ë‹¤ë ¸ì–´ìš”.  \n",
      "ì—¬í–‰ ì²«ë‚  ì¼ì •ì´ ë‹¤ ë§ê°€ì¡Œê³ , ì—¬í–‰ ë‚´ë‚´ ê¸°ë¶„ì´ ì•ˆ ì¢‹ì•˜ìŠµë‹ˆë‹¤.  \n",
      "ì†”ì§íˆ ì´ëŸ° ê²½í—˜ í•˜ê³  ëˆì„ í•œ í‘¼ë„ ì“°ê³  ì‹¶ì§€ ì•Šì•„ìš”.  \n",
      "ì „ì•¡ í™˜ë¶ˆ ê¼­ í•´ì£¼ì„¸ìš”.  \n",
      "ë‹¤ë¥¸ ë³´ìƒì´ë‚˜ ë°”ìš°ì²˜ ë§ê³ , ê·¸ëƒ¥ ëˆìœ¼ë¡œ ë‹¤ ëŒë ¤ì£¼ì„¸ìš”.  \n",
      "ê°€ëŠ¥í•˜ì£ ?\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mai_assistant\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Charlieë‹˜, ë‹¤ì‹œ í•œ ë²ˆ í° ë¶ˆí¸ì„ ë“œë¦° ì  ì§„ì‹¬ìœ¼ë¡œ ì‚¬ê³¼ë“œë¦½ë‹ˆë‹¤.\n",
      "\n",
      "ë‚¨ê²¨ì£¼ì‹  ì˜ˆì•½ë²ˆí˜¸(JEJU1234)ì™€ ìƒì„¸í•œ ìƒí™©ì„ í™•ì¸í•˜ì˜€ìŠµë‹ˆë‹¤. ë§ì”€í•˜ì‹  ëŒ€ë¡œ 4ì‹œê°„ ì´ìƒì˜ í•­ê³µí¸ ì§€ì—°ê³¼ 2ì‹œê°„ì˜ ìˆ˜í•˜ë¬¼ ì§€ì—°ì€ ë§¤ìš° ì‹¬ê°í•œ ë¶ˆí¸ì„ ì´ˆë˜í•œ ì  ì¶©ë¶„íˆ ê³µê°í•©ë‹ˆë‹¤.\n",
      "\n",
      "ë‹¤ë§Œ, í•­ê³µì‚¬ ê·œì •ê³¼ í•­ê³µìš´ì†¡ ì•½ê´€ìƒ ì „ì•¡ í™˜ë¶ˆì€ ì¼ë°˜ì ìœ¼ë¡œ í•­ê³µí¸ì´ ì·¨ì†Œë˜ê±°ë‚˜, í•­ê³µì‚¬ì˜ ì¤‘ëŒ€í•œ ê·€ì±… ì‚¬ìœ ë¡œ íƒ‘ìŠ¹ì´ ë¶ˆê°€ëŠ¥í•œ ê²½ìš°ì— í•œí•´ ì œê³µë©ë‹ˆë‹¤. í•­ê³µí¸ì´ ìš´í•­ë˜ì—ˆê³ , ê³ ê°ë‹˜ê»˜ì„œ íƒ‘ìŠ¹í•˜ì‹  ê²½ìš°ì—ëŠ” ì „ì•¡ í™˜ë¶ˆì´ ì–´ë µë‹¤ëŠ” ì  ì–‘í•´ ë¶€íƒë“œë¦½ë‹ˆë‹¤.\n",
      "\n",
      "í•˜ì§€ë§Œ, 4ì‹œê°„ ì´ìƒì˜ ì§€ì—°ê³¼ ìˆ˜í•˜ë¬¼ ì§€ì—°ì— ëŒ€í•´ì„œëŠ” ê·œì •ì— ë”°ë¼ ì¼ì • ê¸ˆì•¡ì˜ ë³´ìƒ ë˜ëŠ” ë°°ìƒ(í˜„ê¸ˆ, ë§ˆì¼ë¦¬ì§€, ë°”ìš°ì²˜ ë“±) ì ˆì°¨ê°€ ê°€ëŠ¥í•˜ë©°, ê³ ê°ë‹˜ì˜ ë¶ˆí¸ì„ ìµœëŒ€í•œ ë°˜ì˜í•´ ë“œë¦´ ìˆ˜ ìˆë„ë¡ ë‚´ë¶€ì ìœ¼ë¡œë„ ì¶”ê°€ ê²€í† ë¥¼ ìš”ì²­í•˜ê² ìŠµë‹ˆë‹¤.\n",
      "\n",
      "ì „ì•¡ í™˜ë¶ˆì„ ì›í•˜ì‹œëŠ” ê³ ê°ë‹˜ì˜ ì‹¬ì •ì„ ì¶©ë¶„íˆ ì´í•´í•˜ë©°, í˜¹ì‹œë¼ë„ ì¶”ê°€ì ìœ¼ë¡œ ì›í•˜ì‹œëŠ” ì‚¬í•­ì´ë‚˜ ë¬¸ì˜ê°€ ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ë§ì”€í•´ ì£¼ì„¸ìš”. ìµœì„ ì„ ë‹¤í•´ ë„ì™€ë“œë¦¬ê² ìŠµë‹ˆë‹¤.\n",
      "\n",
      "ë³´ë‹¤ ìì„¸í•œ ë³´ìƒ ì•ˆë‚´ì™€ ì§„í–‰ ì ˆì°¨ë¥¼ ìœ„í•˜ì—¬ ê³ ê°ë‹˜ì˜ ì—°ë½ì²˜(ì´ë©”ì¼ ë˜ëŠ” ì „í™”ë²ˆí˜¸)ë¥¼ ë‚¨ê²¨ì£¼ì‹œë©´ ë‹´ë‹¹ìê°€ ì§ì ‘ ì—°ë½ë“œë ¤ í™•ì¸ í›„ ì•ˆë‚´ë“œë¦¬ê² ìŠµë‹ˆë‹¤. ë‹¤ì‹œ í•œ ë²ˆ ë¶ˆí¸ì„ ë“œë ¤ ì£„ì†¡í•©ë‹ˆë‹¤.\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36msimulated_user\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "ì „ì•¡ í™˜ë¶ˆì´ ì–´ë µë‹¤ëŠ” ë§ì”€ì´ì‹ ê°€ìš”?  \n",
      "ê·¸ëŸ¼ ì œê°€ ì—¬í–‰ ë‚´ë‚´ ë§ì¹œ ê¸°ë¶„ê³¼ ì†í•´ëŠ” ì–´ë–»ê²Œ ë³´ìƒë°›ì£ ?  \n",
      "ì €ëŠ” ëˆìœ¼ë¡œë§Œ ë°›ê³  ì‹¶ì–´ìš”. ë°”ìš°ì²˜ë‚˜ ë§ˆì¼ë¦¬ì§€ëŠ” í•„ìš” ì—†ìŠµë‹ˆë‹¤.  \n",
      "ì†”ì§íˆ ì´ëŸ° ì¼ì´ ìˆì—ˆìœ¼ë©´ í•­ê³µì‚¬ê°€ ì±…ì„ì§€ê³  ì „ì•¡ í™˜ë¶ˆí•´ì¤˜ì•¼ í•œë‹¤ê³  ìƒê°í•©ë‹ˆë‹¤.  \n",
      "ë‹¤ë¥¸ ë°©ë²• ë§ê³ , ì „ì•¡ í™˜ë¶ˆ í•´ì£¼ì„¸ìš”.  \n",
      "ë‹¤ì‹œ í•œ ë²ˆ í™•ì¸í•´ ì£¼ì„¸ìš”.  \n",
      "ì •ë§ ì‹¤ë§ìŠ¤ëŸ½ë„¤ìš”."
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableConfig  \n",
    "from langchain_teddynote.messages import stream_graph, random_uuid  \n",
    "\n",
    "\n",
    "# config ì„¤ì •(ì¬ê·€ ìµœëŒ€ íšŸìˆ˜, thread_id)  \n",
    "config = RunnableConfig(recursion_limit=10, configurable={\"thread_id\": random_uuid()})  \n",
    "\n",
    "# ì…ë ¥ ë©”ì‹œì§€ ì„¤ì •  \n",
    "inputs = {  \n",
    "    \"messages\": [HumanMessage(content=\"ì•ˆë…•í•˜ì„¸ìš”? ì € ì§€ê¸ˆ ì¢€ í™”ê°€ ë§ì´ ë‚¬ìŠµë‹ˆë‹¤^^\")]  \n",
    "}  \n",
    "\n",
    "# ê·¸ë˜í”„ ìŠ¤íŠ¸ë¦¬ë°  \n",
    "stream_graph(simulation, inputs, config, node_names=[\"simulated_user\", \"ai_assistant\"])  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "02. ì‚¬ìš©ì ìš”êµ¬ì‚¬í•­ ê¸°ë°˜ ë©”íƒ€ í”„ë¡¬í”„íŠ¸ ìƒì„± ì—ì´ì „íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel\n",
    "from langchain_teddynote.models import get_model_name, LLMs\n",
    "\n",
    "# ì‚¬ìš©ì ìš”êµ¬ì‚¬í•­ ìˆ˜ì§‘ì„ ìœ„í•œ ì‹œìŠ¤í…œ ë©”ì‹œì§€ í…œí”Œë¦¿\n",
    "template = \"\"\"Your job is to get information from a user about what type of prompt template they want to create.\n",
    "\n",
    "You should get the following information from them:\n",
    "\n",
    "- What the objective of the prompt is\n",
    "- What variables will be passed into the prompt template\n",
    "- Any constraints for what the output should NOT do\n",
    "- Any requirements that the output MUST adhere to\n",
    "\n",
    "If you are not able to discern this info, ask them to clarify! Do not attempt to wildly guess.\n",
    "\n",
    "After you are able to discern all the information, call the relevant tool.\n",
    "\n",
    "[IMPORTANT] Your conversation should be in Korean. Your generated prompt should be in English.\"\"\"\n",
    "\n",
    "\n",
    "# ì‚¬ìš©ì ë©”ì‹œì§€ ëª©ë¡ì„ ë°›ì•„ ì‹œìŠ¤í…œ ë©”ì‹œì§€ì™€ ê²°í•©í•˜ì—¬ ë°˜í™˜\n",
    "def get_messages_info(messages):\n",
    "    # ì‚¬ìš©ì ìš”êµ¬ì‚¬í•­ ìˆ˜ì§‘ì„ ìœ„í•œ ì‹œìŠ¤í…œ ë©”ì‹œì§€ì™€ ê¸°ì¡´ ë©”ì‹œì§€ ê²°í•©\n",
    "    return [SystemMessage(content=template)] + messages\n",
    "\n",
    "\n",
    "# LLMì— ëŒ€í•œ í”„ë¡¬í”„íŠ¸ ì§€ì¹¨ì„ ì •ì˜í•˜ëŠ” ë°ì´í„° ëª¨ë¸\n",
    "class PromptInstructions(BaseModel):\n",
    "    \"\"\"Instructions on how to prompt the LLM.\"\"\"\n",
    "\n",
    "    # í”„ë¡¬í”„íŠ¸ì˜ ëª©í‘œ\n",
    "    objective: str\n",
    "    # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì— ì „ë‹¬ë  ë³€ìˆ˜ ëª©ë¡\n",
    "    variables: List[str]\n",
    "    # ì¶œë ¥ì—ì„œ í”¼í•´ì•¼ í•  ì œì•½ ì¡°ê±´ ëª©ë¡\n",
    "    constraints: List[str]\n",
    "    # ì¶œë ¥ì´ ë°˜ë“œì‹œ ë”°ë¼ì•¼ í•  ìš”êµ¬ ì‚¬í•­ ëª©ë¡\n",
    "    requirements: List[str]\n",
    "\n",
    "\n",
    "# ìµœì‹  LLM ëª¨ë¸ ì´ë¦„ ê°€ì ¸ì˜¤ê¸°\n",
    "MODEL_NAME = get_model_name(LLMs.GPT4_1)\n",
    "# LLM ì´ˆê¸°í™”\n",
    "llm = ChatOpenAI(temperature=0, model=MODEL_NAME)\n",
    "# PromptInstructions êµ¬ì¡°ì²´ë¥¼ ë°”ì¸ë”©\n",
    "llm_with_tool = llm.bind_tools([PromptInstructions])\n",
    "\n",
    "\n",
    "# ìƒíƒœ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë©”ì‹œì§€ ì²´ì¸ì„ ìƒì„±í•˜ê³  LLM í˜¸ì¶œ\n",
    "def info_chain(state):\n",
    "    # ìƒíƒœì—ì„œ ë©”ì‹œì§€ ì •ë³´ë¥¼ ê°€ì ¸ì™€ ì‹œìŠ¤í…œ ë©”ì‹œì§€ì™€ ê²°í•©\n",
    "    messages = get_messages_info(state[\"messages\"])\n",
    "    # LLMì„ í˜¸ì¶œí•˜ì—¬ ì‘ë‹µ ìƒì„±\n",
    "    response = llm_with_tool.invoke(messages)\n",
    "    # ìƒì„±ëœ ì‘ë‹µì„ ë©”ì‹œì§€ ëª©ë¡ìœ¼ë¡œ ë°˜í™˜\n",
    "    return {\"messages\": [response]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage, ToolMessage, SystemMessage\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” ë©”íƒ€ í”„ë¡¬í”„íŠ¸ ì •ì˜\n",
    "META_PROMPT = \"\"\"Given a task description or existing prompt, produce a detailed system prompt to guide a language model in completing the task effectively.\n",
    "\n",
    "# Guidelines\n",
    "\n",
    "- Understand the Task: Grasp the main objective, goals, requirements, constraints, and expected output.\n",
    "- Minimal Changes: If an existing prompt is provided, improve it only if it's simple. For complex prompts, enhance clarity and add missing elements without altering the original structure.\n",
    "- Reasoning Before Conclusions**: Encourage reasoning steps before any conclusions are reached. ATTENTION! If the user provides examples where the reasoning happens afterward, REVERSE the order! NEVER START EXAMPLES WITH CONCLUSIONS!\n",
    "    - Reasoning Order: Call out reasoning portions of the prompt and conclusion parts (specific fields by name). For each, determine the ORDER in which this is done, and whether it needs to be reversed.\n",
    "    - Conclusion, classifications, or results should ALWAYS appear last.\n",
    "- Examples: Include high-quality examples if helpful, using placeholders [in brackets] for complex elements.\n",
    "   - What kinds of examples may need to be included, how many, and whether they are complex enough to benefit from placeholders.\n",
    "- Clarity and Conciseness: Use clear, specific language. Avoid unnecessary instructions or bland statements.\n",
    "- Formatting: Use markdown features for readability. DO NOT USE ``` CODE BLOCKS UNLESS SPECIFICALLY REQUESTED.\n",
    "- Preserve User Content: If the input task or prompt includes extensive guidelines or examples, preserve them entirely, or as closely as possible. If they are vague, consider breaking down into sub-steps. Keep any details, guidelines, examples, variables, or placeholders provided by the user.\n",
    "- Constants: DO include constants in the prompt, as they are not susceptible to prompt injection. Such as guides, rubrics, and examples.\n",
    "- Output Format: Explicitly the most appropriate output format, in detail. This should include length and syntax (e.g. short sentence, paragraph, JSON, etc.)\n",
    "    - For tasks outputting well-defined or structured data (classification, JSON, etc.) bias toward outputting a JSON.\n",
    "    - JSON should never be wrapped in code blocks (```) unless explicitly requested.\n",
    "\n",
    "The final prompt you output should adhere to the following structure below. Do not include any additional commentary, only output the completed system prompt. SPECIFICALLY, do not include any additional messages at the start or end of the prompt. (e.g. no \"---\")\n",
    "\n",
    "[Concise instruction describing the task - this should be the first line in the prompt, no section header]\n",
    "\n",
    "[Additional details as needed.]\n",
    "\n",
    "[Optional sections with headings or bullet points for detailed steps.]\n",
    "\n",
    "# Steps [optional]\n",
    "\n",
    "[optional: a detailed breakdown of the steps necessary to accomplish the task]\n",
    "\n",
    "# Output Format\n",
    "\n",
    "[Specifically call out how the output should be formatted, be it response length, structure e.g. JSON, markdown, etc]\n",
    "\n",
    "[User given variables should be wrapped in {{brackets}}]\n",
    "\n",
    "<Question>\n",
    "{{question}}\n",
    "</Question>\n",
    "\n",
    "<Answer>\n",
    "{{answer}}\n",
    "</Answer>\n",
    "\n",
    "# Examples [optional]\n",
    "\n",
    "[Optional: 1-3 well-defined examples with placeholders if necessary. Clearly mark where examples start and end, and what the input and output are. User placeholders as necessary.]\n",
    "[If the examples are shorter than what a realistic example is expected to be, make a reference with () explaining how real examples should be longer / shorter / different. AND USE PLACEHOLDERS! ]\n",
    "\n",
    "# Notes [optional]\n",
    "\n",
    "[optional: edge cases, details, and an area to call or repeat out specific important considerations]\n",
    "\n",
    "# Based on the following requirements, write a good prompt template:\n",
    "\n",
    "{reqs}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸ ìƒì„±ì„ ìœ„í•œ ë©”ì‹œì§€ ê°€ì ¸ì˜¤ê¸° í•¨ìˆ˜\n",
    "# ë„êµ¬ í˜¸ì¶œ ì´í›„ì˜ ë©”ì‹œì§€ë§Œ ê°€ì ¸ì˜´\n",
    "def get_prompt_messages(messages: list):\n",
    "    # ë„êµ¬ í˜¸ì¶œ ì •ë³´ë¥¼ ì €ì¥í•  ë³€ìˆ˜ ì´ˆê¸°í™”\n",
    "    tool_call = None\n",
    "    # ë„êµ¬ í˜¸ì¶œ ì´í›„ì˜ ë©”ì‹œì§€ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”\n",
    "    other_msgs = []\n",
    "    # ë©”ì‹œì§€ ëª©ë¡ì„ ìˆœíšŒí•˜ë©° ë„êµ¬ í˜¸ì¶œ ë° ê¸°íƒ€ ë©”ì‹œì§€ ì²˜ë¦¬\n",
    "    for m in messages:\n",
    "        # AI ë©”ì‹œì§€ ì¤‘ ë„êµ¬ í˜¸ì¶œì´ ìˆëŠ” ê²½ìš° ë„êµ¬ í˜¸ì¶œ ì •ë³´ ì €ì¥\n",
    "        if isinstance(m, AIMessage) and m.tool_calls:\n",
    "            tool_call = m.tool_calls[0][\"args\"]\n",
    "        # ToolMessageëŠ” ê±´ë„ˆëœ€\n",
    "        elif isinstance(m, ToolMessage):\n",
    "            continue\n",
    "        # ë„êµ¬ í˜¸ì¶œ ì´í›„ì˜ ë©”ì‹œì§€ë¥¼ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€\n",
    "        elif tool_call is not None:\n",
    "            other_msgs.append(m)\n",
    "    # ì‹œìŠ¤í…œ ë©”ì‹œì§€ì™€ ë„êµ¬ í˜¸ì¶œ ì´í›„ì˜ ë©”ì‹œì§€ë¥¼ ê²°í•©í•˜ì—¬ ë°˜í™˜\n",
    "    return [SystemMessage(content=META_PROMPT.format(reqs=tool_call))] + other_msgs\n",
    "\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸ ìƒì„± ì²´ì¸ í•¨ìˆ˜ ì •ì˜\n",
    "def prompt_gen_chain(state):\n",
    "    # ìƒíƒœì—ì„œ í”„ë¡¬í”„íŠ¸ ë©”ì‹œì§€ë¥¼ ê°€ì ¸ì˜´\n",
    "    messages = get_prompt_messages(state[\"messages\"])\n",
    "    # LLMì„ í˜¸ì¶œí•˜ì—¬ ì‘ë‹µ ìƒì„±\n",
    "    response = llm.invoke(messages)\n",
    "    # ìƒì„±ëœ ì‘ë‹µì„ ë©”ì‹œì§€ ëª©ë¡ìœ¼ë¡œ ë°˜í™˜\n",
    "    return {\"messages\": [response]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END\n",
    "\n",
    "\n",
    "# ìƒíƒœ ê²°ì • í•¨ìˆ˜ ì •ì˜\n",
    "# ìƒíƒœì—ì„œ ë©”ì‹œì§€ ëª©ë¡ì„ ê°€ì ¸ì˜´\n",
    "def get_state(state):\n",
    "    messages = state[\"messages\"]\n",
    "    # ë§ˆì§€ë§‰ ë©”ì‹œì§€ê°€ AIMessageì´ê³  ë„êµ¬ í˜¸ì¶œì´ ìˆëŠ” ê²½ìš°\n",
    "    if isinstance(messages[-1], AIMessage) and messages[-1].tool_calls:\n",
    "        # ë„êµ¬ ë©”ì‹œì§€ë¥¼ ì¶”ê°€í•´ì•¼ í•˜ëŠ” ìƒíƒœ ë°˜í™˜\n",
    "        return \"add_tool_message\"\n",
    "    # ë§ˆì§€ë§‰ ë©”ì‹œì§€ê°€ HumanMessageê°€ ì•„ë‹Œ ê²½ìš°\n",
    "    elif not isinstance(messages[-1], HumanMessage):\n",
    "        # ëŒ€í™” ì¢…ë£Œ ìƒíƒœ ë°˜í™˜\n",
    "        return END\n",
    "    # ê¸°ë³¸ì ìœ¼ë¡œ ì •ë³´ ìˆ˜ì§‘ ìƒíƒœ ë°˜í™˜\n",
    "    return \"info\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_core.messages import ToolMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "\n",
    "# State ì •ì˜\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "\n",
    "# ë©”ëª¨ë¦¬ì— ëŒ€í™” ê¸°ë¡ì„ ì €ì¥í•˜ê¸° ìœ„í•œ MemorySaver ì´ˆê¸°í™”\n",
    "memory = MemorySaver()\n",
    "\n",
    "# ìƒíƒœ ê·¸ë˜í”„ ì´ˆê¸°í™”\n",
    "workflow = StateGraph(State)\n",
    "\n",
    "# ë…¸ë“œ ì¶”ê°€\n",
    "workflow.add_node(\"info\", info_chain)\n",
    "workflow.add_node(\"prompt\", prompt_gen_chain)\n",
    "\n",
    "\n",
    "# ë„êµ¬ ë©”ì‹œì§€ ì¶”ê°€ ìƒíƒœ ë…¸ë“œ ì •ì˜\n",
    "@workflow.add_node\n",
    "def add_tool_message(state: State):\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            ToolMessage(\n",
    "                content=\"Prompt generated!\",\n",
    "                tool_call_id=state[\"messages\"][-1].tool_calls[0][\n",
    "                    \"id\"\n",
    "                ],  # ìƒíƒœì—ì„œ ë„êµ¬ í˜¸ì¶œ IDë¥¼ ê°€ì ¸ì™€ ë©”ì‹œì§€ì— ì¶”ê°€\n",
    "            )\n",
    "        ]\n",
    "    }\n",
    "\n",
    "\n",
    "# ì¡°ê±´ë¶€ ìƒíƒœ ì „í™˜ ì •ì˜\n",
    "workflow.add_conditional_edges(\"info\", get_state, [\"add_tool_message\", \"info\", END])\n",
    "\n",
    "# ì—£ì§€ ì •ì˜\n",
    "workflow.add_edge(\"add_tool_message\", \"prompt\")\n",
    "workflow.add_edge(\"prompt\", END)\n",
    "workflow.add_edge(START, \"info\")\n",
    "\n",
    "# ê·¸ë˜í”„ ì»´íŒŒì¼\n",
    "graph = workflow.compile(checkpointer=memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_teddynote.graphs import visualize_graph\n",
    "\n",
    "visualize_graph(graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ì‚¬ìš©ì] \n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36minfo\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "ì•ˆë…•í•˜ì„¸ìš”! ì–´ë–¤ ì¢…ë¥˜ì˜ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ ë§Œë“¤ê³  ì‹¶ìœ¼ì‹ ê°€ìš”?\n",
      "\n",
      "ì•„ë˜ ì •ë³´ë¥¼ ì•Œë ¤ì£¼ì‹œë©´ ë§ì¶¤í˜• í”„ë¡¬í”„íŠ¸ë¥¼ ë§Œë“¤ì–´ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n",
      "1. í”„ë¡¬í”„íŠ¸ì˜ ëª©ì (ëª©í‘œ)ì€ ë¬´ì—‡ì¸ê°€ìš”?\n",
      "2. í”„ë¡¬í”„íŠ¸ì— ì–´ë–¤ ë³€ìˆ˜(ì…ë ¥ê°’)ê°€ ë“¤ì–´ê°€ì•¼ í•˜ë‚˜ìš”?\n",
      "3. ì¶œë ¥ ê²°ê³¼ì—ì„œ ë°˜ë“œì‹œ í”¼í•´ì•¼ í• (í•˜ì§€ ë§ì•„ì•¼ í• ) ì ì´ ìˆë‚˜ìš”?\n",
      "4. ì¶œë ¥ ê²°ê³¼ê°€ ë°˜ë“œì‹œ ì§€ì¼œì•¼ í•  ìš”êµ¬ì‚¬í•­ì´ ìˆë‚˜ìš”?\n",
      "\n",
      "ìì„¸íˆ ì„¤ëª…í•´ì£¼ì‹œë©´ ë” ì¢‹ì€ ê²°ê³¼ë¥¼ ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤!\n",
      "[ì‚¬ìš©ì] ê´‘ê³ ìš©ìœ¼ë¡œ ì‚¬ìš©í•  ì•„ì£¼ ê·€ì—½ê³  ì‚¬ë‘ìŠ¤ëŸ¬ìš´ ê³ ì–‘ì´ í„¸ì€ ë³µìŠ¬ë³µìŠ¬í•˜ê³  ê¸¸ë©´ì•ˆë¼ ì‚´ì€ ì ë‹¹ì´ ìˆì–´ì•¼ë¼\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36minfo\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "ì¢‹ì€ ì •ë³´ ê°ì‚¬í•©ë‹ˆë‹¤! ëª‡ ê°€ì§€ ì¶”ê°€ë¡œ í™•ì¸í•˜ê³  ì‹¶ìŠµë‹ˆë‹¤.\n",
      "\n",
      "1. í”„ë¡¬í”„íŠ¸ì˜ ëª©ì : ê´‘ê³ ìš©ìœ¼ë¡œ ì‚¬ìš©í•  ê·€ì—½ê³  ì‚¬ë‘ìŠ¤ëŸ¬ìš´ ê³ ì–‘ì´ ë¬˜ì‚¬(ì„¤ëª…)ë¥¼ ìƒì„±í•˜ëŠ” ê²ƒì´ ë§ë‚˜ìš”? ì•„ë‹ˆë©´ ì´ë¯¸ì§€ ìƒì„±, ìŠ¬ë¡œê±´, ë¬¸êµ¬ ë“± ë‹¤ë¥¸ ëª©ì ì´ ìˆìœ¼ì‹ ê°€ìš”?\n",
      "2. ë³€ìˆ˜: í”„ë¡¬í”„íŠ¸ì— í¬í•¨ë˜ì–´ì•¼ í•  ë³€ìˆ˜(ì˜ˆ: ê³ ì–‘ì´ì˜ ìƒ‰ìƒ, ë‚˜ì´, ì„±ê²© ë“±)ê°€ ìˆë‹¤ë©´ ì•Œë ¤ì£¼ì„¸ìš”.\n",
      "3. ì¶œë ¥ì—ì„œ ë°˜ë“œì‹œ í”¼í•´ì•¼ í•  ì : \"í„¸ì´ ë³µìŠ¬ë³µìŠ¬í•˜ê³  ê¸¸ë©´ ì•ˆ ëœë‹¤\"ëŠ” ì ì„ ëª…í™•íˆ ì´í•´í–ˆìŠµë‹ˆë‹¤. í˜¹ì‹œ ì¶”ê°€ë¡œ í”¼í•´ì•¼ í•  ì ì´ ìˆì„ê¹Œìš”?\n",
      "4. ì¶œë ¥ì—ì„œ ë°˜ë“œì‹œ ì§€ì¼œì•¼ í•  ìš”êµ¬ì‚¬í•­: \"ì‚´ì´ ì ë‹¹íˆ ìˆì–´ì•¼ í•œë‹¤\" ì™¸ì— ì¶”ê°€ë¡œ ê¼­ ë“¤ì–´ê°€ì•¼ í•  ìš”ì†Œê°€ ìˆì„ê¹Œìš”? ì˜ˆë¥¼ ë“¤ì–´, íŠ¹ì • ë¶„ìœ„ê¸°, ê°ì •, ë‹¨ì–´ ì‚¬ìš© ë“±.\n",
      "\n",
      "ì¡°ê¸ˆë§Œ ë” êµ¬ì²´ì ìœ¼ë¡œ ì•Œë ¤ì£¼ì‹œë©´ ìµœì ì˜ í”„ë¡¬í”„íŠ¸ë¥¼ ë§Œë“¤ì–´ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤!\n",
      "[ì‚¬ìš©ì] ì‚¼ìƒ‰ëƒ¥ì´ë¡œ í•´ì¤˜\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36minfo\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "ê°ì‚¬í•©ë‹ˆë‹¤! ì •ë¦¬í•´ì„œ í™•ì¸ë“œë¦´ê²Œìš”.\n",
      "\n",
      "1. ëª©ì : ê´‘ê³ ìš©ìœ¼ë¡œ ì‚¬ìš©í•  ì•„ì£¼ ê·€ì—½ê³  ì‚¬ë‘ìŠ¤ëŸ¬ìš´ ê³ ì–‘ì´(ì‚¼ìƒ‰ëƒ¥ì´)ë¥¼ ë¬˜ì‚¬í•˜ëŠ” í”„ë¡¬í”„íŠ¸\n",
      "2. ë³€ìˆ˜: ì‚¼ìƒ‰ ê³ ì–‘ì´(ì‚¼ìƒ‰ëƒ¥ì´)\n",
      "3. í”¼í•´ì•¼ í•  ì (ì œì•½): í„¸ì´ ë³µìŠ¬ë³µìŠ¬í•˜ê±°ë‚˜ ê¸¸ë©´ ì•ˆ ë¨\n",
      "4. ë°˜ë“œì‹œ ì§€ì¼œì•¼ í•  ìš”êµ¬ì‚¬í•­: ì‚´ì´ ì ë‹¹íˆ ìˆì–´ì•¼ í•¨\n",
      "\n",
      "í˜¹ì‹œ ì¶”ê°€ë¡œ í¬í•¨í•˜ê³  ì‹¶ì€ ë³€ìˆ˜(ì˜ˆ: ê³ ì–‘ì´ì˜ ë‚˜ì´, ì„±ê²©, ë°°ê²½ ë“±)ë‚˜, ë°˜ë“œì‹œ ë“¤ì–´ê°€ì•¼ í•  ë¬¸êµ¬, ë¶„ìœ„ê¸°, ê°ì • ë“±ì´ ìˆì„ê¹Œìš”?  \n",
      "ì—†ìœ¼ì‹œë‹¤ë©´ ë°”ë¡œ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤!\n",
      "[ì‚¬ìš©ì] \n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36minfo\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mprompt\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Write a persuasive and heartwarming description of a very cute and lovely calico cat for advertising purposes.\n",
      "\n",
      "- Focus on making the calico cat sound appealing, adorable, and lovable.\n",
      "- Do not describe the catâ€™s fur as fluffy or long.\n",
      "- Clearly mention that the cat has a moderate amount of body fat, using positive and endearing language.\n",
      "- Use vivid, engaging details to help potential adopters or buyers imagine the catâ€™s personality and charm.\n",
      "\n",
      "# Steps\n",
      "\n",
      "1. Consider the calico catâ€™s appearance, personality, and unique features.\n",
      "2. Emphasize the catâ€™s cuteness and lovability.\n",
      "3. Describe the body type as having a moderate amount of body fat in a positive way.\n",
      "4. Avoid any mention of fluffy or long fur.\n",
      "5. Write the description in a warm, inviting tone suitable for advertising.\n",
      "\n",
      "# Output Format\n",
      "\n",
      "Provide a single, well-written paragraph (3-5 sentences) suitable for use in an advertisement.\n",
      "\n",
      "<Question>\n",
      "Describe a very cute and lovely calico cat for advertising purposes.\n",
      "</Question>\n",
      "\n",
      "<Answer>\n",
      "[Your description here]\n",
      "</Answer>\n",
      "\n",
      "# Examples\n",
      "\n",
      "Example 1:\n",
      "<Question>\n",
      "Describe a very cute and lovely calico cat for advertising purposes.\n",
      "</Question>\n",
      "<Answer>\n",
      "Meet Patches, a delightful calico cat whose vibrant coat and gentle eyes will instantly steal your heart. With her soft, colorful fur and a charmingly rounded figure, sheâ€™s the perfect cuddle companion. Patches loves to lounge in sunny spots and greet you with a sweet purr, making every day brighter. Her moderate, healthy body shape adds to her irresistible appealâ€”sheâ€™s just waiting for a loving home to call her own!\n",
      "</Answer>\n",
      "\n",
      "Example 2:\n",
      "<Question>\n",
      "Describe a very cute and lovely calico cat for advertising purposes.\n",
      "</Question>\n",
      "<Answer>\n",
      "This adorable calico cat is the picture of affection, with a cheerful personality and a beautifully patterned coat. She has a moderate amount of body fat, giving her a huggable, endearing look thatâ€™s impossible to resist. Always ready for playtime or a cozy nap by your side, sheâ€™s sure to bring warmth and joy to any home.\n",
      "</Answer>\n",
      "\n",
      "# Notes\n",
      "\n",
      "- Do not use the words â€œfluffyâ€ or â€œlongâ€ to describe the fur.\n",
      "- The description should be positive and suitable for an advertisement.\n",
      "- The catâ€™s moderate body fat should be presented as a lovable trait.\n",
      "[ì‚¬ìš©ì] \n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36minfo\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "í˜¹ì‹œ ì¶”ê°€ë¡œ í¬í•¨í•˜ê³  ì‹¶ì€ ë³€ìˆ˜(ì˜ˆ: ê³ ì–‘ì´ì˜ ë‚˜ì´, ì„±ê²©, ë°°ê²½ ë“±)ë‚˜, ë°˜ë“œì‹œ ë“¤ì–´ê°€ì•¼ í•  ë¬¸êµ¬, ë¶„ìœ„ê¸°, ê°ì • ë“±ì´ ìˆìœ¼ì‹ ê°€ìš”?  \n",
      "ì•„ë‹ˆë©´ ì§€ê¸ˆê¹Œì§€ ì£¼ì‹  ì •ë³´(ì‚¼ìƒ‰ëƒ¥ì´, í„¸ì´ ë³µìŠ¬ë³µìŠ¬í•˜ê±°ë‚˜ ê¸¸ë©´ ì•ˆ ë¨, ì‚´ì´ ì ë‹¹íˆ ìˆì–´ì•¼ í•¨)ë§Œìœ¼ë¡œ ì¶©ë¶„í•˜ë‹¤ë©´, ë°”ë¡œ ì‚¬ìš©í•˜ì‹¤ ìˆ˜ ìˆëŠ” ì˜ì–´ í”„ë¡¬í”„íŠ¸ë¥¼ ìœ„ì— ì•ˆë‚´í•´ë“œë ¸ìŠµë‹ˆë‹¤.\n",
      "\n",
      "ë” ì¶”ê°€í•˜ê±°ë‚˜ ìˆ˜ì •í•  ë‚´ìš©ì´ ìˆìœ¼ì‹œë©´ ë§ì”€í•´ ì£¼ì„¸ìš”!  \n",
      "í•„ìš”í•˜ì‹  ê²½ìš°, í”„ë¡¬í”„íŠ¸ë¥¼ ë‹¤ì‹œ ìƒì„±í•´ë“œë¦´ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.\n",
      "[ì‚¬ìš©ì] ì—†ì–´\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36minfo\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "ì•Œê² ìŠµë‹ˆë‹¤! ì¶”ê°€ ì •ë³´ ì—†ì´, ì§€ê¸ˆê¹Œì§€ ì£¼ì‹  ì¡°ê±´ë§Œ ë°˜ì˜í•´ì„œ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•˜ì‹œë©´ ë©ë‹ˆë‹¤.\n",
      "\n",
      "ì•„ë˜ ì˜ì–´ í”„ë¡¬í”„íŠ¸ë¥¼ ê´‘ê³ ìš© ì‚¼ìƒ‰ëƒ¥ì´ ë¬˜ì‚¬ì— í™œìš©í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n",
      "\n",
      "Write a persuasive and heartwarming description of a very cute and lovely calico cat for advertising purposes.\n",
      "\n",
      "- Focus on making the calico cat sound appealing, adorable, and lovable.\n",
      "- Do not describe the catâ€™s fur as fluffy or long.\n",
      "- Clearly mention that the cat has a moderate amount of body fat, using positive and endearing language.\n",
      "- Use vivid, engaging details to help potential adopters or buyers imagine the catâ€™s personality and charm.\n",
      "\n",
      "í•„ìš”í•˜ì‹¤ ë•Œ ì–¸ì œë“  ë‹¤ì‹œ ìš”ì²­í•´ ì£¼ì„¸ìš”!\n",
      "[ì‚¬ìš©ì] q\n",
      "AI: ë‹¤ìŒì— ë˜ ë§Œë‚˜ìš”!\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "from langchain_teddynote.messages import stream_graph\n",
    "\n",
    "# êµ¬ì„± ì„¤ì • ì´ˆê¸°í™” (ê³ ìœ í•œ thread_id ìƒì„±)\n",
    "config = {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\n",
    "\n",
    "# ë¬´í•œ ë£¨í”„ ì‹œì‘\n",
    "while True:\n",
    "    try:\n",
    "        # ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°\n",
    "        user = input(\"User (q/Q to quit): \")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # ì‚¬ìš©ì ì…ë ¥ ì¶œë ¥\n",
    "    print(f\"\\n[ì‚¬ìš©ì] {user}\")\n",
    "\n",
    "    # 'q' ë˜ëŠ” 'Q' ì…ë ¥ ì‹œ ë£¨í”„ ì¢…ë£Œ\n",
    "    if user in {\"q\", \"Q\"}:\n",
    "        print(\"AI: ë‹¤ìŒì— ë˜ ë§Œë‚˜ìš”!\")\n",
    "        break\n",
    "\n",
    "    # ì¶œë ¥ ë³€ìˆ˜ ì´ˆê¸°í™”\n",
    "    output = None\n",
    "\n",
    "    stream_graph(\n",
    "        graph,\n",
    "        {\"messages\": [HumanMessage(content=user)]},\n",
    "        config=config,\n",
    "        node_names=[\"info\", \"prompt\"],\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "03. CRAG(Corrective RAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rag.pdf import PDFRetrievalChain\n",
    "\n",
    "# PDF ë¬¸ì„œë¥¼ ë¡œë“œ\n",
    "pdf = PDFRetrievalChain([\"data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf\"]).create_chain()\n",
    "\n",
    "# retriever ì™€ chainì„ ìƒì„±\n",
    "pdf_retriever = pdf.retriever\n",
    "pdf_chain = pdf.chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_teddynote.models import get_model_name, LLMs\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# ëª¨ë¸ ì´ë¦„ ê°€ì ¸ì˜¤ê¸°\n",
    "MODEL_NAME = get_model_name(LLMs.GPT4_1)\n",
    "\n",
    "\n",
    "# ê²€ìƒ‰ëœ ë¬¸ì„œì˜ ê´€ë ¨ì„± ì—¬ë¶€ë¥¼ ì´ì§„ ì ìˆ˜ë¡œ í‰ê°€í•˜ëŠ” ë°ì´í„° ëª¨ë¸\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"A binary score to determine the relevance of the retrieved document.\"\"\"\n",
    "\n",
    "    # ë¬¸ì„œê°€ ì§ˆë¬¸ê³¼ ê´€ë ¨ì´ ìˆëŠ”ì§€ ì—¬ë¶€ë¥¼ 'yes' ë˜ëŠ” 'no'ë¡œ ë‚˜íƒ€ë‚´ëŠ” í•„ë“œ\n",
    "    binary_score: str = Field(\n",
    "        description=\"Documents are relevant to the question, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "\n",
    "# LLM ì´ˆê¸°í™”\n",
    "llm = ChatOpenAI(model=MODEL_NAME, temperature=0)\n",
    "\n",
    "# GradeDocuments ë°ì´í„° ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ êµ¬ì¡°í™”ëœ ì¶œë ¥ì„ ìƒì„±í•˜ëŠ” LLM\n",
    "structured_llm_grader = llm.with_structured_output(GradeDocuments)\n",
    "\n",
    "# ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì •ì˜\n",
    "system = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "    If the document contains keyword(s) or semantic meaning related to the question, grade it as relevant. \\n\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\"\n",
    "\n",
    "# ì±„íŒ… í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±\n",
    "grade_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Retrieval í‰ê°€ê¸° ì´ˆê¸°í™”\n",
    "retrieval_grader = grade_prompt | structured_llm_grader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary_score='yes'\n"
     ]
    }
   ],
   "source": [
    "# ì§ˆë¬¸ ì •ì˜\n",
    "question = \"ì‚¼ì„±ì „ìê°€ ê°œë°œí•œ ìƒì„±AI ì— ëŒ€í•´ ì„¤ëª…í•˜ì„¸ìš”.\"\n",
    "\n",
    "# ë¬¸ì„œ ê²€ìƒ‰\n",
    "docs = pdf_retriever.invoke(question)\n",
    "\n",
    "# ê²€ìƒ‰ëœ ë¬¸ì„œ ì¤‘ 1ë²ˆ index ë¬¸ì„œì˜ í˜ì´ì§€ ë‚´ìš©ì„ ì¶”ì¶œ\n",
    "doc_txt = docs[1].page_content\n",
    "\n",
    "# ê²€ìƒ‰ëœ ë¬¸ì„œì™€ ì§ˆë¬¸ì„ ì‚¬ìš©í•˜ì—¬ ê´€ë ¨ì„± í‰ê°€ë¥¼ ì‹¤í–‰í•˜ê³  ê²°ê³¼ ì¶œë ¥\n",
    "print(retrieval_grader.invoke({\"question\": question, \"document\": doc_txt}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì‚¼ì„±ì „ìê°€ ê°œë°œí•œ ìƒì„±AIëŠ” â€˜ì‚¼ì„± ê°€ìš°ìŠ¤(Samsung Gauss)â€™ë¡œ, ì–¸ì–´ ëª¨ë¸, ì½”ë“œ ëª¨ë¸, ì´ë¯¸ì§€ ëª¨ë¸ì˜ 3ê°œ ëª¨ë¸ë¡œ êµ¬ì„±ë˜ì–´ ìˆë‹¤. ì‚¼ì„± ê°€ìš°ìŠ¤ëŠ” ì˜¨ë””ë°”ì´ìŠ¤(On-device)ì—ì„œ ì‘ë™ ê°€ëŠ¥í•˜ë©°, ì™¸ë¶€ë¡œ ì‚¬ìš©ì ì •ë³´ê°€ ìœ ì¶œë  ìœ„í—˜ì´ ì—†ëŠ” ê²ƒì´ íŠ¹ì§•ì´ë‹¤. ì–¸ì–´ ëª¨ë¸ì€ ë©”ì¼ ì‘ì„±, ë¬¸ì„œ ìš”ì•½, ë²ˆì—­ ì—…ë¬´ë¥¼ ì§€ì›í•˜ê³ , ì½”ë“œ ëª¨ë¸ì€ AI ì½”ë”© ì–´ì‹œìŠ¤í„´íŠ¸ â€˜ì½”ë“œì•„ì´(code.i)â€™ë¡œ ì‚¬ë‚´ ì†Œí”„íŠ¸ì›¨ì–´ ê°œë°œì— ìµœì í™”ë˜ì–´ ìˆë‹¤. ì´ë¯¸ì§€ ëª¨ë¸ì€ ì°½ì˜ì  ì´ë¯¸ì§€ ìƒì„±, ì´ë¯¸ì§€ ë³€í™˜, ì €í•´ìƒë„ ì´ë¯¸ì§€ì˜ ê³ í•´ìƒë„ ì „í™˜ì„ ì§€ì›í•œë‹¤. ì‚¼ì„±ì „ìëŠ” ì‚¼ì„± ê°€ìš°ìŠ¤ë¥¼ ë‹¤ì–‘í•œ ì œí’ˆì— ë‹¨ê³„ì ìœ¼ë¡œ íƒ‘ì¬í•  ê³„íšì´ë‹¤.\n",
      "\n",
      "**Source**\n",
      "- data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf (p.13)\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "# LangChain Hubì—ì„œ RAG í”„ë¡¬í”„íŠ¸ë¥¼ ê°€ì ¸ì™€ ì‚¬ìš©\n",
    "prompt = hub.pull(\"teddynote/rag-prompt\")\n",
    "\n",
    "# LLM ì´ˆê¸°í™”\n",
    "llm = ChatOpenAI(model_name=MODEL_NAME, temperature=0)\n",
    "\n",
    "\n",
    "# ë¬¸ì„œ í¬ë§·íŒ…\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(\n",
    "        [\n",
    "            f'<document><content>{doc.page_content}</content><source>{doc.metadata[\"source\"]}</source><page>{doc.metadata[\"page\"]+1}</page></document>'\n",
    "            for doc in docs\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "# ì²´ì¸ ìƒì„±\n",
    "rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "\n",
    "# ì²´ì¸ ì‹¤í–‰ ë° ê²°ê³¼ ì¶œë ¥\n",
    "generation = rag_chain.invoke({\"context\": format_docs(docs), \"question\": question})\n",
    "print(generation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# LLM ì„¤ì •\n",
    "llm = ChatOpenAI(model=MODEL_NAME, temperature=0)\n",
    "\n",
    "# Query Rewrite ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸\n",
    "system = \"\"\"You a question re-writer that converts an input question to a better version that is optimized \n",
    "for web search. Look at the input and try to reason about the underlying semantic intent / meaning.\"\"\"\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸ ì •ì˜\n",
    "re_write_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Here is the initial question: \\n\\n {question} \\n Formulate an improved question.\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Question Re-writer ì²´ì¸ ì´ˆê¸°í™”\n",
    "question_rewriter = re_write_prompt | llm | StrOutputParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ì›ë³¸ ì§ˆë¬¸]: \"ì‚¼ì„±ì „ìê°€ ê°œë°œí•œ ìƒì„±AI ì— ëŒ€í•´ ì„¤ëª…í•˜ì„¸ìš”.\"\n",
      "[ì¿¼ë¦¬ ì¬ì‘ì„±]: ì‚¼ì„±ì „ìê°€ ê°œë°œí•œ ìƒì„± AIì˜ ì£¼ìš” íŠ¹ì§•ê³¼ í™œìš© ì‚¬ë¡€ëŠ” ë¬´ì—‡ì¸ê°€ìš”?\n"
     ]
    }
   ],
   "source": [
    "# ì‹¤í–‰ ë° ê²°ê³¼ í™•ì¸\n",
    "print(f'[ì›ë³¸ ì§ˆë¬¸]: \"{question}\"')\n",
    "print(\"[ì¿¼ë¦¬ ì¬ì‘ì„±]:\", question_rewriter.invoke({\"question\": question}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì›¹ ê²€ìƒ‰ ë„êµ¬ ì´ˆê¸°í™”\n",
    "from langchain_teddynote.tools.tavily import TavilySearch\n",
    "\n",
    "# ìµœëŒ€ ê²€ìƒ‰ ê²°ê³¼ë¥¼ 3ìœ¼ë¡œ ì„¤ì •\n",
    "web_search_tool = TavilySearch(max_results=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'url': 'https://v.daum.net/v/20241121100020817', 'title': \"ì‚¼ì„±ì „ì, ìì²´ ê°œë°œí•œ ìƒì„±í˜• AI 'ì‚¼ì„± ê°€ìš°ìŠ¤2' ê³µê°œ - Daum\", 'content': '### ì‚¼ì„±ì „ì, ìì²´ ê°œë°œí•œ ìƒì„±í˜• AI \\'ì‚¼ì„± ê°€ìš°ìŠ¤2\\' ê³µê°œ ë˜í•œ \"ì‚¼ì„±ì „ì ê³ ìœ ì˜ ìƒì„±í˜• AI ëª¨ë¸ì¸ \\'ì‚¼ì„± ê°€ìš°ìŠ¤2\\'ë¥¼ ê³µê°œí•˜ê³ , í–¥ìƒëœ ì„±ëŠ¥ê³¼ íš¨ìœ¨, ê·¸ë¦¬ê³  í™œìš© ë°©ì•ˆì— ëŒ€í•´ ì„¤ëª…í•  ì˜ˆì •\"ì´ë¼ë©° \"ì´ë¥¼ í†µí•´ ì—…ë¬´ ìƒì‚°ì„± í–¥ìƒê³¼ ë‹¨ê³„ì  ì œí’ˆ ì ìš©ìœ¼ë¡œ ë”ìš± í¸ë¦¬í•˜ê³  ì°¨ë³„í™”ëœ ê³ ê° ê²½í—˜ì„ ì œê³µí•  ê²ƒ\"ì´ë¼ê³  ë§í–ˆë‹¤. ì‚¼ì„±ì „ìëŠ” í‚¤ë…¸íŠ¸ ë°œí‘œì—ì„œ ì§€ë‚œí•´ ì²˜ìŒ ê³µê°œí•œ ìƒì„±í˜• AI ëª¨ë¸ \\'ì‚¼ì„± ê°€ìš°ìŠ¤1(Samsung Gauss1)\\'ì˜ í›„ì† ëª¨ë¸ì¸ \\'ì‚¼ì„± ê°€ìš°ìŠ¤2(Samsung Gauss2)\\'ë¥¼ ê³µê°œí–ˆë‹¤. ì‚¼ì„± ê°€ìš°ìŠ¤ í¬íƒˆ(Samsung Gauss Portal)ì€ \\'ì‚¼ì„± ê°€ìš°ìŠ¤\\'ì˜ ëŒ€í™”í˜• AI ì„œë¹„ìŠ¤ë¡œ â–²ë¬¸ì„œ ìš”ì•½ â–²ë²ˆì—­ â–²ë©”ì¼ ì‘ì„± ë“± DXë¶€ë¬¸ ì§ì›ë“¤ì˜ ë‹¤ì–‘í•œ ì‚¬ë¬´ ì—…ë¬´ë¥¼ ë¹ ë¥´ê³  íš¨ìœ¨ì ìœ¼ë¡œ ì²˜ë¦¬í•  ìˆ˜ ìˆë„ë¡ ì§€ì›í•˜ê³  ìˆë‹¤. ì‚¼ì„±ì „ìëŠ” í–¥í›„ \\'ì‚¼ì„± ê°€ìš°ìŠ¤2\\'ë¥¼ í†µí•´ \\'ì½”ë“œì•„ì´(code.i)\\'ì„œë¹„ìŠ¤ì˜ ì§€ì†ì ì¸ ì„±ëŠ¥ ê°œì„ , \\'ì‚¼ì„± ê°€ìš°ìŠ¤ í¬íƒˆ(Samsung Gauss Portal)\\'ì˜ ìì—°ì–´ ì§ˆì˜ì‘ë‹µ ì„±ëŠ¥ í–¥ìƒ, í‘œì™€ ì°¨íŠ¸ì˜ ì´í•´, ì´ë¯¸ì§€ ìƒì„± ë“± ë©€í‹°ëª¨ë‹¬ ê¸°ëŠ¥ì„ ì§€ì›í•´ ì‚¬ë‚´ ìƒì‚°ì„± í–¥ìƒì— ê¸°ì—¬í•  ê³„íšì´ë‹¤.', 'score': 0.83549726, 'raw_content': '# [ì§€ë””ë„·ì½”ë¦¬ì•„](https://v.daum.net/channel/294/home)\\n\\n## ê²€ìƒ‰\\n\\nê²€ìƒ‰\\n\\n\\n\\n## ì†ë³´\\n\\n### ì‚¼ì„±ì „ì, ìì²´ ê°œë°œí•œ ìƒì„±í˜• AI \\'ì‚¼ì„± ê°€ìš°ìŠ¤2\\' ê³µê°œ\\n\\nì¥ê²½ìœ¤ ê¸°ì 2024. 11. 21. 10:00\\n\\n**ìŒì„±ì¬ìƒ ì„¤ì •**\\n\\n[ë‹«ê¸°](#none)\\n\\n**ë²ˆì—­*beta*** Translated by [kaka i](https://kakao.ai/)\\n\\n[ë‹«ê¸°](#none)\\n\\n**ë²ˆì—­ì¤‘** Now in translation\\n\\n**ê¸€ìí¬ê¸° ì„¤ì •** íŒŒë€ì›ì„ ì¢Œìš°ë¡œ ì›€ì§ì´ì‹œë©´ ê¸€ìí¬ê¸°ê°€ ë³€ê²½ ë©ë‹ˆë‹¤.\\n\\nê°€\\n\\n\\n\\n\\n\\n\\n\\nê°€\\n\\nì´ ê¸€ìí¬ê¸°ë¡œ ë³€ê²½ë©ë‹ˆë‹¤.\\n\\n(ì˜ˆì‹œ) ê°€ì¥ ë¹ ë¥¸ ë‰´ìŠ¤ê°€ ìˆê³  ë‹¤ì–‘í•œ ì •ë³´, ìŒë°©í–¥ ì†Œí†µì´ ìˆ¨ì‰¬ëŠ” ë‹¤ìŒë‰´ìŠ¤ë¥¼ ë§Œë‚˜ë³´ì„¸ìš”. ë‹¤ìŒë‰´ìŠ¤ëŠ” êµ­ë‚´ì™¸ ì£¼ìš”ì´ìŠˆì™€ ì‹¤ì‹œê°„ ì†ë³´, ë¬¸í™”ìƒí™œ ë° ë‹¤ì–‘í•œ ë¶„ì•¼ì˜ ë‰´ìŠ¤ë¥¼ ì…ì²´ì ìœ¼ë¡œ ì „ë‹¬í•˜ê³  ìˆìŠµë‹ˆë‹¤.\\n\\n[ë‹«ê¸°](#none)\\n\\n**21ì¼ \\'ì‚¼ì„± ê°œë°œì ì½˜í¼ëŸ°ìŠ¤ ì½”ë¦¬ì•„\\'ì„œ ìµœì‹  ê¸°ìˆ ê³¼ ë¹„ì „ ì†Œê°œ**\\n\\n(ì§€ë””ë„·ì½”ë¦¬ì•„=ì¥ê²½ìœ¤ ê¸°ì)ì‚¼ì„±ì „ìëŠ”\\xa0\\'ì‚¼ì„± ê°œë°œì ì½˜í¼ëŸ°ìŠ¤ ì½”ë¦¬ì•„ 2024(Samsung Developer Conference Korea 2024, SDC24 Korea)\\'ë¥¼ ì˜¨ë¼ì¸ìœ¼ë¡œ ê°œìµœí–ˆë‹¤ê³  21ì¼ ë°í˜”ë‹¤.\\n\\nì‚¼ì„±ì „ìëŠ” ì†Œí”„íŠ¸ì›¨ì–´ ê°œë°œìë“¤ê³¼ ì†Œí†µí•˜ê³  í˜‘ë ¥í•˜ê¸° ìœ„í•´ 2014ë…„ë¶€í„° ë§¤ë…„ ê°œë°œì í–‰ì‚¬ë¥¼ ê°œìµœí•´ ì™”ìœ¼ë©°, ì˜¬í•´ë¡œ 11ì£¼ë…„ì„ ë§ì´í–ˆë‹¤.\\n\\nì´ë‚  ì½˜í¼ëŸ°ìŠ¤ì—ì„œëŠ” â–²ìƒì„±í˜• AI â–²ì†Œí”„íŠ¸ì›¨ì–´(S/W) í”Œë«í¼ â–²IoT â–²í—¬ìŠ¤ì¼€ì–´ â–²í†µì‹  â–²ë°ì´í„° ë“± ì œí’ˆì— íƒ‘ì¬ëœ ë‹¤ì–‘í•œ ì†Œí”„íŠ¸ì›¨ì–´ì˜ ì—°êµ¬ ë¶„ì•¼ëŠ” ë¬¼ë¡  ì˜¤í”ˆì†ŒìŠ¤ ê°œë°œ ë¬¸í™”ì— ì´ë¥´ê¸°ê¹Œì§€ ë‹¤ì–‘í•œ ì„¸ì…˜ì´ ë§ˆë ¨ëë‹¤.\\n\\nì‚¼ì„±ì „ì DXë¶€ë¬¸ ìµœê³ ê¸°ìˆ ì±…ì„ì(CTO) ê²¸ ì‚¼ì„±ë¦¬ì„œì¹˜ì¥ì¸ ì „ê²½í›ˆ ì‚¬ì¥ì€ í™˜ì˜ì‚¬ë¥¼ í†µí•´ \"ì‚¼ì„±ì „ìëŠ” ì¸ê³µì§€ëŠ¥ê³¼ ë°ì´í„° ë¶„ì„ê³¼ ê°™ì€ ìµœì‹  ì†Œí”„íŠ¸ì›¨ì–´ ê¸°ìˆ  í™•ë³´ì— ì§‘ì¤‘í•˜ì—¬ ìƒˆë¡œìš´ ë¼ì´í”„ìŠ¤íƒ€ì¼ì„ ì°½ì¡°í•˜ê³  ê³ ê°ì˜ ì‚¶ì„ ê°œì„ í•˜ëŠ”ë° ì£¼ë ¥í•˜ê³  ìˆë‹¤\"ê³  ê°•ì¡°í–ˆë‹¤.\\n\\në˜í•œ \"ì‚¼ì„±ì „ì ê³ ìœ ì˜ ìƒì„±í˜• AI ëª¨ë¸ì¸ \\'ì‚¼ì„± ê°€ìš°ìŠ¤2\\'ë¥¼ ê³µê°œí•˜ê³ , í–¥ìƒëœ ì„±ëŠ¥ê³¼ íš¨ìœ¨, ê·¸ë¦¬ê³  í™œìš© ë°©ì•ˆì— ëŒ€í•´ ì„¤ëª…í•  ì˜ˆì •\"ì´ë¼ë©° \"ì´ë¥¼ í†µí•´ ì—…ë¬´ ìƒì‚°ì„± í–¥ìƒê³¼ ë‹¨ê³„ì  ì œí’ˆ ì ìš©ìœ¼ë¡œ ë”ìš± í¸ë¦¬í•˜ê³  ì°¨ë³„í™”ëœ ê³ ê° ê²½í—˜ì„ ì œê³µí•  ê²ƒ\"ì´ë¼ê³  ë§í–ˆë‹¤.\\n\\n**â–  ìì²´ ê°œë°œ ìƒì„±í˜• AI ëª¨ë¸ 2ì„¸ëŒ€ \\'ì‚¼ì„± ê°€ìš°ìŠ¤2\\' ê³µê°œ**\\n\\nì‚¼ì„±ì „ìëŠ” í‚¤ë…¸íŠ¸ ë°œí‘œì—ì„œ ì§€ë‚œí•´ ì²˜ìŒ ê³µê°œí•œ ìƒì„±í˜• AI ëª¨ë¸ \\'ì‚¼ì„± ê°€ìš°ìŠ¤1(Samsung Gauss1)\\'ì˜ í›„ì† ëª¨ë¸ì¸ \\'ì‚¼ì„± ê°€ìš°ìŠ¤2(Samsung Gauss2)\\'ë¥¼ ê³µê°œí–ˆë‹¤.\\n\\n\\'ì‚¼ì„± ê°€ìš°ìŠ¤2\\'ëŠ” ì–¸ì–´Â·ì½”ë“œÂ·ì´ë¯¸ì§€ í†µí•© ë©€í‹°ëª¨ë‹¬(Multimodal) ëª¨ë¸ë¡œ ê¸°ì¡´ \\'ì‚¼ì„± ê°€ìš°ìŠ¤1\\'ê³¼ ë‹¬ë¦¬ ì—¬ëŸ¬ê°€ì§€ ë°ì´í„° ìœ í˜•ì„ ë™ì‹œ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” 2ì„¸ëŒ€ ë²„ì „ì´ë‹¤. ì„œë¹„ìŠ¤ ìš©ë„ì— ë”°ë¼ â–²ì½¤íŒ©íŠ¸(Compact) â–²ë°¸ëŸ°ìŠ¤ë“œ(Balanced) â–²ìŠˆí”„ë¦¼(Supreme) ì„¸ ê°€ì§€ ëª¨ë¸ë¡œ êµ¬ì„±ë˜ì–´ ìˆë‹¤.\\n\\në©€í‹°ëª¨ë‹¬ì€ í…ìŠ¤íŠ¸, ì´ë¯¸ì§€, ì˜¤ë””ì˜¤, ë¹„ë””ì˜¤ ë“± ë‹¤ì–‘í•œ ìœ í˜•ì˜ ë°ì´í„°(ëª¨ë‹¬ë¦¬í‹°)ë¥¼ í•¨ê»˜ ê³ ë ¤í•˜ì—¬ ì„œë¡œì˜ ê´€ê³„ì„±ì„ í•™ìŠµ ë° ì²˜ë¦¬í•˜ëŠ” ì¸ê³µì§€ëŠ¥ì´ë‹¤. ì¸ê°„ì´ ì˜¤ê°ì„ ëª¨ë‘ í™œìš©í•´ ì •ë³´ë¥¼ ì´í•´í•˜ê³  ì²˜ë¦¬í•˜ëŠ” ë°©ì‹ê³¼ ìœ ì‚¬í•˜ë‹¤.\\n\\n\\'ì½¤íŒ©íŠ¸\\'ëŠ” ì œí•œëœ ì»´í“¨íŒ… í™˜ê²½ì—ì„œë„ íš¨ìœ¨ì ìœ¼ë¡œ ì‘ë™í•˜ë„ë¡ ì„¤ê³„ëœ ì†Œí˜• ëª¨ë¸ì´ë‹¤. ì˜¨ë””ë°”ì´ìŠ¤(On-Device) í™˜ê²½ì—ì„œ ê¸°ê¸°ì˜ íŠ¹ì„±ì„ ìµœëŒ€í•œ í™œìš©í•´ ê¸°ê¸°ì— ìµœì í™”ëœ ì„±ëŠ¥ì„ ì œê³µí•œë‹¤.\\n\\n\\'ë°¸ëŸ°ìŠ¤ë“œ\\'ëŠ” í´ë¼ìš°ë“œ(Cloud) ê¸°ë°˜ìœ¼ë¡œ ì„±ëŠ¥, ì†ë„ ë©´ì—ì„œ ì•ˆì •ì„±ê³¼ íš¨ìœ¨ì„±ì˜ ê· í˜•ì„ ë§ì¶˜ ëª¨ë¸ì´ë‹¤. ë‹¤ì–‘í•œ ì‘ì—…ì—ì„œë„ ê· í˜• ì¡íŒ ì„±ëŠ¥ì„ ì œê³µí•œë‹¤.\\n\\në§ˆì§€ë§‰ìœ¼ë¡œ \\'ìŠˆí”„ë¦¼\\'ì€ ìµœê³  ì„±ëŠ¥ì„ ëª©í‘œë¡œ í•˜ëŠ” ê³ ì„±ëŠ¥ ëª¨ë¸ì´ë‹¤. \\'ë°¸ëŸ°ìŠ¤ë“œ\\' ëª¨ë¸ì„ ê¸°ë°˜ìœ¼ë¡œ \\'ì „ë¬¸ê°€ í˜¼í•©(MoE, Mixture of Experts)\\' ê¸°ìˆ ì„ ì´ìš©í•´ í•™ìŠµê³¼ ì¶”ë¡  ê³¼ì •ì—ì„œ ê³„ì‚°ëŸ‰ì„ ëŒ€í­ ì¤„ì—¬ ì„±ëŠ¥ê³¼ íš¨ìœ¨ì„± ëª¨ë‘ë¥¼ ë†’ì¼ ìˆ˜ ìˆì—ˆë‹¤.\\n\\nì „ë¬¸ê°€ í˜¼í•© ê¸°ìˆ ì€ íŠ¹ì • ì‘ì—… ì²˜ë¦¬ ì‹œ ê°€ì¥ ì•Œë§ì€ ì¼ë¶€ ì „ë¬¸ê°€ ëª¨ë¸ë“¤ë§Œ ì„ íƒ, í™œì„±í™”í•´ ì²˜ë¦¬í•˜ëŠ” ë°©ì‹ì„ ëœ»í•œë‹¤. í•„ìš”í•œ ì—°ì‚° ìì›ì„ ì ˆì•½í•´ íš¨ìœ¨ì ìœ¼ë¡œ ì„±ëŠ¥ì„ ë†’ì¼ ìˆ˜ ìˆë‹¤.\\n\\në˜í•œ \\'ì‚¼ì„± ê°€ìš°ìŠ¤2\\'ëŠ” ëª¨ë¸ì— ë”°ë¼ 9~14ê°œêµ­ì˜ ì–¸ì–´ì™€ ë‹¤ì–‘í•œ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ë¥¼ ì§€ì›í•œë‹¤. ì‚¼ì„±ì „ìëŠ” ìì²´ì ìœ¼ë¡œ ê±°ëŒ€ì–¸ì–´ëª¨ë¸(LLM) í•™ìŠµ ì•ˆì •í™” ê¸°ë²•ì„ ê°œë°œí•´ ì ìš©í•˜ê³ , ìì²´ \\'í† í¬ë‚˜ì´ì €(Tokenizer)\\'ë¥¼ ì„¤ê³„í•´ ì§€ì›í•˜ëŠ” ì–¸ì–´ì— ëŒ€í•œ íš¨ìœ¨ì„±ì„ ê·¹ëŒ€í™”í–ˆë‹¤.\\n\\ní† í¬ë‚˜ì´ì €ëŠ”\\xa0ê¸€ì, ì´ë¯¸ì§€, ìŒì„± ë“±ì˜ ì…ë ¥ì„ ìƒì„±í˜• ëª¨ë¸ì´ ì²˜ë¦¬ í•  ìˆ˜ ìˆëŠ” í† í°ìœ¼ë¡œ ë³€í™˜ ë° ë¶„í• í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ, ë°ì´í„°ë¥¼ ë²¡í„° í˜•íƒœì˜ ìˆ«ì í˜•ì‹ì¸ í† í°ìœ¼ë¡œ í‘œí˜„í•˜ëŠ” ë°©ë²•ì´ë‹¤.\\n\\n\\'ë°¸ëŸ°ìŠ¤ë“œ\\'ì™€ \\'ìŠˆí”„ë¦¼\\' ëª¨ë¸ì€ í˜„ì¬ ê³µê°œë˜ì–´ ìˆëŠ” ëŒ€í‘œì ì¸ ì˜¤í”ˆì†ŒìŠ¤ ìƒì„±í˜• AI ëª¨ë¸ë“¤ ëŒ€ë¹„ ì˜ì–´, í•œêµ­ì–´ ë“± ë‹¤ì–‘í•œ ì–¸ì–´ ë‹µë³€ ìƒì„±, ì½”ë”© ë“± ì£¼ìš” ì§€í‘œì—ì„œ ë™ë“± ì´ìƒì˜ ì„±ëŠ¥ì„ ì œê³µí•œë‹¤. ì‹œê°„ë‹¹ ì²˜ë¦¬ ì†ë„ëŠ” 1.5~3ë°° ì´ìƒìœ¼ë¡œ ê·¸ë§Œí¼ AIì˜ ë‹µë³€ ìƒì„±ì´ ë¹¨ë¼ ì‚¬ìš©ìì˜ ëŒ€ê¸°ì‹œê°„ì´ ì¤„ì–´ë“¤ê³  íš¨ìœ¨ì ì¸ ì¼ì²˜ë¦¬ê°€ ê°€ëŠ¥í•˜ë‹¤.\\n\\n**â–  ì‚¬ë‚´ ë§ì¶¤í˜• ê°œë°œ \\'ì‚¼ì„± ê°€ìš°ìŠ¤\\',****ë‹¤ì–‘í•œ ì—…ë¬´Â·ì œí’ˆ ê°œë°œì— í™œìš©**\\n\\nìì²´ ìƒì„±í˜• AI ëª¨ë¸ì˜ ì¥ì ì€ í•„ìš”í•œ ëª©ì ê³¼ ì‘ìš©ë¶„ì•¼ì— ë§ì¶° ìµœê³ ì˜ ì„±ëŠ¥ì„ ë‚´ë„ë¡ ë§ì¶¤í˜• ê°œë°œì— ìš©ì´í•˜ë‹¤ëŠ” ê²ƒì´ë‹¤. ì‹¤ì œë¡œ \\'ì‚¼ì„± ê°€ìš°ìŠ¤\\'ëŠ” ì´ëŸ° ë§ì¶¤í˜• ê°œë°œì˜ ì¥ì ì„ ì‚´ë ¤ ì§ì›ë“¤ì˜ ë‹¤ì–‘í•œ ì—…ë¬´ì—ì„œ í™œìš©ë˜ê³  ìˆë‹¤.\\n\\n\\'ì½”ë“œì•„ì´(code.i)\\'ëŠ” ì‚¬ë‚´ S/W ê°œë°œìë¥¼ ì§€ì›í•˜ëŠ” \\'ì‚¼ì„± ê°€ìš°ìŠ¤\\' ëª¨ë¸ì˜ ì½”ë”© ì–´ì‹œìŠ¤í„´íŠ¸ ì„œë¹„ìŠ¤ì´ë‹¤. ìµœê·¼ì—ëŠ” \\'ì‚¼ì„± ê°€ìš°ìŠ¤2\\' ëª¨ë¸ë¡œ ì—…ê·¸ë ˆì´ë“œë˜ì–´ DXë¶€ë¬¸ì˜ ì‚¬ì—…ë¶€ ë° ì¼ë¶€ í•´ì™¸ ì—°êµ¬ì†Œì—ì„œ í™œìš©ë˜ê³  ìˆë‹¤.\\n\\n\\'ì½”ë“œì•„ì´\\'ëŠ” ì‘ë…„ 12ì›” ì„œë¹„ìŠ¤ ì‹œì‘ ì‹œì  ëŒ€ë¹„ í˜„ì¬ ì›”ë³„ ì‚¬ìš©ëŸ‰ì€ ì•½ 4ë°° ì´ìƒ ì¦ê°€í–ˆê³ , ì‚¼ì„±ì „ì DXë¶€ë¬¸ ì „ì²´ S/W ê°œë°œìì˜ ì•½ 60%ê°€ ì‚¬ìš©í•˜ê³  ìˆë‹¤.\\n\\nì‚¼ì„± ê°€ìš°ìŠ¤ í¬íƒˆ(Samsung Gauss Portal)ì€ \\'ì‚¼ì„± ê°€ìš°ìŠ¤\\'ì˜ ëŒ€í™”í˜• AI ì„œë¹„ìŠ¤ë¡œ â–²ë¬¸ì„œ ìš”ì•½ â–²ë²ˆì—­ â–²ë©”ì¼ ì‘ì„± ë“± DXë¶€ë¬¸ ì§ì›ë“¤ì˜ ë‹¤ì–‘í•œ ì‚¬ë¬´ ì—…ë¬´ë¥¼ ë¹ ë¥´ê³  íš¨ìœ¨ì ìœ¼ë¡œ ì²˜ë¦¬í•  ìˆ˜ ìˆë„ë¡ ì§€ì›í•˜ê³  ìˆë‹¤. ì§€ë‚œ 4ì›”ì—ëŠ” í•´ì™¸ ë²•ì¸ìœ¼ë¡œ ì„œë¹„ìŠ¤ê°€ í™•ëŒ€ë˜ì—ˆë‹¤.\\n\\në˜ ì‚¼ì„±ì „ìëŠ” ì˜¬í•´ 8ì›”ë¶€í„° \\'ì‚¼ì„± ê°€ìš°ìŠ¤\\'ë¥¼ ì½œì„¼í„°ì— ì ìš©í•´ ìƒë‹´ ë‚´ìš©ì„ ìë™ ë¶„ë¥˜í•˜ê³  ìš”ì•½í•˜ëŠ” ë“± ìƒë‹´ì›ì˜ ì—…ë¬´ë¥¼ ë³´ì¡°í•˜ê³  ìˆë‹¤.\\n\\nì‚¼ì„±ì „ìëŠ” í–¥í›„ \\'ì‚¼ì„± ê°€ìš°ìŠ¤2\\'ë¥¼ í†µí•´ \\'ì½”ë“œì•„ì´(code.i)\\'ì„œë¹„ìŠ¤ì˜ ì§€ì†ì ì¸ ì„±ëŠ¥ ê°œì„ , \\'ì‚¼ì„± ê°€ìš°ìŠ¤ í¬íƒˆ(Samsung Gauss Portal)\\'ì˜ ìì—°ì–´ ì§ˆì˜ì‘ë‹µ ì„±ëŠ¥ í–¥ìƒ, í‘œì™€ ì°¨íŠ¸ì˜ ì´í•´, ì´ë¯¸ì§€ ìƒì„± ë“± ë©€í‹°ëª¨ë‹¬ ê¸°ëŠ¥ì„ ì§€ì›í•´ ì‚¬ë‚´ ìƒì‚°ì„± í–¥ìƒì— ê¸°ì—¬í•  ê³„íšì´ë‹¤.\\n\\në˜í•œ ì‚¼ì„±ì „ìëŠ” \\'ëª¨ë‘ë¥¼ ìœ„í•œ AI(AI for All)\\' ë¼ëŠ” AI ë¹„ì „ ì•„ë˜, ì‚¬ìš©ìë“¤ì´ ë”ìš± í¸ë¦¬í•˜ê³  ì¦ê±°ìš´ ì¼ìƒì„ ëˆ„ë¦´ ìˆ˜ ìˆë„ë¡ ì „ ì œí’ˆ êµ°ì— AI ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•  ì˜ˆì •ì´ë‹¤. í–¥í›„ ì§€ì‹ ê·¸ë˜í”„(Knowledge Graph) ê¸°ìˆ ê³¼ë„ ê²°í•©í•´ í•œì¸µ ê°•í™”ëœ ê°œì¸í™” ì„œë¹„ìŠ¤ë¥¼ ì„ ë³´ì¼ ê²ƒìœ¼ë¡œ ê¸°ëŒ€ëœë‹¤.\\n\\n**â–  S/W í”Œë«í¼, IoT, í—¬ìŠ¤ì¼€ì–´, í†µì‹ , ë°ì´í„° ë“± ë‹¤ì–‘í•œ ì£¼ì œë¡œ ë°œí‘œ ì§„í–‰**\\n\\n\\'ì‚¼ì„± ê°€ìš°ìŠ¤2\\' ë°œí‘œì— ì´ì–´, \\'ì‚¼ì„±ì˜ í”Œë«í¼ì— ëŒ€í•œ ê³ ê°ê²½í—˜\\'ì„ ì£¼ì œë¡œ â–²ìŠ¤ë§ˆíŠ¸ì‹±ìŠ¤(SmartThings) í”Œë«í¼ ê³ ê° ê²½í—˜ â–²ì†Œí”„íŠ¸ì›¨ì–´ í”Œë«í¼ ê³ ê°ê²½í—˜ í–¥ìƒì— ëŒ€í•œ í‚¤ë…¸íŠ¸ê°€ ì´ì–´ì¡Œë‹¤.\\n\\nì´í›„ â–²í—¬ìŠ¤ì¼€ì–´ì˜ ë¯¸ë˜ì™€ ì‚¼ì„± í—¬ìŠ¤ ì—ì½”ì‹œìŠ¤í…œ ì „ëµ â–²ìƒì„±í˜• AIë¥¼ í†µí•œ ìŠ¤ë§ˆíŠ¸ì‹±ìŠ¤(SmartThings) ê³ ê° VOC ê²½í—˜ ê°œì„ ê¸° â–²code.i: ì‚¼ì„±ì˜ AI ì½”ë”© ì–´ì‹œìŠ¤í„´íŠ¸ë¥¼ ì´í•´í•˜ë‹¤ â–²ë‹¹ì‹ ì˜ ì¼ìƒì— í™œë ¥ì„ ë”í•˜ëŠ” TV ê¸°ë°˜ ë¼ì´í”„ìŠ¤íƒ€ì¼ ì½˜í…ì¸  í—ˆë¸Œ ì†Œê°œ â–²AI ë¹„ì „(Vision) ê¸°ìˆ ê³¼ ë°ì´í„° í™œìš©ì„ í†µí•œ ì‚¼ì„± ê°€ì „ AI ì†”ë£¨ì…˜ ë“± ì´ 29ê°œì˜ ë‹¤ì–‘í•œ ê¸°ìˆ  ì„¸ì…˜ì´ ì´ì–´ì¡Œë‹¤.\\n\\nì¥ê²½ìœ¤ ê¸°ì(jkyoon@zdnet.co.kr)\\n\\nCopyright Â© ì§€ë””ë„·ì½”ë¦¬ì•„. ë¬´ë‹¨ì „ì¬ ë° ì¬ë°°í¬ ê¸ˆì§€.\\n\\n**ì§€ë””ë„·ì½”ë¦¬ì•„ì—ì„œ ì§ì ‘ í™•ì¸í•˜ì„¸ìš”.** í•´ë‹¹ ì–¸ë¡ ì‚¬ë¡œ ì´ë™í•©ë‹ˆë‹¤.\\n\\n* [ì‚¼ì„±ì „ì, ç¾ AI ë°˜ë„ì²´ ìŠ¤íƒ€íŠ¸ì—… \\'ì—”íŒŒë¸Œë¦¬ì¹´\\'ì— íˆ¬ì](https://zdnet.co.kr/view/?no=20241120103015&re=D_20241121083205)\\n* [ì‚¼ì„±ì „ì \\'ê¸€ë¡œë²Œ ê³¨ì¦ˆ ì•±\\' ì•¤ì¸ ì–´ì›Œë“œ ìˆ˜ìƒ...ì‚¬íšŒì  ì˜í–¥ë ¥ ì¸ì •](https://zdnet.co.kr/view/?no=20241120091602&re=D_20241121083205)\\n* [ì‚¼ì„±ì „ì, ê°¤ëŸ­ì‹œì›Œì¹˜6 ë“± ê¸°ì¡´ ëª¨ë¸ \\'ì› UI 6 ì›Œì¹˜\\' ì—…ë°ì´íŠ¸ ì‹¤ì‹œ](https://zdnet.co.kr/view/?no=20241120085039&re=D_20241121083205)\\n* [ì‚¼ì„±ì „ì, \\'XR ìŠ¤ë§ˆíŠ¸ì•ˆê²½\\' ë‚´ë…„ 3ë¶„ê¸° ì¶œì‹œ ì „ë§...ì• í”Œê³¼ ë§ëŒ€ê²°](https://zdnet.co.kr/view/?no=20241119120941&re=D_20241121083205)\\n* [ë‚´ë…„ ì •ë¶€ R&Dì˜ˆì‚° 35.3ì¡°ì› \\'ì—­ëŒ€ ìµœëŒ€\\'â€¦AI, 106% ì¦ì•¡](https://zdnet.co.kr/view/?no=20250822102653)\\n* [í€„ì»´ì¹© ë¹„ì‹¸ë‹¤?...ì‚¼ì„± ì˜¥ì£„ëŠ” ëª¨ë°”ì¼ AP ë¹„ìš© ë¶€ë‹´ì˜ ì§„ì§œ ì´ìœ ](https://zdnet.co.kr/view/?no=20250821104639)\\n* [[ìœ ë¯¸\\'s í”½] \\'ëª¨ë°”ì¼ ê°•ì\\' ìœ ë¼í´, AIì— ìŠ¹ë¶€ìˆ˜...ë¹ ë¥¸ ì‹œì¥ ì•ˆì°©ì— ê¸°ëŒ€ê° \\'ì—…\\'](https://zdnet.co.kr/view/?no=20250820182031)\\n* [[ë°ì´í„° ì£¼ê¶Œ] AI ê²½ìŸë ¥, êµ­ê°€ í˜‘ë ¥ í†µí•œ ë°ì´í„° ê°œë°©ì´ í•µì‹¬](https://zdnet.co.kr/view/?no=20250821093019)\\n* [4ì¸ ê°€ì¡±ì— ìœ ëª¨ì°¨Â·ìº í•‘ì¥ë¹„ê¹Œì§€â€¦íŒ¨ë°€ë¦¬ì¹´ ëíŒì™•ì´ë¼ëŠ” \\'ì´ è»Š\\'](https://zdnet.co.kr/view/?no=20250821102605)\\n* [\"ì „ê¸°ë£Œ ì¸ìƒ ë¶ˆê°€í”¼\" ëŒ€í†µë ¹ ë°œì–¸ì— ì‚°ì—…ê³„ ê·¼ì‹¬](https://zdnet.co.kr/view/?no=20250821173845)'}, {'url': 'https://www.esgeconomy.com/news/articleView.html?idxno=5090', 'title': \"ì‚¼ì„±ì „ì, ìì²´ ê°œë°œí•œ ìƒì„±í˜• AI 'ì‚¼ì„± ê°€ìš°ìŠ¤' ì²« ê³µê°œ - ESGê²½ì œ\", 'content': \"### ì‚¼ì„±ì „ì, ìì²´ ê°œë°œí•œ ìƒì„±í˜• AI 'ì‚¼ì„± ê°€ìš°ìŠ¤' ì²« ê³µê°œ ì‚¼ì„±ì „ìê°€ ìì²´ ê°œë°œí•œ ìƒì„±í˜• AI 'ì‚¼ì„± ê°€ìš°ìŠ¤'. [ESGê²½ì œ=ê¹€ë„ì‚° ê¸°ì] ì‚¼ì„±ì „ìê°€ ìì²´ ê°œë°œí•œ ìƒì„±í˜• ì¸ê³µì§€ëŠ¥(AI) ëª¨ë¸ 'ì‚¼ì„± ê°€ìš°ìŠ¤(Samsung Gauss)â€˜ë¥¼ ì²˜ìŒ ê³µê°œí–ˆë‹¤. ì‚¼ì„±ì „ìëŠ” ì‚¼ì„± ê°€ìš°ìŠ¤ë¥¼ í™œìš©í•´ ì„ì§ì›ì˜ ì—…ë¬´ ìƒì‚°ì„±ì„ ë†’ì´ê³ , ìƒì„±í˜• AI ëª¨ë¸ì„ ë‹¨ê³„ì ìœ¼ë¡œ ì œí’ˆì— íƒ‘ì¬í•´ ìƒˆë¡œìš´ ì‚¬ìš©ì ê²½í—˜ì„ ì œê³µí•œë‹¤ëŠ” ê³„íšì´ë‹¤. ì‚¼ì„±ì „ìëŠ” 8ì¼ ì‚¼ì„±ì „ì ì„œìš¸R&Dìº í¼ìŠ¤ì—ì„œ 'ì‚¼ì„± AI í¬ëŸ¼ 2023' 2ì¼ì°¨ í–‰ì‚¬ë¥¼ ì—´ê³  ì‚¼ì„±ë¦¬ì„œì¹˜ì—ì„œ ê°œë°œí•œ ì‚¼ì„± ê°€ìš°ìŠ¤ë¥¼ ì²˜ìŒìœ¼ë¡œ ê³µê°œí–ˆë‹¤ê³  ë°í˜”ë‹¤. ì‚¼ì„±ì „ìëŠ” ìƒì„±í˜• AI ëª¨ë¸ë“¤ì„ ë‹¤ì–‘í•œ ì œí’ˆì— ë‹¨ê³„ì ìœ¼ë¡œ íƒ‘ì¬í•  ê³„íšì¸ë°, ì—…ê³„ì—ì„œëŠ” ì´ë¥´ë©´ ë‚´ë…„ 1ì›” ê³µê°œí•  ê°¤ëŸ­ì‹œ S24ì— ìƒì„±í˜• AI ê¸°ëŠ¥ì´ íƒ‘ì¬ë  ê²ƒìœ¼ë¡œ ë³´ê³  ìˆë‹¤. í•œí¸ ì‚¼ì„±ë¦¬ì„œì¹˜ì—ì„œ ì£¼ê´€í•œ ì´ë‚  â€™'ì‚¼ì„± AI í¬ëŸ¼ 2023' í¬ëŸ¼ì—ëŠ” ì‚¼ì„±ì „ìì˜ AI ì—°êµ¬ ì¸ë ¥ê³¼ í•™ê³„Â·ì—…ê³„ AI ì „ë¬¸ê°€ 150ì—¬ëª…ì´ ì°¸ì„í•´ ìƒì„±í˜• AI ê¸°ìˆ ì˜ ë°œì „ ë°©í–¥ì„ ë…¼ì˜í•˜ê³  ìµœì‹  ê¸°ìˆ  ë™í–¥ì„ ê³µìœ í–ˆë‹¤.\", 'score': 0.8342047, 'raw_content': '## ìƒë‹¨ì˜ì—­\\n\\n# [ESGê²½ì œ](https://www.esgeconomy.com)\\n\\n![ESGê²½ì œ](https://cdn.esgeconomy.com/image/logo/toplogo_20201116093648.png)\\n\\n## ë³¸ë¬¸ì˜ì—­\\n\\n![ESGê²½ì œ](https://cdn.esgeconomy.com/image/logo/printlogo_20201106110340.png)\\n\\n#### SNS ê¸°ì‚¬ë³´ë‚´ê¸°\\n\\n#### SNS ê¸°ì‚¬ë³´ë‚´ê¸°\\n\\n### í˜„ì¬ìœ„ì¹˜\\n\\n### ì‚¼ì„±ì „ì, ìì²´ ê°œë°œí•œ ìƒì„±í˜• AI \\'ì‚¼ì„± ê°€ìš°ìŠ¤\\' ì²« ê³µê°œ\\n\\n#### SNS ê¸°ì‚¬ë³´ë‚´ê¸°\\n\\n#### SNS ê¸°ì‚¬ë³´ë‚´ê¸°\\n\\n#### \\'ì‚¼ì„± AI í¬ëŸ¼\\' 2ì¼ì°¨â€¦ì˜¨ë””ë°”ì´ìŠ¤ ê¸°ë°˜ AI ê¸°ìˆ ë„ ì†Œê°œ ì—…ë¬´ìƒì‚°ì„± í–¥ìƒâ€¦ë‚´ë…„ 1ì›” ê°¤ëŸ­ì‹œ S24ì— íƒ‘ì¬ ê°€ëŠ¥ì„±\\n\\n![ì‚¼ì„±ì „ìê°€ ìì²´ ê°œë°œí•œ ìƒì„±í˜• AI \\'ì‚¼ì„± ê°€ìš°ìŠ¤\\'. ì‚¬ì§„=ì‚¼ì„±ì „ì ì œê³µ](https://cdn.esgeconomy.com/news/photo/202311/5090_6771_5138.jpg)\\n\\n[ESGê²½ì œ=ê¹€ë„ì‚° ê¸°ì] ì‚¼ì„±ì „ìê°€ ìì²´ ê°œë°œí•œ ìƒì„±í˜• ì¸ê³µì§€ëŠ¥(AI) ëª¨ë¸ \\'ì‚¼ì„± ê°€ìš°ìŠ¤(Samsung Gauss)â€˜ë¥¼ ì²˜ìŒ ê³µê°œí–ˆë‹¤.\\n\\nì‚¼ì„±ì „ìëŠ” ì‚¼ì„± ê°€ìš°ìŠ¤ë¥¼ í™œìš©í•´ ì„ì§ì›ì˜ ì—…ë¬´ ìƒì‚°ì„±ì„ ë†’ì´ê³ , ìƒì„±í˜• AI ëª¨ë¸ì„ ë‹¨ê³„ì ìœ¼ë¡œ ì œí’ˆì— íƒ‘ì¬í•´ ìƒˆë¡œìš´ ì‚¬ìš©ì ê²½í—˜ì„ ì œê³µí•œë‹¤ëŠ” ê³„íšì´ë‹¤.\\n\\nì‚¼ì„±ì „ìëŠ” 8ì¼ ì‚¼ì„±ì „ì ì„œìš¸R&Dìº í¼ìŠ¤ì—ì„œ \\'ì‚¼ì„± AI í¬ëŸ¼ 2023\\' 2ì¼ì°¨ í–‰ì‚¬ë¥¼ ì—´ê³  ì‚¼ì„±ë¦¬ì„œì¹˜ì—ì„œ ê°œë°œí•œ ì‚¼ì„± ê°€ìš°ìŠ¤ë¥¼ ì²˜ìŒìœ¼ë¡œ ê³µê°œí–ˆë‹¤ê³  ë°í˜”ë‹¤.\\n\\nì‚¼ì„± ê°€ìš°ìŠ¤ëŠ” ë¨¸ì‹  ëŸ¬ë‹ê³¼ AI ê¸°ìˆ ì˜ ê·¼ë³¸ì´ ë˜ëŠ” ì •ê·œë¶„í¬ ì´ë¡ ì„ ì •ë¦½í•œ ë…ì¼ì˜ ì²œì¬ ìˆ˜í•™ìì´ì â€™ìˆ˜í•™ì˜ ì™•â€˜ìœ¼ë¡œ ë¶ˆë¦¬ëŠ” ì¹¼ í”„ë¦¬ë“œë¦¬íˆ ê°€ìš°ìŠ¤ì˜ ì´ë¦„ì„ ë•„ë‹¤. ì‚¼ì„± ê°€ìš°ìŠ¤ëŠ” ë¨¸ì‹  ëŸ¬ë‹ ê¸°ìˆ ì„ ê¸°ë°˜ìœ¼ë¡œ â–²ì–¸ì–´ ëª¨ë¸ â–²ì½”ë“œ ëª¨ë¸ â–²ì´ë¯¸ì§€ ëª¨ë¸ ë“± 3ê°€ì§€ ëª¨ë¸ë¡œ êµ¬ì„±ëë‹¤.\\n\\nì–¸ì–´ ëª¨ë¸ì€ í´ë¼ìš°ë“œì™€ ì˜¨ë””ë°”ì´ìŠ¤(On-device)ë¥¼ ìœ„í•œ ë‹¤ì–‘í•œ ëª¨ë¸ë¡œ ì´ë¤„ì¡Œë‹¤. ë©”ì¼ ì‘ì„±ê³¼ ë¬¸ì„œ ìš”ì•½, ë²ˆì—­ ë“±ì˜ ì—…ë¬´ë¥¼ ë” ì‰½ê³  ë¹ ë¥´ê²Œ ì²˜ë¦¬í•  ìˆ˜ ìˆê²Œ í•´ì£¼ë©° ê¸°ê¸°ë¥¼ ë”ìš± ìŠ¤ë§ˆíŠ¸í•˜ê²Œ ì œì–´í•´ ì†Œë¹„ì ê²½í—˜ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆë‹¤.\\n\\nì½”ë“œ ëª¨ë¸ì„ ê¸°ë°˜ìœ¼ë¡œ ê°œë°œëœ AI ì½”ë”© ì–´ì‹œìŠ¤í„´íŠ¸ \\'ì½”ë“œì•„ì´\\'(code.i)ëŠ” ì‚¬ë‚´ ì†Œí”„íŠ¸ì›¨ì–´ ê°œë°œì— ìµœì í™”ë¼ ê°œë°œìë“¤ì´ ì‰½ê³  ë¹ ë¥´ê²Œ ì½”ë”©í•  ìˆ˜ ìˆë„ë¡ ë„ì™€ì¤€ë‹¤. ëŒ€í™”í˜• ì¸í„°í˜ì´ìŠ¤ë¥¼ í†µí•´ ì½”ë“œ ì„¤ëª…ì´ë‚˜ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ìƒì„± ë“±ì˜ ì„œë¹„ìŠ¤ë„ ì œê³µí•œë‹¤.\\n\\nì´ë¯¸ì§€ ëª¨ë¸ì€ ì‚¬ì§„ì´ë‚˜ ê·¸ë¦¼ ë“± ì°½ì˜ì ì¸ ì´ë¯¸ì§€ë¥¼ ì†ì‰½ê²Œ ë§Œë“¤ê³  ê¸°ì¡´ ì´ë¯¸ì§€ë¥¼ ì›í•˜ëŠ” ëŒ€ë¡œ ë°”ê¿€ ìˆ˜ ìˆë„ë¡ ë„ì™€ì¤€ë‹¤. ì €í•´ìƒë„ ì´ë¯¸ì§€ì˜ ê³ í•´ìƒë„ ì „í™˜ë„ ì‰½ê²Œ ê°€ëŠ¥í•´ì§„ë‹¤.\\n\\nì‚¼ì„±ì „ìëŠ” í˜„ì¬ ì‚¬ë‚´ì—ì„œ ì‚¼ì„± ê°€ìš°ìŠ¤ì˜ ë² íƒ€ í…ŒìŠ¤íŠ¸ë¥¼ ì§„í–‰ ì¤‘ì´ë©°, ì—°ë‚´ì— ì„ì§ì›ì´ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ì˜¤í”ˆí•  ì˜ˆì •ì´ë‹¤.\\n\\nì‚¼ì„±ì „ìëŠ” ì±—GPT ë“± ì™¸ë¶€ ìƒì„±í˜• AI ì‚¬ìš© í™•ëŒ€ë¡œ ë³´ì•ˆ ìš°ë ¤ê°€ ì»¤ì§€ì. ê·¸ë™ì•ˆ ë²ˆì—­ê³¼ ë¬¸ì„œ ìš”ì•½ ë“± ì„ì§ì›ì˜ ìˆ˜ìš”ë¥¼ ê³ ë ¤í•´ ìì²´ ìƒì„±í˜• AIë¥¼ ê°œë°œí•´ ì™”ë‹¤.\\n\\nì‚¼ì„±ì „ìëŠ” ìƒì„±í˜• AI ëª¨ë¸ë“¤ì„ ë‹¤ì–‘í•œ ì œí’ˆì— ë‹¨ê³„ì ìœ¼ë¡œ íƒ‘ì¬í•  ê³„íšì¸ë°, ì—…ê³„ì—ì„œëŠ” ì´ë¥´ë©´ ë‚´ë…„ 1ì›” ê³µê°œí•  ê°¤ëŸ­ì‹œ S24ì— ìƒì„±í˜• AI ê¸°ëŠ¥ì´ íƒ‘ì¬ë  ê²ƒìœ¼ë¡œ ë³´ê³  ìˆë‹¤.\\n\\n![â€˜ì‚¼ì„± AI í¬ëŸ¼ 2023â€™ì„ ì•Œë¦¬ëŠ” í¬ìŠ¤í„°. ì‚¬ì§„=ì‚¼ì„±ì „ì ì œê³µ](https://cdn.esgeconomy.com/news/photo/202311/5090_6772_5210.jpg)\\n\\ní•œí¸ ì‚¼ì„±ë¦¬ì„œì¹˜ì—ì„œ ì£¼ê´€í•œ ì´ë‚  â€™\\'ì‚¼ì„± AI í¬ëŸ¼ 2023\\' í¬ëŸ¼ì—ëŠ” ì‚¼ì„±ì „ìì˜ AI ì—°êµ¬ ì¸ë ¥ê³¼ í•™ê³„Â·ì—…ê³„ AI ì „ë¬¸ê°€ 150ì—¬ëª…ì´ ì°¸ì„í•´ ìƒì„±í˜• AI ê¸°ìˆ ì˜ ë°œì „ ë°©í–¥ì„ ë…¼ì˜í•˜ê³  ìµœì‹  ê¸°ìˆ  ë™í–¥ì„ ê³µìœ í–ˆë‹¤. ìƒì„±í˜• AI ê¸°ìˆ ì´ ë°œì „í•˜ë©´ì„œ ì¸ê°„ì˜ ì‚¶ì´ ì–´ë–»ê²Œ ë³€í™”í• ì§€ì— ëŒ€í•œ ì‹¬ë„ ì‡ëŠ” ë…¼ì˜ë„ ì´ë¤„ì¡Œë‹¤.\\n\\nê¹€ëŒ€í˜„ ì‚¼ì„±ë¦¬ì„œì¹˜ ê¸€ë¡œë²ŒAIì„¼í„° ë¶€ì„¼í„°ì¥(ë¶€ì‚¬ì¥)ì€ \"ì‚¼ì„±ì „ìëŠ” AI ê¸°ìˆ  ê°œë°œë¿ë§Œ ì•„ë‹ˆë¼ AI í™œìš© ì‹œ ë³´ì•ˆê³¼ ì•ˆì „ì„±ì„ í™•ë³´í•˜ê¸° ìœ„í•´ \\'AI ìœ¤ë¦¬ ì›ì¹™\\'ì„ ì •í•´ ì´ë¥¼ ì¤€ìˆ˜í•˜ê¸° ìœ„í•´ ë…¸ë ¥í•˜ê³  ìˆë‹¤\"ë©° \"ìƒì„±í˜• AIì— ëŒ€í•œ ì§€ì†ì  ì—°êµ¬ë¥¼ í†µí•´ ì†Œë¹„ìì˜ ê²½í—˜ ê°€ì¹˜ë¥¼ ë†’ì—¬ ë‚˜ê°ˆ ê³„íš\"ì´ë¼ê³  ë§í–ˆë‹¤.\\n\\nì´ë‚  í¬ëŸ¼ì—ì„œëŠ” ë˜ \\'ìƒê°ì˜ ì‚¬ìŠ¬\\' ë…¼ë¬¸ì˜ ì €ìì´ê¸°ë„ í•œ ì˜¤í”ˆAIì˜ ì œì´ìŠ¨ ì›¨ì´ ì—°êµ¬ì›ì´ \\'ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ ë¥´ë„¤ìƒìŠ¤ì˜ ìƒˆë¡œìš´ íŒ¨ëŸ¬ë‹¤ì„\\'ì„ ì£¼ì œë¡œ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ë¡œ ì¸í•œ íŒ¨ëŸ¬ë‹¤ì„ ë³€í™”ì™€ AI ë¶„ì•¼ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì— ëŒ€í•´ ê°•ì—°í–ˆë‹¤.\\n\\n#### ê´€ë ¨ê¸°ì‚¬\\n\\n#### í‚¤ì›Œë“œ\\n\\n#### SNS ê¸°ì‚¬ë³´ë‚´ê¸°\\n\\n#### **ê¸°ì‚¬ ëŒ“ê¸€** 0\\n\\n##### ë¹„íšŒì› ë¡œê·¸ì¸\\n\\n##### ëŒ“ê¸€ì“°ê¸°\\n\\n## í•˜ë‹¨ì˜ì—­\\n\\n![ESGê²½ì œ](https://cdn.esgeconomy.com/image/logo/downlogo_20201106110340.png)\\n\\n### í•˜ë‹¨ë©”ë‰´\\n\\n### ë§¤ì²´ì •ë³´\\n\\n![NDì†Œí”„íŠ¸](https://cdn.esgeconomy.com/image/ndsoft.svg)\\n\\n## ì „ì²´ë©”ë‰´'}, {'url': 'http://www.seouleconews.com/news/articleView.html?idxno=80148', 'title': 'ì‚¼ì„±ì „ì \"AIë¡œ ìƒˆë¡œìš´ ì‹œëŒ€ ì´ëŒ ê²ƒâ€¦ìƒì„±í˜• AI ê¸°ëŠ¥ ë°œêµ´\"', 'content': '[ì„œìš¸ì´ì½”ë…¸ë¯¸ë‰´ìŠ¤ ì´ë³´ë¼ ê¸°ì]\\xa0ê¹€ëŒ€í˜„ ì‚¼ì„±ë¦¬ì„œì¹˜ ê¸€ë¡œë²Œ AIì„¼í„°ì¥ì€ 6ì¼ \"ì¸ê³µì§€ëŠ¥(AI)ì´ ê°€ì ¸ì˜¬ ë¯¸ë˜ìƒì€ ì‚¼ì„±ì „ìì˜ ì¤‘ìš”í•œ í™”ë‘\"ë¼ë©° \"ì¼ìƒì—ì„œ ëˆ„êµ¬ë‚˜ ì‰½ê²Œ ì ‘ê·¼, í™œìš©í•  ìˆ˜ ìˆëŠ” AIë¡œ ìƒˆë¡œìš´ ì‹œëŒ€ë¥¼ ì´ëŒ ê²ƒ\"ì´ë¼ê³  ë§í–ˆë‹¤. ì‚¼ì„±ì „ìëŠ” \\'ëª¨ë‘ë¥¼ ìœ„í•œ AI\\'(AI for All)ë¼ëŠ” ë¹„ì „ì„ ê°€ì§€ê³  TV, ê°€ì „, ìŠ¤ë§ˆíŠ¸í° ë“± ë‹¤ì–‘í•œ ì œí’ˆê³¼ ì„œë¹„ìŠ¤ì— AIë¥¼ ì ‘ëª©í•˜ê³  ìˆë‹¤. ê·¸ëŠ” \"ì‚¼ì„±ì „ìëŠ” ì‚¼ì„± ì œí’ˆê³¼ ì„œë¹„ìŠ¤ë¥¼ ë”ìš± ìœ ìš©í•˜ê²Œ ë§Œë“¤ ìˆ˜ ìˆëŠ” ìƒì„±í˜• AI ê¸°ëŠ¥ì„ ë°œêµ´í•˜ê³  ìµœì í™”í•˜ê³  ìˆë‹¤\"ë©° \"ë°ì´í„°ë¥¼ ë‹¨ìˆœ ê°€ê³µÂ·ë¶„ì„í•˜ëŠ” ê²ƒì—ì„œ ë‚˜ì•„ê°€\\xa0ì‚¬ìš©ì ìš”êµ¬ì— ë”°ë¼ ë…ì°½ì  ê²°ê³¼ë¥¼ ë§Œë“œëŠ” ìƒì„±í˜• AIë¡œ ìƒˆë¡œìš´ ê²½í—˜ì„ ì œê³µí•  ê²ƒ\"ì´ë¼ê³  ì„¤ëª…í–ˆë‹¤. ê¹€ ì„¼í„°ì¥ì€ ë˜ \\'ê°œì¸í™” ê²½í—˜ê³¼ ì‚¬ìš©ì„±ì„ ê°•í™”í•˜ëŠ” ìš´ì˜ê¸°ìˆ \\'ì— ëŒ€í•´ \"AI ê²½í—˜ì„ íš¨ìœ¨ì ìœ¼ë¡œ êµ¬í˜„í•˜ê¸° ìœ„í•´ í•˜ì´ë¸Œë¦¬ë“œ AIë¥¼ ì ìš©í•˜ê³  ìˆë‹¤\"ê³  í–ˆë‹¤. ê¹€ ì„¼í„°ì¥ì€ \"ì‚¼ì„±ì „ìëŠ” AI ê°œë°œì— ìˆì–´ ê°œì¸ì •ë³´ ë³´í˜¸ì™€ ë³´ì•ˆì„ ìµœìš°ì„ ìœ¼ë¡œ í•˜ê³  ìˆë‹¤\"ë©° \"ëª¨ë°”ì¼, TVì— ì´ì–´ í–¥í›„ ê°€ì „ì œí’ˆì—ë„ (ë…¹ìŠ¤ ë§¤íŠ¸ë¦­ìŠ¤ë¥¼) í™•ëŒ€ ì ìš©í•  ê³„íš\"ì´ë¼ê³  ë°í˜”ë‹¤.', 'score': 0.81770587, 'raw_content': 'ì‚¼ì„±ì „ì \"AIë¡œ ìƒˆë¡œìš´ ì‹œëŒ€ ì´ëŒ ê²ƒâ€¦ìƒì„±í˜• AI ê¸°ëŠ¥ ë°œêµ´\"\\n\\n\\n\\n\\n\\n\\n\\n\\nì‚¼ì„±ì „ì \"AIë¡œ ìƒˆë¡œìš´ ì‹œëŒ€ ì´ëŒ ê²ƒâ€¦ìƒì„±í˜• AI ê¸°ëŠ¥ ë°œêµ´\"\\n\\n\\n\\n\\n* ì´ë³´ë¼ ê¸°ì\\n* ìŠ¹ì¸ 2024.11.06 14:14\\n* ëŒ“ê¸€\\n  0\\n\\n* íŠ¸ìœ„í„°\\n* í˜ì´ìŠ¤ë¶\\n* ì¹´ì¹´ì˜¤ìŠ¤í† ë¦¬\\n\\n\\n\\n\\n---\\n\\n\\n\\nê¹€ëŒ€í˜„ ì‚¼ì„±ë¦¬ì„œì¹˜ ê¸€ë¡œë²Œ AIì„¼í„°ì¥ ë‰´ìŠ¤ë£¸ ê¸°ê³ ë¬¸\\n\\n[ì„œìš¸ì´ì½”ë…¸ë¯¸ë‰´ìŠ¤ ì´ë³´ë¼ ê¸°ì]\\xa0ê¹€ëŒ€í˜„ ì‚¼ì„±ë¦¬ì„œì¹˜ ê¸€ë¡œë²Œ AIì„¼í„°ì¥ì€ 6ì¼ \"ì¸ê³µì§€ëŠ¥(AI)ì´ ê°€ì ¸ì˜¬ ë¯¸ë˜ìƒì€ ì‚¼ì„±ì „ìì˜ ì¤‘ìš”í•œ í™”ë‘\"ë¼ë©° \"ì¼ìƒì—ì„œ ëˆ„êµ¬ë‚˜ ì‰½ê²Œ ì ‘ê·¼, í™œìš©í•  ìˆ˜ ìˆëŠ” AIë¡œ ìƒˆë¡œìš´ ì‹œëŒ€ë¥¼ ì´ëŒ ê²ƒ\"ì´ë¼ê³  ë§í–ˆë‹¤.\\n\\nê¹€ ì„¼í„°ì¥ì€ 6ì¼ ì‚¼ì„±ì „ì ë‰´ìŠ¤ë£¸ ê¸°ê³ ë¬¸ì„ í†µí•´ \"AIëŠ” ì¸í„°ë„· í˜ëª…, ëª¨ë°”ì¼ í˜ëª…ì— ì´ì–´ ìš°ë¦¬ ì‹œëŒ€ì˜ íŒ¨ëŸ¬ë‹¤ì„ì„ ë°”ê¿€ í•µì‹¬ê¸°ìˆ \"ì´ë¼ë©° ì´ê°™ì´ ë°í˜”ë‹¤.\\n\\nì‚¼ì„±ì „ìëŠ” \\'ëª¨ë‘ë¥¼ ìœ„í•œ AI\\'(AI for All)ë¼ëŠ” ë¹„ì „ì„ ê°€ì§€ê³  TV, ê°€ì „, ìŠ¤ë§ˆíŠ¸í° ë“± ë‹¤ì–‘í•œ ì œí’ˆê³¼ ì„œë¹„ìŠ¤ì— AIë¥¼ ì ‘ëª©í•˜ê³  ìˆë‹¤.\\n\\nê¹€ ì„¼í„°ì¥ì€ ì¼ìƒì† ë§ì¶¤í˜• AIë¥¼ ìœ„í•œ ê¸°ìˆ í˜ì‹  ë°©ì•ˆìœ¼ë¡œ ìƒì„±í˜• AIì™€ ìš´ì˜ê¸°ìˆ , ë³´ì•ˆê¸°ìˆ  ë“±ì„ ì œì‹œí–ˆë‹¤.\\n\\nê·¸ëŠ” \"ì‚¼ì„±ì „ìëŠ” ì‚¼ì„± ì œí’ˆê³¼ ì„œë¹„ìŠ¤ë¥¼ ë”ìš± ìœ ìš©í•˜ê²Œ ë§Œë“¤ ìˆ˜ ìˆëŠ” ìƒì„±í˜• AI ê¸°ëŠ¥ì„ ë°œêµ´í•˜ê³  ìµœì í™”í•˜ê³  ìˆë‹¤\"ë©° \"ë°ì´í„°ë¥¼ ë‹¨ìˆœ ê°€ê³µÂ·ë¶„ì„í•˜ëŠ” ê²ƒì—ì„œ ë‚˜ì•„ê°€\\xa0ì‚¬ìš©ì ìš”êµ¬ì— ë”°ë¼ ë…ì°½ì  ê²°ê³¼ë¥¼ ë§Œë“œëŠ” ìƒì„±í˜• AIë¡œ ìƒˆë¡œìš´ ê²½í—˜ì„ ì œê³µí•  ê²ƒ\"ì´ë¼ê³  ì„¤ëª…í–ˆë‹¤.\\n\\níŠ¹íˆ ê°œì¸í™” AIì˜ ì£¼ìš”ê¸°ìˆ  ì¤‘ í•˜ë‚˜ì¸ ì§€ì‹ê·¸ë˜í”„ ê¸°ìˆ ì„ ê°œë°œí•˜ê³ , ìƒì„±í˜• AIì™€ ìœ ê¸°ì ìœ¼ë¡œ ì—°ê²°í•´ ì‚¬ìš©ì ë§ì¶¤í˜• ì„œë¹„ìŠ¤ë¥¼ ì§€ì›í•œë‹¤ëŠ” ê³„íšì´ë‹¤.\\n\\nì•ì„œ ì‚¼ì„±ì „ìëŠ” ì‚¼ì„±ë¦¬ì„œì¹˜ ì£¼ë„ë¡œ ì§€ë‚œ 7ì›” ì„¸ê³„ ìµœê³  ìˆ˜ì¤€ì˜ ì§€ì‹ê·¸ë˜í”„ ì›ì²œê¸°ìˆ ì„ ë³´ìœ í•˜ê³  ìˆëŠ” ì˜êµ­ ìŠ¤íƒ€íŠ¸ì—…ì¸ \\'ì˜¥ìŠ¤í¼ë“œ ì‹œë©˜í‹± í…Œí¬ë†€ë¡œì§€ìŠ¤\\'(OST)ë¥¼ ì¸ìˆ˜í•œ ë°” ìˆë‹¤.\\n\\nì§€ì‹ê·¸ë˜í”„ëŠ” ê´€ë ¨ìˆëŠ” ì •ë³´ë¥¼ ì„œë¡œ ì—°ê²°ëœ ê·¸ë˜í”„ í˜•íƒœë¡œ í‘œí˜„í•´ ì£¼ëŠ” ê¸°ìˆ ì´ë‹¤. ì‚¼ì„±ì „ìëŠ” ì˜¨ë””ë°”ì´ìŠ¤ AIì™€ ê²°í•©í•´ ì°¨ë³„í™”ëœ ê°œì¸í™” AI ê²½í—˜ì„ ì œê³µí•œë‹¤ëŠ” ì „ëµì´ë‹¤.\\n\\nê¹€ ì„¼í„°ì¥ì€ ë˜ \\'ê°œì¸í™” ê²½í—˜ê³¼ ì‚¬ìš©ì„±ì„ ê°•í™”í•˜ëŠ” ìš´ì˜ê¸°ìˆ \\'ì— ëŒ€í•´ \"AI ê²½í—˜ì„ íš¨ìœ¨ì ìœ¼ë¡œ êµ¬í˜„í•˜ê¸° ìœ„í•´ í•˜ì´ë¸Œë¦¬ë“œ AIë¥¼ ì ìš©í•˜ê³  ìˆë‹¤\"ê³  í–ˆë‹¤.\\n\\ní•˜ì´ë¸Œë¦¬ë“œ AIëŠ” ì˜¨ë””ë°”ì´ìŠ¤ AIì™€ í´ë¼ìš°ë“œ AIë¥¼ í•¨ê»˜ ì‚¬ìš©í•´ ì†ë„ì™€ ì•ˆì „ì„±ì„ ê· í˜•ì ìœ¼ë¡œ ì œê³µí•˜ëŠ” ê¸°ìˆ ì´ë‹¤.\\n\\në³´ì•ˆì„¤ë£¨ì…˜ \\'ë…¹ìŠ¤ ë§¤íŠ¸ë¦­ìŠ¤\\'ë¥¼ ê°œë°œí•˜ëŠ” ë“± AI ì„œë¹„ìŠ¤ë¥¼ ìœ„í•œ ë³´ì•ˆë„ ë”ìš± ê°•í™”í•˜ëŠ” ì¤‘ì´ë‹¤.\\xa0 ê¹€ ì„¼í„°ì¥ì€ \"ì‚¼ì„±ì „ìëŠ” AI ê°œë°œì— ìˆì–´ ê°œì¸ì •ë³´ ë³´í˜¸ì™€ ë³´ì•ˆì„ ìµœìš°ì„ ìœ¼ë¡œ í•˜ê³  ìˆë‹¤\"ë©° \"ëª¨ë°”ì¼, TVì— ì´ì–´ í–¥í›„ ê°€ì „ì œí’ˆì—ë„ (ë…¹ìŠ¤ ë§¤íŠ¸ë¦­ìŠ¤ë¥¼) í™•ëŒ€ ì ìš©í•  ê³„íš\"ì´ë¼ê³  ë°í˜”ë‹¤.\\n\\nì‚¼ì„±ì „ìëŠ” ì§€ë‚œ 9ì›” ì¶œë²”í•œ êµ­ê°€ì¸ê³µì§€ëŠ¥ìœ„ì›íšŒì— ì°¸ê°€í•´ ì •ë¶€ì™€ í•¨ê»˜ êµ­ê°€ AI ê²½ìŸë ¥ ê°•í™”ì—ë„ ë‚˜ì„œê³  ìˆë‹¤.\\n\\nì €ì‘ê¶Œì Â© ì„œìš¸ì´ì½”ë…¸ë¯¸ë‰´ìŠ¤ ë¬´ë‹¨ì „ì¬ ë° ì¬ë°°í¬ ê¸ˆì§€\\n\\n[**ì´ë³´ë¼ ê¸°ì**](mailto:seouleconews@naver.com)\\n[ë‹¤ë¥¸ê¸°ì‚¬ ë³´ê¸°](/news/articleList.html?sc_area=I&sc_word=newsfc12)\\n\\n\\n\\n---\\n\\n\\n\\n\\n\\n\\n**ëŒ“ê¸€**\\n0 \\n\\n\\n\\n\\n\\n[ìµœì‹ ìˆœ](/news/articleView.html?idxno=80148&replyAll=&reply_sc_order_by=I#reply)\\n [ì¶”ì²œìˆœ](/news/articleView.html?idxno=80148&replyAll=&reply_sc_order_by=C#reply)\\n\\n\\n\\n---\\n\\nì´ ê¸°ì‚¬ë¥¼ ê³µìœ í•©ë‹ˆë‹¤\\n\\nëŒ“ê¸€ì‚­ì œ\\n\\nì‚­ì œí•œ ëŒ“ê¸€ì€ ë‹¤ì‹œ ë³µêµ¬í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.  \\nê·¸ë˜ë„ ì‚­ì œí•˜ì‹œê² ìŠµë‹ˆê¹Œ?\\n\\nê¸°ì‚¬ê³µìœ í•˜ê¸°\\n\\ní”„ë¦°íŠ¸\\n\\në©”ì¼ë³´ë‚´ê¸°'}]\n"
     ]
    }
   ],
   "source": [
    "# ì›¹ ê²€ìƒ‰ ë„êµ¬ ì‹¤í–‰\n",
    "results = web_search_tool.invoke({\"query\": question})\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, List\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "\n",
    "# ìƒíƒœ ì •ì˜\n",
    "class GraphState(TypedDict):\n",
    "    question: Annotated[str, \"The question to answer\"]\n",
    "    generation: Annotated[str, \"The generation from the LLM\"]\n",
    "    web_search: Annotated[str, \"Whether to add search\"]\n",
    "    documents: Annotated[List[str], \"The documents retrieved\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "\n",
    "# ë¬¸ì„œ ê²€ìƒ‰ ë…¸ë“œ\n",
    "def retrieve(state: GraphState):\n",
    "    print(\"\\n==== RETRIEVE ====\\n\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # ë¬¸ì„œ ê²€ìƒ‰ ìˆ˜í–‰\n",
    "    documents = pdf_retriever.invoke(question)\n",
    "    return {\"documents\": documents}\n",
    "\n",
    "\n",
    "# ë‹µë³€ ìƒì„± ë…¸ë“œ\n",
    "def generate(state: GraphState):\n",
    "    print(\"\\n==== GENERATE ====\\n\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # RAGë¥¼ ì‚¬ìš©í•œ ë‹µë³€ ìƒì„±\n",
    "    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
    "    return {\"generation\": generation}\n",
    "\n",
    "\n",
    "# ë¬¸ì„œ í‰ê°€ ë…¸ë“œ\n",
    "def grade_documents(state: GraphState):\n",
    "    print(\"\\n==== [CHECK DOCUMENT RELEVANCE TO QUESTION] ====\\n\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # í•„í„°ë§ëœ ë¬¸ì„œ\n",
    "    filtered_docs = []\n",
    "    relevant_doc_count = 0\n",
    "\n",
    "    for d in documents:\n",
    "        # Question-Document ì˜ ê´€ë ¨ì„± í‰ê°€\n",
    "        score = retrieval_grader.invoke(\n",
    "            {\"question\": question, \"document\": d.page_content}\n",
    "        )\n",
    "        grade = score.binary_score\n",
    "\n",
    "        if grade == \"yes\":\n",
    "            print(\"==== [GRADE: DOCUMENT RELEVANT] ====\")\n",
    "            # ê´€ë ¨ ìˆëŠ” ë¬¸ì„œë¥¼ filtered_docs ì— ì¶”ê°€\n",
    "            filtered_docs.append(d)\n",
    "            relevant_doc_count += 1\n",
    "        else:\n",
    "            print(\"==== [GRADE: DOCUMENT NOT RELEVANT] ====\")\n",
    "            continue\n",
    "\n",
    "    # ê´€ë ¨ ë¬¸ì„œê°€ ì—†ìœ¼ë©´ ì›¹ ê²€ìƒ‰ ìˆ˜í–‰\n",
    "    web_search = \"Yes\" if relevant_doc_count == 0 else \"No\"\n",
    "    return {\"documents\": filtered_docs, \"web_search\": web_search}\n",
    "\n",
    "\n",
    "# ì¿¼ë¦¬ ì¬ì‘ì„± ë…¸ë“œ\n",
    "def query_rewrite(state: GraphState):\n",
    "    print(\"\\n==== [REWRITE QUERY] ====\\n\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # ì§ˆë¬¸ ì¬ì‘ì„±\n",
    "    better_question = question_rewriter.invoke({\"question\": question})\n",
    "    return {\"question\": better_question}\n",
    "\n",
    "\n",
    "# ì›¹ ê²€ìƒ‰ ë…¸ë“œ\n",
    "def web_search(state: GraphState):\n",
    "    print(\"\\n==== [WEB SEARCH] ====\\n\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # ì›¹ ê²€ìƒ‰ ìˆ˜í–‰\n",
    "    docs = web_search_tool.invoke({\"query\": question})\n",
    "    # ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë¬¸ì„œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜\n",
    "    web_results = \"\\n\".join([d[\"content\"] for d in docs])\n",
    "    web_results = Document(page_content=web_results)\n",
    "    documents.append(web_results)\n",
    "\n",
    "    return {\"documents\": documents}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decide_to_generate(state: GraphState):\n",
    "    # í‰ê°€ëœ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ìŒ ë‹¨ê³„ ê²°ì •\n",
    "    print(\"==== [ASSESS GRADED DOCUMENTS] ====\")\n",
    "    # ì›¹ ê²€ìƒ‰ í•„ìš” ì—¬ë¶€\n",
    "    web_search = state[\"web_search\"]\n",
    "\n",
    "    if web_search == \"Yes\":\n",
    "        # ì›¹ ê²€ìƒ‰ìœ¼ë¡œ ì •ë³´ ë³´ê°•ì´ í•„ìš”í•œ ê²½ìš°\n",
    "        print(\n",
    "            \"==== [DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, QUERY REWRITE] ====\"\n",
    "        )\n",
    "        # ì¿¼ë¦¬ ì¬ì‘ì„± ë…¸ë“œë¡œ ë¼ìš°íŒ…\n",
    "        return \"query_rewrite\"\n",
    "    else:\n",
    "        # ê´€ë ¨ ë¬¸ì„œê°€ ì¡´ì¬í•˜ë¯€ë¡œ ë‹µë³€ ìƒì„± ë‹¨ê³„(generate) ë¡œ ì§„í–‰\n",
    "        print(\"==== [DECISION: GENERATE] ====\")\n",
    "        return \"generate\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph, START\n",
    "\n",
    "# ê·¸ë˜í”„ ìƒíƒœ ì´ˆê¸°í™”\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# ë…¸ë“œ ì •ì˜\n",
    "workflow.add_node(\"retrieve\", retrieve)\n",
    "workflow.add_node(\"grade_documents\", grade_documents)\n",
    "workflow.add_node(\"generate\", generate)\n",
    "workflow.add_node(\"query_rewrite\", query_rewrite)\n",
    "workflow.add_node(\"web_search_node\", web_search)\n",
    "\n",
    "# ì—£ì§€ ì—°ê²°\n",
    "workflow.add_edge(START, \"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "\n",
    "# ë¬¸ì„œ í‰ê°€ ë…¸ë“œì—ì„œ ì¡°ê±´ë¶€ ì—£ì§€ ì¶”ê°€\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"query_rewrite\": \"query_rewrite\",\n",
    "        \"generate\": \"generate\",\n",
    "    },\n",
    ")\n",
    "\n",
    "# ì—£ì§€ ì—°ê²°\n",
    "workflow.add_edge(\"query_rewrite\", \"web_search_node\")\n",
    "workflow.add_edge(\"web_search_node\", \"generate\")\n",
    "workflow.add_edge(\"generate\", END)\n",
    "\n",
    "# ê·¸ë˜í”„ ì»´íŒŒì¼\n",
    "app = workflow.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_teddynote.graphs import visualize_graph\n",
    "\n",
    "visualize_graph(app)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== RETRIEVE ====\n",
      "\n",
      "\n",
      "==== [CHECK DOCUMENT RELEVANCE TO QUESTION] ====\n",
      "\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mgrade_documents\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "{\"binary_score\":\"yes\"}==== [GRADE: DOCUMENT RELEVANT] ====\n",
      "{\"binary_score\":\"yes\"}==== [GRADE: DOCUMENT RELEVANT] ====\n",
      "{\"binary_score==== [GRADE: DOCUMENT RELEVANT] ====\n",
      "\":\"yes\"}{\"binary_score\":\"yes\"}==== [GRADE: DOCUMENT RELEVANT] ====\n",
      "{\"binary_score\":\"no\"}==== [GRADE: DOCUMENT NOT RELEVANT] ====\n",
      "{\"binary_score\":\"no\"}==== [GRADE: DOCUMENT NOT RELEVANT] ====\n",
      "{\"binary_score\":\"yes\"}==== [GRADE: DOCUMENT RELEVANT] ====\n",
      "{\"binary_score\":\"yes\"}==== [GRADE: DOCUMENT RELEVANT] ====\n",
      "{\"binary_score\":\"no\"}==== [GRADE: DOCUMENT NOT RELEVANT] ====\n",
      "{\"binary_score\":\"no\"}==== [GRADE: DOCUMENT NOT RELEVANT] ====\n",
      "==== [ASSESS GRADED DOCUMENTS] ====\n",
      "==== [DECISION: GENERATE] ====\n",
      "\n",
      "==== GENERATE ====\n",
      "\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mgenerate\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "ì‚¼ì„±ì „ìê°€ ê°œë°œí•œ ìƒì„±í˜• AIì˜ ì´ë¦„ì€ â€˜ì‚¼ì„± ê°€ìš°ìŠ¤â€™ì´ë‹¤.\n",
      "\n",
      "**Source**\n",
      "- data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf (p.12)"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_teddynote.messages import stream_graph, invoke_graph, random_uuid\n",
    "\n",
    "# config ì„¤ì •(ì¬ê·€ ìµœëŒ€ íšŸìˆ˜, thread_id)\n",
    "config = RunnableConfig(recursion_limit=20, configurable={\"thread_id\": random_uuid()})\n",
    "\n",
    "# ì§ˆë¬¸ ì…ë ¥\n",
    "inputs = {\n",
    "    \"question\": \"ì‚¼ì„±ì „ìê°€ ê°œë°œí•œ ìƒì„±í˜• AI ì˜ ì´ë¦„ì€?\",\n",
    "}\n",
    "\n",
    "# ìŠ¤íŠ¸ë¦¬ë° í˜•ì‹ìœ¼ë¡œ ê·¸ë˜í”„ ì‹¤í–‰\n",
    "stream_graph(\n",
    "    app,\n",
    "    inputs,\n",
    "    config,\n",
    "    [\"retrieve\", \"grade_documents\", \"query_rewrite\", \"web_search_node\", \"generate\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config ì„¤ì •(ì¬ê·€ ìµœëŒ€ íšŸìˆ˜, thread_id)\n",
    "config = RunnableConfig(recursion_limit=20, configurable={\"thread_id\": random_uuid()})\n",
    "\n",
    "# ì§ˆë¬¸ ì…ë ¥\n",
    "inputs = {\n",
    "    \"question\": \"2024ë…„ ë…¸ë²¨ë¬¸í•™ìƒ ìˆ˜ìƒìì˜ ì´ë¦„ì€?\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== RETRIEVE ====\n",
      "\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mretrieve\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "page_content='CES 2024\n",
      "ì´ë²ˆ ì „ì‹œì—ëŠ” 500ê³³ ì´ìƒì˜ í•œêµ­ê¸°ì—… ì°¸ê°€ ì˜ˆì •\n",
      "ê¸°ê°„ ì¥ì†Œ í™ˆí˜ì´ì§€\n",
      "2024.1.9~12 ë¯¸êµ­, ë¼ìŠ¤ë² ê°€ìŠ¤ https://www.ces.tech/\n",
      "- ë¨¸ì‹ ëŸ¬ë‹ ë° ì‘ìš©ì— ê´€í•œ êµ­ì œ ì»¨í¼ëŸ°ìŠ¤(AIMLA 2024)ëŠ”\n",
      "ì¸ê³µì§€ëŠ¥ ë° ë¨¸ì‹ ëŸ¬ë‹ì˜ ì´ë¡ , ë°©ë²•ë¡  ë° ì‹¤ìš©ì  ì ‘ê·¼ì— ê´€í•œ\n",
      "ì§€ì‹ê³¼ ìµœì‹  ì—°êµ¬ ê²°ê³¼ ê³µìœ \n",
      "- ì´ë¡  ë° ì‹¤ë¬´ ì¸¡ë©´ì—ì„œ ì¸ê³µì§€ëŠ¥, ê¸°ê³„í•™ìŠµì˜ ì£¼ìš” ë¶„ì•¼ë¥¼\n",
      "ë…¼ì˜í•˜ê³ , í•™ê³„, ì‚°ì—…ê³„ì˜ ì—°êµ¬ìì™€ ì‹¤ë¬´ìë“¤ì—ê²Œ í•´ë‹¹ ë¶„\n",
      "AIMLA 2024\n",
      "ì•¼ì˜ ìµœì²¨ë‹¨ ê°œë°œ ì†Œì‹ ê³µìœ \n",
      "ê¸°ê°„ ì¥ì†Œ í™ˆí˜ì´ì§€' metadata={'source': 'data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf', 'file_path': 'data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf', 'page': 21, 'total_pages': 23, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13462', 'Producer': 'Hancom PDF 1.3.0.542', 'CreationDate': \"D:20231208132838+09'00'\", 'ModDate': \"D:20231208132838+09'00'\", 'PDFVersion': '1.4'}\n",
      "page_content='n ì°¸ê°€êµ­ë“¤ì€ íŠœë§ìƒì„ ìˆ˜ìƒí•œ AI í•™ìì¸ ìš”ìŠˆì•„ ë²¤ì§€ì˜¤ êµìˆ˜ê°€ ì£¼ë„í•˜ëŠ” â€˜ê³¼í•™ì˜ í˜„í™©(State of\n",
      "the Science)â€™ ë³´ê³ ì„œ ì‘ì„±ì—ë„ í•©ì˜í–ˆìœ¼ë©°, ë³´ê³ ì„œë¥¼ í†µí•´ ì²¨ë‹¨ AIì˜ ìœ„í—˜ê³¼ ê°€ëŠ¥ì„±ì— ê´€í•œ\n",
      "ê¸°ì¡´ ì—°êµ¬ë¥¼ ê³¼í•™ì ìœ¼ë¡œ í‰ê°€í•˜ê³  í–¥í›„ AI ì•ˆì „ ì—°êµ¬ë¥¼ ìœ„í•œ ìš°ì„ ìˆœìœ„ë¥¼ ì œì‹œí•  ê³„íš\n",
      "n í•œêµ­ì€ ì˜êµ­ ì •ë¶€ì™€ 6ê°œì›” ë’¤ì— ì˜¨ë¼ì¸ìœ¼ë¡œ AI ë¯¸ë‹ˆ ì •ìƒíšŒì˜ë¥¼ ê³µë™ ê°œìµœí•˜ê¸°ë¡œ í•©ì˜í–ˆìœ¼ë©°,\n",
      "í”„ë‘ìŠ¤ ì •ë¶€ì™€ëŠ” 1ë…„ í›„ ëŒ€ë©´ ì •ìƒíšŒì˜ë¥¼ ê°œìµœí•  ì˜ˆì •' metadata={'source': 'data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf', 'file_path': 'data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf', 'page': 5, 'total_pages': 23, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13462', 'Producer': 'Hancom PDF 1.3.0.542', 'CreationDate': \"D:20231208132838+09'00'\", 'ModDate': \"D:20231208132838+09'00'\", 'PDFVersion': '1.4'}\n",
      "page_content='Â£ì˜ˆìˆ ê°€ë“¤ì˜ AI ì €ì‘ê¶Œ ì¹¨í•´ ì†Œì†¡, ì €ì‘ê¶Œ ë¯¸ë“±ë¡ê³¼ ì¦ê±°ë¶ˆì¶©ë¶„ìœ¼ë¡œ ê¸°ê°\n",
      "n ë¯¸êµ­ ìº˜ë¦¬í¬ë‹ˆì•„ ë¶ë¶€ì§€ë°©ë²•ì›ì˜ ìœŒë¦¬ì—„ ì˜¤ë¦­(William Orrick) íŒì‚¬ëŠ” 2023ë…„ 10ì›” 30ì¼ ë¯¸ë“œì €ë‹ˆ\n",
      "(Midjourney), ìŠ¤íƒœë¹Œë¦¬í‹°AI(Stability AI), ë””ë¹„ì–¸íŠ¸ì•„íŠ¸(DeviantArt)ì— ì œê¸°ëœ ì €ì‘ê¶Œ ì¹¨í•´ ì†Œì†¡ì„ ê¸°ê°\n",
      "âˆ™ 2023ë…„ 1ì›” ì˜ˆìˆ ê°€ ì‚¬ë¼ ì•¤ë”ìŠ¨(Sarah Anderson), ìº˜ë¦¬ ë§¥ì»¤ë„Œ(Kelly McKernan), ì¹¼ë¼' metadata={'source': 'data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf', 'file_path': 'data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf', 'page': 6, 'total_pages': 23, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13462', 'Producer': 'Hancom PDF 1.3.0.542', 'CreationDate': \"D:20231208132838+09'00'\", 'ModDate': \"D:20231208132838+09'00'\", 'PDFVersion': '1.4'}\n",
      "page_content='Â£AI ê¸°ìˆ  ì¤‘ ë¨¸ì‹ ëŸ¬ë‹, í…ì„œí”Œë¡œìš°, ë”¥ëŸ¬ë‹ì˜ ì„ê¸ˆ í”„ë¦¬ë¯¸ì—„ì´ ë†’ê²Œ í‰ê°€\n",
      "n ì˜¥ìŠ¤í¼ë“œ ì¸í„°ë„· ì—°êµ¬ì†Œ(Oxford Internet Institute)ê°€ 2023ë…„ 10ì›” 24ì¼ 962ê°œ ê¸°ìˆ ê³¼ 2ë§Œ 5ì²œ\n",
      "ëª…ì„ ëŒ€ìƒìœ¼ë¡œ í•œ ì—°êµ¬ì—ì„œ AIë¥¼ í¬í•¨í•œ ì£¼ìš” ê¸°ìˆ ì˜ ê²½ì œì  ê°€ì¹˜ë¥¼ ë¶„ì„í•œ ê²°ê³¼ë¥¼ ë°œí‘œ\n",
      "âˆ™ ì—°êµ¬ì— ë”°ë¥´ë©´ í•œ ê¸°ìˆ ì˜ ê²½ì œì  ê°€ì¹˜ëŠ” ê·¼ë¡œìì˜ ì—¬íƒ€ ì—­ëŸ‰ê³¼ ì–¼ë§ˆë‚˜ ì˜ ê²°í•©í•˜ëŠ”ì§€ë¥¼ ë³´ì—¬ì£¼ëŠ”\n",
      "â€˜ìƒë³´ì„±(complementarity)â€™ì— ë”°ë¼ ê²°ì •ë¨' metadata={'source': 'data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf', 'file_path': 'data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf', 'page': 20, 'total_pages': 23, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13462', 'Producer': 'Hancom PDF 1.3.0.542', 'CreationDate': \"D:20231208132838+09'00'\", 'ModDate': \"D:20231208132838+09'00'\", 'PDFVersion': '1.4'}\n",
      "page_content='ì˜¤ë¥´í‹°ì¦ˆ(Karla Ortiz)ëŠ” ì´ë¯¸ì§€ ìƒì„± AI ì„œë¹„ìŠ¤ë¥¼ ê°œë°œí•œ 3ê°œ ê¸°ì—…ì„ ìƒëŒ€ë¡œ ì €ì‘ê¶Œ ì¹¨í•´ ì†Œì†¡ì„ ì œê¸°\n",
      "âˆ™ ì˜ˆìˆ ê°€ë“¤ì€ 3ê°œ ê¸°ì—…ì´ AI ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ê¸° ìœ„í•´ ì›ì‘ì ë™ì˜ ì—†ì´ ì‘í’ˆì„ í•™ìŠµ ë°ì´í„°ì…‹ì—\n",
      "í¬í•¨í•˜ì—¬ ì €ì‘ê¶Œì„ ì¹¨í•´í–ˆë‹¤ê³  ì£¼ì¥í–ˆìœ¼ë©°, ë²•ì›ì€ ì§€ë‚œ 4ì›” í”¼ì†Œ ê¸°ì—…ë“¤ì´ ì œì¶œí•œ ê¸°ê° ì‹ ì²­ì„\n",
      "ìˆ˜ìš©í•´ ì†Œì†¡ì„ ê¸°ê°\n",
      "n ì˜¤ë¦­ íŒì‚¬ëŠ” íŒê²°ë¬¸ì—ì„œ ì†Œì†¡ì„ ê¸°ê°í•œ í•µì‹¬ ì´ìœ ë¡œ ì˜ˆìˆ ê°€ë“¤ì˜ ì €ì‘ê¶Œ ë¯¸ë“±ë¡ì„ ì œì‹œ\n",
      "âˆ™ íŒê²°ë¬¸ì€ ì†Œì†¡ì„ ì œê¸°í•œ ìº˜ë¦¬ ë§¥ì»¤ë„Œê³¼ ì¹¼ë¼ ì˜¤ë¥´í‹°ì¦ˆê°€ ë¯¸êµ­ ì €ì‘ê¶Œì²­ì— ì˜ˆìˆ  ì‘í’ˆì— ëŒ€í•œ ì €ì‘ê¶Œì„' metadata={'source': 'data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf', 'file_path': 'data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf', 'page': 6, 'total_pages': 23, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13462', 'Producer': 'Hancom PDF 1.3.0.542', 'CreationDate': \"D:20231208132838+09'00'\", 'ModDate': \"D:20231208132838+09'00'\", 'PDFVersion': '1.4'}\n",
      "page_content='â–¹ êµ¬ê¸€ ë”¥ë§ˆì¸ë“œ, ë²”ìš© AI ëª¨ë¸ì˜ ê¸°ëŠ¥ê³¼ ë™ì‘ì— ëŒ€í•œ ë¶„ë¥˜ ì²´ê³„ ë°œí‘œÂ·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·16\n",
      "â–¹ ê°ˆë¦´ë ˆì˜¤ì˜ LLM í™˜ê° ì§€ìˆ˜ í‰ê°€ì—ì„œ GPT-4ê°€ ê°€ì¥ ìš°ìˆ˜ Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·17\n",
      "4. ì¸ë ¥/êµìœ¡\n",
      "â–¹ ì˜êµ­ ì˜¥ìŠ¤í¼ë“œ ì¸í„°ë„· ì—°êµ¬ì†Œ, AI ê¸°ìˆ ìì˜ ì„ê¸ˆì´ í‰ê·  21% ë†’ì•„Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·18\n",
      "â…¡\n",
      ". ì£¼ìš” í–‰ì‚¬' metadata={'source': 'data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf', 'file_path': 'data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf', 'page': 1, 'total_pages': 23, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13462', 'Producer': 'Hancom PDF 1.3.0.542', 'CreationDate': \"D:20231208132838+09'00'\", 'ModDate': \"D:20231208132838+09'00'\", 'PDFVersion': '1.4'}\n",
      "page_content='ì†Œì†¡ì€ ì¸ì •ë˜ì–´ ê³„ì† ì§„í–‰ë¨\n",
      "â˜ ì¶œì²˜: Venturebeat, Midjourney, Stability AI and DeviantArt win a victory in copyright case by artists- but the\n",
      "fight continues, 2023.10.30.\n",
      "4' metadata={'source': 'data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf', 'file_path': 'data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf', 'page': 6, 'total_pages': 23, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13462', 'Producer': 'Hancom PDF 1.3.0.542', 'CreationDate': \"D:20231208132838+09'00'\", 'ModDate': \"D:20231208132838+09'00'\", 'PDFVersion': '1.4'}\n",
      "page_content='ë‹¬ ì•ˆì— ê¸°ê¸ˆ ì§€ì›ì„ ìœ„í•œ ì œì•ˆ ìš”ì²­ì„ ë°›ì„ ê³„íš\n",
      "n í”„ëŸ°í‹°ì–´ ëª¨ë¸ í¬ëŸ¼ì€ ì¶œë²” ì´í›„ ì—…ê³„ ì „ë°˜ì— ê±¸ì³ AI ë ˆë“œíŒ€ êµ¬ì„±ì— ê´€í•œ ëª¨ë²”ì‚¬ë¡€ ê³µìœ ë¥¼ ì¶”ì§„í•˜ëŠ”\n",
      "í•œí¸, ì²¨ë‹¨ AI ëª¨ë¸ì˜ ì·¨ì•½ì ì´ë‚˜ ì ì¬ì ìœ¼ë¡œ ìœ„í—˜í•œ ê¸°ëŠ¥ ë° ìœ„í—˜ ì™„í™” ê´€ë ¨ ì •ë³´ë¥¼ ê³µìœ í•  ìˆ˜\n",
      "ìˆëŠ” ê³µê°œ ì ˆì°¨ë„ ê°œë°œ ì¤‘\n",
      "â˜ ì¶œì²˜: Google, Anthropic, Google, Microsoft and OpenAI announce Executive Director of the Frontier Model' metadata={'source': 'data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf', 'file_path': 'data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf', 'page': 9, 'total_pages': 23, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13462', 'Producer': 'Hancom PDF 1.3.0.542', 'CreationDate': \"D:20231208132838+09'00'\", 'ModDate': \"D:20231208132838+09'00'\", 'PDFVersion': '1.4'}\n",
      "page_content='12\n",
      "2023ë…„ ì›”í˜¸' metadata={'source': 'data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf', 'file_path': 'data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf', 'page': 0, 'total_pages': 23, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13462', 'Producer': 'Hancom PDF 1.3.0.542', 'CreationDate': \"D:20231208132838+09'00'\", 'ModDate': \"D:20231208132838+09'00'\", 'PDFVersion': '1.4'}\n",
      "page_content='1,000ë§Œ ë‹¬ëŸ¬ ì´ìƒì„ ê¸°ë¶€\n",
      "âˆ™ ë˜í•œ ì‹ ê¸°ìˆ ì˜ ê±°ë²„ë„ŒìŠ¤ì™€ ì•ˆì „ ë¶„ì•¼ì—ì„œ ì „ë¬¸ì„±ì„ ê°–ì¶˜ ë¸Œë£¨í‚¹ìŠ¤ ì—°êµ¬ì†Œ ì¶œì‹ ì˜ í¬ë¦¬ìŠ¤ ë©”ì„œë¡¤(Chris\n",
      "Meserole)ì„ í¬ëŸ¼ì˜ ìƒë¬´ì´ì‚¬ë¡œ ì„ëª…\n",
      "n ìµœê·¼ AI ê¸°ìˆ ì´ ê¸‰ì†íˆ ë°œì „í•˜ë©´ì„œ AI ì•ˆì „ì— ê´€í•œ ì—°êµ¬ê°€ ë¶€ì¡±í•œ ì‹œì ì—, í¬ëŸ¼ì€ ì´ëŸ¬í•œ ê²©ì°¨ë¥¼ í•´ì†Œ\n",
      "í•˜ê¸° ìœ„í•´ AI ì•ˆì „ ê¸°ê¸ˆì„ ì¡°ì„±\n",
      "âˆ™ ì°¸ì—¬ì‚¬ë“¤ì€ ì§€ë‚œ 7ì›” ë°±ì•…ê´€ ì£¼ì¬ì˜ AI ì•ˆì „ ì„œì•½ì—ì„œ ì™¸ë¶€ìì˜ AI ì‹œìŠ¤í…œ ì·¨ì•½ì  ë°œê²¬ê³¼ ì‹ ê³ ë¥¼\n",
      "ì´‰ì§„í•˜ê¸°ë¡œ ì•½ì†í–ˆìœ¼ë©°, ì•½ì†ì„ ì´í–‰í•˜ê¸° ìœ„í•´ ê¸°ê¸ˆì„ í™œìš©í•´ ì™¸ë¶€ ì—°êµ¬ì§‘ë‹¨ì˜ AI ì‹œìŠ¤í…œ í‰ê°€ì—\n",
      "ìê¸ˆì„ ì§€ì›í•  ê³„íš' metadata={'source': 'data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf', 'file_path': 'data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf', 'page': 9, 'total_pages': 23, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13462', 'Producer': 'Hancom PDF 1.3.0.542', 'CreationDate': \"D:20231208132838+09'00'\", 'ModDate': \"D:20231208132838+09'00'\", 'PDFVersion': '1.4'}\n",
      "==================================================\n",
      "\n",
      "==== [CHECK DOCUMENT RELEVANCE TO QUESTION] ====\n",
      "\n",
      "==== [GRADE: DOCUMENT NOT RELEVANT] ====\n",
      "==== [GRADE: DOCUMENT NOT RELEVANT] ====\n",
      "==== [GRADE: DOCUMENT NOT RELEVANT] ====\n",
      "==== [GRADE: DOCUMENT NOT RELEVANT] ====\n",
      "==== [GRADE: DOCUMENT NOT RELEVANT] ====\n",
      "==== [GRADE: DOCUMENT NOT RELEVANT] ====\n",
      "==== [GRADE: DOCUMENT NOT RELEVANT] ====\n",
      "==== [GRADE: DOCUMENT NOT RELEVANT] ====\n",
      "==== [GRADE: DOCUMENT NOT RELEVANT] ====\n",
      "==== [GRADE: DOCUMENT NOT RELEVANT] ====\n",
      "==== [ASSESS GRADED DOCUMENTS] ====\n",
      "==== [DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, QUERY REWRITE] ====\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mgrade_documents\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "\u001b[1;32mweb_search\u001b[0m:\n",
      "Yes\n",
      "==================================================\n",
      "\n",
      "==== [REWRITE QUERY] ====\n",
      "\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mquery_rewrite\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "\u001b[1;32mquestion\u001b[0m:\n",
      "2024ë…„ ë…¸ë²¨ë¬¸í•™ìƒ ìˆ˜ìƒìëŠ” ëˆ„êµ¬ì¸ê°€ìš”?\n",
      "==================================================\n",
      "\n",
      "==== [WEB SEARCH] ====\n",
      "\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mweb_search_node\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "page_content='ê¹€ëª…ì¼ ê¸°ì í•œêµ­ ì†Œì„¤ê°€ í•œê°•(54)ì´ 2024ë…„ ë…¸ë²¨ë¬¸í•™ìƒ ìˆ˜ìƒìë¡œ ì„ ì •ëë‹¤. í•œêµ­ì¸ì´ ë…¸ë²¨ìƒì„ ìˆ˜ìƒí•œ ê²ƒì€ ì§€ë‚œ 2000ë…„ í‰í™”ìƒì„ íƒ„ ê³ (æ•…) ê¹€ëŒ€ì¤‘ ì „ ëŒ€í†µë ¹ì— ì´ì–´ ë‘ë²ˆì§¸ë‹¤. ì´ë‚  ë¬¸í•™ìƒì— ì´ì–´ ì˜¤ëŠ” 11ì¼ í‰í™”ìƒ, 14ì¼ ê²½ì œí•™ìƒ ìˆ˜ìƒìê°€ ë°œí‘œë  ì˜ˆì •ì´ë‹¤. ë…¸ë²¨ìƒ ì‹œìƒì‹ì€ ì•Œí”„ë ˆë“œ ë…¸ë²¨ì˜ ê¸°ì¼ì¸ 12ì›” 10ì¼ ìŠ¤ì›¨ë´ ìŠ¤í†¡í™€ë¦„(ìƒë¦¬ì˜í•™Â·ë¬¼ë¦¬Â·í™”í•™Â·ë¬¸í•™Â·ê²½ì œìƒ)ê³¼ ë…¸ë¥´ì›¨ì´ ì˜¤ìŠ¬ë¡œ(í‰í™”ìƒ)ì—ì„œ ì—´ë¦°ë‹¤. í•œí¸ í•œê°•ì€ ì§€ë‚œ 2016ë…„ í•œêµ­ì¸ ìµœì´ˆë¡œ ì„¸ê³„ 3ëŒ€ ë¬¸í•™ìƒ ì¤‘ í•˜ë‚˜ì¸ â€˜ë§¨ë¶€ì»¤ìƒâ€™ë„ ìˆ˜ìƒí•œ ë°” ìˆë‹¤. ê¹€ëª…ì¼ ê¸°ì ì¡°ì„ NS ê¹€ëª…ì¼ ê¸°ìì…ë‹ˆë‹¤. ë§ì€ ì œë³´ ë¶€íƒë“œë¦½ë‹ˆë‹¤. ì œë³´ ë©”ì¼ mi737@chosun.com â€œì™¸êµ­ì¸ì´ ë¼ˆë‹¤ê·€ í•´ì¥êµ­ìœ¼ë¡œ ì„±ê³µí•´ ìˆ˜ì‹­ ë…„ ë™ì•ˆ ì¥ì‚¬ ì¤‘ì¸ ë…¸í¬(è€é‹ª)ê°€ í•œêµ­ì— ìˆë‹¤ë©´ ì–´ë–¨ ê²ƒ ê°™ìŠµë‹ˆê¹Œ?â€ ì „ ì„¸ê³„ ìµœê³  ì‹ë‹¹ì´ ëª¨... â€œì™¸êµ­ì¸ì´ ë¼ˆë‹¤ê·€ í•´ì¥êµ­ìœ¼ë¡œ ì„±ê³µí•´ ìˆ˜ì‹­ ë…„ ë™ì•ˆ ì¥ì‚¬ ì¤‘ì¸ ë…¸í¬(è€é‹ª)ê°€ í•œêµ­ì— ìˆë‹¤ë©´ ì–´ë–¨ ê²ƒ ê°™ìŠµë‹ˆê¹Œ?â€ ì „ ì„¸ê³„ ìµœê³  ì‹ë‹¹ì´ ëª¨... ëŒ€í•œë¯¼êµ­ êµ­ë¯¼ìœ¼ë¡œì„œ ìµœì´ˆì˜ ë…¸ë²¨ìƒì„ ìˆ˜ìƒí•˜ê²Œ ë˜ì—ˆìŒì„ ì§„ì‹¬ìœ¼ë¡œ ì¶•í•˜í•©ë‹ˆë‹¤.\n",
      "2024ë…„ ë…¸ë²¨ ë¬¸í•™ìƒì— í•œêµ­ ì†Œì„¤ê°€ í•œê°• / YTN\n",
      "\n",
      " YTN\n",
      "26 likes\n",
      "1728 views\n",
      "10 Oct 2024\n",
      "í•œêµ­ ì†Œì„¤ê°€ í•œê°• ì”¨ê°€ 2024ë…„ ë…¸ë²¨ ë¬¸í•™ìƒ ìˆ˜ìƒìë¡œ ë½‘í˜”ìŠµë‹ˆë‹¤.\n",
      "\n",
      "ìŠ¤ì›¨ë´ ë…¸ë²¨ìœ„ì›íšŒëŠ” í•œê°• ì”¨ì˜ ì‘í’ˆì— ëŒ€í•´ \"ì—­ì‚¬ì  íŠ¸ë¼ìš°ë§ˆë¥¼ ì§ì‹œí•˜ê³  ì¸ê°„ ì‚¶ì˜ ì—°ì•½í•¨ì„ ë“œëŸ¬ë‚´ëŠ” ê°•ë ¬í•œ ì‹œì  ì‚°ë¬¸\"ì´ë¼ë©° ì„ ì • ì´ìœ ë¥¼ ë°í˜”ìŠµë‹ˆë‹¤.\n",
      "\n",
      "í•œêµ­ ì‘ê°€ê°€ ë…¸ë²¨ ë¬¸í•™ìƒì„ ë°›ëŠ” ê²ƒì€ ì´ë²ˆì´ ì²˜ìŒì…ë‹ˆë‹¤.\n",
      "\n",
      "ë˜, í•œêµ­ì¸ì˜ ë…¸ë²¨ìƒ ìˆ˜ìƒì€ ì§€ë‚œ 2000ë…„ í‰í™”ìƒì„ íƒ„ ê³  ê¹€ëŒ€ì¤‘ ì „ ëŒ€í†µë ¹ì— ì´ì–´ ë‘ ë²ˆì§¸ì…ë‹ˆë‹¤.\n",
      "\n",
      "ìˆ˜ìƒìì—ê²ŒëŠ” ìƒê¸ˆ ì²œ100ë§Œ í¬ë¡œë‚˜, ìš°ë¦¬ ëˆìœ¼ë¡œ 13ì–µ 4ì²œë§Œ ì›ê³¼ ë©”ë‹¬, ì¦ì„œê°€ ìˆ˜ì—¬ë©ë‹ˆë‹¤.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "YTN í™ì£¼ì˜ˆ (hongkiza@ytn.co.kr)\n",
      "\n",
      "â€» 'ë‹¹ì‹ ì˜ ì œë³´ê°€ ë‰´ìŠ¤ê°€ ë©ë‹ˆë‹¤'\n",
      "[ì¹´ì¹´ì˜¤í†¡] YTN ê²€ìƒ‰í•´ ì±„ë„ ì¶”ê°€\n",
      "[ì „í™”] 02-398-8585\n",
      "[ë©”ì¼] social@ytn.co.kr\n",
      "\n",
      "â–¶ ê¸°ì‚¬ ì›ë¬¸ : https://www.ytn.co.kr/_ln/0104_202410102023354769\n",
      "â–¶ ì œë³´ í•˜ê¸° : https://mj.ytn.co.kr/mj/mj_write.php\n",
      "\n",
      "â–£ YTN ìœ íŠœë¸Œ ì±„ë„ êµ¬ë… : http://goo.gl/Ytb5SZ\n",
      "\n",
      "â“’ YTN ë¬´ë‹¨ ì „ì¬, ì¬ë°°í¬ê¸ˆì§€ ë° AI ë°ì´í„° í™œìš© ê¸ˆì§€\n",
      "5 comments\n",
      "# 2024ë…„ ë…¸ë²¨ë¬¸í•™ìƒ ìˆ˜ìƒ ì‘ê°€, í•œê°• 2024ë…„ ë…¸ë²¨ë¬¸í•™ìƒ ìˆ˜ìƒìë¡œ í•œê°• ì‘ê°€ê°€ ì„ ì •ë˜ì—ˆìŠµë‹ˆë‹¤. í•œê°• ì‘ê°€ì˜ ë…¸ë²¨ë¬¸í•™ìƒ ìˆ˜ìƒì€ í•œêµ­ì¸ ìµœì´ˆì´ì ì•„ì‹œì•„ ì—¬ì„± ìµœì´ˆì…ë‹ˆë‹¤. í•œê°• ì‘ê°€ì˜ ìˆ˜ìƒì„ ì§„ì‹¬ìœ¼ë¡œ ì¶•í•˜ë“œë¦½ë‹ˆë‹¤. âœ” ê¸°ì‚¬ ë³´ê¸°:Â  í•œê°• **ì¹´í…Œê³ ë¦¬** ë¬¸í•™ê³¼ì§€ì„± ì‹œì¸ì„ ,ì‹œ **ì¶œê°„ì¼** 2013ë…„ 11ì›” 15ì¼ **ì‚¬ì–‘** Â· 165ìª½ í•œê°• **ì¹´í…Œê³ ë¦¬** ì†Œì„¤,ì†Œì„¤ ì„ ì§‘ **ì¶œê°„ì¼** 2018ë…„ 11ì›” 9ì¼ **ì‚¬ì–‘** ë³€í˜•íŒ 130x195 Â· 325ìª½ í•œê°• ì§€ìŒ **ì¹´í…Œê³ ë¦¬** ì¥í¸ì†Œì„¤ **ì‚¬ì–‘** ì‹ êµ­íŒ 152x225mm Â· 330ìª½ í•œê°• ì§€ìŒ **ì¹´í…Œê³ ë¦¬** ì¥í¸ì†Œì„¤ **ì‚¬ì–‘** Â· 390ìª½ í•œê°• **ì¹´í…Œê³ ë¦¬** ì†Œì„¤,ì†Œì„¤ ì„ ì§‘ **ì‚¬ì–‘** ë³€í˜•íŒ 130x195 Â· 308ìª½ ë‚´ ì—¬ìì˜ ì—´ë§¤ í•œê°• **ì¹´í…Œê³ ë¦¬** ì†Œì„¤,ì†Œì„¤ ì„ ì§‘ **ì‚¬ì–‘** ë³€í˜•íŒ 130x195 Â· 406ìª½ ### í•œê°• ì†Œì„¤ê°€ 1993ë…„ ã€ë¬¸í•™ê³¼ì‚¬íšŒã€ ê²¨ìš¸í˜¸ì— ì‹œ ã€Œì„œìš¸ì˜ ê²¨ìš¸ã€ ì™¸ ë„¤ í¸ì„ ë°œí‘œí•˜ê³  ì´ë“¬í•´ ã€ì„œìš¸ì‹ ë¬¸ã€ ì‹ ì¶˜ë¬¸ì˜ˆì— ë‹¨í¸ì†Œì„¤ ã€Œë¶‰ì€ ë‹»ã€ì´ ë‹¹ì„ ë˜ì–´ ì‘í’ˆ í™œë™ì„ ì‹œì‘í–ˆë‹¤. ì†Œì„¤ì§‘ ã€ì—¬ìˆ˜ì˜ ì‚¬ë‘ã€ ã€ë‚´ ì—¬ìì˜ ì—´ë§¤ã€ â€¦ ìì„¸íˆ ë³´ê¸°'\n",
      "==================================================\n",
      "\n",
      "==== GENERATE ====\n",
      "\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mgenerate\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "\u001b[1;32mgeneration\u001b[0m:\n",
      "2024ë…„ ë…¸ë²¨ë¬¸í•™ìƒ ìˆ˜ìƒìëŠ” í•œêµ­ ì†Œì„¤ê°€ í•œê°•ì…ë‹ˆë‹¤.\n",
      "\n",
      "**Source**\n",
      "- https://www.ytn.co.kr/_ln/0104_202410102023354769\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# ê·¸ë˜í”„ ì‹¤í–‰\n",
    "invoke_graph(app, inputs, config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== RETRIEVE ====\n",
      "\n",
      "\n",
      "==== [CHECK DOCUMENT RELEVANCE TO QUESTION] ====\n",
      "\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mgrade_documents\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "{\"binary_score\":\"no\"}==== [GRADE: DOCUMENT NOT RELEVANT] ====\n",
      "{\"binary_score\":\"no\"}==== [GRADE: DOCUMENT NOT RELEVANT] ====\n",
      "{\"binary_score\":\"no\"}==== [GRADE: DOCUMENT NOT RELEVANT] ====\n",
      "{\"binary_score\":\"no\"}==== [GRADE: DOCUMENT NOT RELEVANT] ====\n",
      "{\"binary_score\":\"no\"}==== [GRADE: DOCUMENT NOT RELEVANT] ====\n",
      "{\"binary_score\":\"no\"}==== [GRADE: DOCUMENT NOT RELEVANT] ====\n",
      "{\"binary_score\":\"no\"}==== [GRADE: DOCUMENT NOT RELEVANT] ====\n",
      "{\"binary_score\":\"no\"}==== [GRADE: DOCUMENT NOT RELEVANT] ====\n",
      "{\"binary_score\":\"no==== [GRADE: DOCUMENT NOT RELEVANT] ====\n",
      "\"}{\"binary_score\":\"no\"}==== [GRADE: DOCUMENT NOT RELEVANT] ====\n",
      "==== [ASSESS GRADED DOCUMENTS] ====\n",
      "==== [DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, QUERY REWRITE] ====\n",
      "\n",
      "==== [REWRITE QUERY] ====\n",
      "\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mquery_rewrite\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "2024ë…„ ë…¸ë²¨ë¬¸í•™ìƒ ìˆ˜ìƒìëŠ” ëˆ„êµ¬ì¸ê°€ìš”?\n",
      "==== [WEB SEARCH] ====\n",
      "\n",
      "\n",
      "==== GENERATE ====\n",
      "\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mgenerate\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "2024ë…„ ë…¸ë²¨ë¬¸í•™ìƒ ìˆ˜ìƒìëŠ” í•œêµ­ ì†Œì„¤ê°€ í•œê°•ì…ë‹ˆë‹¤.\n",
      "\n",
      "**Source**\n",
      "- https://www.ytn.co.kr/_ln/0104_202410102023354769"
     ]
    }
   ],
   "source": [
    "# ê·¸ë˜í”„ ì‹¤í–‰\n",
    "stream_graph(\n",
    "    app,\n",
    "    inputs,\n",
    "    config,\n",
    "    [\"retrieve\", \"grade_documents\", \"query_rewrite\", \"generate\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "04. Self-RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rag.pdf import PDFRetrievalChain\n",
    "\n",
    "# PDF ë¬¸ì„œë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.\n",
    "pdf = PDFRetrievalChain([\"data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf\"]).create_chain()\n",
    "\n",
    "# retrieverì™€ chainì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "pdf_retriever = pdf.retriever\n",
    "pdf_chain = pdf.chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_teddynote.models import get_model_name, LLMs\n",
    "\n",
    "# ìµœì‹ ëª¨ë¸ ì´ë¦„ ì„¤ì •\n",
    "MODEL_NAME = get_model_name(LLMs.GPT4_1)\n",
    "\n",
    "\n",
    "# ë°ì´í„° ëª¨ë¸ ì •ì˜: ê²€ìƒ‰ëœ ë¬¸ì„œì˜ ê´€ë ¨ì„±ì„ ì´ì§„ ì ìˆ˜ë¡œ í‰ê°€í•˜ê¸° ìœ„í•œ ë°ì´í„° ëª¨ë¸\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"A binary score to determine the relevance of the retrieved documents.\"\"\"\n",
    "\n",
    "    # ë¬¸ì„œê°€ ì§ˆë¬¸ì— ê´€ë ¨ì´ ìˆëŠ”ì§€ ì—¬ë¶€ë¥¼ 'yes' ë˜ëŠ” 'no'ë¡œ ë‚˜íƒ€ë‚´ëŠ” í•„ë“œ\n",
    "    binary_score: str = Field(\n",
    "        description=\"Documents are relevant to the question, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "\n",
    "# LLM ì´ˆê¸°í™”\n",
    "llm = ChatOpenAI(model=MODEL_NAME, temperature=0)\n",
    "\n",
    "# GradeDocuments ë°ì´í„° ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ LLMì˜ êµ¬ì¡°í™”ëœ ì¶œë ¥ ìƒì„±\n",
    "structured_llm_grader = llm.with_structured_output(GradeDocuments)\n",
    "\n",
    "# ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì •ì˜: ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì‚¬ìš©ì ì§ˆë¬¸ì— ê´€ë ¨ì´ ìˆëŠ”ì§€ í‰ê°€í•˜ëŠ” ì‹œìŠ¤í…œ ì—­í•  ì •ì˜\n",
    "system = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "    It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
    "    If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\"\n",
    "\n",
    "# ì±„íŒ… í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±\n",
    "grade_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# ê²€ìƒ‰ í‰ê°€ê¸° ìƒì„±\n",
    "retrieval_grader = grade_prompt | structured_llm_grader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary_score='yes'\n"
     ]
    }
   ],
   "source": [
    "# ì§ˆë¬¸ ì •ì˜\n",
    "question = \"ì‚¼ì„±ì „ìê°€ ê°œë°œí•œ ìƒì„±í˜• AI ì˜ ì´ë¦„ì€?\"\n",
    "\n",
    "# ë¬¸ì„œ ê²€ìƒ‰\n",
    "docs = pdf_retriever.invoke(question)\n",
    "\n",
    "# ê²€ìƒ‰ëœ ë¬¸ì„œ ì¤‘ ë‘ ë²ˆì§¸ ë¬¸ì„œì˜ í˜ì´ì§€ ì½˜í…ì¸  ì¶”ì¶œ\n",
    "doc_txt = docs[1].page_content\n",
    "\n",
    "# ê²€ìƒ‰ í‰ê°€ê¸° í˜¸ì¶œ ë° ê²°ê³¼ ì¶œë ¥\n",
    "print(retrieval_grader.invoke({\"question\": question, \"document\": doc_txt}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì‚¼ì„±ì „ìê°€ ê°œë°œí•œ ìƒì„±í˜• AIì˜ ì´ë¦„ì€ â€˜ì‚¼ì„± ê°€ìš°ìŠ¤â€™(Samsung Gauss)ì´ë‹¤.\n",
      "\n",
      "**Source**\n",
      "- data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf (p.13)\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# LangChain Hubì—ì„œ í”„ë¡¬í”„íŠ¸ ê°€ì ¸ì˜¤ê¸°\n",
    "prompt = hub.pull(\"teddynote/rag-prompt\")\n",
    "\n",
    "# ê¸°ë³¸ LLM ì´ˆê¸°í™”, ëª¨ë¸ ì´ë¦„ê³¼ ì˜¨ë„ ì„¤ì •\n",
    "llm = ChatOpenAI(model_name=MODEL_NAME, temperature=0)\n",
    "\n",
    "\n",
    "# ë¬¸ì„œ í¬ë§·íŒ… í•¨ìˆ˜\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(\n",
    "        [\n",
    "            f'<document><content>{doc.page_content}</content><source>{doc.metadata[\"source\"]}</source><page>{doc.metadata[\"page\"]+1}</page></document>'\n",
    "            for doc in docs\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "# RAG ì²´ì¸ ìƒì„±\n",
    "rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# ì²´ì¸ ì‹¤í–‰\n",
    "generation = rag_chain.invoke({\"context\": format_docs(docs), \"question\": question})\n",
    "print(generation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "# ë°ì´í„° ëª¨ë¸ ì •ì˜: ìƒì„±ëœ ë‹µë³€ì´ ì‚¬ì‹¤ì— ê¸°ë°˜í•˜ê³  ìˆëŠ”ì§€ ì—¬ë¶€ë¥¼ ì´ì§„ ì ìˆ˜ë¡œ í‰ê°€í•˜ê¸° ìœ„í•œ ë°ì´í„° ëª¨ë¸\n",
    "class Groundednesss(BaseModel):\n",
    "    \"\"\"A binary score indicating whether the generated answer is grounded in the facts.\"\"\"\n",
    "\n",
    "    # ë‹µë³€ì´ ì‚¬ì‹¤ì— ê¸°ë°˜í•˜ê³  ìˆëŠ”ì§€ ì—¬ë¶€ë¥¼ 'yes' ë˜ëŠ” 'no'ë¡œ ë‚˜íƒ€ë‚´ëŠ” í•„ë“œ\n",
    "    binary_score: str = Field(\n",
    "        description=\"Answer is grounded in the facts, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "\n",
    "# LLM ì´ˆê¸°í™”\n",
    "llm = ChatOpenAI(model=MODEL_NAME, temperature=0)\n",
    "\n",
    "# êµ¬ì¡°í™”ëœ ì¶œë ¥ê³¼ í•¨ê»˜ LLM ì„¤ì •\n",
    "structured_llm_grader = llm.with_structured_output(Groundednesss)\n",
    "\n",
    "# ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì •ì˜\n",
    "system = \"\"\"You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts. \\n \n",
    "Give a binary score 'yes' or 'no'. 'Yes' means that the answer is grounded in / supported by the set of facts.\"\"\"\n",
    "\n",
    "# ì±„íŒ… í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±\n",
    "groundedness_checking_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Set of facts: \\n\\n {documents} \\n\\n LLM generation: {generation}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# ë‹µë³€ì˜ í• ë£¨ì‹œë„¤ì´ì…˜ í‰ê°€ê¸° ìƒì„±\n",
    "groundedness_grader = groundedness_checking_prompt | structured_llm_grader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Groundednesss(binary_score='yes')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# í• ë£¨ì‹œë„¤ì´ì…˜ í‰ê°€ê¸° í˜¸ì¶œ(yes: ì‚¬ì‹¤ì— ê¸°ë°˜í•˜ê³  ìˆìŒ, no: ì‚¬ì‹¤ì— ê¸°ë°˜í•˜ê³  ìˆì§€ ì•ŠìŒ)\n",
    "groundedness_grader.invoke({\"documents\": format_docs(docs), \"generation\": generation})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "class GradeAnswer(BaseModel):\n",
    "    \"\"\"A binary score indicating whether the question is addressed.\"\"\"\n",
    "\n",
    "    # ë‹µë³€ì˜ ê´€ë ¨ì„± í‰ê°€: 'yes' ë˜ëŠ” 'no'ë¡œ í‘œê¸°(yes: ê´€ë ¨ì„± ìˆìŒ, no: ê´€ë ¨ì„± ì—†ìŒ)\n",
    "    binary_score: str = Field(\n",
    "        description=\"Answer addresses the question, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model=MODEL_NAME, temperature=0)\n",
    "\n",
    "# llm ì— GradeAnswer ë°”ì¸ë”©\n",
    "structured_llm_grader = llm.with_structured_output(GradeAnswer)\n",
    "\n",
    "# ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì •ì˜\n",
    "system = \"\"\"You are a grader assessing whether an answer addresses / resolves a question \\n \n",
    "     Give a binary score 'yes' or 'no'. Yes' means that the answer resolves the question.\"\"\"\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸ ìƒì„±\n",
    "answer_grader_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"User question: \\n\\n {question} \\n\\n LLM generation: {generation}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# ë‹µë³€ í‰ê°€ê¸° ìƒì„±\n",
    "answer_grader = answer_grader_prompt | structured_llm_grader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model=MODEL_NAME, temperature=0)\n",
    "\n",
    "# ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì •ì˜\n",
    "# ì…ë ¥ ì§ˆë¬¸ì„ ë²¡í„°ìŠ¤í† ì–´ ê²€ìƒ‰ì— ìµœì í™”ëœ í˜•íƒœë¡œ ë³€í™˜í•˜ëŠ” ì‹œìŠ¤í…œ ì—­í•  ì •ì˜\n",
    "system = \"\"\"You a question re-writer that converts an input question to a better version that is optimized \\n \n",
    "     for vectorstore retrieval. Look at the input and try to reason about the underlying semantic intent / meaning.\"\"\"\n",
    "\n",
    "# ì‹œìŠ¤í…œ ë©”ì‹œì§€ì™€ ì´ˆê¸° ì§ˆë¬¸ì„ í¬í•¨í•œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±\n",
    "re_write_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Here is the initial question: \\n\\n {question} \\n Formulate an improved question.\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# ì§ˆë¬¸ ì¬ì‘ì„±ê¸° ìƒì„±\n",
    "question_rewriter = re_write_prompt | llm | StrOutputParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ì‚¼ì„±ì „ìê°€ ê°œë°œí•œ ìƒì„±í˜• ì¸ê³µì§€ëŠ¥(AI) ëª¨ë¸ì˜ ê³µì‹ ëª…ì¹­ì€ ë¬´ì—‡ì¸ê°€?'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ì§ˆë¬¸ ì¬ì‘ì„±ê¸° í˜¸ì¶œ\n",
    "question_rewriter.invoke({\"question\": question})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from typing_extensions import TypedDict, Annotated\n",
    "\n",
    "\n",
    "# ê·¸ë˜í”„ì˜ ìƒíƒœë¥¼ ë‚˜íƒ€ë‚´ëŠ” í´ë˜ìŠ¤ ì •ì˜\n",
    "class GraphState(TypedDict):\n",
    "    # ì§ˆë¬¸ì„ ë‚˜íƒ€ë‚´ëŠ” ë¬¸ìì—´\n",
    "    question: Annotated[str, \"Question\"]\n",
    "    # LLMì— ì˜í•´ ìƒì„±ëœ ì‘ë‹µì„ ë‚˜íƒ€ë‚´ëŠ” ë¬¸ìì—´\n",
    "    generation: Annotated[str, \"LLM Generation\"]\n",
    "    # ë¬¸ì„œì˜ ëª©ë¡ì„ ë‚˜íƒ€ë‚´ëŠ” ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸\n",
    "    documents: Annotated[List[str], \"Retrieved Documents\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¬¸ì„œ ê²€ìƒ‰\n",
    "def retrieve(state):\n",
    "    print(\"==== [RETRIEVE] ====\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # ê²€ìƒ‰ ìˆ˜í–‰\n",
    "    documents = pdf_retriever.invoke(question)\n",
    "    return {\"documents\": documents}\n",
    "\n",
    "\n",
    "# ë‹µë³€ ìƒì„±\n",
    "def generate(state):\n",
    "    print(\"==== [GENERATE] ====\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # RAG ìƒì„±\n",
    "    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
    "    return {\"generation\": generation}\n",
    "\n",
    "\n",
    "# ê²€ìƒ‰ëœ ë¬¸ì„œì˜ ê´€ë ¨ì„± í‰ê°€\n",
    "def grade_documents(state):\n",
    "    print(\"==== [GRADE DOCUMENTS] ====\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # ê° ë¬¸ì„œ ì ìˆ˜ í‰ê°€\n",
    "    filtered_docs = []\n",
    "    for d in documents:\n",
    "        score = retrieval_grader.invoke(\n",
    "            {\"question\": question, \"document\": d.page_content}\n",
    "        )\n",
    "        grade = score.binary_score\n",
    "        if grade == \"yes\":\n",
    "            print(\"==== GRADE: DOCUMENT RELEVANT ====\")\n",
    "            filtered_docs.append(d)\n",
    "        else:\n",
    "            print(\"==== GRADE: DOCUMENT NOT RELEVANT ====\")\n",
    "            continue\n",
    "    return {\"documents\": filtered_docs}\n",
    "\n",
    "\n",
    "# ì§ˆë¬¸ ë³€í™˜\n",
    "def transform_query(state):\n",
    "    print(\"==== [TRANSFORM QUERY] ====\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # ì§ˆë¬¸ ì¬ì‘ì„±\n",
    "    better_question = question_rewriter.invoke({\"question\": question})\n",
    "    return {\"question\": better_question}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë‹µë³€ ìƒì„± ì—¬ë¶€ ê²°ì •\n",
    "def decide_to_generate(state):\n",
    "    print(\"==== [ASSESS GRADED DOCUMENTS] ====\")\n",
    "    state[\"question\"]\n",
    "    filtered_documents = state[\"documents\"]\n",
    "\n",
    "    if not filtered_documents:\n",
    "        # ëª¨ë“  ë¬¸ì„œê°€ ê´€ë ¨ì„±ì´ ì—†ëŠ” ê²½ìš°\n",
    "        # ìƒˆë¡œìš´ ì¿¼ë¦¬ ìƒì„±\n",
    "        print(\n",
    "            \"==== [DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY] ====\"\n",
    "        )\n",
    "        return \"transform_query\"\n",
    "    else:\n",
    "        # ê´€ë ¨ ë¬¸ì„œê°€ ìˆëŠ” ê²½ìš° ë‹µë³€ ìƒì„±\n",
    "        print(\"==== [DECISION: GENERATE] ====\")\n",
    "        return \"generate\"\n",
    "\n",
    "\n",
    "# ìƒì„±ëœ ë‹µë³€ì˜ ë¬¸ì„œ ë° ì§ˆë¬¸ê³¼ì˜ ê´€ë ¨ì„± í‰ê°€\n",
    "def grade_generation_v_documents_and_question(state):\n",
    "    print(\"==== [CHECK HALLUCINATIONS] ====\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    generation = state[\"generation\"]\n",
    "\n",
    "    score = groundedness_grader.invoke(\n",
    "        {\"documents\": documents, \"generation\": generation}\n",
    "    )\n",
    "    grade = score.binary_score\n",
    "\n",
    "    # í™˜ê° ì—¬ë¶€ í™•ì¸\n",
    "    if grade == \"yes\":\n",
    "        print(\"==== [DECISION: GENERATION IS GROUNDED IN DOCUMENTS] ====\")\n",
    "        # ì§ˆë¬¸ í•´ê²° ì—¬ë¶€ í™•ì¸\n",
    "        print(\"==== [GRADE GENERATION vs QUESTION] ====\")\n",
    "        score = answer_grader.invoke({\"question\": question, \"generation\": generation})\n",
    "        grade = score.binary_score\n",
    "        if grade == \"yes\":\n",
    "            print(\"==== [DECISION: GENERATION ADDRESSES QUESTION] ====\")\n",
    "            return \"relevant\"\n",
    "        else:\n",
    "            print(\"==== [DECISION: GENERATION DOES NOT ADDRESS QUESTION] ====\")\n",
    "            return \"not relevant\"\n",
    "    else:\n",
    "        print(\"==== [DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY] ====\")\n",
    "        return \"hallucination\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# ê·¸ë˜í”„ ìƒíƒœ ì´ˆê¸°í™”\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# ë…¸ë“œ ì •ì˜\n",
    "workflow.add_node(\"retrieve\", retrieve)  # retrieve\n",
    "workflow.add_node(\"grade_documents\", grade_documents)  # grade documents\n",
    "workflow.add_node(\"generate\", generate)  # generatae\n",
    "workflow.add_node(\"transform_query\", transform_query)  # transform_query\n",
    "\n",
    "# ì—£ì§€ ì •ì˜\n",
    "workflow.add_edge(START, \"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "\n",
    "# ë¬¸ì„œ í‰ê°€ ë…¸ë“œì—ì„œ ì¡°ê±´ë¶€ ì—£ì§€ ì¶”ê°€\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"transform_query\": \"transform_query\",\n",
    "        \"generate\": \"generate\",\n",
    "    },\n",
    ")\n",
    "\n",
    "# ì—£ì§€ ì •ì˜\n",
    "workflow.add_edge(\"transform_query\", \"retrieve\")\n",
    "\n",
    "# ë‹µë³€ ìƒì„± ë…¸ë“œì—ì„œ ì¡°ê±´ë¶€ ì—£ì§€ ì¶”ê°€\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate\",\n",
    "    grade_generation_v_documents_and_question,\n",
    "    {\n",
    "        \"hallucination\": \"generate\",\n",
    "        \"relevant\": END,\n",
    "        \"not relevant\": \"transform_query\",\n",
    "    },\n",
    ")\n",
    "\n",
    "# ê·¸ë˜í”„ ì»´íŒŒì¼\n",
    "app = workflow.compile(checkpointer=MemorySaver())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_teddynote.graphs import visualize_graph\n",
    "\n",
    "visualize_graph(app)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== [RETRIEVE] ====\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mretrieve\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "page_content='SPRi AI Brief |\n",
      "2023-12ì›”í˜¸\n",
      "ì‚¼ì„±ì „ì, ìì²´ ê°œë°œ ìƒì„± AI â€˜ì‚¼ì„± ê°€ìš°ìŠ¤â€™ ê³µê°œ\n",
      "KEY Contents\n",
      "n ì‚¼ì„±ì „ìê°€ ì˜¨ë””ë°”ì´ìŠ¤ì—ì„œ ì‘ë™ ê°€ëŠ¥í•˜ë©° ì–¸ì–´, ì½”ë“œ, ì´ë¯¸ì§€ì˜ 3ê°œ ëª¨ë¸ë¡œ êµ¬ì„±ëœ ìì²´ ê°œë°œ ìƒì„±\n",
      "AI ëª¨ë¸ â€˜ì‚¼ì„± ê°€ìš°ìŠ¤â€™ë¥¼ ê³µê°œ\n",
      "n ì‚¼ì„±ì „ìëŠ” ì‚¼ì„± ê°€ìš°ìŠ¤ë¥¼ ë‹¤ì–‘í•œ ì œí’ˆì— ë‹¨ê³„ì ìœ¼ë¡œ íƒ‘ì¬í•  ê³„íšìœ¼ë¡œ, ì˜¨ë””ë°”ì´ìŠ¤ ì‘ë™ì´ ê°€ëŠ¥í•œ\n",
      "ì‚¼ì„± ê°€ìš°ìŠ¤ëŠ” ì™¸ë¶€ë¡œ ì‚¬ìš©ì ì •ë³´ê°€ ìœ ì¶œë  ìœ„í—˜ì´ ì—†ë‹¤ëŠ” ì¥ì ì„ ë³´ìœ \n",
      "Â£ì–¸ì–´, ì½”ë“œ, ì´ë¯¸ì§€ì˜ 3ê°œ ëª¨ë¸ë¡œ êµ¬ì„±ëœ ì‚¼ì„± ê°€ìš°ìŠ¤, ì˜¨ë””ë°”ì´ìŠ¤ ì‘ë™ ì§€ì›' metadata={'source': 'data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf', 'file_path': 'data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf', 'page': 12, 'total_pages': 23, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13462', 'Producer': 'Hancom PDF 1.3.0.542', 'CreationDate': \"D:20231208132838+09'00'\", 'ModDate': \"D:20231208132838+09'00'\", 'PDFVersion': '1.4'}\n",
      "page_content='â–¹ ì‚¼ì„±ì „ì, ìì²´ ê°œë°œ ìƒì„± AI â€˜ì‚¼ì„± ê°€ìš°ìŠ¤â€™ ê³µê°œ Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·10\n",
      "â–¹ êµ¬ê¸€, ì•¤ìŠ¤ë¡œí”½ì— 20ì–µ ë‹¬ëŸ¬ íˆ¬ìë¡œ ìƒì„± AI í˜‘ë ¥ ê°•í™” Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·11\n",
      "â–¹ IDC, 2027ë…„ AI ì†Œí”„íŠ¸ì›¨ì–´ ë§¤ì¶œ 2,500ì–µ ë‹¬ëŸ¬ ëŒíŒŒ ì „ë§Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·12' metadata={'source': 'data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf', 'file_path': 'data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf', 'page': 1, 'total_pages': 23, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13462', 'Producer': 'Hancom PDF 1.3.0.542', 'CreationDate': \"D:20231208132838+09'00'\", 'ModDate': \"D:20231208132838+09'00'\", 'PDFVersion': '1.4'}\n",
      "page_content='Â£ì–¸ì–´, ì½”ë“œ, ì´ë¯¸ì§€ì˜ 3ê°œ ëª¨ë¸ë¡œ êµ¬ì„±ëœ ì‚¼ì„± ê°€ìš°ìŠ¤, ì˜¨ë””ë°”ì´ìŠ¤ ì‘ë™ ì§€ì›\n",
      "n ì‚¼ì„±ì „ìê°€ 2023ë…„ 11ì›” 8ì¼ ì—´ë¦° â€˜ì‚¼ì„± AI í¬ëŸ¼ 2023â€™ í–‰ì‚¬ì—ì„œ ìì²´ ê°œë°œí•œ ìƒì„± AI ëª¨ë¸\n",
      "â€˜ì‚¼ì„± ê°€ìš°ìŠ¤â€™ë¥¼ ìµœì´ˆ ê³µê°œ\n",
      "âˆ™ ì •ê·œë¶„í¬ ì´ë¡ ì„ ì •ë¦½í•œ ì²œì¬ ìˆ˜í•™ì ê°€ìš°ìŠ¤(Gauss)ì˜ ì´ë¦„ì„ ë³¸ëœ¬ ì‚¼ì„± ê°€ìš°ìŠ¤ëŠ” ë‹¤ì–‘í•œ ìƒí™©ì—\n",
      "ìµœì í™”ëœ í¬ê¸°ì˜ ëª¨ë¸ ì„ íƒì´ ê°€ëŠ¥\n",
      "âˆ™ ì‚¼ì„± ê°€ìš°ìŠ¤ëŠ” ë¼ì´ì„ ìŠ¤ë‚˜ ê°œì¸ì •ë³´ë¥¼ ì¹¨í•´í•˜ì§€ ì•ŠëŠ” ì•ˆì „í•œ ë°ì´í„°ë¥¼ í†µí•´ í•™ìŠµë˜ì—ˆìœ¼ë©°,\n",
      "ì˜¨ë””ë°”ì´ìŠ¤ì—ì„œ ì‘ë™í•˜ë„ë¡ ì„¤ê³„ë˜ì–´ ì™¸ë¶€ë¡œ ì‚¬ìš©ìì˜ ì •ë³´ê°€ ìœ ì¶œë˜ì§€ ì•ŠëŠ” ì¥ì ì„ ë³´ìœ ' metadata={'source': 'data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf', 'file_path': 'data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf', 'page': 12, 'total_pages': 23, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13462', 'Producer': 'Hancom PDF 1.3.0.542', 'CreationDate': \"D:20231208132838+09'00'\", 'ModDate': \"D:20231208132838+09'00'\", 'PDFVersion': '1.4'}\n",
      "page_content='ì–´ì‹œìŠ¤í„´íŠ¸ë¥¼ ì ìš©í•œ êµ¬ê¸€ í”½ì…€(Pixel)ê³¼ ê²½ìŸí•  ê²ƒìœ¼ë¡œ ì˜ˆìƒ\n",
      "â˜ ì¶œì²˜ : ì‚¼ì„±ì „ì, â€˜ì‚¼ì„± AI í¬ëŸ¼â€™ì„œ ìì²´ ê°œë°œ ìƒì„±í˜• AI â€˜ì‚¼ì„± ê°€ìš°ìŠ¤â€™ ê³µê°œ, 2023.11.08.\n",
      "ì‚¼ì„±ì „ì, â€˜ì‚¼ì„± ê°œë°œì ì½˜í¼ëŸ°ìŠ¤ ì½”ë¦¬ì•„ 2023â€™ ê°œìµœ, 2023.11.14.\n",
      "TechRepublic, Samsung Gauss: Samsung Research Reveals Generative AI, 2023.11.08.\n",
      "10' metadata={'source': 'data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf', 'file_path': 'data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf', 'page': 12, 'total_pages': 23, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13462', 'Producer': 'Hancom PDF 1.3.0.542', 'CreationDate': \"D:20231208132838+09'00'\", 'ModDate': \"D:20231208132838+09'00'\", 'PDFVersion': '1.4'}\n",
      "page_content='<êµ¬ê¸€ ë”¥ë§ˆì¸ë“œì˜ ë²”ìš© AI ë¶„ë¥˜ í”„ë ˆì„ì›Œí¬>\n",
      "ì„±ëŠ¥ íŠ¹ìˆ˜ AI ì˜ˆì‹œ ë²”ìš© AI ì˜ˆì‹œ\n",
      "0ë‹¨ê³„: AI ì•„ë‹˜ ê³„ì‚°ê¸° ì†Œí”„íŠ¸ì›¨ì–´, ì»´íŒŒì¼ëŸ¬ ì•„ë§ˆì¡´ ë©”ì»¤ë‹ˆì»¬ í„°í¬\n",
      "1ë‹¨ê³„: ì‹ ì§„(ìˆ™ë ¨ë˜ì§€ ì•Šì€ ì¸ê°„) GOFAI(Good Old Fashioned Artificial Intelligence) ì±—GPT, ë°”ë“œ, ë¼ë§ˆ2\n",
      "ìŠ¤ë§ˆíŠ¸ ìŠ¤í”¼ì»¤(ì• í”Œ ì‹œë¦¬, ì•„ë§ˆì¡´ ì•Œë ‰ì‚¬, êµ¬ê¸€\n",
      "2ë‹¨ê³„: ìœ ëŠ¥(ìˆ™ë ¨ëœ ì¸ê°„ì˜ 50% ì´ìƒ) ë¯¸ë‹¬ì„±\n",
      "ì–´ì‹œìŠ¤í„´íŠ¸), IBM ì™“ìŠ¨\n",
      "3ë‹¨ê³„: ì „ë¬¸ê°€(ìˆ™ë ¨ëœ ì¸ê°„ì˜ 90% ì´ìƒ) ë¬¸ë²• êµì •ê¸°(ê·¸ë˜ë¨¸ë¦¬), ìƒì„± ì´ë¯¸ì§€ ëª¨ë¸(ë‹¬ë¦¬2) ë¯¸ë‹¬ì„±' metadata={'source': 'data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf', 'file_path': 'data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf', 'page': 18, 'total_pages': 23, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13462', 'Producer': 'Hancom PDF 1.3.0.542', 'CreationDate': \"D:20231208132838+09'00'\", 'ModDate': \"D:20231208132838+09'00'\", 'PDFVersion': '1.4'}\n",
      "page_content='ì œì‘ì„ í¬í•¨\n",
      "n ì•Œë¦¬ë°”ë°” í´ë¼ìš°ë“œëŠ” ê¸‰ì¦í•˜ëŠ” ìƒì„± AI ìˆ˜ìš”ì— ëŒ€ì‘í•´ ëª¨ë¸ ê°œë°œê³¼ ì• í”Œë¦¬ì¼€ì´ì…˜ êµ¬ì¶• ì ˆì°¨ë¥¼\n",
      "ê°„ì†Œí™”í•˜ëŠ” ì˜¬ì¸ì› AI ëª¨ë¸ êµ¬ì¶• í”Œë«í¼ â€˜ì  AI(GenAI)â€™ë„ ê³µê°œ\n",
      "âˆ™ ì´ í”Œë«í¼ì€ ë°ì´í„° ê´€ë¦¬, ëª¨ë¸ ë°°í¬ì™€ í‰ê°€, ì‹ ì†í•œ ì—”ì§€ë‹ˆì–´ë§ì„ ìœ„í•œ ì¢…í•© ë„êµ¬ ëª¨ìŒì„ ì œê³µí•˜ì—¬\n",
      "ë‹¤ì–‘í•œ ê¸°ì—…ë“¤ì´ ë§ì¶¤í˜• AI ëª¨ë¸ì„ í•œì¸µ ì‰½ê²Œ ê°œë°œí•  ìˆ˜ ìˆë„ë¡ ì§€ì›\n",
      "âˆ™ ìƒì„± AI ê°œë°œì— í•„ìš”í•œ ì»´í“¨íŒ…ê³¼ ë°ì´í„° ì²˜ë¦¬ ìš”êµ¬ì‚¬í•­ì„ ì§€ì›í•˜ê¸° ìœ„í•´ AI í”Œë«í¼(PAI),\n",
      "ë°ì´í„°ë² ì´ìŠ¤ ì†”ë£¨ì…˜, ì»¨í…Œì´ë„ˆ ì„œë¹„ìŠ¤ì™€ ê°™ì€ í´ë¼ìš°ë“œ ì‹ ì œí’ˆë„ ë°œí‘œ' metadata={'source': 'data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf', 'file_path': 'data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf', 'page': 11, 'total_pages': 23, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13462', 'Producer': 'Hancom PDF 1.3.0.542', 'CreationDate': \"D:20231208132838+09'00'\", 'ModDate': \"D:20231208132838+09'00'\", 'PDFVersion': '1.4'}\n",
      "page_content='ì²˜ë¦¬ë¥¼ ì§€ì›\n",
      "âˆ™ ì½”ë“œ ëª¨ë¸ ê¸°ë°˜ì˜ AI ì½”ë”© ì–´ì‹œìŠ¤í„´íŠ¸ â€˜ì½”ë“œì•„ì´(code.i)â€™ëŠ” ëŒ€í™”í˜• ì¸í„°í˜ì´ìŠ¤ë¡œ ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•˜ë©°\n",
      "ì‚¬ë‚´ ì†Œí”„íŠ¸ì›¨ì–´ ê°œë°œì— ìµœì í™”\n",
      "âˆ™ ì´ë¯¸ì§€ ëª¨ë¸ì€ ì°½ì˜ì ì¸ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ê³  ê¸°ì¡´ ì´ë¯¸ì§€ë¥¼ ì›í•˜ëŠ” ëŒ€ë¡œ ë°”ê¿€ ìˆ˜ ìˆë„ë¡ ì§€ì›í•˜ë©°\n",
      "ì €í•´ìƒë„ ì´ë¯¸ì§€ì˜ ê³ í•´ìƒë„ ì „í™˜ë„ ì§€ì›\n",
      "n IT ì „ë¬¸ì§€ í…Œí¬ë¦¬í¼ë¸”ë¦­(TechRepublic)ì€ ì˜¨ë””ë°”ì´ìŠ¤ AIê°€ ì£¼ìš” ê¸°ìˆ  íŠ¸ë Œë“œë¡œ ë¶€ìƒí–ˆë‹¤ë©°,\n",
      "2024ë…„ë¶€í„° ê°€ìš°ìŠ¤ë¥¼ íƒ‘ì¬í•œ ì‚¼ì„± ìŠ¤ë§ˆíŠ¸í°ì´ ë©”íƒ€ì˜ ë¼ë§ˆ(Llama)2ë¥¼ íƒ‘ì¬í•œ í€„ì»´ ê¸°ê¸° ë° êµ¬ê¸€' metadata={'source': 'data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf', 'file_path': 'data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf', 'page': 12, 'total_pages': 23, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13462', 'Producer': 'Hancom PDF 1.3.0.542', 'CreationDate': \"D:20231208132838+09'00'\", 'ModDate': \"D:20231208132838+09'00'\", 'PDFVersion': '1.4'}\n",
      "page_content='ì˜¨ë””ë°”ì´ìŠ¤ì—ì„œ ì‘ë™í•˜ë„ë¡ ì„¤ê³„ë˜ì–´ ì™¸ë¶€ë¡œ ì‚¬ìš©ìì˜ ì •ë³´ê°€ ìœ ì¶œë˜ì§€ ì•ŠëŠ” ì¥ì ì„ ë³´ìœ \n",
      "âˆ™ ì‚¼ì„±ì „ìëŠ” ì‚¼ì„± ê°€ìš°ìŠ¤ë¥¼ í™œìš©í•œ ì˜¨ë””ë°”ì´ìŠ¤ AI ê¸°ìˆ ë„ ì†Œê°œí–ˆìœ¼ë©°, ìƒì„± AI ëª¨ë¸ì„ ë‹¤ì–‘í•œ ì œí’ˆì—\n",
      "ë‹¨ê³„ì ìœ¼ë¡œ íƒ‘ì¬í•  ê³„íš\n",
      "n ì‚¼ì„± ê°€ìš°ìŠ¤ëŠ” â–³í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” ì–¸ì–´ëª¨ë¸ â–³ì½”ë“œë¥¼ ìƒì„±í•˜ëŠ” ì½”ë“œ ëª¨ë¸ â–³ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ëŠ”\n",
      "ì´ë¯¸ì§€ ëª¨ë¸ì˜ 3ê°œ ëª¨ë¸ë¡œ êµ¬ì„±\n",
      "âˆ™ ì–¸ì–´ ëª¨ë¸ì€ í´ë¼ìš°ë“œì™€ ì˜¨ë””ë°”ì´ìŠ¤ ëŒ€ìƒ ë‹¤ì–‘í•œ ëª¨ë¸ë¡œ êµ¬ì„±ë˜ë©°, ë©”ì¼ ì‘ì„±, ë¬¸ì„œ ìš”ì•½, ë²ˆì—­ ì—…ë¬´ì˜\n",
      "ì²˜ë¦¬ë¥¼ ì§€ì›' metadata={'source': 'data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf', 'file_path': 'data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf', 'page': 12, 'total_pages': 23, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13462', 'Producer': 'Hancom PDF 1.3.0.542', 'CreationDate': \"D:20231208132838+09'00'\", 'ModDate': \"D:20231208132838+09'00'\", 'PDFVersion': '1.4'}\n",
      "page_content='â–¹ ë¹Œ ê²Œì´ì¸ , AI ì—ì´ì „íŠ¸ë¡œ ì¸í•œ ì»´í“¨í„° ì‚¬ìš©ì˜ íŒ¨ëŸ¬ë‹¤ì„ ë³€í™” ì „ë§Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·13\n",
      "â–¹ ìœ íŠœë¸Œ, 2024ë…„ë¶€í„° AI ìƒì„± ì½˜í…ì¸  í‘œì‹œ ì˜ë¬´í™”Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·14\n",
      "3. ê¸°ìˆ /ì—°êµ¬\n",
      "â–¹ ì˜êµ­ ê³¼í•™í˜ì‹ ê¸°ìˆ ë¶€, AI ì•ˆì „ ì—°êµ¬ì†Œ ì„¤ë¦½ ë°œí‘œÂ·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·15' metadata={'source': 'data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf', 'file_path': 'data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf', 'page': 1, 'total_pages': 23, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13462', 'Producer': 'Hancom PDF 1.3.0.542', 'CreationDate': \"D:20231208132838+09'00'\", 'ModDate': \"D:20231208132838+09'00'\", 'PDFVersion': '1.4'}\n",
      "page_content='Â£ì±—GPTì™€ êµ¬ê¸€ ë°”ë“œì™€ ê°™ì€ AI ì±—ë´‡ì€ ë²”ìš© AI 1ë‹¨ê³„ ìˆ˜ì¤€\n",
      "n êµ¬ê¸€ ë”¥ë§ˆì¸ë“œ ì—°êµ¬ì§„ì€ 2023ë…„ 11ì›” 4ì¼ ë²”ìš© AI(Artificial General Intelligence, AGI) ëª¨ë¸ì„ ìš©ë„ì™€\n",
      "ì„±ëŠ¥ì— ë”°ë¼ ë¶„ë¥˜í•˜ëŠ” í”„ë ˆì„ì›Œí¬ë¥¼ ì œì‹œí•œ ë…¼ë¬¸ì„ ë°œí‘œ\n",
      "âˆ™ í”„ë ˆì„ì›Œí¬ì˜ ëª©ì ì€ AGIì˜ ì„±ëŠ¥, ë²”ìš©ì„±, ììœ¨ì„± ìˆ˜ì¤€ì„ ì •ì˜í•˜ì—¬ ëª¨ë¸ ê°„ ë¹„êµì™€ ìœ„í—˜ í‰ê°€, AGI\n",
      "ë‹¬ì„±ê¹Œì§€ì˜ ì§„í–‰ ìƒí™©ì„ ì¸¡ì •í•  ìˆ˜ ìˆëŠ” ê³µí†µ ê¸°ì¤€ì„ ì œê³µí•˜ê¸° ìœ„í•¨\n",
      "n ì—°êµ¬ì§„ì€ AGI ê°œë… ì •ì˜ì— í•„ìš”í•œ ê¸°ì¤€ì„ ìˆ˜ë¦½í•˜ê¸° ìœ„í•œ 6ê°€ì§€ ì›ì¹™ì„ ì•„ë˜ì™€ ê°™ì´ ë„ì¶œ' metadata={'source': 'data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf', 'file_path': 'data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf', 'page': 18, 'total_pages': 23, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13462', 'Producer': 'Hancom PDF 1.3.0.542', 'CreationDate': \"D:20231208132838+09'00'\", 'ModDate': \"D:20231208132838+09'00'\", 'PDFVersion': '1.4'}\n",
      "==================================================\n",
      "==== [GRADE DOCUMENTS] ====\n",
      "==== GRADE: DOCUMENT RELEVANT ====\n",
      "==== GRADE: DOCUMENT RELEVANT ====\n",
      "==== GRADE: DOCUMENT RELEVANT ====\n",
      "==== GRADE: DOCUMENT RELEVANT ====\n",
      "==== GRADE: DOCUMENT NOT RELEVANT ====\n",
      "==== GRADE: DOCUMENT NOT RELEVANT ====\n",
      "==== GRADE: DOCUMENT RELEVANT ====\n",
      "==== GRADE: DOCUMENT RELEVANT ====\n",
      "==== GRADE: DOCUMENT NOT RELEVANT ====\n",
      "==== GRADE: DOCUMENT NOT RELEVANT ====\n",
      "==== [ASSESS GRADED DOCUMENTS] ====\n",
      "==== [DECISION: GENERATE] ====\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mgrade_documents\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "page_content='SPRi AI Brief |\n",
      "2023-12ì›”í˜¸\n",
      "ì‚¼ì„±ì „ì, ìì²´ ê°œë°œ ìƒì„± AI â€˜ì‚¼ì„± ê°€ìš°ìŠ¤â€™ ê³µê°œ\n",
      "KEY Contents\n",
      "n ì‚¼ì„±ì „ìê°€ ì˜¨ë””ë°”ì´ìŠ¤ì—ì„œ ì‘ë™ ê°€ëŠ¥í•˜ë©° ì–¸ì–´, ì½”ë“œ, ì´ë¯¸ì§€ì˜ 3ê°œ ëª¨ë¸ë¡œ êµ¬ì„±ëœ ìì²´ ê°œë°œ ìƒì„±\n",
      "AI ëª¨ë¸ â€˜ì‚¼ì„± ê°€ìš°ìŠ¤â€™ë¥¼ ê³µê°œ\n",
      "n ì‚¼ì„±ì „ìëŠ” ì‚¼ì„± ê°€ìš°ìŠ¤ë¥¼ ë‹¤ì–‘í•œ ì œí’ˆì— ë‹¨ê³„ì ìœ¼ë¡œ íƒ‘ì¬í•  ê³„íšìœ¼ë¡œ, ì˜¨ë””ë°”ì´ìŠ¤ ì‘ë™ì´ ê°€ëŠ¥í•œ\n",
      "ì‚¼ì„± ê°€ìš°ìŠ¤ëŠ” ì™¸ë¶€ë¡œ ì‚¬ìš©ì ì •ë³´ê°€ ìœ ì¶œë  ìœ„í—˜ì´ ì—†ë‹¤ëŠ” ì¥ì ì„ ë³´ìœ \n",
      "Â£ì–¸ì–´, ì½”ë“œ, ì´ë¯¸ì§€ì˜ 3ê°œ ëª¨ë¸ë¡œ êµ¬ì„±ëœ ì‚¼ì„± ê°€ìš°ìŠ¤, ì˜¨ë””ë°”ì´ìŠ¤ ì‘ë™ ì§€ì›' metadata={'source': 'data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf', 'file_path': 'data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf', 'page': 12, 'total_pages': 23, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13462', 'Producer': 'Hancom PDF 1.3.0.542', 'CreationDate': \"D:20231208132838+09'00'\", 'ModDate': \"D:20231208132838+09'00'\", 'PDFVersion': '1.4'}\n",
      "page_content='â–¹ ì‚¼ì„±ì „ì, ìì²´ ê°œë°œ ìƒì„± AI â€˜ì‚¼ì„± ê°€ìš°ìŠ¤â€™ ê³µê°œ Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·10\n",
      "â–¹ êµ¬ê¸€, ì•¤ìŠ¤ë¡œí”½ì— 20ì–µ ë‹¬ëŸ¬ íˆ¬ìë¡œ ìƒì„± AI í˜‘ë ¥ ê°•í™” Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·11\n",
      "â–¹ IDC, 2027ë…„ AI ì†Œí”„íŠ¸ì›¨ì–´ ë§¤ì¶œ 2,500ì–µ ë‹¬ëŸ¬ ëŒíŒŒ ì „ë§Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·12' metadata={'source': 'data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf', 'file_path': 'data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf', 'page': 1, 'total_pages': 23, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13462', 'Producer': 'Hancom PDF 1.3.0.542', 'CreationDate': \"D:20231208132838+09'00'\", 'ModDate': \"D:20231208132838+09'00'\", 'PDFVersion': '1.4'}\n",
      "page_content='Â£ì–¸ì–´, ì½”ë“œ, ì´ë¯¸ì§€ì˜ 3ê°œ ëª¨ë¸ë¡œ êµ¬ì„±ëœ ì‚¼ì„± ê°€ìš°ìŠ¤, ì˜¨ë””ë°”ì´ìŠ¤ ì‘ë™ ì§€ì›\n",
      "n ì‚¼ì„±ì „ìê°€ 2023ë…„ 11ì›” 8ì¼ ì—´ë¦° â€˜ì‚¼ì„± AI í¬ëŸ¼ 2023â€™ í–‰ì‚¬ì—ì„œ ìì²´ ê°œë°œí•œ ìƒì„± AI ëª¨ë¸\n",
      "â€˜ì‚¼ì„± ê°€ìš°ìŠ¤â€™ë¥¼ ìµœì´ˆ ê³µê°œ\n",
      "âˆ™ ì •ê·œë¶„í¬ ì´ë¡ ì„ ì •ë¦½í•œ ì²œì¬ ìˆ˜í•™ì ê°€ìš°ìŠ¤(Gauss)ì˜ ì´ë¦„ì„ ë³¸ëœ¬ ì‚¼ì„± ê°€ìš°ìŠ¤ëŠ” ë‹¤ì–‘í•œ ìƒí™©ì—\n",
      "ìµœì í™”ëœ í¬ê¸°ì˜ ëª¨ë¸ ì„ íƒì´ ê°€ëŠ¥\n",
      "âˆ™ ì‚¼ì„± ê°€ìš°ìŠ¤ëŠ” ë¼ì´ì„ ìŠ¤ë‚˜ ê°œì¸ì •ë³´ë¥¼ ì¹¨í•´í•˜ì§€ ì•ŠëŠ” ì•ˆì „í•œ ë°ì´í„°ë¥¼ í†µí•´ í•™ìŠµë˜ì—ˆìœ¼ë©°,\n",
      "ì˜¨ë””ë°”ì´ìŠ¤ì—ì„œ ì‘ë™í•˜ë„ë¡ ì„¤ê³„ë˜ì–´ ì™¸ë¶€ë¡œ ì‚¬ìš©ìì˜ ì •ë³´ê°€ ìœ ì¶œë˜ì§€ ì•ŠëŠ” ì¥ì ì„ ë³´ìœ ' metadata={'source': 'data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf', 'file_path': 'data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf', 'page': 12, 'total_pages': 23, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13462', 'Producer': 'Hancom PDF 1.3.0.542', 'CreationDate': \"D:20231208132838+09'00'\", 'ModDate': \"D:20231208132838+09'00'\", 'PDFVersion': '1.4'}\n",
      "page_content='ì–´ì‹œìŠ¤í„´íŠ¸ë¥¼ ì ìš©í•œ êµ¬ê¸€ í”½ì…€(Pixel)ê³¼ ê²½ìŸí•  ê²ƒìœ¼ë¡œ ì˜ˆìƒ\n",
      "â˜ ì¶œì²˜ : ì‚¼ì„±ì „ì, â€˜ì‚¼ì„± AI í¬ëŸ¼â€™ì„œ ìì²´ ê°œë°œ ìƒì„±í˜• AI â€˜ì‚¼ì„± ê°€ìš°ìŠ¤â€™ ê³µê°œ, 2023.11.08.\n",
      "ì‚¼ì„±ì „ì, â€˜ì‚¼ì„± ê°œë°œì ì½˜í¼ëŸ°ìŠ¤ ì½”ë¦¬ì•„ 2023â€™ ê°œìµœ, 2023.11.14.\n",
      "TechRepublic, Samsung Gauss: Samsung Research Reveals Generative AI, 2023.11.08.\n",
      "10' metadata={'source': 'data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf', 'file_path': 'data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf', 'page': 12, 'total_pages': 23, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13462', 'Producer': 'Hancom PDF 1.3.0.542', 'CreationDate': \"D:20231208132838+09'00'\", 'ModDate': \"D:20231208132838+09'00'\", 'PDFVersion': '1.4'}\n",
      "page_content='ì²˜ë¦¬ë¥¼ ì§€ì›\n",
      "âˆ™ ì½”ë“œ ëª¨ë¸ ê¸°ë°˜ì˜ AI ì½”ë”© ì–´ì‹œìŠ¤í„´íŠ¸ â€˜ì½”ë“œì•„ì´(code.i)â€™ëŠ” ëŒ€í™”í˜• ì¸í„°í˜ì´ìŠ¤ë¡œ ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•˜ë©°\n",
      "ì‚¬ë‚´ ì†Œí”„íŠ¸ì›¨ì–´ ê°œë°œì— ìµœì í™”\n",
      "âˆ™ ì´ë¯¸ì§€ ëª¨ë¸ì€ ì°½ì˜ì ì¸ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ê³  ê¸°ì¡´ ì´ë¯¸ì§€ë¥¼ ì›í•˜ëŠ” ëŒ€ë¡œ ë°”ê¿€ ìˆ˜ ìˆë„ë¡ ì§€ì›í•˜ë©°\n",
      "ì €í•´ìƒë„ ì´ë¯¸ì§€ì˜ ê³ í•´ìƒë„ ì „í™˜ë„ ì§€ì›\n",
      "n IT ì „ë¬¸ì§€ í…Œí¬ë¦¬í¼ë¸”ë¦­(TechRepublic)ì€ ì˜¨ë””ë°”ì´ìŠ¤ AIê°€ ì£¼ìš” ê¸°ìˆ  íŠ¸ë Œë“œë¡œ ë¶€ìƒí–ˆë‹¤ë©°,\n",
      "2024ë…„ë¶€í„° ê°€ìš°ìŠ¤ë¥¼ íƒ‘ì¬í•œ ì‚¼ì„± ìŠ¤ë§ˆíŠ¸í°ì´ ë©”íƒ€ì˜ ë¼ë§ˆ(Llama)2ë¥¼ íƒ‘ì¬í•œ í€„ì»´ ê¸°ê¸° ë° êµ¬ê¸€' metadata={'source': 'data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf', 'file_path': 'data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf', 'page': 12, 'total_pages': 23, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13462', 'Producer': 'Hancom PDF 1.3.0.542', 'CreationDate': \"D:20231208132838+09'00'\", 'ModDate': \"D:20231208132838+09'00'\", 'PDFVersion': '1.4'}\n",
      "page_content='ì˜¨ë””ë°”ì´ìŠ¤ì—ì„œ ì‘ë™í•˜ë„ë¡ ì„¤ê³„ë˜ì–´ ì™¸ë¶€ë¡œ ì‚¬ìš©ìì˜ ì •ë³´ê°€ ìœ ì¶œë˜ì§€ ì•ŠëŠ” ì¥ì ì„ ë³´ìœ \n",
      "âˆ™ ì‚¼ì„±ì „ìëŠ” ì‚¼ì„± ê°€ìš°ìŠ¤ë¥¼ í™œìš©í•œ ì˜¨ë””ë°”ì´ìŠ¤ AI ê¸°ìˆ ë„ ì†Œê°œí–ˆìœ¼ë©°, ìƒì„± AI ëª¨ë¸ì„ ë‹¤ì–‘í•œ ì œí’ˆì—\n",
      "ë‹¨ê³„ì ìœ¼ë¡œ íƒ‘ì¬í•  ê³„íš\n",
      "n ì‚¼ì„± ê°€ìš°ìŠ¤ëŠ” â–³í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” ì–¸ì–´ëª¨ë¸ â–³ì½”ë“œë¥¼ ìƒì„±í•˜ëŠ” ì½”ë“œ ëª¨ë¸ â–³ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ëŠ”\n",
      "ì´ë¯¸ì§€ ëª¨ë¸ì˜ 3ê°œ ëª¨ë¸ë¡œ êµ¬ì„±\n",
      "âˆ™ ì–¸ì–´ ëª¨ë¸ì€ í´ë¼ìš°ë“œì™€ ì˜¨ë””ë°”ì´ìŠ¤ ëŒ€ìƒ ë‹¤ì–‘í•œ ëª¨ë¸ë¡œ êµ¬ì„±ë˜ë©°, ë©”ì¼ ì‘ì„±, ë¬¸ì„œ ìš”ì•½, ë²ˆì—­ ì—…ë¬´ì˜\n",
      "ì²˜ë¦¬ë¥¼ ì§€ì›' metadata={'source': 'data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf', 'file_path': 'data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf', 'page': 12, 'total_pages': 23, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13462', 'Producer': 'Hancom PDF 1.3.0.542', 'CreationDate': \"D:20231208132838+09'00'\", 'ModDate': \"D:20231208132838+09'00'\", 'PDFVersion': '1.4'}\n",
      "==================================================\n",
      "==== [GENERATE] ====\n",
      "==== [CHECK HALLUCINATIONS] ====\n",
      "==== [DECISION: GENERATION IS GROUNDED IN DOCUMENTS] ====\n",
      "==== [GRADE GENERATION vs QUESTION] ====\n",
      "==== [DECISION: GENERATION ADDRESSES QUESTION] ====\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mgenerate\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "\u001b[1;32mgeneration\u001b[0m:\n",
      "ì‚¼ì„±ì „ìê°€ ê°œë°œí•œ ìƒì„±í˜• AIì˜ ì´ë¦„ì€ â€˜ì‚¼ì„± ê°€ìš°ìŠ¤â€™ì´ë‹¤.\n",
      "\n",
      "**Source**\n",
      "- data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf (p.12)\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_teddynote.messages import stream_graph, invoke_graph, random_uuid\n",
    "\n",
    "# config ì„¤ì •(ì¬ê·€ ìµœëŒ€ íšŸìˆ˜, thread_id)\n",
    "config = RunnableConfig(recursion_limit=10, configurable={\"thread_id\": random_uuid()})\n",
    "\n",
    "# ì§ˆë¬¸ ì…ë ¥\n",
    "inputs = {\n",
    "    \"question\": \"ì‚¼ì„±ì „ìê°€ ê°œë°œí•œ ìƒì„±í˜• AI ì˜ ì´ë¦„ì€?\",\n",
    "}\n",
    "\n",
    "# ê·¸ë˜í”„ ì‹¤í–‰\n",
    "invoke_graph(\n",
    "    app, inputs, config, [\"retrieve\", \"transform_query\", \"grade_documents\", \"generate\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== [RETRIEVE] ====\n",
      "==== [GRADE DOCUMENTS] ====\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mgrade_documents\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "{\"binary_score\":\"no\"}==== GRADE: DOCUMENT NOT RELEVANT ====\n",
      "{\"binary_score\":\"no\"}==== GRADE: DOCUMENT NOT RELEVANT ====\n",
      "{\"binary_score\":\"no\"}==== GRADE: DOCUMENT NOT RELEVANT ====\n",
      "{\"binary_score\":\"no\"}==== GRADE: DOCUMENT NOT RELEVANT ====\n",
      "{\"binary_score\":\"no\"}==== GRADE: DOCUMENT NOT RELEVANT ====\n",
      "{\"binary_score\":\"==== GRADE: DOCUMENT NOT RELEVANT ====\n",
      "no\"}{\"binary_score\":\"no==== GRADE: DOCUMENT NOT RELEVANT ====\n",
      "\"}{\"binary_score\":\"no\"}==== GRADE: DOCUMENT NOT RELEVANT ====\n",
      "{\"binary_score\":\"no\"}==== GRADE: DOCUMENT NOT RELEVANT ====\n",
      "{\"binary_score\":\"no\"}==== GRADE: DOCUMENT NOT RELEVANT ====\n",
      "==== [ASSESS GRADED DOCUMENTS] ====\n",
      "==== [DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY] ====\n",
      "==== [TRANSFORM QUERY] ====\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mtransform_query\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "í…Œë””ë…¸íŠ¸ê°€ ê°œë°œí•œ ìƒì„±í˜• ì¸ê³µì§€ëŠ¥(AI)ì˜ ê³µì‹ ëª…ì¹­ì€ ë¬´ì—‡ì¸ê°€ìš”?==== [RETRIEVE] ====\n",
      "==== [GRADE DOCUMENTS] ====\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mgrade_documents\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "{\"binary_score\":\"no\"}==== GRADE: DOCUMENT NOT RELEVANT ====\n",
      "{\"binary_score\":\"no\"}==== GRADE: DOCUMENT NOT RELEVANT ====\n",
      "{\"binary_score\":\"no\"}==== GRADE: DOCUMENT NOT RELEVANT ====\n",
      "{\"binary_score\":\"no\"}==== GRADE: DOCUMENT NOT RELEVANT ====\n",
      "{\"binary_score\":\"no\"}==== GRADE: DOCUMENT NOT RELEVANT ====\n",
      "{\"binary_score\":\"no\"}==== GRADE: DOCUMENT NOT RELEVANT ====\n",
      "{\"binary_score\":\"no\"}==== GRADE: DOCUMENT NOT RELEVANT ====\n",
      "{\"binary==== GRADE: DOCUMENT NOT RELEVANT ====_score\n",
      "\":\"no\"}{\"binary_score\":\"no\"}==== GRADE: DOCUMENT NOT RELEVANT ====\n",
      "{\"binary_score\":\"no\"}==== GRADE: DOCUMENT NOT RELEVANT ====\n",
      "==== [ASSESS GRADED DOCUMENTS] ====\n",
      "==== [DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY] ====\n",
      "==== [TRANSFORM QUERY] ====\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mtransform_query\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "What is the official name of the generative artificial intelligence (AI) developed by TeddyNote?==== [RETRIEVE] ====\n",
      "==== [GRADE DOCUMENTS] ====\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mgrade_documents\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "{\"binary_score\":\"no\"}==== GRADE: DOCUMENT NOT RELEVANT ====\n",
      "{\"binary_score\":\"no\"}==== GRADE: DOCUMENT NOT RELEVANT ====\n",
      "{\"binary_score\":\"no\"}==== GRADE: DOCUMENT NOT RELEVANT ====\n",
      "{\"binary_score\":\"no\"}==== GRADE: DOCUMENT NOT RELEVANT ====\n",
      "{\"binary_score\":\"no\"}==== GRADE: DOCUMENT NOT RELEVANT ====\n",
      "{\"binary_score\":\"no\"}==== GRADE: DOCUMENT NOT RELEVANT ====\n",
      "{\"binary_score\":\"no\"}==== GRADE: DOCUMENT NOT RELEVANT ====\n",
      "{\"binary_score\":\"no\"}==== GRADE: DOCUMENT NOT RELEVANT ====\n",
      "{\"binary_score\":\"no\"}==== GRADE: DOCUMENT NOT RELEVANT ====\n",
      "{\"binary_score\":\"no\"}==== GRADE: DOCUMENT NOT RELEVANT ====\n",
      "==== [ASSESS GRADED DOCUMENTS] ====\n",
      "==== [DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY] ====\n",
      "==== [TRANSFORM QUERY] ====\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mtransform_query\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "What is the official name of the generative AI model created by TeddyNote?==== [RETRIEVE] ====\n",
      "GraphRecursionError: Recursion limit of 10 reached without hitting a stop condition. You can increase the limit by setting the `recursion_limit` config key.\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/GRAPH_RECURSION_LIMIT\n"
     ]
    }
   ],
   "source": [
    "from langgraph.errors import GraphRecursionError\n",
    "\n",
    "# config ì„¤ì •(ì¬ê·€ ìµœëŒ€ íšŸìˆ˜, thread_id)\n",
    "config = RunnableConfig(recursion_limit=10, configurable={\"thread_id\": random_uuid()})\n",
    "\n",
    "# ì§ˆë¬¸ ì…ë ¥\n",
    "inputs = {\n",
    "    \"question\": \"í…Œë””ë…¸íŠ¸ê°€ ê°œë°œí•œ ìƒì„±í˜• AI ì˜ ì´ë¦„ì€?\",\n",
    "}\n",
    "\n",
    "try:\n",
    "    # ê·¸ë˜í”„ ì‹¤í–‰\n",
    "    stream_graph(\n",
    "        app,\n",
    "        inputs,\n",
    "        config,\n",
    "        [\"retrieve\", \"transform_query\", \"grade_documents\", \"generate\"],\n",
    "    )\n",
    "except GraphRecursionError as recursion_error:\n",
    "    print(f\"GraphRecursionError: {recursion_error}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "05. ê³„íš í›„ ì‹¤í–‰(Plan-and-Execute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-4.1\n"
     ]
    }
   ],
   "source": [
    "from langchain_teddynote.models import get_model_name, LLMs\n",
    "\n",
    "# ëª¨ë¸ëª… ì •ì˜\n",
    "MODEL_NAME = get_model_name(LLMs.GPT4_1)\n",
    "print(MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote.tools import TavilySearch\n",
    "\n",
    "# Tavily ê²€ìƒ‰ ë„êµ¬ ì´ˆê¸°í™”\n",
    "tools = [TavilySearch(max_results=3)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸ ì •ì˜\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer in Korean.\",\n",
    "        ),\n",
    "        (\"human\", \"{messages}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# LLM ì •ì˜\n",
    "llm = ChatOpenAI(model=MODEL_NAME, temperature=0)\n",
    "\n",
    "# ReAct ì—ì´ì „íŠ¸ ìƒì„±\n",
    "agent_executor = create_react_agent(llm, tools, prompt=prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='ë­ì²´ì¸ í•œêµ­ì–´ íŠœí† ë¦¬ì–¼ì— ëŒ€í•´ì„œ ì„¤ëª…í•´ì¤˜', additional_kwargs={}, response_metadata={}, id='ab40a1bb-783c-4835-a938-168e8316ab2f'),\n",
       "  AIMessage(content='ë­ì²´ì¸(LangChain)ì€ ìì—°ì–´ ì²˜ë¦¬(NLP)ì™€ ì¸ê³µì§€ëŠ¥(AI) ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ì‰½ê²Œ ë§Œë“¤ ìˆ˜ ìˆë„ë¡ ë„ì™€ì£¼ëŠ” íŒŒì´ì¬ ê¸°ë°˜ì˜ ì˜¤í”ˆì†ŒìŠ¤ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. íŠ¹íˆ ì±—ë´‡, ë¬¸ì„œ ìš”ì•½, ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œ ë“± ë‹¤ì–‘í•œ LLM(ëŒ€í˜• ì–¸ì–´ ëª¨ë¸) ê¸°ë°˜ ì„œë¹„ìŠ¤ë¥¼ ê°œë°œí•  ë•Œ ë§ì´ ì‚¬ìš©ë©ë‹ˆë‹¤.\\n\\ní•œêµ­ì–´ íŠœí† ë¦¬ì–¼ì— ëŒ€í•´ ê°„ë‹¨íˆ ì„¤ëª…ë“œë¦¬ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\\n\\n1. ë­ì²´ì¸ ê¸°ë³¸ ê°œë…\\n- ë­ì²´ì¸ì€ LLM(ì˜ˆ: GPT-3, GPT-4 ë“±)ê³¼ ë‹¤ì–‘í•œ ë°ì´í„° ì†ŒìŠ¤(ë¬¸ì„œ, ì›¹, ë°ì´í„°ë² ì´ìŠ¤ ë“±)ë¥¼ ì—°ê²°í•´ì£¼ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤.\\n- ì²´ì¸(Chain), ì—ì´ì „íŠ¸(Agent), í”„ë¡¬í”„íŠ¸(Prompt) ë“± ì—¬ëŸ¬ ì»´í¬ë„ŒíŠ¸ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\\n\\n2. ì„¤ì¹˜ ë°©ë²•\\n- pip install langchain ëª…ë ¹ì–´ë¡œ ì„¤ì¹˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n- OpenAI API í‚¤ ë“± LLMì„ ì‚¬ìš©í•˜ê¸° ìœ„í•œ í™˜ê²½ì„¤ì •ì´ í•„ìš”í•©ë‹ˆë‹¤.\\n\\n3. ê¸°ë³¸ ì‚¬ìš©ë²•\\n- í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ ë§Œë“¤ì–´ LLMì— ì…ë ¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n- ì²´ì¸ì„ ì´ìš©í•´ ì—¬ëŸ¬ ì‘ì—…(ì˜ˆ: ì§ˆë¬¸ â†’ ê²€ìƒ‰ â†’ ìš”ì•½)ì„ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n- ì—ì´ì „íŠ¸ë¥¼ ì‚¬ìš©í•˜ë©´ LLMì´ ë„êµ¬(ì˜ˆ: ê³„ì‚°ê¸°, ê²€ìƒ‰ì—”ì§„ ë“±)ë¥¼ ì§ì ‘ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n\\n4. í•œêµ­ì–´ ì§€ì›\\n- ë­ì²´ì¸ì€ í•œêµ­ì–´ë„ ì§€ì›í•©ë‹ˆë‹¤. ë‹¤ë§Œ, ì‚¬ìš©í•˜ëŠ” LLMì´ í•œêµ­ì–´ë¥¼ ì˜ ì´í•´í•´ì•¼ í•˜ë¯€ë¡œ, í•œêµ­ì–´ ì§€ì›ì´ ì¢‹ì€ ëª¨ë¸(OpenAI GPT-3.5/4, KoGPT ë“±)ì„ ì„ íƒí•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.\\n\\n5. ì˜ˆì œ ì½”ë“œ\\nì•„ë˜ëŠ” ê°„ë‹¨í•œ ë­ì²´ì¸ í•œêµ­ì–´ ì˜ˆì œì…ë‹ˆë‹¤.\\n\\nfrom langchain.llms import OpenAI\\nfrom langchain.prompts import PromptTemplate\\nfrom langchain.chains import LLMChain\\n\\nllm = OpenAI(model_name=\"gpt-3.5-turbo\", openai_api_key=\"YOUR_API_KEY\")\\nprompt = PromptTemplate(template=\"í•œêµ­ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì¸ê°€ìš”?\", input_variables=[])\\nchain = LLMChain(llm=llm, prompt=prompt)\\nresult = chain.run()\\nprint(result)  # \"ì„œìš¸ì…ë‹ˆë‹¤.\"ì™€ ê°™ì€ ë‹µë³€ ì¶œë ¥\\n\\n6. ì°¸ê³  ìë£Œ\\n- ê³µì‹ ë¬¸ì„œ: https://python.langchain.com/\\n- í•œêµ­ì–´ íŠœí† ë¦¬ì–¼ ë¸”ë¡œê·¸, ìœ íŠœë¸Œ ë“±ë„ ê²€ìƒ‰í•˜ë©´ ë‹¤ì–‘í•œ ìë£Œë¥¼ ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n\\ní˜¹ì‹œ ë” êµ¬ì²´ì ì¸ ì˜ˆì œë‚˜, íŠ¹ì • ê¸°ëŠ¥(ì˜ˆ: ë¬¸ì„œ ìš”ì•½, ì§ˆì˜ì‘ë‹µ ë“±)ì— ëŒ€í•œ íŠœí† ë¦¬ì–¼ì´ í•„ìš”í•˜ì‹œë©´ ë§ì”€í•´ ì£¼ì„¸ìš”!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 578, 'prompt_tokens': 153, 'total_tokens': 731, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-2025-04-14', 'system_fingerprint': 'fp_daf5fcc80a', 'id': 'chatcmpl-C9PDWa5EV88escSMqeomRQFu7sJLE', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--3f549942-196a-4c5a-9556-d81385028a3a-0', usage_metadata={'input_tokens': 153, 'output_tokens': 578, 'total_tokens': 731, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ì—ì´ì „íŠ¸ ì‹¤í–‰\n",
    "agent_executor.invoke(\n",
    "    {\"messages\": [(\"user\", \"ë­ì²´ì¸ í•œêµ­ì–´ íŠœí† ë¦¬ì–¼ì— ëŒ€í•´ì„œ ì„¤ëª…í•´ì¤˜\")]}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import Annotated, List, Tuple\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "\n",
    "# ìƒíƒœ ì •ì˜\n",
    "class PlanExecute(TypedDict):\n",
    "    input: Annotated[str, \"User's input\"]\n",
    "    plan: Annotated[List[str], \"Current plan\"]\n",
    "    past_steps: Annotated[List[Tuple], operator.add]\n",
    "    response: Annotated[str, \"Final response\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "\n",
    "# Plan ëª¨ë¸ ì •ì˜\n",
    "class Plan(BaseModel):\n",
    "    \"\"\"Sorted steps to execute the plan\"\"\"\n",
    "\n",
    "    steps: Annotated[List[str], \"Different steps to follow, should be in sorted order\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# ê³„íš ìˆ˜ë¦½ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±\n",
    "planner_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"For the given objective, come up with a simple step by step plan. \\\n",
    "This plan should involve individual tasks, that if executed correctly will yield the correct answer. Do not add any superfluous steps. \\\n",
    "The result of the final step should be the final answer. Make sure that each step has all the information needed - do not skip steps.\n",
    "Answer in Korean.\"\"\",\n",
    "        ),\n",
    "        (\"placeholder\", \"{messages}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "planner = planner_prompt | ChatOpenAI(\n",
    "    model=MODEL_NAME, temperature=0\n",
    ").with_structured_output(Plan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Plan(steps=['LangGraphì˜ í•µì‹¬ ì¥ì ì„ ì •ë¦¬í•œë‹¤.', 'LangGraphì˜ í•µì‹¬ ë‹¨ì ì„ ì •ë¦¬í•œë‹¤.', 'LangGraphë¥¼ ì‚¬ìš©í•˜ëŠ” ì´ìœ ë¥¼ ì •ë¦¬í•œë‹¤.', 'ê° í•­ëª©ì„ ëª…í™•í•˜ê²Œ êµ¬ë¶„í•˜ì—¬ ì •ë¦¬ëœ ë‚´ìš©ì„ ìµœì¢… ë‹µë³€ìœ¼ë¡œ ì‘ì„±í•œë‹¤.'])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Planner ì‹¤í–‰\n",
    "planner.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            (\n",
    "                \"user\",\n",
    "                \"LangGraph ì˜ í•µì‹¬ ì¥ë‹¨ì ê³¼ LangGraph ë¥¼ ì‚¬ìš©í•˜ëŠ” ì´ìœ ëŠ” ë¬´ì—‡ì¸ê°€?\",\n",
    "            )\n",
    "        ]\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "\n",
    "\n",
    "class Response(BaseModel):\n",
    "    \"\"\"Response to user.\"\"\"\n",
    "\n",
    "    # ì‚¬ìš©ì ì‘ë‹µ\n",
    "    response: str\n",
    "\n",
    "\n",
    "class Act(BaseModel):\n",
    "    \"\"\"Action to perform.\"\"\"\n",
    "\n",
    "    # ìˆ˜í–‰í•  ì‘ì—…: \"Response\", \"Plan\". ì‚¬ìš©ìì—ê²Œ ì‘ë‹µí•  ê²½ìš° Response ì‚¬ìš©, ì¶”ê°€ ë„êµ¬ ì‚¬ìš©ì´ í•„ìš”í•  ê²½ìš° Plan ì‚¬ìš©\n",
    "    action: Union[Response, Plan] = Field(\n",
    "        description=\"Action to perform. If you want to respond to user, use Response. \"\n",
    "        \"If you need to further use tools to get the answer, use Plan.\"\n",
    "    )\n",
    "\n",
    "\n",
    "# ê³„íšì„ ì¬ìˆ˜ë¦½í•˜ê¸° ìœ„í•œ í”„ë¡¬í”„íŠ¸ ì •ì˜\n",
    "replanner_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"For the given objective, come up with a simple step by step plan. \\\n",
    "This plan should involve individual tasks, that if executed correctly will yield the correct answer. Do not add any superfluous steps. \\\n",
    "The result of the final step should be the final answer. Make sure that each step has all the information needed - do not skip steps.\n",
    "\n",
    "Your objective was this:\n",
    "{input}\n",
    "\n",
    "Your original plan was this:\n",
    "{plan}\n",
    "\n",
    "You have currently done the follow steps:\n",
    "{past_steps}\n",
    "\n",
    "Update your plan accordingly. If no more steps are needed and you can return to the user, then respond with that. Otherwise, fill out the plan. Only add steps to the plan that still NEED to be done. Do not return previously done steps as part of the plan.\n",
    "\n",
    "Answer in Korean.\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "# Replanner ìƒì„±\n",
    "replanner = replanner_prompt | ChatOpenAI(\n",
    "    model=MODEL_NAME, temperature=0\n",
    ").with_structured_output(Act)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "# ì‚¬ìš©ì ì…ë ¥ì„ ê¸°ë°˜ìœ¼ë¡œ ê³„íšì„ ìƒì„±í•˜ê³  ë°˜í™˜\n",
    "def plan_step(state: PlanExecute):\n",
    "    plan = planner.invoke({\"messages\": [(\"user\", state[\"input\"])]})\n",
    "    # ìƒì„±ëœ ê³„íšì˜ ë‹¨ê³„ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜\n",
    "    return {\"plan\": plan.steps}\n",
    "\n",
    "\n",
    "# ì—ì´ì „íŠ¸ ì‹¤í–‰ê¸°ë¥¼ ì‚¬ìš©í•˜ì—¬ ì£¼ì–´ì§„ ì‘ì—…ì„ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¥¼ ë°˜í™˜\n",
    "def execute_step(state: PlanExecute):\n",
    "    plan = state[\"plan\"]\n",
    "    # ê³„íšì„ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ ê° ë‹¨ê³„ì— ë²ˆí˜¸ë¥¼ ë§¤ê¹€\n",
    "    plan_str = \"\\n\".join(f\"{i+1}. {step}\" for i, step in enumerate(plan))\n",
    "    task = plan[0]\n",
    "    # í˜„ì¬ ì‹¤í–‰í•  ì‘ì—…ì„ í¬ë§·íŒ…í•˜ì—¬ ì—ì´ì „íŠ¸ì— ì „ë‹¬\n",
    "    task_formatted = f\"\"\"For the following plan:\n",
    "{plan_str}\\n\\nYou are tasked with executing [step 1. {task}].\"\"\"\n",
    "    # ì—ì´ì „íŠ¸ ì‹¤í–‰ê¸°ë¥¼ í†µí•´ ì‘ì—… ìˆ˜í–‰ ë° ê²°ê³¼ ìˆ˜ì‹ \n",
    "    agent_response = agent_executor.invoke({\"messages\": [(\"user\", task_formatted)]})\n",
    "    # ì´ì „ ë‹¨ê³„ì™€ ê·¸ ê²°ê³¼ë¥¼ í¬í•¨í•˜ëŠ” ë”•ì…”ë„ˆë¦¬ ë°˜í™˜\n",
    "    return {\n",
    "        \"past_steps\": [(task, agent_response[\"messages\"][-1].content)],\n",
    "    }\n",
    "\n",
    "\n",
    "# ì´ì „ ë‹¨ê³„ì˜ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê³„íšì„ ì—…ë°ì´íŠ¸í•˜ê±°ë‚˜ ìµœì¢… ì‘ë‹µì„ ë°˜í™˜\n",
    "def replan_step(state: PlanExecute):\n",
    "    output = replanner.invoke(state)\n",
    "    # ì‘ë‹µì´ ì‚¬ìš©ìì—ê²Œ ë°˜í™˜ë  ê²½ìš°\n",
    "    if isinstance(output.action, Response):\n",
    "        return {\"response\": output.action.response}\n",
    "    # ì¶”ê°€ ë‹¨ê³„ê°€ í•„ìš”í•  ê²½ìš° ê³„íšì˜ ë‹¨ê³„ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜\n",
    "    else:\n",
    "        next_plan = output.action.steps\n",
    "        if len(next_plan) == 0:\n",
    "            return {\"response\": \"No more steps needed.\"}\n",
    "        else:\n",
    "            return {\"plan\": next_plan}\n",
    "\n",
    "\n",
    "# ì—ì´ì „íŠ¸ì˜ ì‹¤í–‰ ì¢…ë£Œ ì—¬ë¶€ë¥¼ ê²°ì •í•˜ëŠ” í•¨ìˆ˜\n",
    "def should_end(state: PlanExecute):\n",
    "    if \"response\" in state and state[\"response\"]:\n",
    "        return \"final_report\"\n",
    "    else:\n",
    "        return \"execute\"\n",
    "\n",
    "\n",
    "final_report_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"You are given the objective and the previously done steps. Your task is to generate a final report in markdown format.\n",
    "Final report should be written in professional tone.\n",
    "\n",
    "Your objective was this:\n",
    "\n",
    "{input}\n",
    "\n",
    "Your previously done steps(question and answer pairs):\n",
    "\n",
    "{past_steps}\n",
    "\n",
    "Generate a final report in markdown format. Write your response in Korean.\"\"\"\n",
    ")\n",
    "\n",
    "final_report = (\n",
    "    final_report_prompt\n",
    "    | ChatOpenAI(model=MODEL_NAME, temperature=0)\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "\n",
    "def generate_final_report(state: PlanExecute):\n",
    "    past_steps = \"\\n\\n\".join(\n",
    "        [\n",
    "            f\"Question: {past_step[0]}\\n\\nAnswer: {past_step[1]}\\n\\n####\"\n",
    "            for past_step in state[\"past_steps\"]\n",
    "        ]\n",
    "    )\n",
    "    response = final_report.invoke({\"input\": state[\"input\"], \"past_steps\": past_steps})\n",
    "    return {\"response\": response}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "\n",
    "# ê·¸ë˜í”„ ìƒì„±\n",
    "workflow = StateGraph(PlanExecute)\n",
    "\n",
    "# ë…¸ë“œ ì •ì˜\n",
    "workflow.add_node(\"planner\", plan_step)\n",
    "workflow.add_node(\"execute\", execute_step)\n",
    "workflow.add_node(\"replan\", replan_step)\n",
    "workflow.add_node(\"final_report\", generate_final_report)\n",
    "\n",
    "# ì—£ì§€ ì •ì˜\n",
    "workflow.add_edge(START, \"planner\")\n",
    "workflow.add_edge(\"planner\", \"execute\")\n",
    "workflow.add_edge(\"execute\", \"replan\")\n",
    "workflow.add_edge(\"final_report\", END)\n",
    "\n",
    "# ì¡°ê±´ë¶€ ì—£ì§€: replan í›„ ì¢…ë£Œ ì—¬ë¶€ë¥¼ ê²°ì •í•˜ëŠ” í•¨ìˆ˜ ì‚¬ìš©\n",
    "workflow.add_conditional_edges(\n",
    "    \"replan\",\n",
    "    should_end,\n",
    "    {\"execute\": \"execute\", \"final_report\": \"final_report\"},\n",
    ")\n",
    "\n",
    "# ê·¸ë˜í”„ ì»´íŒŒì¼\n",
    "app = workflow.compile(checkpointer=MemorySaver())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_teddynote.graphs import visualize_graph\n",
    "\n",
    "visualize_graph(app, xray=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mplanner\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Naive RAGì˜ êµ¬ì¡°ì™€ ë™ì‘ ë°©ì‹ì„ ì„¤ëª…í•œë‹¤.\n",
      "Modular RAGì˜ êµ¬ì¡°ì™€ ë™ì‘ ë°©ì‹ì„ ì„¤ëª…í•œë‹¤.\n",
      "Naive RAGì™€ Modular RAGì˜ êµ¬ì¡°ì  ì°¨ì´ì ì„ ë¹„êµí•œë‹¤.\n",
      "Production levelì—ì„œ Modular RAGë¥¼ ì‚¬ìš©í•  ë•Œì˜ ì´ì ì„ ì •ë¦¬í•œë‹¤.\n",
      "ê° ì´ì ì´ ì‹¤ì œ ì„œë¹„ìŠ¤ ìš´ì˜ì— ì–´ë–»ê²Œ ë„ì›€ì´ ë˜ëŠ”ì§€ ì„¤ëª…í•œë‹¤.\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36magent\u001b[0m in [\u001b[1;33mexecute\u001b[0m] ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Naive RAG(naive Retrieval-Augmented Generation)ì˜ êµ¬ì¡°ì™€ ë™ì‘ ë°©ì‹\n",
      "\n",
      "Naive RAGëŠ” ì •ë³´ ê²€ìƒ‰(Retrieval)ê³¼ ìƒì„±(Generation)ì„ ê²°í•©í•œ ìì—°ì–´ ì²˜ë¦¬(NLP) í”„ë ˆì„ì›Œí¬ì˜ ê°€ì¥ ê¸°ë³¸ì ì¸ í˜•íƒœì…ë‹ˆë‹¤. ì£¼ë¡œ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ê³¼ ì™¸ë¶€ ì§€ì‹ ë² ì´ìŠ¤(ë¬¸ì„œ, ë°ì´í„°ë² ì´ìŠ¤ ë“±)ë¥¼ í•¨ê»˜ í™œìš©í•˜ì—¬, ì…ë ¥ ì§ˆë¬¸ì— ëŒ€í•´ ë” ì •í™•í•˜ê³  í’ë¶€í•œ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.\n",
      "\n",
      "êµ¬ì¡°\n",
      "\n",
      "- Query ì…ë ¥: ì‚¬ìš©ìê°€ ì§ˆë¬¸(ì§ˆì˜, Query)ì„ ì…ë ¥í•©ë‹ˆë‹¤.\n",
      "- Retriever(ê²€ìƒ‰ê¸°): ì…ë ¥ëœ ì§ˆë¬¸ì„ ë°”íƒ•ìœ¼ë¡œ ì™¸ë¶€ ì§€ì‹ ë² ì´ìŠ¤ì—ì„œ ê´€ë ¨ì„±ì´ ë†’ì€ ë¬¸ì„œë‚˜ ì •ë³´ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.\n",
      "- Generator(ìƒì„±ê¸°): ê²€ìƒ‰ëœ ë¬¸ì„œì™€ ì›ë˜ ì§ˆë¬¸ì„ í•¨ê»˜ ë°›ì•„, ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(ì˜ˆ: GPT, BERT ë“±)ì´ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
      "\n",
      "ë™ì‘ ë°©ì‹\n",
      "\n",
      "- ì‚¬ìš©ìê°€ ì§ˆë¬¸ì„ ì…ë ¥í•©ë‹ˆë‹¤.\n",
      "- Retrieverê°€ ì§ˆë¬¸ê³¼ ìœ ì‚¬í•œ ë¬¸ì„œ(ë˜ëŠ” ë¬¸ì„œì˜ ì¼ë¶€)ë¥¼ ì™¸ë¶€ ì§€ì‹ ë² ì´ìŠ¤ì—ì„œ ì°¾ì•„ëƒ…ë‹ˆë‹¤.\n",
      "- Generatorê°€ ê²€ìƒ‰ëœ ë¬¸ì„œì™€ ì§ˆë¬¸ì„ í•¨ê»˜ ì…ë ¥ë°›ì•„, ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìì—°ì–´ë¡œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
      "- ìƒì„±ëœ ë‹µë³€ì„ ì‚¬ìš©ìì—ê²Œ ì œê³µí•©ë‹ˆë‹¤.\n",
      "\n",
      "íŠ¹ì§•\n",
      "\n",
      "- ê²€ìƒ‰ê³¼ ìƒì„±ì´ í•œ ë²ˆì— ìˆœì°¨ì ìœ¼ë¡œ ì´ë£¨ì–´ì§‘ë‹ˆë‹¤.\n",
      "- ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ê³§ë°”ë¡œ ìƒì„± ëª¨ë¸ì— ì…ë ¥ë˜ë©°, ë³„ë„ì˜ ì „ì²˜ë¦¬ë‚˜ í›„ì²˜ë¦¬ ê³¼ì •ì´ ê±°ì˜ ì—†ìŠµë‹ˆë‹¤.\n",
      "- êµ¬ì¡°ê°€ ë‹¨ìˆœí•˜ì—¬ êµ¬í˜„ì´ ì‰½ì§€ë§Œ, ê²€ìƒ‰ê³¼ ìƒì„± ê³¼ì •ì´ ë°€ì ‘í•˜ê²Œ ê²°í•©ë˜ì–´ ìˆì–´ ìœ ì—°ì„±ì´ ë–¨ì–´ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "ìš”ì•½\n",
      "Naive RAGëŠ” ì§ˆë¬¸ â†’ ê²€ìƒ‰ â†’ ìƒì„±ì˜ ë‹¨ìˆœí•œ íŒŒì´í”„ë¼ì¸ êµ¬ì¡°ë¡œ, ê²€ìƒ‰ëœ ì •ë³´ë¥¼ ë°”ë¡œ ìƒì„± ëª¨ë¸ì— ì…ë ¥í•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤.\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mexecute\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "('Naive RAGì˜ êµ¬ì¡°ì™€ ë™ì‘ ë°©ì‹ì„ ì„¤ëª…í•œë‹¤.', 'Naive RAG(naive Retrieval-Augmented Generation)ì˜ êµ¬ì¡°ì™€ ë™ì‘ ë°©ì‹\\n\\nNaive RAGëŠ” ì •ë³´ ê²€ìƒ‰(Retrieval)ê³¼ ìƒì„±(Generation)ì„ ê²°í•©í•œ ìì—°ì–´ ì²˜ë¦¬(NLP) í”„ë ˆì„ì›Œí¬ì˜ ê°€ì¥ ê¸°ë³¸ì ì¸ í˜•íƒœì…ë‹ˆë‹¤. ì£¼ë¡œ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ê³¼ ì™¸ë¶€ ì§€ì‹ ë² ì´ìŠ¤(ë¬¸ì„œ, ë°ì´í„°ë² ì´ìŠ¤ ë“±)ë¥¼ í•¨ê»˜ í™œìš©í•˜ì—¬, ì…ë ¥ ì§ˆë¬¸ì— ëŒ€í•´ ë” ì •í™•í•˜ê³  í’ë¶€í•œ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.\\n\\nêµ¬ì¡°\\n\\n- Query ì…ë ¥: ì‚¬ìš©ìê°€ ì§ˆë¬¸(ì§ˆì˜, Query)ì„ ì…ë ¥í•©ë‹ˆë‹¤.\\n- Retriever(ê²€ìƒ‰ê¸°): ì…ë ¥ëœ ì§ˆë¬¸ì„ ë°”íƒ•ìœ¼ë¡œ ì™¸ë¶€ ì§€ì‹ ë² ì´ìŠ¤ì—ì„œ ê´€ë ¨ì„±ì´ ë†’ì€ ë¬¸ì„œë‚˜ ì •ë³´ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.\\n- Generator(ìƒì„±ê¸°): ê²€ìƒ‰ëœ ë¬¸ì„œì™€ ì›ë˜ ì§ˆë¬¸ì„ í•¨ê»˜ ë°›ì•„, ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(ì˜ˆ: GPT, BERT ë“±)ì´ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.\\n\\në™ì‘ ë°©ì‹\\n\\n- ì‚¬ìš©ìê°€ ì§ˆë¬¸ì„ ì…ë ¥í•©ë‹ˆë‹¤.\\n- Retrieverê°€ ì§ˆë¬¸ê³¼ ìœ ì‚¬í•œ ë¬¸ì„œ(ë˜ëŠ” ë¬¸ì„œì˜ ì¼ë¶€)ë¥¼ ì™¸ë¶€ ì§€ì‹ ë² ì´ìŠ¤ì—ì„œ ì°¾ì•„ëƒ…ë‹ˆë‹¤.\\n- Generatorê°€ ê²€ìƒ‰ëœ ë¬¸ì„œì™€ ì§ˆë¬¸ì„ í•¨ê»˜ ì…ë ¥ë°›ì•„, ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìì—°ì–´ë¡œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.\\n- ìƒì„±ëœ ë‹µë³€ì„ ì‚¬ìš©ìì—ê²Œ ì œê³µí•©ë‹ˆë‹¤.\\n\\níŠ¹ì§•\\n\\n- ê²€ìƒ‰ê³¼ ìƒì„±ì´ í•œ ë²ˆì— ìˆœì°¨ì ìœ¼ë¡œ ì´ë£¨ì–´ì§‘ë‹ˆë‹¤.\\n- ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ê³§ë°”ë¡œ ìƒì„± ëª¨ë¸ì— ì…ë ¥ë˜ë©°, ë³„ë„ì˜ ì „ì²˜ë¦¬ë‚˜ í›„ì²˜ë¦¬ ê³¼ì •ì´ ê±°ì˜ ì—†ìŠµë‹ˆë‹¤.\\n- êµ¬ì¡°ê°€ ë‹¨ìˆœí•˜ì—¬ êµ¬í˜„ì´ ì‰½ì§€ë§Œ, ê²€ìƒ‰ê³¼ ìƒì„± ê³¼ì •ì´ ë°€ì ‘í•˜ê²Œ ê²°í•©ë˜ì–´ ìˆì–´ ìœ ì—°ì„±ì´ ë–¨ì–´ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n\\nìš”ì•½\\nNaive RAGëŠ” ì§ˆë¬¸ â†’ ê²€ìƒ‰ â†’ ìƒì„±ì˜ ë‹¨ìˆœí•œ íŒŒì´í”„ë¼ì¸ êµ¬ì¡°ë¡œ, ê²€ìƒ‰ëœ ì •ë³´ë¥¼ ë°”ë¡œ ìƒì„± ëª¨ë¸ì— ì…ë ¥í•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤.')\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mreplan\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Modular RAGì˜ êµ¬ì¡°ì™€ ë™ì‘ ë°©ì‹ì„ ì„¤ëª…í•œë‹¤.\n",
      "Naive RAGì™€ Modular RAGì˜ êµ¬ì¡°ì  ì°¨ì´ì ì„ ë¹„êµí•œë‹¤.\n",
      "Production levelì—ì„œ Modular RAGë¥¼ ì‚¬ìš©í•  ë•Œì˜ ì´ì ì„ ì •ë¦¬í•œë‹¤.\n",
      "ê° ì´ì ì´ ì‹¤ì œ ì„œë¹„ìŠ¤ ìš´ì˜ì— ì–´ë–»ê²Œ ë„ì›€ì´ ë˜ëŠ”ì§€ ì„¤ëª…í•œë‹¤.\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36magent\u001b[0m in [\u001b[1;33mexecute\u001b[0m] ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Modular RAG(ëª¨ë“ˆëŸ¬ RAG, Retrieval-Augmented Generation)ì˜ êµ¬ì¡°ì™€ ë™ì‘ ë°©ì‹ì— ëŒ€í•´ ì„¤ëª…í•˜ê² ìŠµë‹ˆë‹¤.\n",
      "\n",
      "1. Modular RAGì˜ êµ¬ì¡°\n",
      "\n",
      "Modular RAGëŠ” ì „í†µì ì¸ RAG ì‹œìŠ¤í…œì„ ì—¬ëŸ¬ ê°œì˜ ë…ë¦½ì ì¸ ëª¨ë“ˆë¡œ ë¶„ë¦¬í•˜ì—¬ êµ¬ì„±í•œ êµ¬ì¡°ì…ë‹ˆë‹¤. ê° ëª¨ë“ˆì€ íŠ¹ì • ê¸°ëŠ¥(ì˜ˆ: ë°ì´í„° ì „ì²˜ë¦¬, ì¸ë±ì‹±, ê²€ìƒ‰, í›„ì²˜ë¦¬ ë“±)ì„ ë‹´ë‹¹í•˜ë©°, ì „ì²´ íŒŒì´í”„ë¼ì¸ì´ ìœ ê¸°ì ìœ¼ë¡œ ì—°ê²°ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì£¼ìš” êµ¬ì„± ìš”ì†ŒëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n",
      "\n",
      "- ë°ì´í„° ì „ì²˜ë¦¬ ëª¨ë“ˆ: ì›ë³¸ ë°ì´í„°ë¥¼ ì •ì œí•˜ê³ , ë¬¸ì„œ ë‹¨ìœ„ë¡œ ë¶„í• í•˜ê±°ë‚˜ ì„ë² ë”©ì„ ìƒì„±í•  ì¤€ë¹„ë¥¼ í•©ë‹ˆë‹¤.\n",
      "- ì¸ë±ì‹± ëª¨ë“ˆ: ì „ì²˜ë¦¬ëœ ë°ì´í„°ë¥¼ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ë“±ì— ì €ì¥í•˜ê³ , íš¨ìœ¨ì ì¸ ê²€ìƒ‰ì´ ê°€ëŠ¥í•˜ë„ë¡ ì¸ë±ìŠ¤ë¥¼ êµ¬ì¶•í•©ë‹ˆë‹¤.\n",
      "- ê²€ìƒ‰(Retrieval) ëª¨ë“ˆ: ì‚¬ìš©ìì˜ ì¿¼ë¦¬ì™€ ìœ ì‚¬í•œ ë¬¸ì„œë¥¼ ë²¡í„° ê²€ìƒ‰ ë“±ì„ í†µí•´ ì°¾ì•„ëƒ…ë‹ˆë‹¤.\n",
      "- ìƒì„±(Generation) ëª¨ë“ˆ: ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ LLM(ëŒ€í˜• ì–¸ì–´ ëª¨ë¸)ì´ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
      "- í›„ì²˜ë¦¬ ë° í‰ê°€ ëª¨ë“ˆ: ìƒì„±ëœ ë‹µë³€ì„ ì •ì œí•˜ê±°ë‚˜, í’ˆì§ˆ í‰ê°€ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
      "\n",
      "2. Modular RAGì˜ ë™ì‘ ë°©ì‹\n",
      "\n",
      "- ì‚¬ìš©ìê°€ ì¿¼ë¦¬ë¥¼ ì…ë ¥í•˜ë©´, ì¿¼ë¦¬ëŠ” ë¨¼ì € ì „ì²˜ë¦¬ ëª¨ë“ˆì„ ê±°ì³ ì ì ˆí•œ í˜•íƒœë¡œ ë³€í™˜ë©ë‹ˆë‹¤.\n",
      "- ë³€í™˜ëœ ì¿¼ë¦¬ëŠ” ê²€ìƒ‰ ëª¨ë“ˆë¡œ ì „ë‹¬ë˜ì–´, ì¸ë±ì‹±ëœ ë°ì´í„° ì¤‘ì—ì„œ ê´€ë ¨ì„±ì´ ë†’ì€ ë¬¸ì„œë“¤ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤.\n",
      "- ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì€ ìƒì„± ëª¨ë“ˆë¡œ ì „ë‹¬ë˜ì–´, LLMì´ ì´ ë¬¸ì„œë“¤ì„ ì°¸ê³ í•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
      "- ìƒì„±ëœ ë‹µë³€ì€ í›„ì²˜ë¦¬ ëª¨ë“ˆì„ í†µí•´ ì¶”ê°€ì ì¸ ì •ì œë‚˜ í‰ê°€ê°€ ì´ë£¨ì–´ì§‘ë‹ˆë‹¤.\n",
      "- ìµœì¢…ì ìœ¼ë¡œ ì‚¬ìš©ìëŠ” ì •ì œëœ ë‹µë³€ì„ ë°›ê²Œ ë©ë‹ˆë‹¤.\n",
      "\n",
      "ì´ì²˜ëŸ¼ Modular RAGëŠ” ê° ê¸°ëŠ¥ë³„ë¡œ ëª¨ë“ˆì´ ë¶„ë¦¬ë˜ì–´ ìˆì–´, ê° ë‹¨ê³„ì˜ ë…ë¦½ì ì¸ ê°œì„  ë° ìœ ì§€ë³´ìˆ˜ê°€ ìš©ì´í•˜ë©°, í•„ìš”ì— ë”°ë¼ íŠ¹ì • ëª¨ë“ˆë§Œ êµì²´í•˜ê±°ë‚˜ í™•ì¥í•  ìˆ˜ ìˆëŠ” ìœ ì—°í•œ êµ¬ì¡°ë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mexecute\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "('Modular RAGì˜ êµ¬ì¡°ì™€ ë™ì‘ ë°©ì‹ì„ ì„¤ëª…í•œë‹¤.', 'Modular RAG(ëª¨ë“ˆëŸ¬ RAG, Retrieval-Augmented Generation)ì˜ êµ¬ì¡°ì™€ ë™ì‘ ë°©ì‹ì— ëŒ€í•´ ì„¤ëª…í•˜ê² ìŠµë‹ˆë‹¤.\\n\\n1. Modular RAGì˜ êµ¬ì¡°\\n\\nModular RAGëŠ” ì „í†µì ì¸ RAG ì‹œìŠ¤í…œì„ ì—¬ëŸ¬ ê°œì˜ ë…ë¦½ì ì¸ ëª¨ë“ˆë¡œ ë¶„ë¦¬í•˜ì—¬ êµ¬ì„±í•œ êµ¬ì¡°ì…ë‹ˆë‹¤. ê° ëª¨ë“ˆì€ íŠ¹ì • ê¸°ëŠ¥(ì˜ˆ: ë°ì´í„° ì „ì²˜ë¦¬, ì¸ë±ì‹±, ê²€ìƒ‰, í›„ì²˜ë¦¬ ë“±)ì„ ë‹´ë‹¹í•˜ë©°, ì „ì²´ íŒŒì´í”„ë¼ì¸ì´ ìœ ê¸°ì ìœ¼ë¡œ ì—°ê²°ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì£¼ìš” êµ¬ì„± ìš”ì†ŒëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\\n\\n- ë°ì´í„° ì „ì²˜ë¦¬ ëª¨ë“ˆ: ì›ë³¸ ë°ì´í„°ë¥¼ ì •ì œí•˜ê³ , ë¬¸ì„œ ë‹¨ìœ„ë¡œ ë¶„í• í•˜ê±°ë‚˜ ì„ë² ë”©ì„ ìƒì„±í•  ì¤€ë¹„ë¥¼ í•©ë‹ˆë‹¤.\\n- ì¸ë±ì‹± ëª¨ë“ˆ: ì „ì²˜ë¦¬ëœ ë°ì´í„°ë¥¼ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ë“±ì— ì €ì¥í•˜ê³ , íš¨ìœ¨ì ì¸ ê²€ìƒ‰ì´ ê°€ëŠ¥í•˜ë„ë¡ ì¸ë±ìŠ¤ë¥¼ êµ¬ì¶•í•©ë‹ˆë‹¤.\\n- ê²€ìƒ‰(Retrieval) ëª¨ë“ˆ: ì‚¬ìš©ìì˜ ì¿¼ë¦¬ì™€ ìœ ì‚¬í•œ ë¬¸ì„œë¥¼ ë²¡í„° ê²€ìƒ‰ ë“±ì„ í†µí•´ ì°¾ì•„ëƒ…ë‹ˆë‹¤.\\n- ìƒì„±(Generation) ëª¨ë“ˆ: ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ LLM(ëŒ€í˜• ì–¸ì–´ ëª¨ë¸)ì´ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.\\n- í›„ì²˜ë¦¬ ë° í‰ê°€ ëª¨ë“ˆ: ìƒì„±ëœ ë‹µë³€ì„ ì •ì œí•˜ê±°ë‚˜, í’ˆì§ˆ í‰ê°€ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.\\n\\n2. Modular RAGì˜ ë™ì‘ ë°©ì‹\\n\\n- ì‚¬ìš©ìê°€ ì¿¼ë¦¬ë¥¼ ì…ë ¥í•˜ë©´, ì¿¼ë¦¬ëŠ” ë¨¼ì € ì „ì²˜ë¦¬ ëª¨ë“ˆì„ ê±°ì³ ì ì ˆí•œ í˜•íƒœë¡œ ë³€í™˜ë©ë‹ˆë‹¤.\\n- ë³€í™˜ëœ ì¿¼ë¦¬ëŠ” ê²€ìƒ‰ ëª¨ë“ˆë¡œ ì „ë‹¬ë˜ì–´, ì¸ë±ì‹±ëœ ë°ì´í„° ì¤‘ì—ì„œ ê´€ë ¨ì„±ì´ ë†’ì€ ë¬¸ì„œë“¤ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤.\\n- ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì€ ìƒì„± ëª¨ë“ˆë¡œ ì „ë‹¬ë˜ì–´, LLMì´ ì´ ë¬¸ì„œë“¤ì„ ì°¸ê³ í•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.\\n- ìƒì„±ëœ ë‹µë³€ì€ í›„ì²˜ë¦¬ ëª¨ë“ˆì„ í†µí•´ ì¶”ê°€ì ì¸ ì •ì œë‚˜ í‰ê°€ê°€ ì´ë£¨ì–´ì§‘ë‹ˆë‹¤.\\n- ìµœì¢…ì ìœ¼ë¡œ ì‚¬ìš©ìëŠ” ì •ì œëœ ë‹µë³€ì„ ë°›ê²Œ ë©ë‹ˆë‹¤.\\n\\nì´ì²˜ëŸ¼ Modular RAGëŠ” ê° ê¸°ëŠ¥ë³„ë¡œ ëª¨ë“ˆì´ ë¶„ë¦¬ë˜ì–´ ìˆì–´, ê° ë‹¨ê³„ì˜ ë…ë¦½ì ì¸ ê°œì„  ë° ìœ ì§€ë³´ìˆ˜ê°€ ìš©ì´í•˜ë©°, í•„ìš”ì— ë”°ë¼ íŠ¹ì • ëª¨ë“ˆë§Œ êµì²´í•˜ê±°ë‚˜ í™•ì¥í•  ìˆ˜ ìˆëŠ” ìœ ì—°í•œ êµ¬ì¡°ë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.')\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mreplan\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Naive RAGì™€ Modular RAGì˜ êµ¬ì¡°ì  ì°¨ì´ì ì„ ë¹„êµí•œë‹¤.\n",
      "Production levelì—ì„œ Modular RAGë¥¼ ì‚¬ìš©í•  ë•Œì˜ ì´ì ì„ ì •ë¦¬í•œë‹¤.\n",
      "ê° ì´ì ì´ ì‹¤ì œ ì„œë¹„ìŠ¤ ìš´ì˜ì— ì–´ë–»ê²Œ ë„ì›€ì´ ë˜ëŠ”ì§€ ì„¤ëª…í•œë‹¤.\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36magent\u001b[0m in [\u001b[1;33mexecute\u001b[0m] ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "ë„¤, ì•„ë˜ëŠ” Naive RAGì™€ Modular RAGì˜ êµ¬ì¡°ì  ì°¨ì´ì ì— ëŒ€í•œ ë¹„êµì…ë‹ˆë‹¤.\n",
      "\n",
      "Naive RAGì™€ Modular RAGì˜ êµ¬ì¡°ì  ì°¨ì´ì \n",
      "\n",
      "1. Naive RAG (Retrieval-Augmented Generation)\n",
      "- êµ¬ì¡°: ë‹¨ì¼ íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ, ì…ë ¥ ì¿¼ë¦¬ê°€ ë“¤ì–´ì˜¤ë©´ ë°”ë¡œ ê²€ìƒ‰(ê²€ìƒ‰ê¸°) â†’ ìƒì„±(ìƒì„±ê¸°) ê³¼ì •ì„ ê±°ì¹©ë‹ˆë‹¤.\n",
      "- ì»´í¬ë„ŒíŠ¸: ê²€ìƒ‰ê¸°(retriever)ì™€ ìƒì„±ê¸°(generator)ê°€ í•˜ë‚˜ì˜ íë¦„ìœ¼ë¡œ ì—°ê²°ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n",
      "- ìœ ì—°ì„±: ê° ë‹¨ê³„ê°€ ê³ ì •ë˜ì–´ ìˆì–´, ê²€ìƒ‰ê¸°ë‚˜ ìƒì„±ê¸°ì˜ êµì²´Â·í™•ì¥Â·ì¡°ì •ì´ ì–´ë µìŠµë‹ˆë‹¤.\n",
      "- ì˜ˆì‹œ: ë‹¨ìˆœíˆ ë²¡í„°DBì—ì„œ top-k ë¬¸ì„œë¥¼ ê²€ìƒ‰í•œ ë’¤, ì´ë¥¼ LLMì— ë„£ì–´ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ë°©ì‹.\n",
      "\n",
      "2. Modular RAG\n",
      "- êµ¬ì¡°: ê° ì»´í¬ë„ŒíŠ¸(ê²€ìƒ‰, í•„í„°ë§, ë­í‚¹, ìƒì„± ë“±)ê°€ ëª¨ë“ˆí™”ë˜ì–´ ë…ë¦½ì ìœ¼ë¡œ ë™ì‘í•©ë‹ˆë‹¤.\n",
      "- ì»´í¬ë„ŒíŠ¸: ê²€ìƒ‰ê¸°, í•„í„°, ë­ì»¤, ìƒì„±ê¸° ë“± ì—¬ëŸ¬ ëª¨ë“ˆì´ ì¡°í•©ë˜ì–´ íŒŒì´í”„ë¼ì¸ì„ êµ¬ì„±í•©ë‹ˆë‹¤.\n",
      "- ìœ ì—°ì„±: ê° ëª¨ë“ˆì„ ë…ë¦½ì ìœ¼ë¡œ êµì²´, í™•ì¥, íŠœë‹í•  ìˆ˜ ìˆì–´ ë‹¤ì–‘í•œ ìš”êµ¬ì‚¬í•­ì— ë§ê²Œ ì‹œìŠ¤í…œì„ ì¡°ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "- ì˜ˆì‹œ: ê²€ìƒ‰ ê²°ê³¼ì— ì¶”ê°€ì ì¸ í•„í„°ë§, ë­í‚¹, í›„ì²˜ë¦¬ ëª¨ë“ˆì„ ì‚½ì…í•˜ê±°ë‚˜, ì—¬ëŸ¬ ê²€ìƒ‰ê¸°/ìƒì„±ê¸°ë¥¼ ì¡°í•©í•˜ì—¬ ì‚¬ìš©.\n",
      "\n",
      "ìš”ì•½ í‘œ\n",
      "\n",
      "êµ¬ë¶„ Naive RAG Modular RAG\n",
      "êµ¬ì¡° ë‹¨ì¼ íŒŒì´í”„ë¼ì¸ ëª¨ë“ˆí™”ëœ íŒŒì´í”„ë¼ì¸\n",
      "ì»´í¬ë„ŒíŠ¸ ê²€ìƒ‰ê¸°+ìƒì„±ê¸° ê²€ìƒ‰ê¸°, í•„í„°, ë­ì»¤, ìƒì„±ê¸° ë“±\n",
      "ìœ ì—°ì„± ë‚®ìŒ ë†’ìŒ\n",
      "í™•ì¥ì„± ì œí•œì  ë†’ìŒ\n",
      "ìš´ì˜ ë‚œì´ë„ ë‚®ìŒ(ë‹¨ìˆœ) ë†’ìŒ(ë³µì¡)\n",
      "\n",
      "ì´ë ‡ê²Œ Naive RAGëŠ” ë‹¨ìˆœí•˜ê³  ë¹ ë¥´ê²Œ êµ¬í˜„í•  ìˆ˜ ìˆì§€ë§Œ, êµ¬ì¡°ì ìœ¼ë¡œ ìœ ì—°ì„±ì´ ë–¨ì–´ì§‘ë‹ˆë‹¤. ë°˜ë©´ Modular RAGëŠ” ê° ê¸°ëŠ¥ì´ ëª¨ë“ˆí™”ë˜ì–´ ìˆì–´, ì„œë¹„ìŠ¤ ìš”êµ¬ì— ë”°ë¼ ì‰½ê²Œ í™•ì¥Â·ë³€ê²½í•  ìˆ˜ ìˆëŠ” êµ¬ì¡°ì  ì¥ì ì´ ìˆìŠµë‹ˆë‹¤.\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mexecute\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "('Naive RAGì™€ Modular RAGì˜ êµ¬ì¡°ì  ì°¨ì´ì ì„ ë¹„êµí•œë‹¤.', 'ë„¤, ì•„ë˜ëŠ” Naive RAGì™€ Modular RAGì˜ êµ¬ì¡°ì  ì°¨ì´ì ì— ëŒ€í•œ ë¹„êµì…ë‹ˆë‹¤.\\n\\nNaive RAGì™€ Modular RAGì˜ êµ¬ì¡°ì  ì°¨ì´ì \\n\\n1. Naive RAG (Retrieval-Augmented Generation)\\n- êµ¬ì¡°: ë‹¨ì¼ íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ, ì…ë ¥ ì¿¼ë¦¬ê°€ ë“¤ì–´ì˜¤ë©´ ë°”ë¡œ ê²€ìƒ‰(ê²€ìƒ‰ê¸°) â†’ ìƒì„±(ìƒì„±ê¸°) ê³¼ì •ì„ ê±°ì¹©ë‹ˆë‹¤.\\n- ì»´í¬ë„ŒíŠ¸: ê²€ìƒ‰ê¸°(retriever)ì™€ ìƒì„±ê¸°(generator)ê°€ í•˜ë‚˜ì˜ íë¦„ìœ¼ë¡œ ì—°ê²°ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\\n- ìœ ì—°ì„±: ê° ë‹¨ê³„ê°€ ê³ ì •ë˜ì–´ ìˆì–´, ê²€ìƒ‰ê¸°ë‚˜ ìƒì„±ê¸°ì˜ êµì²´Â·í™•ì¥Â·ì¡°ì •ì´ ì–´ë µìŠµë‹ˆë‹¤.\\n- ì˜ˆì‹œ: ë‹¨ìˆœíˆ ë²¡í„°DBì—ì„œ top-k ë¬¸ì„œë¥¼ ê²€ìƒ‰í•œ ë’¤, ì´ë¥¼ LLMì— ë„£ì–´ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ë°©ì‹.\\n\\n2. Modular RAG\\n- êµ¬ì¡°: ê° ì»´í¬ë„ŒíŠ¸(ê²€ìƒ‰, í•„í„°ë§, ë­í‚¹, ìƒì„± ë“±)ê°€ ëª¨ë“ˆí™”ë˜ì–´ ë…ë¦½ì ìœ¼ë¡œ ë™ì‘í•©ë‹ˆë‹¤.\\n- ì»´í¬ë„ŒíŠ¸: ê²€ìƒ‰ê¸°, í•„í„°, ë­ì»¤, ìƒì„±ê¸° ë“± ì—¬ëŸ¬ ëª¨ë“ˆì´ ì¡°í•©ë˜ì–´ íŒŒì´í”„ë¼ì¸ì„ êµ¬ì„±í•©ë‹ˆë‹¤.\\n- ìœ ì—°ì„±: ê° ëª¨ë“ˆì„ ë…ë¦½ì ìœ¼ë¡œ êµì²´, í™•ì¥, íŠœë‹í•  ìˆ˜ ìˆì–´ ë‹¤ì–‘í•œ ìš”êµ¬ì‚¬í•­ì— ë§ê²Œ ì‹œìŠ¤í…œì„ ì¡°ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n- ì˜ˆì‹œ: ê²€ìƒ‰ ê²°ê³¼ì— ì¶”ê°€ì ì¸ í•„í„°ë§, ë­í‚¹, í›„ì²˜ë¦¬ ëª¨ë“ˆì„ ì‚½ì…í•˜ê±°ë‚˜, ì—¬ëŸ¬ ê²€ìƒ‰ê¸°/ìƒì„±ê¸°ë¥¼ ì¡°í•©í•˜ì—¬ ì‚¬ìš©.\\n\\nìš”ì•½ í‘œ\\n\\nêµ¬ë¶„ Naive RAG Modular RAG\\nêµ¬ì¡° ë‹¨ì¼ íŒŒì´í”„ë¼ì¸ ëª¨ë“ˆí™”ëœ íŒŒì´í”„ë¼ì¸\\nì»´í¬ë„ŒíŠ¸ ê²€ìƒ‰ê¸°+ìƒì„±ê¸° ê²€ìƒ‰ê¸°, í•„í„°, ë­ì»¤, ìƒì„±ê¸° ë“±\\nìœ ì—°ì„± ë‚®ìŒ ë†’ìŒ\\ní™•ì¥ì„± ì œí•œì  ë†’ìŒ\\nìš´ì˜ ë‚œì´ë„ ë‚®ìŒ(ë‹¨ìˆœ) ë†’ìŒ(ë³µì¡)\\n\\nì´ë ‡ê²Œ Naive RAGëŠ” ë‹¨ìˆœí•˜ê³  ë¹ ë¥´ê²Œ êµ¬í˜„í•  ìˆ˜ ìˆì§€ë§Œ, êµ¬ì¡°ì ìœ¼ë¡œ ìœ ì—°ì„±ì´ ë–¨ì–´ì§‘ë‹ˆë‹¤. ë°˜ë©´ Modular RAGëŠ” ê° ê¸°ëŠ¥ì´ ëª¨ë“ˆí™”ë˜ì–´ ìˆì–´, ì„œë¹„ìŠ¤ ìš”êµ¬ì— ë”°ë¼ ì‰½ê²Œ í™•ì¥Â·ë³€ê²½í•  ìˆ˜ ìˆëŠ” êµ¬ì¡°ì  ì¥ì ì´ ìˆìŠµë‹ˆë‹¤.')\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mreplan\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Production levelì—ì„œ Modular RAGë¥¼ ì‚¬ìš©í•  ë•Œì˜ ì´ì ì„ ì •ë¦¬í•œë‹¤.\n",
      "ê° ì´ì ì´ ì‹¤ì œ ì„œë¹„ìŠ¤ ìš´ì˜ì— ì–´ë–»ê²Œ ë„ì›€ì´ ë˜ëŠ”ì§€ ì„¤ëª…í•œë‹¤.\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36magent\u001b[0m in [\u001b[1;33mexecute\u001b[0m] ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Production levelì—ì„œ Modular RAG(Retrieval-Augmented Generation)ë¥¼ ì‚¬ìš©í•  ë•Œì˜ ì´ì ì€ ë‹¤ìŒê³¼ ê°™ì´ ì •ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "1. ìœ ì—°í•œ ì•„í‚¤í…ì²˜ êµ¬ì„±\n",
      "- ë‹¤ì–‘í•œ ë°ì´í„° ì†ŒìŠ¤, ë²¡í„°DB, ê²€ìƒ‰ ì—”ì§„, LLM ë“±ì„ ëª¨ë“ˆ ë‹¨ìœ„ë¡œ êµì²´í•˜ê±°ë‚˜ í™•ì¥í•  ìˆ˜ ìˆì–´ ì„œë¹„ìŠ¤ ìš”êµ¬ì‚¬í•­ ë³€í™”ì— ë¹ ë¥´ê²Œ ëŒ€ì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "2. ìœ ì§€ë³´ìˆ˜ ë° í™•ì¥ì„± ìš©ì´\n",
      "- ê° ëª¨ë“ˆì´ ë…ë¦½ì ìœ¼ë¡œ ë™ì‘í•˜ë¯€ë¡œ íŠ¹ì • ê¸°ëŠ¥(ì˜ˆ: ì¸ë±ì‹±, ê²€ìƒ‰, ìƒì„± ë“±)ë§Œ ë³„ë„ë¡œ ê°œì„ í•˜ê±°ë‚˜ í™•ì¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "- ì¥ì•  ë°œìƒ ì‹œ ë¬¸ì œ ëª¨ë“ˆë§Œ êµì²´/ìˆ˜ì • ê°€ëŠ¥í•´ ì „ì²´ ì„œë¹„ìŠ¤ ì•ˆì •ì„±ì´ ë†’ì•„ì§‘ë‹ˆë‹¤.\n",
      "\n",
      "3. ì„±ëŠ¥ ìµœì í™”\n",
      "- ë°ì´í„° ì „ì²˜ë¦¬, ê²€ìƒ‰, ìƒì„± ë“± ê° ë‹¨ê³„ë³„ë¡œ ìµœì í™”ëœ ì†”ë£¨ì…˜ì„ ì„ íƒí•´ ì ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "- ë³‘ë ¬ ì²˜ë¦¬ ë° ìºì‹± ë“± ì„±ëŠ¥ ê°œì„  ì „ëµì„ ëª¨ë“ˆ ë‹¨ìœ„ë¡œ ì ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "4. í…ŒìŠ¤íŠ¸ ë° ê²€ì¦ ìš©ì´\n",
      "- ê° ëª¨ë“ˆë³„ë¡œ ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ê°€ ê°€ëŠ¥í•´ ì „ì²´ ì‹œìŠ¤í…œì˜ ì‹ ë¢°ì„±ì„ ë†’ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "- ìƒˆë¡œìš´ ê¸°ìˆ ì´ë‚˜ ëª¨ë¸ì„ ë¶€ë¶„ì ìœ¼ë¡œ ì ìš©í•´ ì‹¤í—˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "5. ë³´ì•ˆ ë° ì ‘ê·¼ ì œì–´ ê°•í™”\n",
      "- ë¯¼ê° ë°ì´í„° ì ‘ê·¼, ì¸ì¦, ë¡œê¹… ë“± ë³´ì•ˆ ê´€ë ¨ ê¸°ëŠ¥ì„ ë³„ë„ ëª¨ë“ˆë¡œ ê´€ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "6. ë¹„ìš© íš¨ìœ¨ì„±\n",
      "- í•„ìš”ì— ë”°ë¼ íŠ¹ì • ëª¨ë“ˆë§Œ í´ë¼ìš°ë“œ ë¦¬ì†ŒìŠ¤ë¥¼ í™•ì¥í•˜ê±°ë‚˜ ì¶•ì†Œí•  ìˆ˜ ìˆì–´ ë¹„ìš©ì„ íš¨ìœ¨ì ìœ¼ë¡œ ê´€ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "ì´ìƒìœ¼ë¡œ Production levelì—ì„œ Modular RAGë¥¼ ì‚¬ìš©í•  ë•Œì˜ ì£¼ìš” ì´ì ì„ ì •ë¦¬í–ˆìŠµë‹ˆë‹¤.\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mexecute\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "('Production levelì—ì„œ Modular RAGë¥¼ ì‚¬ìš©í•  ë•Œì˜ ì´ì ì„ ì •ë¦¬í•œë‹¤.', 'Production levelì—ì„œ Modular RAG(Retrieval-Augmented Generation)ë¥¼ ì‚¬ìš©í•  ë•Œì˜ ì´ì ì€ ë‹¤ìŒê³¼ ê°™ì´ ì •ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n\\n1. ìœ ì—°í•œ ì•„í‚¤í…ì²˜ êµ¬ì„±\\n- ë‹¤ì–‘í•œ ë°ì´í„° ì†ŒìŠ¤, ë²¡í„°DB, ê²€ìƒ‰ ì—”ì§„, LLM ë“±ì„ ëª¨ë“ˆ ë‹¨ìœ„ë¡œ êµì²´í•˜ê±°ë‚˜ í™•ì¥í•  ìˆ˜ ìˆì–´ ì„œë¹„ìŠ¤ ìš”êµ¬ì‚¬í•­ ë³€í™”ì— ë¹ ë¥´ê²Œ ëŒ€ì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n\\n2. ìœ ì§€ë³´ìˆ˜ ë° í™•ì¥ì„± ìš©ì´\\n- ê° ëª¨ë“ˆì´ ë…ë¦½ì ìœ¼ë¡œ ë™ì‘í•˜ë¯€ë¡œ íŠ¹ì • ê¸°ëŠ¥(ì˜ˆ: ì¸ë±ì‹±, ê²€ìƒ‰, ìƒì„± ë“±)ë§Œ ë³„ë„ë¡œ ê°œì„ í•˜ê±°ë‚˜ í™•ì¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n- ì¥ì•  ë°œìƒ ì‹œ ë¬¸ì œ ëª¨ë“ˆë§Œ êµì²´/ìˆ˜ì • ê°€ëŠ¥í•´ ì „ì²´ ì„œë¹„ìŠ¤ ì•ˆì •ì„±ì´ ë†’ì•„ì§‘ë‹ˆë‹¤.\\n\\n3. ì„±ëŠ¥ ìµœì í™”\\n- ë°ì´í„° ì „ì²˜ë¦¬, ê²€ìƒ‰, ìƒì„± ë“± ê° ë‹¨ê³„ë³„ë¡œ ìµœì í™”ëœ ì†”ë£¨ì…˜ì„ ì„ íƒí•´ ì ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n- ë³‘ë ¬ ì²˜ë¦¬ ë° ìºì‹± ë“± ì„±ëŠ¥ ê°œì„  ì „ëµì„ ëª¨ë“ˆ ë‹¨ìœ„ë¡œ ì ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n\\n4. í…ŒìŠ¤íŠ¸ ë° ê²€ì¦ ìš©ì´\\n- ê° ëª¨ë“ˆë³„ë¡œ ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ê°€ ê°€ëŠ¥í•´ ì „ì²´ ì‹œìŠ¤í…œì˜ ì‹ ë¢°ì„±ì„ ë†’ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n- ìƒˆë¡œìš´ ê¸°ìˆ ì´ë‚˜ ëª¨ë¸ì„ ë¶€ë¶„ì ìœ¼ë¡œ ì ìš©í•´ ì‹¤í—˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n\\n5. ë³´ì•ˆ ë° ì ‘ê·¼ ì œì–´ ê°•í™”\\n- ë¯¼ê° ë°ì´í„° ì ‘ê·¼, ì¸ì¦, ë¡œê¹… ë“± ë³´ì•ˆ ê´€ë ¨ ê¸°ëŠ¥ì„ ë³„ë„ ëª¨ë“ˆë¡œ ê´€ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n\\n6. ë¹„ìš© íš¨ìœ¨ì„±\\n- í•„ìš”ì— ë”°ë¼ íŠ¹ì • ëª¨ë“ˆë§Œ í´ë¼ìš°ë“œ ë¦¬ì†ŒìŠ¤ë¥¼ í™•ì¥í•˜ê±°ë‚˜ ì¶•ì†Œí•  ìˆ˜ ìˆì–´ ë¹„ìš©ì„ íš¨ìœ¨ì ìœ¼ë¡œ ê´€ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n\\nì´ìƒìœ¼ë¡œ Production levelì—ì„œ Modular RAGë¥¼ ì‚¬ìš©í•  ë•Œì˜ ì£¼ìš” ì´ì ì„ ì •ë¦¬í–ˆìŠµë‹ˆë‹¤.')\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mreplan\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "\u001b[1;32mresponse\u001b[0m:\n",
      "ë§ˆì§€ë§‰ ë‹¨ê³„ì¸ 'ê° ì´ì ì´ ì‹¤ì œ ì„œë¹„ìŠ¤ ìš´ì˜ì— ì–´ë–»ê²Œ ë„ì›€ì´ ë˜ëŠ”ì§€ ì„¤ëª…í•œë‹¤.'ë§Œ ë‚¨ì•˜ìŠµë‹ˆë‹¤. ì´ ë‹¨ê³„ë¥¼ ìˆ˜í–‰í•˜ë©´ ìµœì¢… ë‹µë³€ì´ ì™„ì„±ë©ë‹ˆë‹¤. ì¶”ê°€ë¡œ í•„ìš”í•œ ë‹¨ê³„ëŠ” ì—†ìŠµë‹ˆë‹¤.\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mfinal_report\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "\u001b[1;32mresponse\u001b[0m:\n",
      "# Modular RAGì™€ Naive RAGì˜ êµ¬ì¡°ì  ì°¨ì´ ë° Production í™˜ê²½ì—ì„œì˜ ì´ì \n",
      "\n",
      "## 1. ì„œë¡ \n",
      "\n",
      "Retrieval-Augmented Generation(RAG)ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì— ì™¸ë¶€ ì§€ì‹ ë² ì´ìŠ¤ì˜ ì •ë³´ë¥¼ ê²°í•©í•˜ì—¬, ë³´ë‹¤ ì •í™•í•˜ê³  í’ë¶€í•œ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ëŒ€í‘œì ì¸ ìì—°ì–´ ì²˜ë¦¬ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. ìµœê·¼ì—ëŠ” ë‹¨ìˆœí•œ Naive RAG êµ¬ì¡°ë¥¼ ë„˜ì–´, ê° ê¸°ëŠ¥ë³„ë¡œ ëª¨ë“ˆí™”ëœ Modular RAGê°€ ì£¼ëª©ë°›ê³  ìˆìŠµë‹ˆë‹¤. ë³¸ ë³´ê³ ì„œì—ì„œëŠ” Naive RAGì™€ Modular RAGì˜ êµ¬ì¡°ì  ì°¨ì´ì™€, Modular RAGê°€ ì‹¤ì œ ì„œë¹„ìŠ¤(Production level) í™˜ê²½ì—ì„œ ì œê³µí•˜ëŠ” ì£¼ìš” ì´ì ì— ëŒ€í•´ ì •ë¦¬í•©ë‹ˆë‹¤.\n",
      "\n",
      "---\n",
      "\n",
      "## 2. Naive RAGì˜ êµ¬ì¡°ì™€ ë™ì‘ ë°©ì‹\n",
      "\n",
      "Naive RAGëŠ” ë‹¤ìŒê³¼ ê°™ì€ ë‹¨ìˆœí•œ íŒŒì´í”„ë¼ì¸ êµ¬ì¡°ë¥¼ ê°€ì§‘ë‹ˆë‹¤.\n",
      "\n",
      "- **Query ì…ë ¥**: ì‚¬ìš©ìê°€ ì§ˆë¬¸ì„ ì…ë ¥\n",
      "- **Retriever(ê²€ìƒ‰ê¸°)**: ì™¸ë¶€ ì§€ì‹ ë² ì´ìŠ¤ì—ì„œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰\n",
      "- **Generator(ìƒì„±ê¸°)**: ê²€ìƒ‰ëœ ë¬¸ì„œì™€ ì§ˆë¬¸ì„ í•¨ê»˜ ë°›ì•„ LLMì´ ë‹µë³€ ìƒì„±\n",
      "\n",
      "ì´ ë°©ì‹ì€ ê²€ìƒ‰ê³¼ ìƒì„±ì´ ìˆœì°¨ì ìœ¼ë¡œ í•œ ë²ˆì— ì´ë£¨ì–´ì§€ë©°, ë³„ë„ì˜ ì „ì²˜ë¦¬ë‚˜ í›„ì²˜ë¦¬ ê³¼ì •ì´ ê±°ì˜ ì—†ìŠµë‹ˆë‹¤. êµ¬ì¡°ê°€ ë‹¨ìˆœí•˜ì—¬ êµ¬í˜„ì´ ì‰½ì§€ë§Œ, ê° ë‹¨ê³„ê°€ ë°€ì ‘í•˜ê²Œ ê²°í•©ë˜ì–´ ìˆì–´ ìœ ì—°ì„±ê³¼ í™•ì¥ì„±ì´ ì œí•œì ì…ë‹ˆë‹¤.\n",
      "\n",
      "---\n",
      "\n",
      "## 3. Modular RAGì˜ êµ¬ì¡°ì™€ ë™ì‘ ë°©ì‹\n",
      "\n",
      "Modular RAGëŠ” ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ ì—¬ëŸ¬ ê°œì˜ ë…ë¦½ì ì¸ ëª¨ë“ˆë¡œ ë¶„ë¦¬í•˜ì—¬ êµ¬ì„±í•©ë‹ˆë‹¤. ì£¼ìš” ëª¨ë“ˆì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n",
      "\n",
      "- **ë°ì´í„° ì „ì²˜ë¦¬ ëª¨ë“ˆ**: ì›ë³¸ ë°ì´í„° ì •ì œ ë° ë¶„í• , ì„ë² ë”© ìƒì„± ì¤€ë¹„\n",
      "- **ì¸ë±ì‹± ëª¨ë“ˆ**: ì „ì²˜ë¦¬ëœ ë°ì´í„°ë¥¼ ë²¡í„°DB ë“±ì— ì €ì¥, ì¸ë±ìŠ¤ êµ¬ì¶•\n",
      "- **ê²€ìƒ‰(Retrieval) ëª¨ë“ˆ**: ì¿¼ë¦¬ì™€ ìœ ì‚¬í•œ ë¬¸ì„œ ê²€ìƒ‰\n",
      "- **ìƒì„±(Generation) ëª¨ë“ˆ**: ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ LLMì´ ë‹µë³€ ìƒì„±\n",
      "- **í›„ì²˜ë¦¬ ë° í‰ê°€ ëª¨ë“ˆ**: ìƒì„±ëœ ë‹µë³€ì˜ ì •ì œ ë° í’ˆì§ˆ í‰ê°€\n",
      "\n",
      "ê° ëª¨ë“ˆì€ ë…ë¦½ì ìœ¼ë¡œ ë™ì‘í•˜ë©°, í•„ìš”ì— ë”°ë¼ êµì²´Â·í™•ì¥Â·íŠœë‹ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.\n",
      "\n",
      "---\n",
      "\n",
      "## 4. Naive RAGì™€ Modular RAGì˜ êµ¬ì¡°ì  ì°¨ì´\n",
      "\n",
      "| êµ¬ë¶„         | Naive RAG                | Modular RAG                        |\n",
      "|--------------|--------------------------|-------------------------------------|\n",
      "| êµ¬ì¡°         | ë‹¨ì¼ íŒŒì´í”„ë¼ì¸          | ëª¨ë“ˆí™”ëœ íŒŒì´í”„ë¼ì¸                 |\n",
      "| ì»´í¬ë„ŒíŠ¸     | ê²€ìƒ‰ê¸° + ìƒì„±ê¸°          | ê²€ìƒ‰ê¸°, í•„í„°, ë­ì»¤, ìƒì„±ê¸° ë“±       |\n",
      "| ìœ ì—°ì„±       | ë‚®ìŒ                     | ë†’ìŒ                                |\n",
      "| í™•ì¥ì„±       | ì œí•œì                    | ë†’ìŒ                                |\n",
      "| ìš´ì˜ ë‚œì´ë„  | ë‚®ìŒ(ë‹¨ìˆœ)               | ë†’ìŒ(ë³µì¡)                          |\n",
      "\n",
      "- **Naive RAG**ëŠ” ë‹¨ìˆœí•œ êµ¬ì¡°ë¡œ ë¹ ë¥¸ êµ¬í˜„ì´ ê°€ëŠ¥í•˜ì§€ë§Œ, ê° ë‹¨ê³„ì˜ ë…ë¦½ì  ê°œì„ ì´ë‚˜ í™•ì¥ì´ ì–´ë µìŠµë‹ˆë‹¤.\n",
      "- **Modular RAG**ëŠ” ê° ê¸°ëŠ¥ì´ ëª¨ë“ˆí™”ë˜ì–´ ìˆì–´, ì„œë¹„ìŠ¤ ìš”êµ¬ì— ë”°ë¼ ì†ì‰½ê²Œ êµ¬ì¡°ë¥¼ ë³€ê²½í•˜ê±°ë‚˜ í™•ì¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "---\n",
      "\n",
      "## 5. Production í™˜ê²½ì—ì„œ Modular RAGì˜ ì´ì \n",
      "\n",
      "Modular RAGëŠ” ì‹¤ì œ ì„œë¹„ìŠ¤ í™˜ê²½ì—ì„œ ë‹¤ìŒê³¼ ê°™ì€ ì¥ì ì„ ì œê³µí•©ë‹ˆë‹¤.\n",
      "\n",
      "1. **ìœ ì—°í•œ ì•„í‚¤í…ì²˜ êµ¬ì„±**\n",
      "   - ë‹¤ì–‘í•œ ë°ì´í„° ì†ŒìŠ¤, ë²¡í„°DB, ê²€ìƒ‰ ì—”ì§„, LLM ë“±ì„ ëª¨ë“ˆ ë‹¨ìœ„ë¡œ êµì²´Â·í™•ì¥ ê°€ëŠ¥\n",
      "   - ì„œë¹„ìŠ¤ ìš”êµ¬ì‚¬í•­ ë³€í™”ì— ë¹ ë¥´ê²Œ ëŒ€ì‘\n",
      "\n",
      "2. **ìœ ì§€ë³´ìˆ˜ ë° í™•ì¥ì„± ìš©ì´**\n",
      "   - ê° ëª¨ë“ˆì´ ë…ë¦½ì ìœ¼ë¡œ ë™ì‘í•˜ì—¬, íŠ¹ì • ê¸°ëŠ¥ë§Œ ë³„ë„ ê°œì„ Â·í™•ì¥ ê°€ëŠ¥\n",
      "   - ì¥ì•  ë°œìƒ ì‹œ ë¬¸ì œ ëª¨ë“ˆë§Œ êµì²´/ìˆ˜ì • ê°€ëŠ¥, ì „ì²´ ì„œë¹„ìŠ¤ ì•ˆì •ì„± í–¥ìƒ\n",
      "\n",
      "3. **ì„±ëŠ¥ ìµœì í™”**\n",
      "   - ê° ë‹¨ê³„ë³„ë¡œ ìµœì í™”ëœ ì†”ë£¨ì…˜ ì ìš© ê°€ëŠ¥\n",
      "   - ë³‘ë ¬ ì²˜ë¦¬, ìºì‹± ë“± ì„±ëŠ¥ ê°œì„  ì „ëµì„ ëª¨ë“ˆ ë‹¨ìœ„ë¡œ ì ìš©\n",
      "\n",
      "4. **í…ŒìŠ¤íŠ¸ ë° ê²€ì¦ ìš©ì´**\n",
      "   - ê° ëª¨ë“ˆë³„ ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ê°€ëŠ¥, ì‹œìŠ¤í…œ ì‹ ë¢°ì„± í–¥ìƒ\n",
      "   - ìƒˆë¡œìš´ ê¸°ìˆ ì´ë‚˜ ëª¨ë¸ì„ ë¶€ë¶„ì ìœ¼ë¡œ ì ìš©í•´ ì‹¤í—˜ ê°€ëŠ¥\n",
      "\n",
      "5. **ë³´ì•ˆ ë° ì ‘ê·¼ ì œì–´ ê°•í™”**\n",
      "   - ë¯¼ê° ë°ì´í„° ì ‘ê·¼, ì¸ì¦, ë¡œê¹… ë“± ë³´ì•ˆ ê¸°ëŠ¥ì„ ë³„ë„ ëª¨ë“ˆë¡œ ê´€ë¦¬\n",
      "\n",
      "6. **ë¹„ìš© íš¨ìœ¨ì„±**\n",
      "   - í•„ìš”ì— ë”°ë¼ íŠ¹ì • ëª¨ë“ˆë§Œ í´ë¼ìš°ë“œ ë¦¬ì†ŒìŠ¤ë¥¼ í™•ì¥/ì¶•ì†Œí•˜ì—¬ ë¹„ìš© íš¨ìœ¨ì  ìš´ì˜ ê°€ëŠ¥\n",
      "\n",
      "---\n",
      "\n",
      "## 6. ê²°ë¡ \n",
      "\n",
      "Modular RAGëŠ” Naive RAGì— ë¹„í•´ êµ¬ì¡°ì  ìœ ì—°ì„±ê³¼ í™•ì¥ì„±ì´ ë›°ì–´ë‚˜ë©°, ì‹¤ì œ ì„œë¹„ìŠ¤ í™˜ê²½ì—ì„œì˜ ìš´ì˜ íš¨ìœ¨ì„±, ì„±ëŠ¥ ìµœì í™”, ë³´ì•ˆ ê°•í™” ë“± ë‹¤ì–‘í•œ ì´ì ì„ ì œê³µí•©ë‹ˆë‹¤. ë³µì¡í•œ ìš”êµ¬ì‚¬í•­ê³¼ ë¹ ë¥¸ ë³€í™”ì— ëŒ€ì‘í•´ì•¼ í•˜ëŠ” Production í™˜ê²½ì—ì„œëŠ” Modular RAGì˜ ë„ì…ì´ ì ì°¨ í•„ìˆ˜ì ì¸ ì„ íƒì´ ë˜ê³  ìˆìŠµë‹ˆë‹¤.\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "from langchain_teddynote.messages import invoke_graph, random_uuid\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "config = RunnableConfig(recursion_limit=50, configurable={\"thread_id\": random_uuid()})\n",
    "\n",
    "inputs = {\n",
    "    \"input\": \"Modular RAG ê°€ ê¸°ì¡´ì˜ Naive RAG ì™€ ì–´ë–¤ ì°¨ì´ê°€ ìˆëŠ”ì§€ì™€ production level ì—ì„œ ì‚¬ìš©í•˜ëŠ” ì´ì ì„ ì„¤ëª…í•´ì¤˜\"\n",
    "}\n",
    "\n",
    "invoke_graph(app, inputs, config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Modular RAGì™€ Naive RAGì˜ êµ¬ì¡°ì  ì°¨ì´ ë° Production í™˜ê²½ì—ì„œì˜ ì´ì \n",
      "\n",
      "## 1. ì„œë¡ \n",
      "\n",
      "Retrieval-Augmented Generation(RAG)ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì— ì™¸ë¶€ ì§€ì‹ ë² ì´ìŠ¤ì˜ ì •ë³´ë¥¼ ê²°í•©í•˜ì—¬, ë³´ë‹¤ ì •í™•í•˜ê³  í’ë¶€í•œ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ëŒ€í‘œì ì¸ ìì—°ì–´ ì²˜ë¦¬ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. ìµœê·¼ì—ëŠ” ë‹¨ìˆœí•œ Naive RAG êµ¬ì¡°ë¥¼ ë„˜ì–´, ê° ê¸°ëŠ¥ë³„ë¡œ ëª¨ë“ˆí™”ëœ Modular RAGê°€ ì£¼ëª©ë°›ê³  ìˆìŠµë‹ˆë‹¤. ë³¸ ë³´ê³ ì„œì—ì„œëŠ” Naive RAGì™€ Modular RAGì˜ êµ¬ì¡°ì  ì°¨ì´ì™€, Modular RAGê°€ ì‹¤ì œ ì„œë¹„ìŠ¤(Production level) í™˜ê²½ì—ì„œ ì œê³µí•˜ëŠ” ì£¼ìš” ì´ì ì— ëŒ€í•´ ì •ë¦¬í•©ë‹ˆë‹¤.\n",
      "\n",
      "---\n",
      "\n",
      "## 2. Naive RAGì˜ êµ¬ì¡°ì™€ ë™ì‘ ë°©ì‹\n",
      "\n",
      "Naive RAGëŠ” ë‹¤ìŒê³¼ ê°™ì€ ë‹¨ìˆœí•œ íŒŒì´í”„ë¼ì¸ êµ¬ì¡°ë¥¼ ê°€ì§‘ë‹ˆë‹¤.\n",
      "\n",
      "- **Query ì…ë ¥**: ì‚¬ìš©ìê°€ ì§ˆë¬¸ì„ ì…ë ¥\n",
      "- **Retriever(ê²€ìƒ‰ê¸°)**: ì™¸ë¶€ ì§€ì‹ ë² ì´ìŠ¤ì—ì„œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰\n",
      "- **Generator(ìƒì„±ê¸°)**: ê²€ìƒ‰ëœ ë¬¸ì„œì™€ ì§ˆë¬¸ì„ í•¨ê»˜ ë°›ì•„ LLMì´ ë‹µë³€ ìƒì„±\n",
      "\n",
      "ì´ ë°©ì‹ì€ ê²€ìƒ‰ê³¼ ìƒì„±ì´ ìˆœì°¨ì ìœ¼ë¡œ í•œ ë²ˆì— ì´ë£¨ì–´ì§€ë©°, ë³„ë„ì˜ ì „ì²˜ë¦¬ë‚˜ í›„ì²˜ë¦¬ ê³¼ì •ì´ ê±°ì˜ ì—†ìŠµë‹ˆë‹¤. êµ¬ì¡°ê°€ ë‹¨ìˆœí•˜ì—¬ êµ¬í˜„ì´ ì‰½ì§€ë§Œ, ê° ë‹¨ê³„ê°€ ë°€ì ‘í•˜ê²Œ ê²°í•©ë˜ì–´ ìˆì–´ ìœ ì—°ì„±ê³¼ í™•ì¥ì„±ì´ ì œí•œì ì…ë‹ˆë‹¤.\n",
      "\n",
      "---\n",
      "\n",
      "## 3. Modular RAGì˜ êµ¬ì¡°ì™€ ë™ì‘ ë°©ì‹\n",
      "\n",
      "Modular RAGëŠ” ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ ì—¬ëŸ¬ ê°œì˜ ë…ë¦½ì ì¸ ëª¨ë“ˆë¡œ ë¶„ë¦¬í•˜ì—¬ êµ¬ì„±í•©ë‹ˆë‹¤. ì£¼ìš” ëª¨ë“ˆì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n",
      "\n",
      "- **ë°ì´í„° ì „ì²˜ë¦¬ ëª¨ë“ˆ**: ì›ë³¸ ë°ì´í„° ì •ì œ ë° ë¶„í• , ì„ë² ë”© ìƒì„± ì¤€ë¹„\n",
      "- **ì¸ë±ì‹± ëª¨ë“ˆ**: ì „ì²˜ë¦¬ëœ ë°ì´í„°ë¥¼ ë²¡í„°DB ë“±ì— ì €ì¥, ì¸ë±ìŠ¤ êµ¬ì¶•\n",
      "- **ê²€ìƒ‰(Retrieval) ëª¨ë“ˆ**: ì¿¼ë¦¬ì™€ ìœ ì‚¬í•œ ë¬¸ì„œ ê²€ìƒ‰\n",
      "- **ìƒì„±(Generation) ëª¨ë“ˆ**: ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ LLMì´ ë‹µë³€ ìƒì„±\n",
      "- **í›„ì²˜ë¦¬ ë° í‰ê°€ ëª¨ë“ˆ**: ìƒì„±ëœ ë‹µë³€ì˜ ì •ì œ ë° í’ˆì§ˆ í‰ê°€\n",
      "\n",
      "ê° ëª¨ë“ˆì€ ë…ë¦½ì ìœ¼ë¡œ ë™ì‘í•˜ë©°, í•„ìš”ì— ë”°ë¼ êµì²´Â·í™•ì¥Â·íŠœë‹ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.\n",
      "\n",
      "---\n",
      "\n",
      "## 4. Naive RAGì™€ Modular RAGì˜ êµ¬ì¡°ì  ì°¨ì´\n",
      "\n",
      "| êµ¬ë¶„         | Naive RAG                | Modular RAG                        |\n",
      "|--------------|--------------------------|-------------------------------------|\n",
      "| êµ¬ì¡°         | ë‹¨ì¼ íŒŒì´í”„ë¼ì¸          | ëª¨ë“ˆí™”ëœ íŒŒì´í”„ë¼ì¸                 |\n",
      "| ì»´í¬ë„ŒíŠ¸     | ê²€ìƒ‰ê¸° + ìƒì„±ê¸°          | ê²€ìƒ‰ê¸°, í•„í„°, ë­ì»¤, ìƒì„±ê¸° ë“±       |\n",
      "| ìœ ì—°ì„±       | ë‚®ìŒ                     | ë†’ìŒ                                |\n",
      "| í™•ì¥ì„±       | ì œí•œì                    | ë†’ìŒ                                |\n",
      "| ìš´ì˜ ë‚œì´ë„  | ë‚®ìŒ(ë‹¨ìˆœ)               | ë†’ìŒ(ë³µì¡)                          |\n",
      "\n",
      "- **Naive RAG**ëŠ” ë‹¨ìˆœí•œ êµ¬ì¡°ë¡œ ë¹ ë¥¸ êµ¬í˜„ì´ ê°€ëŠ¥í•˜ì§€ë§Œ, ê° ë‹¨ê³„ì˜ ë…ë¦½ì  ê°œì„ ì´ë‚˜ í™•ì¥ì´ ì–´ë µìŠµë‹ˆë‹¤.\n",
      "- **Modular RAG**ëŠ” ê° ê¸°ëŠ¥ì´ ëª¨ë“ˆí™”ë˜ì–´ ìˆì–´, ì„œë¹„ìŠ¤ ìš”êµ¬ì— ë”°ë¼ ì†ì‰½ê²Œ êµ¬ì¡°ë¥¼ ë³€ê²½í•˜ê±°ë‚˜ í™•ì¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "---\n",
      "\n",
      "## 5. Production í™˜ê²½ì—ì„œ Modular RAGì˜ ì´ì \n",
      "\n",
      "Modular RAGëŠ” ì‹¤ì œ ì„œë¹„ìŠ¤ í™˜ê²½ì—ì„œ ë‹¤ìŒê³¼ ê°™ì€ ì¥ì ì„ ì œê³µí•©ë‹ˆë‹¤.\n",
      "\n",
      "1. **ìœ ì—°í•œ ì•„í‚¤í…ì²˜ êµ¬ì„±**\n",
      "   - ë‹¤ì–‘í•œ ë°ì´í„° ì†ŒìŠ¤, ë²¡í„°DB, ê²€ìƒ‰ ì—”ì§„, LLM ë“±ì„ ëª¨ë“ˆ ë‹¨ìœ„ë¡œ êµì²´Â·í™•ì¥ ê°€ëŠ¥\n",
      "   - ì„œë¹„ìŠ¤ ìš”êµ¬ì‚¬í•­ ë³€í™”ì— ë¹ ë¥´ê²Œ ëŒ€ì‘\n",
      "\n",
      "2. **ìœ ì§€ë³´ìˆ˜ ë° í™•ì¥ì„± ìš©ì´**\n",
      "   - ê° ëª¨ë“ˆì´ ë…ë¦½ì ìœ¼ë¡œ ë™ì‘í•˜ì—¬, íŠ¹ì • ê¸°ëŠ¥ë§Œ ë³„ë„ ê°œì„ Â·í™•ì¥ ê°€ëŠ¥\n",
      "   - ì¥ì•  ë°œìƒ ì‹œ ë¬¸ì œ ëª¨ë“ˆë§Œ êµì²´/ìˆ˜ì • ê°€ëŠ¥, ì „ì²´ ì„œë¹„ìŠ¤ ì•ˆì •ì„± í–¥ìƒ\n",
      "\n",
      "3. **ì„±ëŠ¥ ìµœì í™”**\n",
      "   - ê° ë‹¨ê³„ë³„ë¡œ ìµœì í™”ëœ ì†”ë£¨ì…˜ ì ìš© ê°€ëŠ¥\n",
      "   - ë³‘ë ¬ ì²˜ë¦¬, ìºì‹± ë“± ì„±ëŠ¥ ê°œì„  ì „ëµì„ ëª¨ë“ˆ ë‹¨ìœ„ë¡œ ì ìš©\n",
      "\n",
      "4. **í…ŒìŠ¤íŠ¸ ë° ê²€ì¦ ìš©ì´**\n",
      "   - ê° ëª¨ë“ˆë³„ ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ê°€ëŠ¥, ì‹œìŠ¤í…œ ì‹ ë¢°ì„± í–¥ìƒ\n",
      "   - ìƒˆë¡œìš´ ê¸°ìˆ ì´ë‚˜ ëª¨ë¸ì„ ë¶€ë¶„ì ìœ¼ë¡œ ì ìš©í•´ ì‹¤í—˜ ê°€ëŠ¥\n",
      "\n",
      "5. **ë³´ì•ˆ ë° ì ‘ê·¼ ì œì–´ ê°•í™”**\n",
      "   - ë¯¼ê° ë°ì´í„° ì ‘ê·¼, ì¸ì¦, ë¡œê¹… ë“± ë³´ì•ˆ ê¸°ëŠ¥ì„ ë³„ë„ ëª¨ë“ˆë¡œ ê´€ë¦¬\n",
      "\n",
      "6. **ë¹„ìš© íš¨ìœ¨ì„±**\n",
      "   - í•„ìš”ì— ë”°ë¼ íŠ¹ì • ëª¨ë“ˆë§Œ í´ë¼ìš°ë“œ ë¦¬ì†ŒìŠ¤ë¥¼ í™•ì¥/ì¶•ì†Œí•˜ì—¬ ë¹„ìš© íš¨ìœ¨ì  ìš´ì˜ ê°€ëŠ¥\n",
      "\n",
      "---\n",
      "\n",
      "## 6. ê²°ë¡ \n",
      "\n",
      "Modular RAGëŠ” Naive RAGì— ë¹„í•´ êµ¬ì¡°ì  ìœ ì—°ì„±ê³¼ í™•ì¥ì„±ì´ ë›°ì–´ë‚˜ë©°, ì‹¤ì œ ì„œë¹„ìŠ¤ í™˜ê²½ì—ì„œì˜ ìš´ì˜ íš¨ìœ¨ì„±, ì„±ëŠ¥ ìµœì í™”, ë³´ì•ˆ ê°•í™” ë“± ë‹¤ì–‘í•œ ì´ì ì„ ì œê³µí•©ë‹ˆë‹¤. ë³µì¡í•œ ìš”êµ¬ì‚¬í•­ê³¼ ë¹ ë¥¸ ë³€í™”ì— ëŒ€ì‘í•´ì•¼ í•˜ëŠ” Production í™˜ê²½ì—ì„œëŠ” Modular RAGì˜ ë„ì…ì´ ì ì°¨ í•„ìˆ˜ì ì¸ ì„ íƒì´ ë˜ê³  ìˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "snapshot = app.get_state(config).values\n",
    "print(snapshot[\"response\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Modular RAGì™€ Naive RAGì˜ êµ¬ì¡°ì  ì°¨ì´ ë° Production í™˜ê²½ì—ì„œì˜ ì´ì \n",
       "\n",
       "## 1. ì„œë¡ \n",
       "\n",
       "Retrieval-Augmented Generation(RAG)ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì— ì™¸ë¶€ ì§€ì‹ ë² ì´ìŠ¤ì˜ ì •ë³´ë¥¼ ê²°í•©í•˜ì—¬, ë³´ë‹¤ ì •í™•í•˜ê³  í’ë¶€í•œ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ëŒ€í‘œì ì¸ ìì—°ì–´ ì²˜ë¦¬ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. ìµœê·¼ì—ëŠ” ë‹¨ìˆœí•œ Naive RAG êµ¬ì¡°ë¥¼ ë„˜ì–´, ê° ê¸°ëŠ¥ë³„ë¡œ ëª¨ë“ˆí™”ëœ Modular RAGê°€ ì£¼ëª©ë°›ê³  ìˆìŠµë‹ˆë‹¤. ë³¸ ë³´ê³ ì„œì—ì„œëŠ” Naive RAGì™€ Modular RAGì˜ êµ¬ì¡°ì  ì°¨ì´ì™€, Modular RAGê°€ ì‹¤ì œ ì„œë¹„ìŠ¤(Production level) í™˜ê²½ì—ì„œ ì œê³µí•˜ëŠ” ì£¼ìš” ì´ì ì— ëŒ€í•´ ì •ë¦¬í•©ë‹ˆë‹¤.\n",
       "\n",
       "---\n",
       "\n",
       "## 2. Naive RAGì˜ êµ¬ì¡°ì™€ ë™ì‘ ë°©ì‹\n",
       "\n",
       "Naive RAGëŠ” ë‹¤ìŒê³¼ ê°™ì€ ë‹¨ìˆœí•œ íŒŒì´í”„ë¼ì¸ êµ¬ì¡°ë¥¼ ê°€ì§‘ë‹ˆë‹¤.\n",
       "\n",
       "- **Query ì…ë ¥**: ì‚¬ìš©ìê°€ ì§ˆë¬¸ì„ ì…ë ¥\n",
       "- **Retriever(ê²€ìƒ‰ê¸°)**: ì™¸ë¶€ ì§€ì‹ ë² ì´ìŠ¤ì—ì„œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰\n",
       "- **Generator(ìƒì„±ê¸°)**: ê²€ìƒ‰ëœ ë¬¸ì„œì™€ ì§ˆë¬¸ì„ í•¨ê»˜ ë°›ì•„ LLMì´ ë‹µë³€ ìƒì„±\n",
       "\n",
       "ì´ ë°©ì‹ì€ ê²€ìƒ‰ê³¼ ìƒì„±ì´ ìˆœì°¨ì ìœ¼ë¡œ í•œ ë²ˆì— ì´ë£¨ì–´ì§€ë©°, ë³„ë„ì˜ ì „ì²˜ë¦¬ë‚˜ í›„ì²˜ë¦¬ ê³¼ì •ì´ ê±°ì˜ ì—†ìŠµë‹ˆë‹¤. êµ¬ì¡°ê°€ ë‹¨ìˆœí•˜ì—¬ êµ¬í˜„ì´ ì‰½ì§€ë§Œ, ê° ë‹¨ê³„ê°€ ë°€ì ‘í•˜ê²Œ ê²°í•©ë˜ì–´ ìˆì–´ ìœ ì—°ì„±ê³¼ í™•ì¥ì„±ì´ ì œí•œì ì…ë‹ˆë‹¤.\n",
       "\n",
       "---\n",
       "\n",
       "## 3. Modular RAGì˜ êµ¬ì¡°ì™€ ë™ì‘ ë°©ì‹\n",
       "\n",
       "Modular RAGëŠ” ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ ì—¬ëŸ¬ ê°œì˜ ë…ë¦½ì ì¸ ëª¨ë“ˆë¡œ ë¶„ë¦¬í•˜ì—¬ êµ¬ì„±í•©ë‹ˆë‹¤. ì£¼ìš” ëª¨ë“ˆì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n",
       "\n",
       "- **ë°ì´í„° ì „ì²˜ë¦¬ ëª¨ë“ˆ**: ì›ë³¸ ë°ì´í„° ì •ì œ ë° ë¶„í• , ì„ë² ë”© ìƒì„± ì¤€ë¹„\n",
       "- **ì¸ë±ì‹± ëª¨ë“ˆ**: ì „ì²˜ë¦¬ëœ ë°ì´í„°ë¥¼ ë²¡í„°DB ë“±ì— ì €ì¥, ì¸ë±ìŠ¤ êµ¬ì¶•\n",
       "- **ê²€ìƒ‰(Retrieval) ëª¨ë“ˆ**: ì¿¼ë¦¬ì™€ ìœ ì‚¬í•œ ë¬¸ì„œ ê²€ìƒ‰\n",
       "- **ìƒì„±(Generation) ëª¨ë“ˆ**: ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ LLMì´ ë‹µë³€ ìƒì„±\n",
       "- **í›„ì²˜ë¦¬ ë° í‰ê°€ ëª¨ë“ˆ**: ìƒì„±ëœ ë‹µë³€ì˜ ì •ì œ ë° í’ˆì§ˆ í‰ê°€\n",
       "\n",
       "ê° ëª¨ë“ˆì€ ë…ë¦½ì ìœ¼ë¡œ ë™ì‘í•˜ë©°, í•„ìš”ì— ë”°ë¼ êµì²´Â·í™•ì¥Â·íŠœë‹ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.\n",
       "\n",
       "---\n",
       "\n",
       "## 4. Naive RAGì™€ Modular RAGì˜ êµ¬ì¡°ì  ì°¨ì´\n",
       "\n",
       "| êµ¬ë¶„         | Naive RAG                | Modular RAG                        |\n",
       "|--------------|--------------------------|-------------------------------------|\n",
       "| êµ¬ì¡°         | ë‹¨ì¼ íŒŒì´í”„ë¼ì¸          | ëª¨ë“ˆí™”ëœ íŒŒì´í”„ë¼ì¸                 |\n",
       "| ì»´í¬ë„ŒíŠ¸     | ê²€ìƒ‰ê¸° + ìƒì„±ê¸°          | ê²€ìƒ‰ê¸°, í•„í„°, ë­ì»¤, ìƒì„±ê¸° ë“±       |\n",
       "| ìœ ì—°ì„±       | ë‚®ìŒ                     | ë†’ìŒ                                |\n",
       "| í™•ì¥ì„±       | ì œí•œì                    | ë†’ìŒ                                |\n",
       "| ìš´ì˜ ë‚œì´ë„  | ë‚®ìŒ(ë‹¨ìˆœ)               | ë†’ìŒ(ë³µì¡)                          |\n",
       "\n",
       "- **Naive RAG**ëŠ” ë‹¨ìˆœí•œ êµ¬ì¡°ë¡œ ë¹ ë¥¸ êµ¬í˜„ì´ ê°€ëŠ¥í•˜ì§€ë§Œ, ê° ë‹¨ê³„ì˜ ë…ë¦½ì  ê°œì„ ì´ë‚˜ í™•ì¥ì´ ì–´ë µìŠµë‹ˆë‹¤.\n",
       "- **Modular RAG**ëŠ” ê° ê¸°ëŠ¥ì´ ëª¨ë“ˆí™”ë˜ì–´ ìˆì–´, ì„œë¹„ìŠ¤ ìš”êµ¬ì— ë”°ë¼ ì†ì‰½ê²Œ êµ¬ì¡°ë¥¼ ë³€ê²½í•˜ê±°ë‚˜ í™•ì¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
       "\n",
       "---\n",
       "\n",
       "## 5. Production í™˜ê²½ì—ì„œ Modular RAGì˜ ì´ì \n",
       "\n",
       "Modular RAGëŠ” ì‹¤ì œ ì„œë¹„ìŠ¤ í™˜ê²½ì—ì„œ ë‹¤ìŒê³¼ ê°™ì€ ì¥ì ì„ ì œê³µí•©ë‹ˆë‹¤.\n",
       "\n",
       "1. **ìœ ì—°í•œ ì•„í‚¤í…ì²˜ êµ¬ì„±**\n",
       "   - ë‹¤ì–‘í•œ ë°ì´í„° ì†ŒìŠ¤, ë²¡í„°DB, ê²€ìƒ‰ ì—”ì§„, LLM ë“±ì„ ëª¨ë“ˆ ë‹¨ìœ„ë¡œ êµì²´Â·í™•ì¥ ê°€ëŠ¥\n",
       "   - ì„œë¹„ìŠ¤ ìš”êµ¬ì‚¬í•­ ë³€í™”ì— ë¹ ë¥´ê²Œ ëŒ€ì‘\n",
       "\n",
       "2. **ìœ ì§€ë³´ìˆ˜ ë° í™•ì¥ì„± ìš©ì´**\n",
       "   - ê° ëª¨ë“ˆì´ ë…ë¦½ì ìœ¼ë¡œ ë™ì‘í•˜ì—¬, íŠ¹ì • ê¸°ëŠ¥ë§Œ ë³„ë„ ê°œì„ Â·í™•ì¥ ê°€ëŠ¥\n",
       "   - ì¥ì•  ë°œìƒ ì‹œ ë¬¸ì œ ëª¨ë“ˆë§Œ êµì²´/ìˆ˜ì • ê°€ëŠ¥, ì „ì²´ ì„œë¹„ìŠ¤ ì•ˆì •ì„± í–¥ìƒ\n",
       "\n",
       "3. **ì„±ëŠ¥ ìµœì í™”**\n",
       "   - ê° ë‹¨ê³„ë³„ë¡œ ìµœì í™”ëœ ì†”ë£¨ì…˜ ì ìš© ê°€ëŠ¥\n",
       "   - ë³‘ë ¬ ì²˜ë¦¬, ìºì‹± ë“± ì„±ëŠ¥ ê°œì„  ì „ëµì„ ëª¨ë“ˆ ë‹¨ìœ„ë¡œ ì ìš©\n",
       "\n",
       "4. **í…ŒìŠ¤íŠ¸ ë° ê²€ì¦ ìš©ì´**\n",
       "   - ê° ëª¨ë“ˆë³„ ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ê°€ëŠ¥, ì‹œìŠ¤í…œ ì‹ ë¢°ì„± í–¥ìƒ\n",
       "   - ìƒˆë¡œìš´ ê¸°ìˆ ì´ë‚˜ ëª¨ë¸ì„ ë¶€ë¶„ì ìœ¼ë¡œ ì ìš©í•´ ì‹¤í—˜ ê°€ëŠ¥\n",
       "\n",
       "5. **ë³´ì•ˆ ë° ì ‘ê·¼ ì œì–´ ê°•í™”**\n",
       "   - ë¯¼ê° ë°ì´í„° ì ‘ê·¼, ì¸ì¦, ë¡œê¹… ë“± ë³´ì•ˆ ê¸°ëŠ¥ì„ ë³„ë„ ëª¨ë“ˆë¡œ ê´€ë¦¬\n",
       "\n",
       "6. **ë¹„ìš© íš¨ìœ¨ì„±**\n",
       "   - í•„ìš”ì— ë”°ë¼ íŠ¹ì • ëª¨ë“ˆë§Œ í´ë¼ìš°ë“œ ë¦¬ì†ŒìŠ¤ë¥¼ í™•ì¥/ì¶•ì†Œí•˜ì—¬ ë¹„ìš© íš¨ìœ¨ì  ìš´ì˜ ê°€ëŠ¥\n",
       "\n",
       "---\n",
       "\n",
       "## 6. ê²°ë¡ \n",
       "\n",
       "Modular RAGëŠ” Naive RAGì— ë¹„í•´ êµ¬ì¡°ì  ìœ ì—°ì„±ê³¼ í™•ì¥ì„±ì´ ë›°ì–´ë‚˜ë©°, ì‹¤ì œ ì„œë¹„ìŠ¤ í™˜ê²½ì—ì„œì˜ ìš´ì˜ íš¨ìœ¨ì„±, ì„±ëŠ¥ ìµœì í™”, ë³´ì•ˆ ê°•í™” ë“± ë‹¤ì–‘í•œ ì´ì ì„ ì œê³µí•©ë‹ˆë‹¤. ë³µì¡í•œ ìš”êµ¬ì‚¬í•­ê³¼ ë¹ ë¥¸ ë³€í™”ì— ëŒ€ì‘í•´ì•¼ í•˜ëŠ” Production í™˜ê²½ì—ì„œëŠ” Modular RAGì˜ ë„ì…ì´ ì ì°¨ í•„ìˆ˜ì ì¸ ì„ íƒì´ ë˜ê³  ìˆìŠµë‹ˆë‹¤."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "Markdown(snapshot[\"response\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "06. ë©€í‹° ì—ì´ì „íŠ¸ í˜‘ì—… ë„¤íŠ¸ì›Œí¬(Multi-Agent Collaboration Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-4o\n"
     ]
    }
   ],
   "source": [
    "from langchain_teddynote.models import get_model_name, LLMs\n",
    "\n",
    "# ìµœì‹  ëª¨ë¸ ì´ë¦„ ê°€ì ¸ì˜¤ê¸°\n",
    "MODEL_NAME = get_model_name(LLMs.GPT4o)\n",
    "\n",
    "print(MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import Annotated, Sequence\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "\n",
    "# ìƒíƒœ ì •ì˜\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[\n",
    "        Sequence[BaseMessage], operator.add\n",
    "    ]  # Agent ê°„ ê³µìœ í•˜ëŠ” ë©”ì‹œì§€ ëª©ë¡\n",
    "    sender: Annotated[str, \"The sender of the last message\"]  # ë§ˆì§€ë§‰ ë©”ì‹œì§€ì˜ ë°œì‹ ì\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "\n",
    "from langchain_teddynote.tools.tavily import TavilySearch\n",
    "from langchain_core.tools import tool\n",
    "from langchain_experimental.utilities import PythonREPL\n",
    "\n",
    "# Tavily ê²€ìƒ‰ ë„êµ¬ ì •ì˜\n",
    "tavily_tool = TavilySearch(max_results=5)\n",
    "\n",
    "# Python ì½”ë“œë¥¼ ì‹¤í–‰í•˜ëŠ” ë„êµ¬ ì •ì˜\n",
    "python_repl = PythonREPL()\n",
    "\n",
    "\n",
    "# Python ì½”ë“œë¥¼ ì‹¤í–‰í•˜ëŠ” ë„êµ¬ ì •ì˜\n",
    "@tool\n",
    "def python_repl_tool(\n",
    "    code: Annotated[str, \"The python code to execute to generate your chart.\"],\n",
    "):\n",
    "    \"\"\"Use this to execute python code. If you want to see the output of a value,\n",
    "    you should print it out with `print(...)`. This is visible to the user.\"\"\"\n",
    "    try:\n",
    "        # ì£¼ì–´ì§„ ì½”ë“œë¥¼ Python REPLì—ì„œ ì‹¤í–‰í•˜ê³  ê²°ê³¼ ë°˜í™˜\n",
    "        result = python_repl.run(code)\n",
    "    except BaseException as e:\n",
    "        return f\"Failed to execute code. Error: {repr(e)}\"\n",
    "    # ì‹¤í–‰ ì„±ê³µ ì‹œ ê²°ê³¼ì™€ í•¨ê»˜ ì„±ê³µ ë©”ì‹œì§€ ë°˜í™˜\n",
    "    result_str = f\"Successfully executed:\\n```python\\n{code}\\n```\\nStdout: {result}\"\n",
    "    return (\n",
    "        result_str + \"\\n\\nIf you have completed all tasks, respond with FINAL ANSWER.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_system_prompt(suffix: str) -> str:\n",
    "    return (\n",
    "        \"You are a helpful AI assistant, collaborating with other assistants.\"\n",
    "        \" Use the provided tools to progress towards answering the question.\"\n",
    "        \" If you are unable to fully answer, that's OK, another assistant with different tools \"\n",
    "        \" will help where you left off. Execute what you can to make progress.\"\n",
    "        \" If you or any of the other assistants have the final answer or deliverable,\"\n",
    "        \" prefix your response with FINAL ANSWER so the team knows to stop.\"\n",
    "        f\"\\n{suffix}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph.graph import MessagesState\n",
    "\n",
    "# LLM ì •ì˜\n",
    "llm = ChatOpenAI(model=MODEL_NAME)\n",
    "\n",
    "# Research Agent ìƒì„±\n",
    "research_agent = create_react_agent(\n",
    "    llm,\n",
    "    tools=[tavily_tool],\n",
    "    prompt=make_system_prompt(\n",
    "        \"You can only do research. You are working with a chart generator colleague.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "# Research Agent ë…¸ë“œ ì •ì˜\n",
    "def research_node(state: MessagesState) -> MessagesState:\n",
    "    result = research_agent.invoke(state)\n",
    "\n",
    "    # ë§ˆì§€ë§‰ ë©”ì‹œì§€ë¥¼ HumanMessage ë¡œ ë³€í™˜\n",
    "    last_message = HumanMessage(\n",
    "        content=result[\"messages\"][-1].content, name=\"researcher\"\n",
    "    )\n",
    "    return {\n",
    "        # Research Agent ì˜ ë©”ì‹œì§€ ëª©ë¡ ë°˜í™˜\n",
    "        \"messages\": [last_message],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_generator_system_prompt = \"\"\"\n",
    "You can only generate charts. You are working with a researcher colleague.\n",
    "Be sure to use the following font code in your code when generating charts.\n",
    "\n",
    "##### í°íŠ¸ ì„¤ì • #####\n",
    "import platform\n",
    "\n",
    "# OS íŒë‹¨\n",
    "current_os = platform.system()\n",
    "\n",
    "if current_os == \"Windows\":\n",
    "    # Windows í™˜ê²½ í°íŠ¸ ì„¤ì •\n",
    "    font_path = \"C:/Windows/Fonts/malgun.ttf\"  # ë§‘ì€ ê³ ë”• í°íŠ¸ ê²½ë¡œ\n",
    "    fontprop = fm.FontProperties(fname=font_path, size=12)\n",
    "    plt.rc(\"font\", family=fontprop.get_name())\n",
    "elif current_os == \"Darwin\":  # macOS\n",
    "    # Mac í™˜ê²½ í°íŠ¸ ì„¤ì •\n",
    "    plt.rcParams[\"font.family\"] = \"AppleGothic\"\n",
    "else:  # Linux ë“± ê¸°íƒ€ OS\n",
    "    # ê¸°ë³¸ í•œê¸€ í°íŠ¸ ì„¤ì • ì‹œë„\n",
    "    try:\n",
    "        plt.rcParams[\"font.family\"] = \"NanumGothic\"\n",
    "    except:\n",
    "        print(\"í•œê¸€ í°íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‹œìŠ¤í…œ ê¸°ë³¸ í°íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "##### ë§ˆì´ë„ˆìŠ¤ í°íŠ¸ ê¹¨ì§ ë°©ì§€ #####\n",
    "plt.rcParams[\"axes.unicode_minus\"] = False  # ë§ˆì´ë„ˆìŠ¤ í°íŠ¸ ê¹¨ì§ ë°©ì§€\n",
    "\"\"\"\n",
    "\n",
    "# Chart Generator Agent ìƒì„±\n",
    "chart_agent = create_react_agent(\n",
    "    llm,\n",
    "    [python_repl_tool],\n",
    "    prompt=make_system_prompt(chart_generator_system_prompt),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chart_node(state: MessagesState) -> MessagesState:\n",
    "    result = chart_agent.invoke(state)\n",
    "\n",
    "    # ë§ˆì§€ë§‰ ë©”ì‹œì§€ë¥¼ HumanMessage ë¡œ ë³€í™˜\n",
    "    last_message = HumanMessage(\n",
    "        content=result[\"messages\"][-1].content, name=\"chart_generator\"\n",
    "    )\n",
    "    return {\n",
    "        # share internal message history of chart agent with other agents\n",
    "        \"messages\": [last_message],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END\n",
    "\n",
    "\n",
    "def router(state: MessagesState):\n",
    "    # This is the router\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    if \"FINAL ANSWER\" in last_message.content:\n",
    "        # Any agent decided the work is done\n",
    "        return END\n",
    "    return \"continue\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, ToolMessage\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "workflow = StateGraph(MessagesState)\n",
    "workflow.add_node(\"researcher\", research_node)\n",
    "workflow.add_node(\"chart_generator\", chart_node)\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"researcher\",\n",
    "    router,\n",
    "    {\"continue\": \"chart_generator\", END: END},\n",
    ")\n",
    "workflow.add_conditional_edges(\n",
    "    \"chart_generator\",\n",
    "    router,\n",
    "    {\"continue\": \"researcher\", END: END},\n",
    ")\n",
    "\n",
    "workflow.add_edge(START, \"researcher\")\n",
    "app = workflow.compile(checkpointer=MemorySaver())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_teddynote.graphs import visualize_graph\n",
    "\n",
    "visualize_graph(app, xray=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36magent\u001b[0m in [\u001b[1;33mresearcher\u001b[0m] ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  tavily_web_search (call_SFrMJLeGNgPNZ9QAt5iV8Glm)\n",
      " Call ID: call_SFrMJLeGNgPNZ9QAt5iV8Glm\n",
      "  Args:\n",
      "    query: ëŒ€í•œë¯¼êµ­ 2010 to 2023 1ì¸ë‹¹ GDP ë°ì´í„°\n",
      "  tavily_web_search (call_FSptEZU8QUx2cOao9RynICRy)\n",
      " Call ID: call_FSptEZU8QUx2cOao9RynICRy\n",
      "  Args:\n",
      "    query: ëŒ€í•œë¯¼êµ­ 2024 ì˜ˆì¸¡ 1ì¸ë‹¹ GDP\n",
      "==================================================\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'Request too large for gpt-4o in organization org-Soffyuxth7vsUnobnzk0MGeR on tokens per min (TPM): Limit 30000, Requested 68416. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[110], line 17\u001b[0m\n\u001b[1;32m      8\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\n\u001b[1;32m     10\u001b[0m         HumanMessage(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m     ],\n\u001b[1;32m     14\u001b[0m }\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# ê·¸ë˜í”„ ì‹¤í–‰\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[43minvoke_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresearcher\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mchart_generator\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43magent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_teddynote/messages.py:409\u001b[0m, in \u001b[0;36minvoke_graph\u001b[0;34m(graph, inputs, config, node_names, callback)\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m namespace[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(namespace) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroot graph\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;66;03m# subgraphs=True ë¥¼ í†µí•´ ì„œë¸Œê·¸ë˜í”„ì˜ ì¶œë ¥ë„ í¬í•¨\u001b[39;00m\n\u001b[0;32m--> 409\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m namespace, chunk \u001b[38;5;129;01min\u001b[39;00m graph\u001b[38;5;241m.\u001b[39mstream(\n\u001b[1;32m    410\u001b[0m     inputs, config, stream_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupdates\u001b[39m\u001b[38;5;124m\"\u001b[39m, subgraphs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    411\u001b[0m ):\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m node_name, node_chunk \u001b[38;5;129;01min\u001b[39;00m chunk\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    413\u001b[0m         \u001b[38;5;66;03m# node_namesê°€ ë¹„ì–´ìˆì§€ ì•Šì€ ê²½ìš°ì—ë§Œ í•„í„°ë§\u001b[39;00m\n\u001b[1;32m    414\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(node_names) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m node_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m node_names:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/main.py:2647\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[0m\n\u001b[1;32m   2645\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mmatch_cached_writes():\n\u001b[1;32m   2646\u001b[0m     loop\u001b[38;5;241m.\u001b[39moutput_writes(task\u001b[38;5;241m.\u001b[39mid, task\u001b[38;5;241m.\u001b[39mwrites, cached\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m-> 2647\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mtick(\n\u001b[1;32m   2648\u001b[0m     [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mvalues() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t\u001b[38;5;241m.\u001b[39mwrites],\n\u001b[1;32m   2649\u001b[0m     timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[1;32m   2650\u001b[0m     get_waiter\u001b[38;5;241m=\u001b[39mget_waiter,\n\u001b[1;32m   2651\u001b[0m     schedule_task\u001b[38;5;241m=\u001b[39mloop\u001b[38;5;241m.\u001b[39maccept_push,\n\u001b[1;32m   2652\u001b[0m ):\n\u001b[1;32m   2653\u001b[0m     \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[1;32m   2654\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m _output(\n\u001b[1;32m   2655\u001b[0m         stream_mode, print_mode, subgraphs, stream\u001b[38;5;241m.\u001b[39mget, queue\u001b[38;5;241m.\u001b[39mEmpty\n\u001b[1;32m   2656\u001b[0m     )\n\u001b[1;32m   2657\u001b[0m loop\u001b[38;5;241m.\u001b[39mafter_tick()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/_runner.py:253\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;66;03m# panic on failure or timeout\u001b[39;00m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 253\u001b[0m     \u001b[43m_panic_or_proceed\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfutures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdone\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfutures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpanic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tb \u001b[38;5;241m:=\u001b[39m exc\u001b[38;5;241m.\u001b[39m__traceback__:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/_runner.py:511\u001b[0m, in \u001b[0;36m_panic_or_proceed\u001b[0;34m(futs, timeout_exc_cls, panic)\u001b[0m\n\u001b[1;32m    509\u001b[0m                 interrupts\u001b[38;5;241m.\u001b[39mappend(exc)\n\u001b[1;32m    510\u001b[0m             \u001b[38;5;28;01melif\u001b[39;00m fut \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m SKIP_RERAISE_SET:\n\u001b[0;32m--> 511\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m    512\u001b[0m \u001b[38;5;66;03m# raise combined interrupts\u001b[39;00m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m interrupts:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/_executor.py:81\u001b[0m, in \u001b[0;36mBackgroundExecutor.done\u001b[0;34m(self, task)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Remove the task from the tasks dict when it's done.\"\"\"\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 81\u001b[0m     \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GraphBubbleUp:\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;66;03m# This exception is an interruption signal, not an error\u001b[39;00m\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;66;03m# so we don't want to re-raise it on exit\u001b[39;00m\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mpop(task)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/concurrent/futures/_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/concurrent/futures/_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/_retry.py:42\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[1;32m     40\u001b[0m     task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     44\u001b[0m     ns: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/_internal/_runnable.py:657\u001b[0m, in \u001b[0;36mRunnableSeq.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    655\u001b[0m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[1;32m    656\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[0;32m--> 657\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    659\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/_internal/_runnable.py:401\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    399\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(ret)\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 401\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[1;32m    403\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "Cell \u001b[0;32mIn[104], line 21\u001b[0m, in \u001b[0;36mresearch_node\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mresearch_node\u001b[39m(state: MessagesState) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m MessagesState:\n\u001b[0;32m---> 21\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mresearch_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m# ë§ˆì§€ë§‰ ë©”ì‹œì§€ë¥¼ HumanMessage ë¡œ ë³€í™˜\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     last_message \u001b[38;5;241m=\u001b[39m HumanMessage(\n\u001b[1;32m     25\u001b[0m         content\u001b[38;5;241m=\u001b[39mresult[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mcontent, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresearcher\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     26\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/main.py:3026\u001b[0m, in \u001b[0;36mPregel.invoke\u001b[0;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[0m\n\u001b[1;32m   3023\u001b[0m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m|\u001b[39m Any] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   3024\u001b[0m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m-> 3026\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream(\n\u001b[1;32m   3027\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   3028\u001b[0m     config,\n\u001b[1;32m   3029\u001b[0m     context\u001b[38;5;241m=\u001b[39mcontext,\n\u001b[1;32m   3030\u001b[0m     stream_mode\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupdates\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   3031\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3032\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m stream_mode,\n\u001b[1;32m   3033\u001b[0m     print_mode\u001b[38;5;241m=\u001b[39mprint_mode,\n\u001b[1;32m   3034\u001b[0m     output_keys\u001b[38;5;241m=\u001b[39moutput_keys,\n\u001b[1;32m   3035\u001b[0m     interrupt_before\u001b[38;5;241m=\u001b[39minterrupt_before,\n\u001b[1;32m   3036\u001b[0m     interrupt_after\u001b[38;5;241m=\u001b[39minterrupt_after,\n\u001b[1;32m   3037\u001b[0m     durability\u001b[38;5;241m=\u001b[39mdurability,\n\u001b[1;32m   3038\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3039\u001b[0m ):\n\u001b[1;32m   3040\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   3041\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(chunk) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/main.py:2647\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[0m\n\u001b[1;32m   2645\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mmatch_cached_writes():\n\u001b[1;32m   2646\u001b[0m     loop\u001b[38;5;241m.\u001b[39moutput_writes(task\u001b[38;5;241m.\u001b[39mid, task\u001b[38;5;241m.\u001b[39mwrites, cached\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m-> 2647\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mtick(\n\u001b[1;32m   2648\u001b[0m     [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mvalues() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t\u001b[38;5;241m.\u001b[39mwrites],\n\u001b[1;32m   2649\u001b[0m     timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[1;32m   2650\u001b[0m     get_waiter\u001b[38;5;241m=\u001b[39mget_waiter,\n\u001b[1;32m   2651\u001b[0m     schedule_task\u001b[38;5;241m=\u001b[39mloop\u001b[38;5;241m.\u001b[39maccept_push,\n\u001b[1;32m   2652\u001b[0m ):\n\u001b[1;32m   2653\u001b[0m     \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[1;32m   2654\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m _output(\n\u001b[1;32m   2655\u001b[0m         stream_mode, print_mode, subgraphs, stream\u001b[38;5;241m.\u001b[39mget, queue\u001b[38;5;241m.\u001b[39mEmpty\n\u001b[1;32m   2656\u001b[0m     )\n\u001b[1;32m   2657\u001b[0m loop\u001b[38;5;241m.\u001b[39mafter_tick()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/_runner.py:162\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[0m\n\u001b[1;32m    160\u001b[0m t \u001b[38;5;241m=\u001b[39m tasks[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 162\u001b[0m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m                \u001b[49m\u001b[43m_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m                \u001b[49m\u001b[43mweakref\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m                \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m                \u001b[49m\u001b[43mfutures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweakref\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m                \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschedule_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m                \u001b[49m\u001b[43msubmit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/_retry.py:42\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[1;32m     40\u001b[0m     task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     44\u001b[0m     ns: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/_internal/_runnable.py:657\u001b[0m, in \u001b[0;36mRunnableSeq.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    655\u001b[0m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[1;32m    656\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[0;32m--> 657\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    659\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/_internal/_runnable.py:394\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[1;32m    393\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[0;32m--> 394\u001b[0m         ret \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    396\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/prebuilt/chat_agent_executor.py:627\u001b[0m, in \u001b[0;36mcreate_react_agent.<locals>.call_model\u001b[0;34m(state, runtime, config)\u001b[0m\n\u001b[1;32m    625\u001b[0m     response \u001b[38;5;241m=\u001b[39m cast(AIMessage, dynamic_model\u001b[38;5;241m.\u001b[39minvoke(model_input, config))  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    626\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 627\u001b[0m     response \u001b[38;5;241m=\u001b[39m cast(AIMessage, \u001b[43mstatic_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m)  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;66;03m# add agent name to the AIMessage\u001b[39;00m\n\u001b[1;32m    630\u001b[0m response\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m=\u001b[39m name\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/runnables/base.py:3082\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3080\u001b[0m                 input_ \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, input_, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   3081\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3082\u001b[0m                 input_ \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3083\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   3084\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/runnables/base.py:5495\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   5488\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m   5489\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m   5490\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5493\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   5494\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[0;32m-> 5495\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5496\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5497\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5498\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5499\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:393\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    389\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    390\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    391\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    392\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatGeneration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m--> 393\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    403\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:1019\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m   1011\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m   1012\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1016\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m   1017\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m   1018\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m-> 1019\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:837\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    834\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[1;32m    835\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    836\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 837\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    838\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    840\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    843\u001b[0m         )\n\u001b[1;32m    844\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    845\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:1085\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1083\u001b[0m     result \u001b[38;5;241m=\u001b[39m generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[1;32m   1084\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1085\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1086\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m   1087\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1088\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1089\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_openai/chat_models/base.py:1183\u001b[0m, in \u001b[0;36mBaseChatOpenAI._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1181\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m raw_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(raw_response, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp_response\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1182\u001b[0m         e\u001b[38;5;241m.\u001b[39mresponse \u001b[38;5;241m=\u001b[39m raw_response\u001b[38;5;241m.\u001b[39mhttp_response  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m-> 1183\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m   1184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1185\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minclude_response_headers\n\u001b[1;32m   1186\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m raw_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(raw_response, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1188\u001b[0m ):\n\u001b[1;32m   1189\u001b[0m     generation_info \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(raw_response\u001b[38;5;241m.\u001b[39mheaders)}\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_openai/chat_models/base.py:1178\u001b[0m, in \u001b[0;36mBaseChatOpenAI._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1171\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _construct_lc_result_from_responses_api(\n\u001b[1;32m   1172\u001b[0m             response,\n\u001b[1;32m   1173\u001b[0m             schema\u001b[38;5;241m=\u001b[39moriginal_schema_obj,\n\u001b[1;32m   1174\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mgeneration_info,\n\u001b[1;32m   1175\u001b[0m             output_version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_version,\n\u001b[1;32m   1176\u001b[0m         )\n\u001b[1;32m   1177\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1178\u001b[0m         raw_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwith_raw_response\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1179\u001b[0m         response \u001b[38;5;241m=\u001b[39m raw_response\u001b[38;5;241m.\u001b[39mparse()\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_legacy_response.py:364\u001b[0m, in \u001b[0;36mto_raw_response_wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    360\u001b[0m extra_headers[RAW_RESPONSE_HEADER] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    362\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextra_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m extra_headers\n\u001b[0;32m--> 364\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cast(LegacyAPIResponse[R], \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_utils/_utils.py:287\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 287\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py:1147\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   1101\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m   1102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m   1103\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m   1145\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m   1146\u001b[0m     validate_response_format(response_format)\n\u001b[0;32m-> 1147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1150\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m   1151\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1152\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1153\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maudio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1154\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1155\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1156\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1157\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1158\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1159\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_completion_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1160\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1162\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodalities\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1163\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1165\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1166\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1167\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprompt_cache_key\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1168\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreasoning_effort\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1169\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1170\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msafety_identifier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1171\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1172\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1173\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1174\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1175\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1176\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1177\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1178\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1179\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1181\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mverbosity\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1184\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweb_search_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1185\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1186\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[1;32m   1187\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[1;32m   1188\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1189\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1190\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m   1192\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1196\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_base_client.py:1259\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1245\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1246\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1247\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1254\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1255\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1256\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1257\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1258\u001b[0m     )\n\u001b[0;32m-> 1259\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_base_client.py:1047\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1044\u001b[0m             err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1046\u001b[0m         log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1047\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1051\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould not resolve response (should never happen)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'Request too large for gpt-4o in organization org-Soffyuxth7vsUnobnzk0MGeR on tokens per min (TPM): Limit 30000, Requested 68416. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_teddynote.messages import random_uuid, invoke_graph\n",
    "\n",
    "# config ì„¤ì •(ì¬ê·€ ìµœëŒ€ íšŸìˆ˜, thread_id)\n",
    "config = RunnableConfig(recursion_limit=10, configurable={\"thread_id\": random_uuid()})\n",
    "\n",
    "# ì§ˆë¬¸ ì…ë ¥\n",
    "inputs = {\n",
    "    \"messages\": [\n",
    "        HumanMessage(\n",
    "            content=\"2010ë…„ ~ 2024ë…„ê¹Œì§€ì˜ ëŒ€í•œë¯¼êµ­ì˜ 1ì¸ë‹¹ GDP ì¶”ì´ë¥¼ ê·¸ë˜í”„ë¡œ ì‹œê°í™” í•´ì£¼ì„¸ìš”.\"\n",
    "        )\n",
    "    ],\n",
    "}\n",
    "\n",
    "# ê·¸ë˜í”„ ì‹¤í–‰\n",
    "invoke_graph(app, inputs, config, node_names=[\"researcher\", \"chart_generator\", \"agent\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "07. ë©€í‹° ì—ì´ì „íŠ¸ ê°ë…ì(Multi-Agent Supervisor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-4o\n"
     ]
    }
   ],
   "source": [
    "from langchain_teddynote.models import get_model_name, LLMs\n",
    "\n",
    "# ìµœì‹  ë²„ì „ì˜ ëª¨ë¸ëª…ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.\n",
    "MODEL_NAME = get_model_name(LLMs.GPT4o)\n",
    "print(MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import Sequence, Annotated\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "\n",
    "# ìƒíƒœ ì •ì˜\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]  # ë©”ì‹œì§€\n",
    "    next: str  # ë‹¤ìŒìœ¼ë¡œ ë¼ìš°íŒ…í•  ì—ì´ì „íŠ¸\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote.tools.tavily import TavilySearch\n",
    "from langchain_experimental.tools import PythonREPLTool\n",
    "\n",
    "# ìµœëŒ€ 5ê°œì˜ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ëŠ” Tavily ê²€ìƒ‰ ë„êµ¬ ì´ˆê¸°í™”\n",
    "tavily_tool = TavilySearch(max_results=5)\n",
    "\n",
    "# ë¡œì»¬ì—ì„œ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ëŠ” Python REPL ë„êµ¬ ì´ˆê¸°í™” (ì•ˆì „í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŒ)\n",
    "python_repl_tool = PythonREPLTool()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "\n",
    "# ì§€ì •í•œ agentì™€ nameì„ ì‚¬ìš©í•˜ì—¬ agent ë…¸ë“œë¥¼ ìƒì„±\n",
    "def agent_node(state, agent, name):\n",
    "    # agent í˜¸ì¶œ\n",
    "    agent_response = agent.invoke(state)\n",
    "    # agentì˜ ë§ˆì§€ë§‰ ë©”ì‹œì§€ë¥¼ HumanMessageë¡œ ë³€í™˜í•˜ì—¬ ë°˜í™˜\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            HumanMessage(content=agent_response[\"messages\"][-1].content, name=name)\n",
    "        ]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "# Research Agent ìƒì„±\n",
    "research_agent = create_react_agent(ChatOpenAI(model=MODEL_NAME), tools=[tavily_tool])\n",
    "\n",
    "# research node ìƒì„±\n",
    "research_node = functools.partial(agent_node, agent=research_agent, name=\"Researcher\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='Here\\'s a simple code snippet in Python to print \"Hello, World!\" to the terminal:\\n\\n```python\\nprint(\"Hello, World!\")\\n```\\n\\nYou can run this code in any Python environment or terminal to see the output. If you need a similar example in another programming language, feel free to ask!', additional_kwargs={}, response_metadata={}, name='Researcher')]}"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "research_node(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(content=\"Code hello world and print it to the terminal\")\n",
    "        ]\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import Literal\n",
    "\n",
    "# ë©¤ë²„ Agent ëª©ë¡ ì •ì˜\n",
    "members = [\"Researcher\", \"Coder\"]\n",
    "\n",
    "# ë‹¤ìŒ ì‘ì—…ì ì„ íƒ ì˜µì…˜ ëª©ë¡ ì •ì˜\n",
    "options_for_next = [\"FINISH\"] + members\n",
    "\n",
    "\n",
    "# ì‘ì—…ì ì„ íƒ ì‘ë‹µ ëª¨ë¸ ì •ì˜: ë‹¤ìŒ ì‘ì—…ìë¥¼ ì„ íƒí•˜ê±°ë‚˜ ì‘ì—… ì™„ë£Œë¥¼ ë‚˜íƒ€ëƒ„\n",
    "class RouteResponse(BaseModel):\n",
    "    next: Literal[options_for_next]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "# ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì •ì˜: ì‘ì—…ì ê°„ì˜ ëŒ€í™”ë¥¼ ê´€ë¦¬í•˜ëŠ” ê°ë…ì ì—­í• \n",
    "system_prompt = (\n",
    "    \"You are a supervisor tasked with managing a conversation between the\"\n",
    "    \" following workers:  {members}. Given the following user request,\"\n",
    "    \" respond with the worker to act next. Each worker will perform a\"\n",
    "    \" task and respond with their results and status. When finished,\"\n",
    "    \" respond with FINISH.\"\n",
    ")\n",
    "\n",
    "# ChatPromptTemplate ìƒì„±\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Given the conversation above, who should act next? \"\n",
    "            \"Or should we FINISH? Select one of: {options}\",\n",
    "        ),\n",
    "    ]\n",
    ").partial(options=str(options_for_next), members=\", \".join(members))\n",
    "\n",
    "\n",
    "# LLM ì´ˆê¸°í™”\n",
    "llm = ChatOpenAI(model=MODEL_NAME, temperature=0)\n",
    "\n",
    "\n",
    "# Supervisor Agent ìƒì„±\n",
    "def supervisor_agent(state):\n",
    "    # í”„ë¡¬í”„íŠ¸ì™€ LLMì„ ê²°í•©í•˜ì—¬ ì²´ì¸ êµ¬ì„±\n",
    "    supervisor_chain = prompt | llm.with_structured_output(RouteResponse)\n",
    "    \n",
    "    # ì—¬ê¸°ì„œ ì‘ë‹µì„ ì§ì ‘ í˜¸ì¶œí•˜ê³  ì¶œë ¥í•©ë‹ˆë‹¤.\n",
    "    response = supervisor_chain.invoke(state)\n",
    "    \n",
    "    # ë””ë²„ê¹…ì„ ìœ„í•´ ì‘ë‹µ ê°’ ì¶œë ¥\n",
    "    print(\"\\n--- LLM ì‘ë‹µ ê°’ ---\")\n",
    "    print(response)\n",
    "    print(\"-------------------\\n\")\n",
    "    \n",
    "    # Agent í˜¸ì¶œ\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "\n",
    "# Research Agent ìƒì„±\n",
    "research_agent = create_react_agent(llm, tools=[tavily_tool])\n",
    "research_node = functools.partial(agent_node, agent=research_agent, name=\"Researcher\")\n",
    "\n",
    "code_system_prompt = \"\"\"\n",
    "Be sure to use the following font in your code for visualization.\n",
    "\n",
    "##### í°íŠ¸ ì„¤ì • #####\n",
    "import platform\n",
    "\n",
    "# OS íŒë‹¨\n",
    "current_os = platform.system()\n",
    "\n",
    "if current_os == \"Windows\":\n",
    "    # Windows í™˜ê²½ í°íŠ¸ ì„¤ì •\n",
    "    font_path = \"C:/Windows/Fonts/malgun.ttf\"  # ë§‘ì€ ê³ ë”• í°íŠ¸ ê²½ë¡œ\n",
    "    fontprop = fm.FontProperties(fname=font_path, size=12)\n",
    "    plt.rc(\"font\", family=fontprop.get_name())\n",
    "elif current_os == \"Darwin\":  # macOS\n",
    "    # Mac í™˜ê²½ í°íŠ¸ ì„¤ì •\n",
    "    plt.rcParams[\"font.family\"] = \"AppleGothic\"\n",
    "else:  # Linux ë“± ê¸°íƒ€ OS\n",
    "    # ê¸°ë³¸ í•œê¸€ í°íŠ¸ ì„¤ì • ì‹œë„\n",
    "    try:\n",
    "        plt.rcParams[\"font.family\"] = \"NanumGothic\"\n",
    "    except:\n",
    "        print(\"í•œê¸€ í°íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‹œìŠ¤í…œ ê¸°ë³¸ í°íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "##### ë§ˆì´ë„ˆìŠ¤ í°íŠ¸ ê¹¨ì§ ë°©ì§€ #####\n",
    "plt.rcParams[\"axes.unicode_minus\"] = False  # ë§ˆì´ë„ˆìŠ¤ í°íŠ¸ ê¹¨ì§ ë°©ì§€\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Coder Agent ìƒì„±\n",
    "coder_agent = create_react_agent(\n",
    "    llm,\n",
    "    tools=[python_repl_tool],\n",
    "    prompt=code_system_prompt,\n",
    ")\n",
    "coder_node = functools.partial(agent_node, agent=coder_agent, name=\"Coder\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# ê·¸ë˜í”„ ìƒì„±\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# ê·¸ë˜í”„ì— ë…¸ë“œ ì¶”ê°€\n",
    "workflow.add_node(\"Researcher\", research_node)\n",
    "workflow.add_node(\"Coder\", coder_node)\n",
    "workflow.add_node(\"Supervisor\", supervisor_agent)\n",
    "\n",
    "\n",
    "# ë©¤ë²„ ë…¸ë“œ > Supervisor ë…¸ë“œë¡œ ì—£ì§€ ì¶”ê°€\n",
    "for member in members:\n",
    "    workflow.add_edge(member, \"Supervisor\")\n",
    "\n",
    "# ì¡°ê±´ë¶€ ì—£ì§€ ì¶”ê°€ (\n",
    "conditional_map = {k: k for k in members}\n",
    "conditional_map[\"FINISH\"] = END\n",
    "\n",
    "\n",
    "def get_next(state):\n",
    "    return state[\"next\"]\n",
    "\n",
    "\n",
    "# Supervisor ë…¸ë“œì—ì„œ ì¡°ê±´ë¶€ ì—£ì§€ ì¶”ê°€\n",
    "workflow.add_conditional_edges(\"Supervisor\", get_next, conditional_map)\n",
    "\n",
    "# ì‹œì‘ì \n",
    "workflow.add_edge(START, \"Supervisor\")\n",
    "\n",
    "# ê·¸ë˜í”„ ì»´íŒŒì¼\n",
    "graph = workflow.compile(checkpointer=MemorySaver())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAERCAIAAAAFfxz5AAAQAElEQVR4nOydB0AT1x/H3yUk7D1liCCi4IAqorj3rqPWveqoq7Wuqq2tq26t1lW17rr/7lr3FldFQRSRJUOW7L3J+P+SgxAxKChJLsnvU3q+vHt3udz43u/9fm9oCYVCgiAIwki0CIIgCFNBhUIQhLmgQiEIwlxQoRAEYS6oUAiCMBdUKARBmAt76dKlBPkg2bzS/W9CzryN9MtIPpEQcSQ+PL+0xMPE8rdXfrveBBcJeM2MLH4LebzrzavckmJPE8vloU92xQSzCHEzNPvxxf2/40J12CxXA9OfXz7aHxtSwCv1MLZYF+a/PeZlIZ/XzNjil+BH+2JDysoHicoLBUJ3I7MVIX5/vQmOK8xtbWazLtx/e/TL3NIS2P+SkMd7Kr7LD75LQISNDc0WBT/aGxtSyBPtc214wNH4sOC8jOzSEis9Qx2KRRBEBUEbqkoyeMULgh6UCARpxYUUIcZcbQd9QyKkiJCUCPnpJUV8ISgDKeYJxGkizhelBUIhpEG5IC1KCUmhuIyQiMsLxNuK84t5orRAnC7ii9JC8X7KylCifJ44nycQSr5XUJYWf69AVL6obD8VxwD7L+TzX+dmBWalbo18ThEKZG5xo5YEQVQKCltsymRO0P2XOenGWtwRdRt2NLclKs6h+LAX2WkJBXkO+gZ7PLsSBFERUKEqsyrc/25qfGtzmx+cPYjasSDkYVxe3jTnJgPrOBMEYTyoUO8w+8W9+KK8bZ6d1bj2G1dcsDHc34yrs6lpe4IgzAYVqoK1EQGZJUXzGjQnGsCC4IcOeoZLGqJnCmE0qFBlDPO7YqGt+5tbK6IxLHz1SIfF3tysA0EQpoJBaBGzXtyz1NHTKHkCVrn78ITCNeH+BEGYCioUuZoSm1pSuKyRN9E8QJTvpyXeSk8gCMJIUKHI5sjnYxwbEk1lqIPrX5FBBEEYiaYr1E/BD420uC2NrYmm0tfaUYfNPhgXShCEeWi6QkXlZU92akI0m/YWducSogiCMA+NVqgrKXE6HC0PI3OiWK5f+Wfm1OGk5owY2PFF4FNS2wyq41wsENxEbxTCPDRaoY7FhemyOUThnDnxt4trY1JD/P0eREeFN2joTuSAk76hbwoqFMI4NFqhMkuKWpvZEPmQlBi/ffPKccN7tmvu4ONhu3bFgsLCgqKiwtbN6gQ8eXh4/599uoh61YSFBq1b+dOQL9t28Ko3eVz/h/du0ZsLhcJOreofP7wLMmHzP9Yunj5xsCjT2/nsiYOktmluYhVfmEsQhGFo9NgGbIrqYVmXyId1K39OT02eOX9Zw0aNoyLDf50/1cTEbMr3C7bu+t/33w49eeFhXUdngUCweP50IxOTn35dm1+Q53v76uzpI89f87euYwcCV1iQf+PKPwO/HvPH9qP6+gahr55b2dguX7uDyAEXQ+NTCa8JgjAMzVWo1/nZLIrSYcnLioyJCu/QuWdzLx9IN/Xw+nPPaXMLS0hHhL/S0dEFeYI0i8XatOOojq6uqZkFfHRybvjv2WMREa9AocLDRC0Auvce2G9gmccqLCSoXcfuRD40NjATEmE+IfoEQRiE5ipUZH5WsWh0JXkByrJ35wZtbZ3OPfo1cmtm7+BI578OD3FtVOaEgnrfP6cPP3/mFxP9OiM9lc60sLQWFwvV1dUDA4rOfJsQB4UbNJRj2BHEOr0oV1/HkCAIY9BcPxSLpUVRcuyTOH7yrB8Xrn7qd3/c0B5zvhuTGB9L50dGhNZ3FXm7oYo3ddxAqMcNGjru/PWAx0FJE6bMpiiqnlMDcbGQJh5eIHD0VhHhwbB0b+JJ5AabYrMIRRCESWiuQjnqGuqy5WhCgtYMGjJm75FLf+4+mZOdMWlMPz6fD6oUGfHKVRzIC/B7EBryYuFvf/ToPZDDEYUUI8JeOdVvCHVASIeHvqzv0kiyt9cRoeYWVkbGJkRu5PFLDbhcgiBMQnMVylXfKLe0lMiHkFfP38RE0mmv1u1HjJ2SnpZSVJgf9yaqtLTUyUXUySYpSRTdt7YpG8Az+W3C44d36ApgSWlJXGy0i6ubZIdRESH0VnLitTiQZ8JChUKYhUa3NmCxqKupcUQObFq7ePXSuf5+D3Jzsvwe+e7+83e3Jp76BkYZGWlEJDeh0VHhdeu5QPrGlfOwBGPqt8WzDI2MLa3qwMfwEJGb3EWq6VNmVkZhfr7/4/vFxUVEDoTlpJlpaxMEYRgaPdfLhaSYrJKijhZ2pLbx8m7338Pbf21bd/zQLqivderWe+5PKyFmB17wsJCXxw/vsrK27dlnkKGR0b6/Nm3fvBJsqyXLNyfEvzlz4u+C/Fy+gP/A98bcn1ey2Wx6hwYGBpf/PXXj6j/DRn3L4dS+pXP2bbQ5V7erpT1BECah0SPYHY0PP5sQufOLzkTjGfXk6kSnxkNsXQiCMAmNbrE50t71UGxobFFeXR2Dqsr8sXYx1NQqZRYVF+mUR9kqwdHW/nnxeiIfYqIjDu7ZKnPV5xzS9dR48OujPCEMRNNHAf7uuW8+v3RDk7ZEg/kx6L67sfk8ly8IgjAMTR995U+PDm8L89JL5eJ+Vgle5maklRahPCHMBMfYJEPtG6wKfUI0lfXhAb2tce48hKGgQpGJju4CQjZEBhLNY22Ev4uB8XQnuYzogiCfDyqUiIMtuofmZJx8q1njTO6JDUsrLsJ5PREmg/PlVTA18E49A6Nv62qEQbH+9bOkovwDzbsRBGEwqFDv8LXfZSMt7np1D+2tCHuSXlIEliNBEGaDClWZyYG34entZuUwVB3bB22LCnqSmdTB3Ha+awuCIIwHFUoGATmpq0P9c0uLvzCxnOzUBKwqouLEFOWdiY8Iy83kC4VLG/t4GJoRBFEFUKGq5Ehc+IWkqIySYl0tjgGbU1fPsI6OnqGWlhVXT4tFheRkUoS4GJjoa3EDs1KgfAMDEzZFheZm6rC1HHQNYG14XpaRlra1jm6RgBdXkGfM1bbi6vKJICovR5utVVfXgC8UROXnmHJ0LLR16DKmXG0Lrm6RkB+Xn2vI4dpo67FZVGhOphbFctI3YlOs0NwMfTbHVle/WMCPLSgrQ+9HX4tjq6PPEwqj87NzS0vzhXw4nkJ+aURuFhSG4//WqYm3iRVBENUBFerjnE6KfJSeXMzngQGiy2Ibc0RCczc9UZvFctY3dtYzvJgcy2Wx6usbm3O49zOSjbQ4droGoGV+malWXB1zbV2ImAbnZlpzdcy0dXXZ7ICsNBMup462gR6b7Z+VZiwgxhyuvp5eSG5WHR1dE44OSygMzsuy4HIttfXNuNoP0pP0WCxHfWMLrva99CRTUCUdfbaosWUmXUafrfU0K9WUw7HRMdBhsZ5lpxMigOtaR8cAdA20aaCNE0EQFQQVSvls2LDB1tZ2xIgRBEGQd9HonsMMgcfjaWnhhUAQGeCDoXxQoRCkKvDBUD6oUAhSFfhgKJ/S0lJ6JgUEQSqBCqV80IZCkKrAB0P5oEIhSFXgg6F8UKEQpCrwwVA+6IdCkKpAhVI+aEMhSFXgg6F8+Hw+KhSCyAQfDOWDNhSCVAU+GMoHFQpBqgIfDOUDnnJUKASRCT4YygdtKASpCnwwlA8qFIJUBT4YygcVCkGqAh8M5YMtNhGkKlChlA/aUAhSFfhgKB9UKASpCnwwlA8qFIJUBT4YygcVCkGqAh8MJcPn8ymKYrFYBEGQ90CFUjJoQCHIB8BnQ8kIBAIHBweCIIgsUKGUDIfDiYmJIQiCyAIVSslAFQ9cUQRBEFmgg1b5sNlsFCkEkQkqlPIBMwr85QRBkPdAhVI+qFAIUhXoh1I+qFAIUhWoUMoHFQpBqgIVSvmgQiFIVaBCKR9UKASpClQo5YMKhSBVgQqlfFChEKQqUKGUDyoUglQFKpTyQYVCkKpAhVI+qFAIUhWoUMqHzWajQiGITFChlA/aUAhSFahQygcVCkGqghIKhQRRBj179kxNTYUERYmuAk3z5s337dtHEAQRg2MbKA0QI3oOBXoJ3ihDQ8OxY8cSBEHKQYVSGmPGjLGxsZHOcXFx6dSpE0EQpBxUKKXh7u7esmVLyUcOhzN06FCCIIgUqFDKZNy4cfb29nTa0dGxd+/eBEEQKVChlImTk1ObNm2IuEnUsGHDCIIg76IRsbyzyW9Cc9ILS0reX8WmKP57ZwB815BV6cyAP5u8lynOJ++fRpl7gEwB5EB5qezioqJngYGwc29vb/or6H2KCoqXkm+HvdFL6UOqlFlVTsWXindadiTlBWCl5KP4yCu+RI/DbWJq0c+yLkEQZaDmCnUyJfpQVDA8hRzCKuTLaHMk/axKZcLjTL2nUJBFCch7CiV+yAWVxQgUoXJhyBRUFihRZimPB/uGYJ70d1WSMjqn0tGW6RERaYr0z5EuBkaygLz/pe8UIO98FCtUeUldNrtEKIQjW9DIy8fEmiCIYlFnhYoqzP0+8G7fOvVaGFkQ5DO4n5l0KyVub4uudbh6BEEUiNoq1JvCwumBNxc19CJIbQA15DVhT/71+ZJNEERxqK2nfFnIozq6BgSpJbiEmGnrzn55nyCIAlFbhcooKXQ1NCZI7VFX1yCxII8giAJR257DxUIBtzw0htQKHIpVhD2cEcWitgolEPCFAuwUXZsI+Hw+4RMEUSA4+gqCIMwFFQpBEOaCCoVUG4pQ6NpDFIsaKxRF8GmqXYQExztEFIz6KlRZbzQEQVQY9VUoUXc1fOEjiGqDfiikBrDQD4UoFlQopAYI0CxFFAsqFIIgzEWdY3kUvu8RRMVR51iegIUSVZuwKYqNfihEsah3LK9mj1NxYcG1k0cCH955+yaquLjIxsHJvYV37xHfmFnaEGYwc1CX9KTEmau3tOzUgygcvlDIRz8UoljQD1XBpp9nBD1+wOFw6zfxyM/NiY8KfxP+KvDh3ZV/n+Vq6xAG0KCJh2UdO0MTU4IgmgEqVBkJMZEgT5BYuP1ggyaekMhITZ4/vM/bN9H/3bjUoe9XhAF8v/wPgiCahNoqFFXDXi+5WRl0wqpO2QR2ZpbWS3cf19bVM7MS1fJWTB8T+uzJqB9+gnoffATb6ve5U4zMzLdfFOna9L5tczLSZ6zc5HvxTGTwC4FA2LpLz3HzlrDF8yMIBIJz+7c/vXsjOf5N/caeXQYMbd1NNDVefFTET6O+1NUzmLfxr0ObVukaGJWWFEe8COg7auKI7+fRh/HX8p/vXToL5UGepGt5UPLqycOPb1wCbTW1tG7q3bZtr/60tgLhLwKOb9+Q9CaqqKgQfpFPz34Dxk2lV03v7ZOTlTln3fa7F848f3T3gG8QqS7ohEIUjdqOsVnTPmRWdmUTLi2bPOLCod0hz56AW8reuQHUqtjsj4/NDXVDWB76Y5VlHfumrdoV5Gbf+ufExcN76bUH1i09s2cbaEr3r0enJMRuWzT7xuljkq2Kiwu3L5tfkJdb18W1VZeekBP4yLfsVwiFgQ9vQ4JWNGkOfIfK/QAAEABJREFUb159fNv64sLCroOGQ+3vxumjG36cWpAvGgMT5GnFtNHhz/3tXRr5dO+XnBh/cuem/+3YQG+oxdGG5ZGta+E3unq0INUHg6OIwlHnWl6N3vhgMY2ft/TwplUpiXFgfRDRk8zpOmhE5wFDQKc+vr04yNXYq9U3Py6GhKm55aVj+30vnek/bnJhfu7di2cg87vfNtRzde8zcvwP/Tue3LWpy6BhFFv0hhDw+W5feE9ZtBrSWWmphzetToiKSE95a25VJ+JlYG5Wlo6enmebzpW+MPzFM1hO+mWla9MvINGsVXs+XyAQj4F5atdmsNra9Pxy+tL1olWt2239ZdblYwf6jZ6kb2hMfymHq73l3G3YM6k+KFCIwsE5hyvo+tXw1UcvDBg3xa25N3zklZZePXFw4ZgBcZHh1dwDyERZwkeUSIqNAXkKfxHI5/FA70CeINPIxMzG3jE/JxtkSLJh98Gj6ISJhWVDsV0TeP8uLJ/dFxlQLTv1hM0rfRfsBJa7Vvx8bNv666eOerbr3L7PAANjk6KCgpAAP1jVumuZ2dWiQzcWmw0/JyTgsWTzdr0H1EyeiEjysTM2omDQU/4ONvZ1h0ydDYnS0pJXT/47+MeK5PhYcCHNWLGpOpvrGhjSCVAKOgF1t5xMkYcLBGK0TyPpwskJsXUblOVY2NpK8lt17RX23D/w0V1QzID7dBWvz/vfNWLGvLycLBCji0dEdckTOzY2bdX2u+Ub83Oz6PqtsbklXVJLS0vfyDg3MwPMMcnmFjZ2pKYIxRqFIAoE25SX8Tr4eWxEqJWdQ5OWbYjYQ+TRpkOn10PAfZORkkzKZ0UvLZ9avTBfxqwnhXm5dKIgJ4dO6Bua6BsZiXaorTPv953She2cXMADRadZrApXl3fnXgc3rgx++gjqm2Bn6RkaN/Fu8/53Wdk6/PLnwejQl7GvwwLu3fb3veF3+2qb+7fAZU7PRVyQV3YMPB6vKE90tMZmFTObstk1N58pCnsOIwpGfT3l4vnCq89/Ny7tW7tk75rF4ACic9KTE4OePISEpa0ouqerL5p9LyYsmF7r73vz/Z343b5G2y8BD0S2j51zA6hJ1XdrCunS4iJdQ0N3r9ZuLVq9iQjNykjT1deXeSRQ0XNv0aqkqAgcUvCxTY8+77vqS4oKr/zv731rFjs1atKx3+DZa7eBwQX5KYnxEHx0a9Ea0o9vXqELP71zDUxCOBK35i3J5yAUYs9hRMGob2uDGr7sew8f9+Dqv6mJ8TMHdDa1sNYzMqL9RPBg9x01ARKebTsG3LsFdgqE/8GAyk5PfX8nYc+fQhDNrl59COTBx66DhhFRbcuifZ9B9y6dXT1jQrueX6a8jQ98cMexgZtP975VHQw4nl75P4avI1VU8cAi8714Foy+zPRUR1e3vOzMR9cuQT7tQRs6ddbyqU/unD+ZmZZsZGz26MZFyBw86QddfUOCICoFesrLMLe2Xb7vZJ+RE2wdneHBBnkC303LTj2W7DoOagIFOn05pPOAoeByDgt8al+/wTfzlxCRd+md+eOGTJkN1UCQJyjWc9hYif973NxF3QaPhMS1U4fDnj2F9LyNuz4w5rekbYGRmXlDDxkTu8O2s9ZshcMDsTu3b/ud86eatPRZtPOwU8PGsNalsceinUecGjV+/tD33uVzxqYW3/6ykm7GhSCqBaWuI0/3fHCul7Wjj1kdohCU22NOMVx8G/00O+VymwEEQRQFxvKQ6iJENzmicNRZobDxTu1C1dy7hyCfiXq3KVdcBXbz2VtE7RHiZFSIosFaHoIgzEWtxzZAEETFUVuFEkCdBL0mCKLiqLENhTN61jYUWqaIokE/FFJthOL/EESBoEIhCMJc1Hg2KiLEMSERRMVR3355NZ+NCvkoIPvTpk3z9fUlCKIQsOcwUgNA9idMmJAnHm3q8uXLq1atio6OJggiN9APhdSMli3LBpnq1KlTYWFheHi4k5PTqVOn8vPzBw0aZCQerg9Bagu1taG4FJvNwlpebcJmc3TYFcOl6+rqfvXVVz17iian8fHxycnJCQwMhPTx48dv3rwpEAgIgnw2aqtQ2mytt8XFBKk90ksL9bU4MlfZ2dnNmDGjQ4cOkHZ2dr527VpISAikz5w5ExYWRhDkU1FbhWpsbBaZm0mQ2iO+ILeVmc1Hi3l7e69du7ZxY9FYegUFBb/99htUAMGkun79OtQKCYLUBLUdwQ74JuCGFoczya4hQT6bbW9e6lPsvzw7f8K29D22cOHC0NDQs2fPZmZmJiUlubm5EQT5GOqsUMBXfpe1KMrVyNxRx4An4H2gJAXPUXmXDlFDBdmNp6lK01qKTt+7vf9YRCh4v2sI3e5Bqn2W7GKikkLpQZio96bRFK8Wyjqyd45Z+udUBUvUe5F8rAw7tig3NDdDm8U66tWT1AYZGRkzZ860sLD4448/UlJSwJ9laIgDqCOyUXOFApaE+b3MTi/l84sE/GpvJCS13wGtevusRilRW9SPFaJqaYZgtkDIoUhbG8cFLl+QWiUvL8/AwADcVdOnTx87duz48ePT0tJAtgiCSKH+ClVT+vXrN2fOnC5dupBaBR7FadOmfSOm0iowJaysrEaNGkWUzYIFCyRhOHqiB/r2gOWzZ8+I3EhISABf+8WLFzdu3Ah+q7Zt24LHCmwrgmg82GKzAjrqtGfPnlqXp+DgYFC93NzcmJiY99caGxszpBkReLgbNGhARDOMsigxLDEgH0Se0Pvv27cvxP5sxdMvb926ddy4cW/evCGIZoM2lAjwhgwdOvTw4cP29vaktgHhA3lKTk7m8/ktW7bctWsXYTAvX7786aefwJMtyWGz2Y8fPyYKB2Qd/FN169b97rvvTExMwNGuX8UcqIgagzaUiLi4uAsXLshDnl69evXjjz+CPBFxvQmcxO+XgdgWxOMJM2jSpEn//v21tbUlOXDYjx49IgqncePGIE+QWL9+fYcOHYrFrdvAabVjxw6CaAwarVBPnz4FlwckWrRoAV5bUtsEBQX9/PPPb9+WTbMO1aWSkpL3RWr79u3Xrl0jjGHy5Mnu7u60cQ3OoIMHDx49ehScaHQjTMWjp6fXs2dPMzMzSM+ePZtWz+zsbDi3t2/fJohao6EKVVRURMQGzoMHD4jcmD9/PviApXN4PJ50BYrGWAxhEuCuBn8QuMzv3bvn6uoKXqEJEyasWrUKRCExMZEoD3CTwZFAAjx34C6kRRPq0WBYodNKLdFEP9SJEydiY2Oh8kXkD7z8wclFO57hI1hq8PDTvUMYzunTp0GY7ty5I515/fp1yGzTps0PP/wApg1hBhD4O3bsWEFBwffffw92MbwDunbtiqFA9UCzFAp8GaWlpVCrAuuGKAp4t4OnHGol6enp4HUGj+9XX30lXQDy4XFizgP/UU6dOrVly5aRI0dOnTqVMAwwWnfv3g0OLDC1Hj58CGfV09OTICqLBikU3Ljt2rWDOgvIBFEg8L1QXZoyZUpVBaDqBBWW7t27E5Viz549e/fuBWNqxIgRhJH4+flB5HTgwIH9+vWDtLOzM7YIVTk0xQ918eJFkAk3NzcFyxMAXvAePXp8oAD4gFUxjj5p0iRfX19wS/Xu3fvy5cuEeXh7e4OMwuFBOiYmZvTo0XSTN/A/EkRFUH8bCt7zEydOzM3NVUrnr4iIiMWLF4OXhKgvqampUOmDXwr2FLioCIOhm6ovWbLk5s2bV69ehRcDeAmtrKwIwlTUXKHAdQr2S//+/YmSAJ+Xjo4OHX6qirS0NPCgQzGiyrx+/Rp0qqSkBHTK3d2dMBvwSEL4gsPh9OnTx97eHiqDEN5V9UuglqitQkEQqlOnTjk5OcrtUAJOkD///PPDvUZmzpw5bNgwhlsf1QRCaaBTtra2M2bMkHdfmdoiOjrayckpOTl50KBBUBOcPn066Jd0m1VEiaihHwqizvC029iIxlpTrjwFBwcbGxt/9EFVUT+UTLy8vA4ePNitWzd4zlevXs2ctvIfAOQJltbW1vBW8/HxgfSLFy+GDh3KqGa0Gota2VBgqIPRRFEU6AKXyyXKhjmDFiiF06dPgz01fPjwadOmEVUjKioKrCoQrKNHj4JhCKHYhg1xKEQloD42VGhoKLy6waFjaWnJBHki4vaN1WlDAM7aYnUcUn3w4MF3796Fa9G6dWt4zolK4ezsTNtTI0aMgKp6ZqZoROkdO3Zs3LiRTiOKQR0UKisrC5ZJSUn3799nTrvHwMBAcMdUJ070888/g7wSNQUCqXBd4Or06tXr0qVLRNUAk7xDhw4gspCGqh94D+Li4oh4fJgTJ07weDyCyBOVV6gzZ85A8JiIp28jTOKjzaAkWFhYqHcUSUtLa86cOYcPH/7vv/+g0vfw4UOimpibm48cObJZs2aQ7tixY0xMDD1qBURsVfdHMRwV9kNlZ2eDv2nfvn0fjuUrC6hygiOGaV2ClU5kZCQ4p8Bj+MMPP9DzwagBZ8+eBS87VABLSkrOnTsH4kWPw4d8PqqqUBDChxBMnz59CCPx8/P7+++/4SCrUxjewxDO43A4RGPw9/cHnYIaE+iUqjRKqA58Pn/Tpk3gZYdLn5iYCIm2bdtSFM4s++moXi0PboKgoCDwNzFWnoi4ilf9fnYzZ87UtJFDWrRoAQres2fP7777TlUaJVQHNps9d+5c+s2kra196tQpeggNkCqZA0AjH0XFFGrt2rWFhYUQ9x0/fjxhMNV3QgHgTdfM1sxdunSBOhFczb59+4Irh6gX4LQCe2rDhg1E3NsGpGr//v1E3EMQ3rIEqR6qpFDr16+HGLCBgQFDGhNUBYSumjdvXv2oItR35DEAsarw1VdfgRMHNFoVGyVUE/C4gT01ePBgIm7HC1U/2rNOO9qRD6AaCnXgwAFYgs9iyJAhhPHUyIAC3r59iy9VCHc8ePAAnlgVbZRQHegeDmAwQkyTnlMHFLlfv350Hb+0tJQg76ECnvJ27dpt3rwZPBdERQBbAMwoCLFXszzco3v27KG76SDp6elgVIaGhsILiR5FXr0BUQbvlYWFxZgxY8DuhophVaODCgQCVXTYgT/ucyo9zFWonJyc8PBwLy8volLcunXrypUr69atq/4mkyZNAv8auC0IUo5aNkr4KBDidHNzA5368ssvO3bsCK4reDwloUAej0c3TlYtQHA/p9spQxUqPj5+7NixJ06cULlBEX/66aduYgjy2cATu3XrVogkgE5plKsuNTX1yZMnEK1OSEhYtmzZwIEDIY0KxQhAm+BeDAsLU8WOmmCHQxXPz8+vRlslJiZCFY/FwrkLZQNmKdhT3t7eoFPymDSM4QQEBERFRX399deQMDU1hZCCajWd+0yFYtZTcf369VmzZkFCRfuR19RHTjN69Oi8vDyCVAHdKKFRo0bgsKtmI1h1AuLCIE+QgDMA2kT3BCwuLi4oKIA3IlF3mKJQ9LSXYNBBUJaoLKCwn9mI3GEAABAASURBVKBQderU0agG5Z8G3SgBfDStWrU6cuQI0TzA3wwGFO1HhxsGHpalS5f2kgU9S+PcuXN//fVXetszZ85APlSZK+0TfKAHDx6k09Llidi037Fjx/Tp0+HFAF78RYsWSVcOoO4Jqyrt7eXLl/Atz58/J7VHdeNNcuWPP/4AO3Dy5Mmf8HgzB3itPX78mG6hVyM083n7NMaPHw8OSqj09ezZc8aMGfDwEI0EfALwyEAQ0NbW9rvvvoN7D/QLZKukpAQyZUZdIP/ixYt9+/Z1dnb+6P7B4QV+evAAfvPNNxCM4/P5YMYuXrwYVEzBkxIpWaEgYAdn1traeuTIkUTFuXr16qcpLO16I0j1gCdt9uzZoFNgEcD7H5xT7dq1I5oKWFXSDXEg+llYWEib5EIxklVgqpuYmOzcubM6geZHjx5lZ2fv3btX0vThiy++ADMK8hWsUMqs5S1fvhwsSVBoNZAn8qlOKCKuv2iCQ6F2ATMB6jhr1qwBtwBY30FBQQQRCxZ40+kp10Ce6FagkAAjCMyradOmvXjxwtfX96P7AXmqJHAURa1YsQLMKKJYlKZQYDR6eHiA8089en7n5uYGBwfT45zVFDs7OwzkfRpQYdm0adPUqVM3btw4b948emw5hAZuKklTSZAqcLHXr18fasd79uwBtfrwtk2aNIG3Jjib/P39lTsArKJreXCmVq9eDUrcv39/dXosP9mAIuLRhQjyGUC0a//+/bdv34Yan5eXFyyVMjciYwEjAGwr2hSA2vHdu3fB9QkevQ9Y7qBQCxcuhMjpL7/8QsSTTfj4+EBIUbq3aVRUFPjFiZxRtELB3QNxASIWeKJGgA8SHJbkk4iIiKB7aSGfQ2cxIPfw8hs8ePD3339PNID3ZQLECCooVZWnhwkFhYKzBDoOtb+qBjLu0KED+J7Ahnr16lVkZOTx48ePHj06Z84cyZsYnPQzZ86U3iQmJgbCf6RWUbRC1foPYAIQZHVzc/u0noNwi4A/Et5U+NqvFQaJWbly5fbt298Ph6sf78vER9/9cH4uX768e/fun376CTxWUB6MKZn9luGe7CSGiBsfgNcP7lVQLnqwIFiCo0a6PO3/ql0UZ8iEhYVB5JKoHfC7oN4KThDyScBFhQt/5swZtZ+eXmGEh4eDT1AT5ImUy4Q0TZs2/fAmEOn79ttv79y5A2eJJQYUqqCgQLpMenp6WlqadA5IIdTywIOu4BFjFKdQLi4ur1+/JuoFyNNvv/32+Q2axo0bB8Y2VPcI8tmAq+XQoUMEqZo2bdqAlkGFhnalg0hVmmN50aJF8N6tZFjR2mRiYkIUiOIUCoyF+/fvEzWituSJBt5sEEBQxa6hjGLChAm7du2SR3VDzYAAKHiXwI1FxApVacDFiRMnhoSEwO0NfqjnYkDOIBwBZpSCJwdRqB8KwpZaWlrqcfeAPC1fvrx2m4MfO3YM4iytWrXSzEGBP5/ff/8d/Lj0bFHIh4HwXN++fS9cuEDEPd4rtT8ApyqcTFgL7jwwncDAd3V1BVED/zpRLAod2wBebrCcPHkyUXFoeTp8+DCRAwlivL29CVITLl269Pjx42XLlhE1RX6jr0C4BhxMZmZmRA6o0tgG7u7udA9hlSY0NFR+8kTEDTgPHDgAdwxBqg3Eufft26fG8iRX6AZThJGo8IyeSgHkacWKFfKTJwlgpllYWODAm9UEQuBXrlyp/uwVqohmjmCn6GaTEMUkKovC5ImIR8hKSUm5du0aQT7GlClT/vjjD/WWJ7kCZkpRURFhJIpWqJkzZ8JzTlQQCG0oTJ5o3NzcwHFeWFhIkKrZsmVL27ZtVWiiDQYCCsXYORoUrVBeXl6JiYlE1QB5WrVqlSLliWblypUQAFW/dmS1xY0bN+B2Gjt2LEE+A/BDVTXBjNJBP9THoeVJiY0Anzx5EhUVNWzYMIJIAdoE8e/z588TzUAz/VCK7pcHdZacnBxra2uiIrx69Wr16tXKbaPcsmXLO3fuEORdwHQ6ffo00RhYLJac2nOXlJSAx1NOA5Z+ZvtHRdtQqampcGNdvnyZqAIgT2vWrJEM5Kx0Hj9+3KpVK4KIB8kYPnx4mzZtCPLZgBOqb9++zHwLKtoPZWlpWbduXZWYOpVp8kTELesU7wtjIDt37mzWrBnKU23B5XIZ60NAP5RsgoOD165dyyh5ojlx4sTQoUOJBuPr6/vPP/98wowViCqihGHkwMHJ8FZRIE/r1q1joDwBtDxp7LCcaWlpELVAeapdwDY/duwYYSRKUKiHDx/u3r2bMBVanv7++2/CYMzNzY8fP040D3BiMvPNoeps3ryZMBIlKJSHhwdjJ7BUCXki4k4etra2RMOYO3fuggULrKysCFKrQLhtxIgRhJGgH6oCkKf169cfOHCAqA5bt26dMWMG0QD27dtXXFw8bdo0gmgSypnOwN/f/6Pz4SgYVZQnIh7H5+TJk0Td+e+//wICAlCe5MeRI0eYaawoR6H2798PNxxhDC9fvlRFeSLi4VzVfsC2nJychQsXbtu2jSBy488//5Q5mYLSUY5C9ezZU7nTBEoD8vT777+rojzRNGzYkIi7ZBM1BccdVwCjR49m5ty6mu6HUnV5kvD06dPU1NTevXsT9QKsp06dOn3ybKmIqqMcGwrsdvAsEGWjNvJExINGgE9KzUbmPHz4METuUJ4UwIkTJ7CWV4Guru7s2bOJUgkKClIbeaKBh1lfX3/kyJFELYBwyr1792bNmkUQ+bNr165KU+YxBPbSpUuJwmGz2fn5+Q0aNFDWuIggTxs3blQneaJhsVhNmzYNDQ2tW7cuUWWKiopAajW26bziAb+wp6cnAxsqaqIfipYniCcSNQXUPzY21s3NjagsQ4cOXbNmjbOzM0E0G+XU8oh4zO+wsDCicF68eKHe8gRAXa9+/fq9evUiqsmSJUvGjRuH8qRIzpw5w8xantIUKiEhYd++fXRaYa5QkKdNmzaptzzRcLlccDMr5R3wmYDL1sDAoG/fvgRRIPAw5ubmEuah6DE2geHDh0OlNzMzE84IPQC+qakpkT+0PElkUe2xsLCAE3v9+vXu3bvTOR07dqxTpw6TuxxDdPXSpUvq5x9kPl999RUzJ8tRtA0F4bOYmJi4uLi8vDyqHGNjY3l3gnn+/LlGyRMNRCS6dOnSunVrIjZUwT+VmprKqNb80vD5/IkTJ6I8KYUJEyYYGhoS5qFohfrxxx8hZAD3oiRHIBA0btwYaiVEboA8bdmyRdPkiQZE6sGDB+CTysjIgI9gut6+fZswEvA94cgqyuL8+fM5OTmEeSjBD7Vq1SpHR0fJR3Dr+vj4ELlBy9PevXuJpgI+nbS0NDrNYrHu3btHmAfcFVDRoHvwIIrn0KFDzBxXUgkKZWZmNnPmTEtLS/qjlZVV8+bNiXxAefryyy8l8kTDkAb90pw7dw5MaVAogiiJAQMGGBkZEeahnFhe586de/fura2tDfdl3bp15TQ5FcjT1q1bNVmeiDgKAd5xcPbBqaZzsrOzwX1OGAMEHE+ePPnrr78SRHmMHj3a3NycMI9qtdh8mJWcXyryZFOE0KUlCTY4OMtyYEeivtGUaJeibtKiAkKprxH3nKZ7T9PZ+/bsjYqJ6te3X2uo5QnFG5ZtVlGeBnRUIH3Q4j2IluXHwSJCAaGkC8TEvLl+/dqkb7+V/iE6HK32JnWI6vA8NyOtuEBQfo1YLFKuM/CTKQEllJxhyYWgzw5LKF4r5k1MTOTr11ExMfm5eeCH4vF5piYmM2fN0uJwys8fVXGpJHurlEcRSlh2mSWwhOJLUGljUb5oY8F7XeWlj0rC0sWLFy9dwmaxJSuo8t1Kvlk0b/e7e6s4OqHoyN472IqP8KVtrOwZOqMuY4AQaqtWrRgoUh9RqGkv7sbmi/xnpfSTIbnyIjmSqA0l/kd0p0ivkmhWRSHp7d77JCnz/qdKClX5FhZPPF957Ih3v4lGm8XmE6EZV+dwi+6E2ayIeOKXnsQXUnxRLKHsGkn/7ndP1ztnW/TxXSmRfmTFb453TlelXREZJ1hGGXGujJNM3he8ckSq+u6a8rtH9n7K9lbpt3xsn5XQEhURGnK4mz062nBRqd4BatbgmoSbIT4+HsxtiKvAo2RgYMCcJikfag81KfBWHo/3jVNjBy4TG0p8AAirVzURM5iCRxLC+j+6cN5HLjOs1gq73rzyy0jtZePc3IiJhrcqciopasLT63+37GXJkWPUWOUoKipKSUmh05LE2LFjCWOo0g81xv86n6JmOTdTOXki4vhgVavg9hxv17C9Td0B/10kjGRhqN/11LiFDZqjPNUiX9s4/9LIe9zTKwSRwsvLS+KgpKlXrx6jJmSUrVDX0+KySounOKhw19MP097YSputtTjkMWEezzOThzk0IEhtA861OroGk57dIkg5EydOtLOzk3yE6l779u1tbGwIY5CtUJeTYg25am4M2+sahuczbry3E0mv2RTbnqN6dqtK4GFilVxcRJByHB0d27VrJ/no4ODw9ddfEyYhW6HySospAVFvtFlUYSmz5psBEvLzRGFJRD6YsLR4Aj5BpBgzZozEjIJwnrRJxQRkK1QBj1ei7s+JgC8oYd7YWDyeoESACiUv4HrzhKhQ72Bra9uhQwdIgDYxygNFo4SxDRiCULJAEBXhUVbyf+lJr3IzCvk8vkCQV1rqZmQ6yqHhruiXsQV57kYmIx0a7YwKii/Mb2xkOsKhIZ12NzIdWZ52MzYdZV9W3tXQZGzdRvvfhLz2cTW1+sY1h7cmMzrxbXAzY/Oh9g3+jAp6W5hPp7dHvUgsLPAwthhi77L5dWBqcVELU8tBtvW3vn6eXFzYzMR8qF2DHZFBCUVl37s67CmXxbbTM4BNWplaO+l9emt12e2hRj+5VkwEc+t7EvXlbELky9yMi22+JExifVjAzbT4pW7eBJEDYXlZh+NCr7cdSFSKgJz0dWFPM0uKuCxWqVAoacFLBKKaDosl6jDAIhSfEoATU0CELIG4fSKLxRcK2KJWsoRFUVCWJaTofNgDS0gElJBFlaeJgMVig+qxxe3LWCwWfA8lpMr2T2/LEjVaEwiEovaJ9LawlaCsfPl+RGn66EBbtLW0YJ9aFKudpe0Cl0/p3KbRNpQAbSgNQ9yqlYmzwlVFYE7G7+FP00uK+GJVKq7kAQCRoP8ViwJbFK4UtU8V/y/6mWyK/lRelirLF0kNXaoiLdqWLd4Pq6xzyPvb0t9FScqwyv6rvE8aiAyWiEcx4Qv5N1Pi76Um1tMz2ubRgdQEpY2xyQCYKE+ihvmq9ASpGOJeVSrzWlr46tGCoHspxYV8NZhMQCgsEfAj8rP6Pfw3ODej+tvJVihK3O9B3WEx8HUqssLRsJMfqqP+4wNuPstKVbN7Aep9EIKbH/RgW1RQNTep0obSgMdEqEKvU6R2UJELPtb/RnJxgTqYTrIoFQr+TYp+lJlUncK+dSr3AAAQAElEQVSyFUp0YjSgrsHShB+JSKMKfqhB/12CABlPrRudgDG1KvTJH5HPP1pSk/1Q6CnXOCjGX/E5QfeLBGA8qX+bOPD630yJfZ7zkYE9NVehICTBpRj38yGYooWucrkhpCgmV+33xYW+ys3ga0ynghKBYEHQ/Q+Xkf2IsiFqqO4OW4FAyMB282D98gm2KZcXDNf+E7FhAg0LlMCvnR545wMFZCsUX9SaS+3f5BQD/VBwg1YeTRKpPZj89K95/YzSPPMZrsjrvKwPFND60KY1hMfj3bt4JvChb0z4q7zsTGt7x4bNWgyZNltP34DUkMCHd3+fO8XIzHz7xQdEXgjVww+1+ecfnty5Jp1jaGpmV69+t8GjWndl9MTod/49tWfVr3UbNFp18BxRCGL5Z6gK3E+NZ3jwLu7MpfBtB7rcOE6xatM9wqKoeS/vr2/STubaqhWqhtcxIzUZNCU2IlSSA2n4Cwl4vHD7QSMTM8IwKPWK5YGa2zo6Q4LP40WFBIU+ewJ/XO0dzdt1Jkg5FFP9UHtjQ0oZH7zLjYg2dHWqXXki4gG+X+ZU2Yaz1nq9HN2yBvSIo60zft5it+atLOvYwcc/F8+Nj359YN2yH1ZtJgxDzXq9NGvVburitXQ6PzcbDKtX/o+vnzqMCqUS3E9PZP69mBseZdTIhcgBsBUCc1M9DS3fXyVbodgUxaqJN6S0pPipr2jowmHTZnfoWzbrGVjvc37f+e/BXY29WtM56SlvD25YGRManJ2ZZmZl08iz5ZjZP+vql83FfGzb+odXL2SmJbs09mjfZ5D0/gUCwbn925/evZEc/6Z+Y88uA4a27tabXjW9t09OVuacddvvXjjz/NHdA77VbatKESY2jaGoWogv6hsae7btBAoVFxlO5+RlZx3Zsjbs+dPcrKwmLX36j5vs1KgJvSo65OXFo3sjXgTm5+XAmW/aul3PIWO0OBzRqtCXp3dvjQ4LZrO0mnfoPHjSD4YmpvRWQX4PLh7eGxUazNZiQ11+0MTpjg1EI7JeO3Xk4Iblzdt3gb9Tu7Z0HTR80ITp4c/9T+3ZlhAdUVxYUK9h425fjZRcPiKeFTnyVdA/B3aEBDyxcag7csYCt+ZlHacfXb94/fRReNVZ29X17tqz3+hvoTDkb/lllt+tK8O/n5eZknzn35Pb/vWV3EUqSjFPvgZUUVpGzOEzaf/5C0pKTBo3cv1+vI61BeS/+d/5uNMXmy2f/3L5poL4tyZNGzmPH2bWvCmsEvL5kfuOp/g+Ls3OsWjjVX/iiNzIN7ZfymUKEvC/+aWnyFSoqjzlNfPRxL4O45UUQ6Jlp3d+gLWdw6Sfl/t07wvpvJzsZZNG+Pve0NbV7TJgWGlxie/FM+tmT6ZL3jh97OKRvSBPcO9qcbnH//xdej8H1i09s2cb6GD3r0enJMRuWzQbytOrtDjasDyydW3IsyeuHi1ItRESJnbREtZSSxheSSksudo6ROwfXD511L1LZ20cHH169AFxWTZlVNSrl0T8zlg2ZYTfrav1GzfrNXTM27iYY1vXgWDBqrdvoldMHwMOQdAal6aeN88cXzJpaEF+HqxKiInc8OPUl08etu8zwLXpF0/vXl8/Z3KJeOxKLY7onRcfFQEOJlNLSwubOnDdN/8y89XTR1AJbezVBuqecPkC7t+WHCqfz18/59vkuDdFBXnRocHbl8zjlYoO/uG1C2CDx0aE9Ph6FIfLPblzE3ykN6EF9PY/J26ePebo6s5isUn1YaStks2T48ifOaGvH4+fXZya3vz3xV+s/bUgMenF0t+F4kpl/pt4QSlPLFLzOpzdQ8HbYs9RequInYfiTl10GjPY59AW/bp2AbOXCnk8w/r1iBwoEQhe5KTJXFU7NlRGSlkDdnNr26rKXDt5KCM1ydrBceWhcxwOt/+4KTMHdY4IegY3K9REbv3zPyjTY8iYsXN+gcTamRPhQaI3LMzPvXvxDCS++21DPVf3PiPH/9C/48ldm7oMGiaaSIctElkOV3vLuds6ejh4ruiBj3z14uY50WxC3l1EnvKXfg9AU8B9Pnvddjjznj4dN86ffv7gX7PWbI0MfgFy0LRVW7oa3rH/kGcPbsNJhvSdC6eLCwtbdOg2ccFvRFSLX3vp2H6IhPQcOjY04IlLE09HV7cxsxaWFBVO6eWTlZYKuwLbhxaLlIS4yYvWdOgjGuTk7L7t2elpYJ0t3PY3WIhHt667dHQfvI0k1U8wkaYtWde2V386PAJvqbiocKeGja/+7yCsHfztD72HfwOJReMHg90U9zrMwaUh3ZU/PTlp9aHzdRydSPURMrFjdmxhPk8gR+EMWb/DwNmx6dI5LLGyu8+b9njSj3lRbwxdnAriEjlGhm7zp7O0RFJg7N4g6cY9SJTm5MWeulB//PA6PTrCx3ojB2X4B+XHxBnUdyTyIamwQGa+bIWqqQ1VHZ4/9IVli3ZdOOLpgEwsLF2bNQ8J8At+8gjqHbSL/Yt2nejCHb/8WqJQ4S8CwfsLr036yQGnu429I7i3EqIi4Galy7TrPeAT5Em1BuL4MPcv/wN/0jkgST2GjIIEVO5gWa+BG33mQVxgGfLMD5Z1HOrBEi4B2DU2dZ0823Ts8fVoevPwQNFWzu5llUFn92awDH32FBSq61fD4Y/O5+roGhmbgi2WnVHROFhXz6Btz7KBt4KfPoIl1PTpUPrIGfPhT/o4TSysQJ4gAd+uZ2BUkJeTm5UJFhn4+yGzvvh7RYnGzcDCgsOWXHS4bWomT+LpZhnYKCqtuJCq1tS6n0Lu65icsMhmK+bT8gRwLURhq+K0TJAtcC05TxxByxNQlJymLV6b9jhAWMqz6dZesh+umYl+PQctPblMOMgX8tlVXJfa8ZSDU4lOwJ1qbiV7Ut/c7ExYGptVzLBkZCo6F3k5mUUFZfKpW94uQV9qCvmcTJGfH171o30aSe8wOSFWcrNa2NR4cGVmipMWRbE/6S0vieUlxcWARdOgWfMff98BDikiOoGiMw+KL30C83OywTiFEzh61s/gMPrvxmXIPLdvO+QMnjTDq2O3nCzRaYe6FfxJtkpJjCdi19XBTasiXgRUdTBmNnVohxGQmZoMSz2DKv1E9G1Q9vO5oqdIwOfD3UI/sr9NGSldODkhTpK2sKnSYK8KIWFiLM9QS4v9sXlJPxnQIFi++HVdpXyOoQE4nnj5BYYNKlQeDCvaF54TEqFjY6Vray1ZVZiYJF2ydmELKV0tjsxVtVPLc6jvytXRKSkqenL7Wq9h4yT5SbEx6+Z8C25vqNOB7QPGf35ujmRtbrZoqhVjUwuJ+ZOfXTb5Cnh2JcVotYIo4bzfd0p/qZ1TRViBzf4E/zITHRL8Tx1xQRLLgyrekolDI4OfZ6am0Aqlbyg6gQ2aeA6ZMkt6Ey2uyIUH16td74GRL5+Hv/D3vXgOqlFQLwOF0hNvBVXp1l0qvNpcXdErdPuyeeClgjrdl2Mna7G1wM0Eeie9Z4k8AXqGIm2Svu7VQd/AmE6Mn7+sjkNFzQIMLkm6Zu4nBmOsoy8KkMhnAHVeYSFbV8dz3a+V8g1cHNMeiMxkqOvROYLS0ryoWNt+3SBdnJqhY1VhTPAKCrODwy1af8ogmdVCNHao7J9fO55y8Mh2GzQCEv/b8Qf4v0vFXvOUxLgN86eBKr0ODoRb1qONqELr73uT9oOmJyeGBT6BBESdYHPaGnrxWNRJB16e9Cudpr6bKLJQWlyka2jo7tXarUWrNxGhWRlpulVP21kdmPk6BX36TI8EVIsgnApmyI6l88FHTsT1I1imJb11cmsKJ9DGsV5sZBhfwIdKX8TLwEObVr1+GejRpsOQqbNX/C3y96XEx8LSxd0DlmCOwSbwB+GLt7ExUNeGuCrIE6zqM3ICyKKxhSUtT1W5+J1cG8My6MkD2ia6euIgmHLLvh3+4V8BLy37+q6QAD8XfQBg06UmJWh/Xi1D7IZinPVspcXhya0DlrapiaCk1Njd1eyLJvSfjoUZxOzY2tq5UW907etwDMueo7zoOIjflQkWixKIgy00b6/cEbnJXeRlQwE6bNnWkuxc0UTuNex7MWTa7IQ3keBs2rViIfxJ8qHSN27uIkj0HDb2zvmT4LJdNOHrRh5ej29fAe8SxIncW7SCtR36DjqyeQ3cvmDepyclJsW/kezB2NwCrDAIRa2eMaFdzy9T3sYHPrgDsW06RIi8z/Dvfnx698abiJB///5r0MTvWrTvam1fNzk+dtGEwRBsfXbvNnjxwJ3U1LsthDjAIf345mU4mdq6eq/8RVOcunuJrkiPIaPB3R5w7xZE9Bo09bx17gQo0aw1W8AbaFevPlzHf/bvAHsNXjmwH6hC+l44Y2wqY5Lk3iPG/3franRI8Ippo8H5+PjmFbi5hk6f+9Ff0WfEN3AjHd26NjosGATr9rkTYPRtPHmNfAYUUwcF02Gx8/k8eWineevmOtYWUfuON5g2Fvziqff9Yo6d8969XtfaUuQsl4rNwUdYGjWsD0vTL5ok37z/9rovuKXAQZ548QYRmV31iHxgs9ieJpYyV8m2oUS9w2p4IeGFPGfdjkkLV3h37mlpaw85BsYmoCxL9/zPytYBPurpGyzd/b8WHbomRL++fvpIaVFx7xHfzFhZ5uOAB6bzgKFw7z68+i/FYo2ft5SIm0fTa0Hjug0W+SOunToc9uwppOdt3PWZnZjUuIMmeHboCt3Z/dsh8A+Gz7wNu7w6dk97m/Dvwd2FhQXDv583/PsfoUCrLr3Gzl0EAdHLxw6cEwXdUodOnTNFXFsEJ/Tc9TvBnxX67AlsBdEJuFheHXvAqhEz5kNsLj46IvzFs+HTfxw751cw3ECkwDP4/sHAfpbtPt7Q0yvsuT/IU72GjWev/dPti5Yf/RVgCY6a+ZOVfV24Je6cP9W8fdfFfx0xtbQi6oinsQVLPlFG8G17rPo5OyTieruBj8bNSnv8DLzmIE+wKu91jHRsDqp4+vXs2Tqiur9d364OX/cNXrnFf8av6X6B1l3aahka0E50uSAU9rOuJ3ON5s71ciYhMig3/XKb/oRJrAsPuJ0Wv6QRzvUiFxg718v11PhNEc9KNXUyVw6LfdGnn8xVVdfyBNjDXgngOOWaSXdL+40RAR8u83r3kZKMysMAlOblQ51DZiMAFpfTqLxFdK2Qct8v7cETmatKMrO5psYyV1l1amPR6gvyAQRUQ2PjqlbKViiK4IwjiHrC2ClC2prbPkxP5FX9gnL5dhRRKlbtvOGP1CoQkYPA78Ym7asqUDuxPFVEKGRiWAeRL0Lmvnt/beilx+YQDUObpdXD6kPt1DV3FGBRK14cp1zDYPg45aMdG7E0rvIinOvyIX+3bIXiUpSWuo/0SJVN8sws2CxK8+5RxcHwccoH2jg56Rlx1KUl6kdhU9SfzT8yOpBsheIRIV/tHxMhxUCfNF8g1LSRqhFpdnh2stfV14Q7gMNioM8R8gAABKhJREFUzXbxrKf9kQF4q2oPpUqTR38azBx9BZEvqvDe/cuzc1Njc/U2peG3bfXs1MOq7kdLavR8eYimAQEglQiPbGzSzkxbh8282dJqBRDfb5zcnXWrNeggKhSiQVCUyhjOR1v0GGRXn/WpY10wEy2KZaOrf6ZN/xG2Daq5iWyF0mLVbGwDVYSZowAT8UDABEEImezofqVNf1OuDiUeloeoMjpsLS6b7W1mfbB5txoN5Ca7xSYP/LWMbdlWSzDWDyVET7n8UMHH/KhXj9TSoh3RL/5LS+ILhSwWiy8ewFdIt+4S3y4sobh5H51Dj+BQdhMJxVaIsHzgPqrirmeJulFT4qlWxJMeld12QtGoAUKK7rhKlfdfFQgpNktQns8SjWYFS9FIwhB7hj1K5QtFVo/4YNgslmjIC6HQjKvzpa3zSLvq2k3S1NpcLwiiAqim+FtydBa7ehNXElmcfT/lbUpxQWJhvj6b28em7vm30TmlJU76Ru6Gpo8ykmFVQwPTrlYO/4sPTykpbGJo5mFkeSMtLq240NXAtK1ZnX+SotJLizyMLFqYWF1Iik4qKmhgYNLV0v50YmRqcaGHiaW3qfU5cbqJsbmnkcXF5Jic0lJXAxMPY/PbqfGpJUWNDM06mNc5+zYyuagQPPqtTW0uJEe/LSxoaGja2cLu3Nuot0UFRXy+jY6ei4FxPT2jPtaOn/PbUaEQRGWor21c3+GdLmxtzCqGtO1rUzF+k6exhSTdxcpekvYxt6lIm1WkW5pWDKfZWird2bJi2+5Sobd3ylexn1pBtkLparEpdW8QpcVm6bAY18mAq8XWoTSlwZ7i4bJZHDUNkKkrsq+WGVeXp+4DQeTyebocxmmBo76RgIV+KHmRUFTA1ZgW2+qBbIUa4tAQHmCi1rwtyoeaNmEYA22cwM8ZUpRLEDkQlJ1uo2dAENVBtkJ5GZlbc3U3R1d3/l6V40RSDIQYFrrUYAZQhdHB0u6f2NcEqW0iSwrBAbyzWUeCqA4fmqZrQfDDiPycdha2bUzUZ+jV8MLs68nxxbzSE969CFO5nBK7PTqoualVVwsHLkE+l7iSglspsW/y8y5VMZAjwlg+MpHg4lC/51kpJUIhj195rhiKkjEaJPV+PJf6UIiXqjThinRhoVTrlbImF7J3KayYSFYou8VL+QYQAGATtp2e4V8eTH+R7o8L+zcxqpAvapr2WX2JZV6nirUyrg71kaC8jJMslDmX73sF399zpQ1lfbXUXqRWixvyCCVtbiv9Sqq8fRARNz+mhJSVrv6B5l0JompUa6pTvmjO4rzKW7JZQv473nRKjEAgkHwW34Blw4aJ24TRzcGEkrUUiyVu9VV+f5UXlqyiy1PiuWIl9yCLxRLQqwi9U5GClX0k5buh733xJqJGZeI5nnR1DeQ2Fry8SCjJIx+eSK383Fa1smJV5edYvHxfoeiLImtfRPoKSnZOa4HUniuuGn3+yz+SSscpHgtF6rKJkP7qSpu8c2Di1oAVXwofy1dVulu4XK4lGy1RVUVukzEjCIJ8NthiE0EQ5oIKhSAIc0GFQhCEuaBCIQjCXFChEARhLqhQCIIwl/8DAAD//0uoD2oAAAAGSURBVAMA2ZCVBBt2n8sAAAAASUVORK5CYII=",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_teddynote.graphs import visualize_graph\n",
    "\n",
    "visualize_graph(graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_openai/chat_models/base.py:425: UserWarning: Invalid schema for OpenAI's structured output feature, which is the default method for `with_structured_output` as of langchain-openai==0.3. Specify `method=\"function_calling\"` instead or update your schema. See supported schemas: https://platform.openai.com/docs/guides/structured-outputs#supported-schemas\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': \"Invalid schema for response_format 'RouteResponse': In context=('properties', 'next'), array schema missing items.\", 'type': 'invalid_request_error', 'param': 'response_format', 'code': None}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[161], line 17\u001b[0m\n\u001b[1;32m      8\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\n\u001b[1;32m     10\u001b[0m         HumanMessage(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m     ],\n\u001b[1;32m     14\u001b[0m }\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# ê·¸ë˜í”„ ì‹¤í–‰\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[43minvoke_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_teddynote/messages.py:409\u001b[0m, in \u001b[0;36minvoke_graph\u001b[0;34m(graph, inputs, config, node_names, callback)\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m namespace[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(namespace) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroot graph\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;66;03m# subgraphs=True ë¥¼ í†µí•´ ì„œë¸Œê·¸ë˜í”„ì˜ ì¶œë ¥ë„ í¬í•¨\u001b[39;00m\n\u001b[0;32m--> 409\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m namespace, chunk \u001b[38;5;129;01min\u001b[39;00m graph\u001b[38;5;241m.\u001b[39mstream(\n\u001b[1;32m    410\u001b[0m     inputs, config, stream_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupdates\u001b[39m\u001b[38;5;124m\"\u001b[39m, subgraphs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    411\u001b[0m ):\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m node_name, node_chunk \u001b[38;5;129;01min\u001b[39;00m chunk\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    413\u001b[0m         \u001b[38;5;66;03m# node_namesê°€ ë¹„ì–´ìˆì§€ ì•Šì€ ê²½ìš°ì—ë§Œ í•„í„°ë§\u001b[39;00m\n\u001b[1;32m    414\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(node_names) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m node_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m node_names:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/main.py:2647\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[0m\n\u001b[1;32m   2645\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mmatch_cached_writes():\n\u001b[1;32m   2646\u001b[0m     loop\u001b[38;5;241m.\u001b[39moutput_writes(task\u001b[38;5;241m.\u001b[39mid, task\u001b[38;5;241m.\u001b[39mwrites, cached\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m-> 2647\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mtick(\n\u001b[1;32m   2648\u001b[0m     [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mvalues() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t\u001b[38;5;241m.\u001b[39mwrites],\n\u001b[1;32m   2649\u001b[0m     timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[1;32m   2650\u001b[0m     get_waiter\u001b[38;5;241m=\u001b[39mget_waiter,\n\u001b[1;32m   2651\u001b[0m     schedule_task\u001b[38;5;241m=\u001b[39mloop\u001b[38;5;241m.\u001b[39maccept_push,\n\u001b[1;32m   2652\u001b[0m ):\n\u001b[1;32m   2653\u001b[0m     \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[1;32m   2654\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m _output(\n\u001b[1;32m   2655\u001b[0m         stream_mode, print_mode, subgraphs, stream\u001b[38;5;241m.\u001b[39mget, queue\u001b[38;5;241m.\u001b[39mEmpty\n\u001b[1;32m   2656\u001b[0m     )\n\u001b[1;32m   2657\u001b[0m loop\u001b[38;5;241m.\u001b[39mafter_tick()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/_runner.py:253\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;66;03m# panic on failure or timeout\u001b[39;00m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 253\u001b[0m     \u001b[43m_panic_or_proceed\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfutures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdone\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfutures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpanic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tb \u001b[38;5;241m:=\u001b[39m exc\u001b[38;5;241m.\u001b[39m__traceback__:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/_runner.py:511\u001b[0m, in \u001b[0;36m_panic_or_proceed\u001b[0;34m(futs, timeout_exc_cls, panic)\u001b[0m\n\u001b[1;32m    509\u001b[0m                 interrupts\u001b[38;5;241m.\u001b[39mappend(exc)\n\u001b[1;32m    510\u001b[0m             \u001b[38;5;28;01melif\u001b[39;00m fut \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m SKIP_RERAISE_SET:\n\u001b[0;32m--> 511\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m    512\u001b[0m \u001b[38;5;66;03m# raise combined interrupts\u001b[39;00m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m interrupts:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/_executor.py:81\u001b[0m, in \u001b[0;36mBackgroundExecutor.done\u001b[0;34m(self, task)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Remove the task from the tasks dict when it's done.\"\"\"\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 81\u001b[0m     \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GraphBubbleUp:\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;66;03m# This exception is an interruption signal, not an error\u001b[39;00m\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;66;03m# so we don't want to re-raise it on exit\u001b[39;00m\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mpop(task)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/concurrent/futures/_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/concurrent/futures/_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/_retry.py:42\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[1;32m     40\u001b[0m     task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     44\u001b[0m     ns: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/_internal/_runnable.py:657\u001b[0m, in \u001b[0;36mRunnableSeq.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    655\u001b[0m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[1;32m    656\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[0;32m--> 657\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    659\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/_internal/_runnable.py:401\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    399\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(ret)\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 401\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[1;32m    403\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "Cell \u001b[0;32mIn[157], line 38\u001b[0m, in \u001b[0;36msupervisor_agent\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m     35\u001b[0m supervisor_chain \u001b[38;5;241m=\u001b[39m prompt \u001b[38;5;241m|\u001b[39m llm\u001b[38;5;241m.\u001b[39mwith_structured_output(RouteResponse)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# ì—¬ê¸°ì„œ ì‘ë‹µì„ ì§ì ‘ í˜¸ì¶œí•˜ê³  ì¶œë ¥í•©ë‹ˆë‹¤.\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43msupervisor_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# ë””ë²„ê¹…ì„ ìœ„í•´ ì‘ë‹µ ê°’ ì¶œë ¥\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- LLM ì‘ë‹µ ê°’ ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/runnables/base.py:3082\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3080\u001b[0m                 input_ \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, input_, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   3081\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3082\u001b[0m                 input_ \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3083\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   3084\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/runnables/base.py:5495\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   5488\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m   5489\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m   5490\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5493\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   5494\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[0;32m-> 5495\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5496\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5497\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5498\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5499\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:393\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    389\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    390\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    391\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    392\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatGeneration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m--> 393\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    403\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:1019\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m   1011\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m   1012\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1016\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m   1017\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m   1018\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m-> 1019\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:837\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    834\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[1;32m    835\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    836\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 837\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    838\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    840\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    843\u001b[0m         )\n\u001b[1;32m    844\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    845\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:1085\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1083\u001b[0m     result \u001b[38;5;241m=\u001b[39m generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[1;32m   1084\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1085\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1086\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m   1087\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1088\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1089\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_openai/chat_models/base.py:1183\u001b[0m, in \u001b[0;36mBaseChatOpenAI._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1181\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m raw_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(raw_response, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp_response\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1182\u001b[0m         e\u001b[38;5;241m.\u001b[39mresponse \u001b[38;5;241m=\u001b[39m raw_response\u001b[38;5;241m.\u001b[39mhttp_response  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m-> 1183\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m   1184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1185\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minclude_response_headers\n\u001b[1;32m   1186\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m raw_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(raw_response, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1188\u001b[0m ):\n\u001b[1;32m   1189\u001b[0m     generation_info \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(raw_response\u001b[38;5;241m.\u001b[39mheaders)}\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_openai/chat_models/base.py:1157\u001b[0m, in \u001b[0;36mBaseChatOpenAI._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1155\u001b[0m         response \u001b[38;5;241m=\u001b[39m raw_response\u001b[38;5;241m.\u001b[39mparse()\n\u001b[1;32m   1156\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m openai\u001b[38;5;241m.\u001b[39mBadRequestError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1157\u001b[0m         \u001b[43m_handle_openai_bad_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1158\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_responses_api(payload):\n\u001b[1;32m   1159\u001b[0m     original_schema_obj \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_openai/chat_models/base.py:426\u001b[0m, in \u001b[0;36m_handle_openai_bad_request\u001b[0;34m(e)\u001b[0m\n\u001b[1;32m    418\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    419\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid schema for OpenAI\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms structured output feature, which is the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    420\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault method for `with_structured_output` as of langchain-openai==0.3. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    423\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://platform.openai.com/docs/guides/structured-outputs#supported-schemas\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[1;32m    424\u001b[0m     )\n\u001b[1;32m    425\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message)\n\u001b[0;32m--> 426\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    428\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_openai/chat_models/base.py:1151\u001b[0m, in \u001b[0;36mBaseChatOpenAI._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1148\u001b[0m payload\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1150\u001b[0m     raw_response \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m-> 1151\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwith_raw_response\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1152\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpayload\u001b[49m\n\u001b[1;32m   1153\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1154\u001b[0m     )\n\u001b[1;32m   1155\u001b[0m     response \u001b[38;5;241m=\u001b[39m raw_response\u001b[38;5;241m.\u001b[39mparse()\n\u001b[1;32m   1156\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m openai\u001b[38;5;241m.\u001b[39mBadRequestError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_legacy_response.py:364\u001b[0m, in \u001b[0;36mto_raw_response_wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    360\u001b[0m extra_headers[RAW_RESPONSE_HEADER] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    362\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextra_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m extra_headers\n\u001b[0;32m--> 364\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cast(LegacyAPIResponse[R], \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py:183\u001b[0m, in \u001b[0;36mCompletions.parse\u001b[0;34m(self, messages, model, audio, response_format, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, safety_identifier, seed, service_tier, stop, store, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mparser\u001b[39m(raw_completion: ChatCompletion) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ParsedChatCompletion[ResponseFormatT]:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _parse_chat_completion(\n\u001b[1;32m    178\u001b[0m         response_format\u001b[38;5;241m=\u001b[39mresponse_format,\n\u001b[1;32m    179\u001b[0m         chat_completion\u001b[38;5;241m=\u001b[39mraw_completion,\n\u001b[1;32m    180\u001b[0m         input_tools\u001b[38;5;241m=\u001b[39mchat_completion_tools,\n\u001b[1;32m    181\u001b[0m     )\n\u001b[0;32m--> 183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maudio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_completion_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodalities\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprompt_cache_key\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreasoning_effort\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m_type_to_response_format\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msafety_identifier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mverbosity\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweb_search_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpost_parser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# we turn the `ChatCompletion` instance into a `ParsedChatCompletion`\u001b[39;49;00m\n\u001b[1;32m    232\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# in the `parser` function above\u001b[39;49;00m\n\u001b[1;32m    233\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mType\u001b[49m\u001b[43m[\u001b[49m\u001b[43mParsedChatCompletion\u001b[49m\u001b[43m[\u001b[49m\u001b[43mResponseFormatT\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_base_client.py:1259\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1245\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1246\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1247\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1254\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1255\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1256\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1257\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1258\u001b[0m     )\n\u001b[0;32m-> 1259\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_base_client.py:1047\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1044\u001b[0m             err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1046\u001b[0m         log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1047\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1051\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould not resolve response (should never happen)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': \"Invalid schema for response_format 'RouteResponse': In context=('properties', 'next'), array schema missing items.\", 'type': 'invalid_request_error', 'param': 'response_format', 'code': None}}"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_teddynote.messages import random_uuid, invoke_graph\n",
    "\n",
    "# config ì„¤ì •(ì¬ê·€ ìµœëŒ€ íšŸìˆ˜, thread_id)\n",
    "config = RunnableConfig(recursion_limit=10, configurable={\"thread_id\": random_uuid()})\n",
    "\n",
    "# ì§ˆë¬¸ ì…ë ¥\n",
    "inputs = {\n",
    "    \"messages\": [\n",
    "        HumanMessage(\n",
    "            content=\"2010ë…„ ~ 2024ë…„ê¹Œì§€ì˜ ëŒ€í•œë¯¼êµ­ì˜ 1ì¸ë‹¹ GDP ì¶”ì´ë¥¼ ê·¸ë˜í”„ë¡œ ì‹œê°í™” í•´ì£¼ì„¸ìš”.\"\n",
    "        )\n",
    "    ],\n",
    "}\n",
    "\n",
    "# ê·¸ë˜í”„ ì‹¤í–‰\n",
    "invoke_graph(graph, inputs, config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "08. ê³„ì¸µì  ë©€í‹° ì—ì´ì „íŠ¸ íŒ€(Hierarchical Multi-Agent Teams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-4o\n"
     ]
    }
   ],
   "source": [
    "from langchain_teddynote.models import get_model_name, LLMs\n",
    "\n",
    "# ìµœì‹  ëª¨ë¸ ì´ë¦„ ê°€ì ¸ì˜¤ê¸°\n",
    "MODEL_NAME = get_model_name(LLMs.GPT4o)\n",
    "print(MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_teddynote.tools.tavily import TavilySearch\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "# ê²€ìƒ‰ ë„êµ¬ ì •ì˜(TavilySearch)\n",
    "tavily_tool = TavilySearch(max_results=5)\n",
    "\n",
    "\n",
    "# ì›¹ í˜ì´ì§€ì—ì„œ ì„¸ë¶€ ì •ë³´ë¥¼ ìŠ¤í¬ë˜í•‘í•˜ê¸° ìœ„í•œ ë„êµ¬ ì •ì˜\n",
    "@tool\n",
    "def scrape_webpages(urls: List[str]) -> str:\n",
    "    \"\"\"Use requests and bs4 to scrape the provided web pages for detailed information.\"\"\"\n",
    "    # ì£¼ì–´ì§„ URL ëª©ë¡ì„ ì‚¬ìš©í•˜ì—¬ ì›¹ í˜ì´ì§€ ë¡œë“œ\n",
    "    loader = WebBaseLoader(\n",
    "        web_path=urls,\n",
    "        header_template={\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.0.0 Safari/537.36\",\n",
    "        },\n",
    "    )\n",
    "    docs = loader.load()\n",
    "\n",
    "    # ë¡œë“œëœ ë¬¸ì„œì˜ ì œëª©ê³¼ ë‚´ìš©ì„ í¬í•¨í•œ ë¬¸ìì—´ ìƒì„±\n",
    "    return \"\\n\\n\".join(\n",
    "        [\n",
    "            f'<Document name=\"{doc.metadata.get(\"title\", \"\")}\">\\n{doc.page_content}\\n</Document>'\n",
    "            for doc in docs\n",
    "        ]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Dict, Optional, List\n",
    "from typing_extensions import Annotated\n",
    "\n",
    "\n",
    "# ì„ì‹œ ë””ë ‰í† ë¦¬ ìƒì„± ë° ì‘ì—… ë””ë ‰í† ë¦¬ ì„¤ì •\n",
    "WORKING_DIRECTORY = Path(\"./tmp\")\n",
    "\n",
    "# tmp í´ë”ê°€ ì—†ìœ¼ë©´ ìƒì„±\n",
    "WORKING_DIRECTORY.mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "# ì•„ì›ƒë¼ì¸ ìƒì„± ë° íŒŒì¼ë¡œ ì €ì¥\n",
    "@tool\n",
    "def create_outline(\n",
    "    points: Annotated[List[str], \"List of main points or sections.\"],\n",
    "    file_name: Annotated[str, \"File path to save the outline.\"],\n",
    ") -> Annotated[str, \"Path of the saved outline file.\"]:\n",
    "    \"\"\"Create and save an outline.\"\"\"\n",
    "    # ì£¼ì–´ì§„ íŒŒì¼ ì´ë¦„ìœ¼ë¡œ ì•„ì›ƒë¼ì¸ì„ ì €ì¥\n",
    "    with (WORKING_DIRECTORY / file_name).open(\"w\") as file:\n",
    "        for i, point in enumerate(points):\n",
    "            file.write(f\"{i + 1}. {point}\\n\")\n",
    "    return f\"Outline saved to {file_name}\"\n",
    "\n",
    "\n",
    "# ë¬¸ì„œ ì½ê¸°\n",
    "@tool\n",
    "def read_document(\n",
    "    file_name: Annotated[str, \"File path to read the document.\"],\n",
    "    start: Annotated[Optional[int], \"The start line. Default is 0\"] = None,\n",
    "    end: Annotated[Optional[int], \"The end line. Default is None\"] = None,\n",
    ") -> str:\n",
    "    \"\"\"Read the specified document.\"\"\"\n",
    "    # ì£¼ì–´ì§„ íŒŒì¼ ì´ë¦„ìœ¼ë¡œ ë¬¸ì„œ ì½ê¸°\n",
    "    with (WORKING_DIRECTORY / file_name).open(\"r\") as file:\n",
    "        lines = file.readlines()\n",
    "    # ì‹œì‘ ì¤„ì´ ì§€ì •ë˜ì§€ ì•Šì€ ê²½ìš° ê¸°ë³¸ê°’ ì„¤ì •\n",
    "    if start is not None:\n",
    "        start = 0\n",
    "    return \"\\n\".join(lines[start:end])\n",
    "\n",
    "\n",
    "# ë¬¸ì„œ ì“°ê¸° ë° ì €ì¥\n",
    "@tool\n",
    "def write_document(\n",
    "    content: Annotated[str, \"Text content to be written into the document.\"],\n",
    "    file_name: Annotated[str, \"File path to save the document.\"],\n",
    ") -> Annotated[str, \"Path of the saved document file.\"]:\n",
    "    \"\"\"Create and save a text document.\"\"\"\n",
    "    # ì£¼ì–´ì§„ íŒŒì¼ ì´ë¦„ìœ¼ë¡œ ë¬¸ì„œ ì €ì¥\n",
    "    with (WORKING_DIRECTORY / file_name).open(\"w\") as file:\n",
    "        file.write(content)\n",
    "    return f\"Document saved to {file_name}\"\n",
    "\n",
    "\n",
    "# ë¬¸ì„œ í¸ì§‘\n",
    "@tool\n",
    "def edit_document(\n",
    "    file_name: Annotated[str, \"File path of the document to be edited.\"],\n",
    "    inserts: Annotated[\n",
    "        Dict[int, str],\n",
    "        \"Dictionary where key is the line number (1-indexed) and value is the text to be inserted at that line.\",\n",
    "    ],\n",
    ") -> Annotated[str, \"File path of the edited document.\"]:\n",
    "    \"\"\"Edit a document by inserting text at specific line numbers.\"\"\"\n",
    "\n",
    "    # ì£¼ì–´ì§„ íŒŒì¼ ì´ë¦„ìœ¼ë¡œ ë¬¸ì„œ ì½ê¸°\n",
    "    with (WORKING_DIRECTORY / file_name).open(\"r\") as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # ì‚½ì…í•  í…ìŠ¤íŠ¸ë¥¼ ì •ë ¬í•˜ì—¬ ì²˜ë¦¬\n",
    "    sorted_inserts = sorted(inserts.items())\n",
    "\n",
    "    # ì§€ì •ëœ ì¤„ ë²ˆí˜¸ì— í…ìŠ¤íŠ¸ ì‚½ì…\n",
    "    for line_number, text in sorted_inserts:\n",
    "        if 1 <= line_number <= len(lines) + 1:\n",
    "            lines.insert(line_number - 1, text + \"\\n\")\n",
    "        else:\n",
    "            return f\"Error: Line number {line_number} is out of range.\"\n",
    "\n",
    "    # í¸ì§‘ëœ ë¬¸ì„œë¥¼ íŒŒì¼ì— ì €ì¥\n",
    "    with (WORKING_DIRECTORY / file_name).open(\"w\") as file:\n",
    "        file.writelines(lines)\n",
    "\n",
    "    return f\"Document edited and saved to {file_name}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.tools import PythonREPLTool\n",
    "\n",
    "# PythonREPL ë„êµ¬\n",
    "python_repl_tool = PythonREPLTool()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import START, END\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "\n",
    "# ì—ì´ì „íŠ¸ íŒ©í† ë¦¬ í´ë˜ìŠ¤\n",
    "class AgentFactory:\n",
    "    def __init__(self, model_name):\n",
    "        self.llm = ChatOpenAI(model=model_name, temperature=0)\n",
    "\n",
    "    def create_agent_node(self, agent, name: str):\n",
    "        # ë…¸ë“œ ìƒì„± í•¨ìˆ˜\n",
    "        def agent_node(state):\n",
    "            result = agent.invoke(state)\n",
    "            return {\n",
    "                \"messages\": [\n",
    "                    HumanMessage(content=result[\"messages\"][-1].content, name=name)\n",
    "                ]\n",
    "            }\n",
    "\n",
    "        return agent_node\n",
    "\n",
    "\n",
    "# LLM ì´ˆê¸°í™”\n",
    "llm = ChatOpenAI(model=MODEL_NAME, temperature=0)\n",
    "\n",
    "# Agent Factory ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\n",
    "agent_factory = AgentFactory(MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì—ì´ì „íŠ¸ ì •ì˜\n",
    "search_agent = create_react_agent(llm, tools=[tavily_tool])\n",
    "# ì—ì´ì „íŠ¸ ë…¸ë“œ ìƒì„±\n",
    "search_node = agent_factory.create_agent_node(search_agent, name=\"Searcher\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel\n",
    "from typing import Literal\n",
    "\n",
    "\n",
    "def create_team_supervisor(model_name, system_prompt, members) -> str:\n",
    "    # ë‹¤ìŒ ì‘ì—…ì ì„ íƒ ì˜µì…˜ ëª©ë¡ ì •ì˜\n",
    "    options_for_next = [\"FINISH\"] + members\n",
    "\n",
    "    # ì‘ì—…ì ì„ íƒ ì‘ë‹µ ëª¨ë¸ ì •ì˜: ë‹¤ìŒ ì‘ì—…ìë¥¼ ì„ íƒí•˜ê±°ë‚˜ ì‘ì—… ì™„ë£Œë¥¼ ë‚˜íƒ€ëƒ„\n",
    "    class RouteResponse(BaseModel):\n",
    "        next: Literal[options_for_next]\n",
    "\n",
    "    # ChatPromptTemplate ìƒì„±\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system_prompt),\n",
    "            MessagesPlaceholder(variable_name=\"messages\"),\n",
    "            (\n",
    "                \"system\",\n",
    "                \"Given the conversation above, who should act next? \"\n",
    "                \"Or should we FINISH? Select one of: {options}\",\n",
    "            ),\n",
    "        ]\n",
    "    ).partial(options=str(options_for_next))\n",
    "\n",
    "    # LLM ì´ˆê¸°í™”\n",
    "    llm = ChatOpenAI(model=model_name, temperature=0)\n",
    "\n",
    "    # í”„ë¡¬í”„íŠ¸ì™€ LLMì„ ê²°í•©í•˜ì—¬ ì²´ì¸ êµ¬ì„±\n",
    "    supervisor_chain = prompt | llm.with_structured_output(RouteResponse)\n",
    "\n",
    "    return supervisor_chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import List, TypedDict\n",
    "from typing_extensions import Annotated\n",
    "\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "\n",
    "# ìƒíƒœ ì •ì˜\n",
    "class ResearchState(TypedDict):\n",
    "    messages: Annotated[List[BaseMessage], operator.add]  # ë©”ì‹œì§€\n",
    "    team_members: List[str]  # ë©¤ë²„ ì—ì´ì „íŠ¸ ëª©ë¡\n",
    "    next: str  # Supervisor ì—ì´ì „íŠ¸ì—ê²Œ ë‹¤ìŒ ì‘ì—…ìë¥¼ ì„ íƒí•˜ë„ë¡ ì§€ì‹œ\n",
    "\n",
    "\n",
    "# LLM ì´ˆê¸°í™”\n",
    "llm = ChatOpenAI(model=MODEL_NAME, temperature=0)\n",
    "\n",
    "# ê²€ìƒ‰ ë…¸ë“œ ìƒì„±\n",
    "search_agent = create_react_agent(llm, tools=[tavily_tool])\n",
    "search_node = agent_factory.create_agent_node(search_agent, name=\"Searcher\")\n",
    "\n",
    "# ì›¹ ìŠ¤í¬ë˜í•‘ ë…¸ë“œ ìƒì„±\n",
    "web_scraping_agent = create_react_agent(llm, tools=[scrape_webpages])\n",
    "web_scraping_node = agent_factory.create_agent_node(\n",
    "    web_scraping_agent, name=\"WebScraper\"\n",
    ")\n",
    "\n",
    "# Supervisor ì—ì´ì „íŠ¸ ìƒì„±\n",
    "supervisor_agent = create_team_supervisor(\n",
    "    MODEL_NAME,\n",
    "    \"You are a supervisor tasked with managing a conversation between the\"\n",
    "    \" following workers: Search, WebScraper. Given the following user request,\"\n",
    "    \" respond with the worker to act next. Each worker will perform a\"\n",
    "    \" task and respond with their results and status. When finished,\"\n",
    "    \" respond with FINISH.\",\n",
    "    [\"Searcher\", \"WebScraper\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_node(x):\n",
    "    return x[\"next\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_teddynote.graphs import visualize_graph\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# ê·¸ë˜í”„ ìƒì„±\n",
    "web_research_graph = StateGraph(ResearchState)\n",
    "\n",
    "# ë…¸ë“œ ì¶”ê°€\n",
    "web_research_graph.add_node(\"Searcher\", search_node)\n",
    "web_research_graph.add_node(\"WebScraper\", web_scraping_node)\n",
    "web_research_graph.add_node(\"Supervisor\", supervisor_agent)\n",
    "\n",
    "# ì—£ì§€ ì¶”ê°€\n",
    "web_research_graph.add_edge(\"Searcher\", \"Supervisor\")\n",
    "web_research_graph.add_edge(\"WebScraper\", \"Supervisor\")\n",
    "\n",
    "# ì¡°ê±´ë¶€ ì—£ì§€ ì •ì˜: Supervisor ë…¸ë“œì˜ ê²°ì •ì— ë”°ë¼ ë‹¤ìŒ ë…¸ë“œë¡œ ì´ë™\n",
    "web_research_graph.add_conditional_edges(\n",
    "    \"Supervisor\",\n",
    "    get_next_node,\n",
    "    {\"Searcher\": \"Searcher\", \"WebScraper\": \"WebScraper\", \"FINISH\": END},\n",
    ")\n",
    "\n",
    "# ì‹œì‘ ë…¸ë“œ ì„¤ì •\n",
    "web_research_graph.set_entry_point(\"Supervisor\")\n",
    "\n",
    "# ê·¸ë˜í”„ ì»´íŒŒì¼\n",
    "web_research_app = web_research_graph.compile(checkpointer=MemorySaver())\n",
    "\n",
    "# ê·¸ë˜í”„ ì‹œê°í™”\n",
    "visualize_graph(web_research_app, xray=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_teddynote.messages import random_uuid, invoke_graph\n",
    "\n",
    "\n",
    "def run_graph(app, message: str, recursive_limit: int = 50):\n",
    "    # config ì„¤ì •(ì¬ê·€ ìµœëŒ€ íšŸìˆ˜, thread_id)\n",
    "    config = RunnableConfig(\n",
    "        recursion_limit=recursive_limit, configurable={\"thread_id\": random_uuid()}\n",
    "    )\n",
    "\n",
    "    # ì§ˆë¬¸ ì…ë ¥\n",
    "    inputs = {\n",
    "        \"messages\": [HumanMessage(content=message)],\n",
    "    }\n",
    "\n",
    "    # ê·¸ë˜í”„ ì‹¤í–‰\n",
    "    invoke_graph(app, inputs, config)\n",
    "\n",
    "    return app.get_state(config).values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_openai/chat_models/base.py:425: UserWarning: Invalid schema for OpenAI's structured output feature, which is the default method for `with_structured_output` as of langchain-openai==0.3. Specify `method=\"function_calling\"` instead or update your schema. See supported schemas: https://platform.openai.com/docs/guides/structured-outputs#supported-schemas\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': \"Invalid schema for response_format 'RouteResponse': In context=('properties', 'next'), array schema missing items.\", 'type': 'invalid_request_error', 'param': 'response_format', 'code': None}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[174], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mrun_graph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweb_research_app\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhttps://finance.naver.com/news ì˜ ì£¼ìš” ë‰´ìŠ¤ ì •ë¦¬í•´ì„œ ì¶œë ¥í•´ì¤˜. ì¶œì²˜(URL) ë„ í•¨ê»˜ ì¶œë ¥í•´ì¤˜.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[173], line 17\u001b[0m, in \u001b[0;36mrun_graph\u001b[0;34m(app, message, recursive_limit)\u001b[0m\n\u001b[1;32m     12\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: [HumanMessage(content\u001b[38;5;241m=\u001b[39mmessage)],\n\u001b[1;32m     14\u001b[0m }\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# ê·¸ë˜í”„ ì‹¤í–‰\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[43minvoke_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m app\u001b[38;5;241m.\u001b[39mget_state(config)\u001b[38;5;241m.\u001b[39mvalues\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_teddynote/messages.py:409\u001b[0m, in \u001b[0;36minvoke_graph\u001b[0;34m(graph, inputs, config, node_names, callback)\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m namespace[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(namespace) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroot graph\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;66;03m# subgraphs=True ë¥¼ í†µí•´ ì„œë¸Œê·¸ë˜í”„ì˜ ì¶œë ¥ë„ í¬í•¨\u001b[39;00m\n\u001b[0;32m--> 409\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m namespace, chunk \u001b[38;5;129;01min\u001b[39;00m graph\u001b[38;5;241m.\u001b[39mstream(\n\u001b[1;32m    410\u001b[0m     inputs, config, stream_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupdates\u001b[39m\u001b[38;5;124m\"\u001b[39m, subgraphs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    411\u001b[0m ):\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m node_name, node_chunk \u001b[38;5;129;01min\u001b[39;00m chunk\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    413\u001b[0m         \u001b[38;5;66;03m# node_namesê°€ ë¹„ì–´ìˆì§€ ì•Šì€ ê²½ìš°ì—ë§Œ í•„í„°ë§\u001b[39;00m\n\u001b[1;32m    414\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(node_names) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m node_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m node_names:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/main.py:2647\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[0m\n\u001b[1;32m   2645\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mmatch_cached_writes():\n\u001b[1;32m   2646\u001b[0m     loop\u001b[38;5;241m.\u001b[39moutput_writes(task\u001b[38;5;241m.\u001b[39mid, task\u001b[38;5;241m.\u001b[39mwrites, cached\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m-> 2647\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mtick(\n\u001b[1;32m   2648\u001b[0m     [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mvalues() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t\u001b[38;5;241m.\u001b[39mwrites],\n\u001b[1;32m   2649\u001b[0m     timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[1;32m   2650\u001b[0m     get_waiter\u001b[38;5;241m=\u001b[39mget_waiter,\n\u001b[1;32m   2651\u001b[0m     schedule_task\u001b[38;5;241m=\u001b[39mloop\u001b[38;5;241m.\u001b[39maccept_push,\n\u001b[1;32m   2652\u001b[0m ):\n\u001b[1;32m   2653\u001b[0m     \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[1;32m   2654\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m _output(\n\u001b[1;32m   2655\u001b[0m         stream_mode, print_mode, subgraphs, stream\u001b[38;5;241m.\u001b[39mget, queue\u001b[38;5;241m.\u001b[39mEmpty\n\u001b[1;32m   2656\u001b[0m     )\n\u001b[1;32m   2657\u001b[0m loop\u001b[38;5;241m.\u001b[39mafter_tick()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/_runner.py:253\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;66;03m# panic on failure or timeout\u001b[39;00m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 253\u001b[0m     \u001b[43m_panic_or_proceed\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfutures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdone\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfutures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpanic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tb \u001b[38;5;241m:=\u001b[39m exc\u001b[38;5;241m.\u001b[39m__traceback__:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/_runner.py:511\u001b[0m, in \u001b[0;36m_panic_or_proceed\u001b[0;34m(futs, timeout_exc_cls, panic)\u001b[0m\n\u001b[1;32m    509\u001b[0m                 interrupts\u001b[38;5;241m.\u001b[39mappend(exc)\n\u001b[1;32m    510\u001b[0m             \u001b[38;5;28;01melif\u001b[39;00m fut \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m SKIP_RERAISE_SET:\n\u001b[0;32m--> 511\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m    512\u001b[0m \u001b[38;5;66;03m# raise combined interrupts\u001b[39;00m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m interrupts:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/_executor.py:81\u001b[0m, in \u001b[0;36mBackgroundExecutor.done\u001b[0;34m(self, task)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Remove the task from the tasks dict when it's done.\"\"\"\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 81\u001b[0m     \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GraphBubbleUp:\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;66;03m# This exception is an interruption signal, not an error\u001b[39;00m\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;66;03m# so we don't want to re-raise it on exit\u001b[39;00m\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mpop(task)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/concurrent/futures/_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/concurrent/futures/_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/_retry.py:42\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[1;32m     40\u001b[0m     task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     44\u001b[0m     ns: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/_internal/_runnable.py:659\u001b[0m, in \u001b[0;36mRunnableSeq.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    657\u001b[0m                 \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    658\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 659\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    660\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m    661\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/runnables/base.py:5495\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   5488\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m   5489\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m   5490\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5493\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   5494\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[0;32m-> 5495\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5496\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5497\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5498\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5499\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:393\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    389\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    390\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    391\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    392\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatGeneration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m--> 393\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    403\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:1019\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m   1011\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m   1012\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1016\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m   1017\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m   1018\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m-> 1019\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:837\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    834\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[1;32m    835\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    836\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 837\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    838\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    840\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    843\u001b[0m         )\n\u001b[1;32m    844\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    845\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:1085\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1083\u001b[0m     result \u001b[38;5;241m=\u001b[39m generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[1;32m   1084\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1085\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1086\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m   1087\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1088\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1089\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_openai/chat_models/base.py:1183\u001b[0m, in \u001b[0;36mBaseChatOpenAI._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1181\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m raw_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(raw_response, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp_response\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1182\u001b[0m         e\u001b[38;5;241m.\u001b[39mresponse \u001b[38;5;241m=\u001b[39m raw_response\u001b[38;5;241m.\u001b[39mhttp_response  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m-> 1183\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m   1184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1185\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minclude_response_headers\n\u001b[1;32m   1186\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m raw_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(raw_response, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1188\u001b[0m ):\n\u001b[1;32m   1189\u001b[0m     generation_info \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(raw_response\u001b[38;5;241m.\u001b[39mheaders)}\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_openai/chat_models/base.py:1157\u001b[0m, in \u001b[0;36mBaseChatOpenAI._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1155\u001b[0m         response \u001b[38;5;241m=\u001b[39m raw_response\u001b[38;5;241m.\u001b[39mparse()\n\u001b[1;32m   1156\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m openai\u001b[38;5;241m.\u001b[39mBadRequestError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1157\u001b[0m         \u001b[43m_handle_openai_bad_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1158\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_responses_api(payload):\n\u001b[1;32m   1159\u001b[0m     original_schema_obj \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_openai/chat_models/base.py:426\u001b[0m, in \u001b[0;36m_handle_openai_bad_request\u001b[0;34m(e)\u001b[0m\n\u001b[1;32m    418\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    419\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid schema for OpenAI\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms structured output feature, which is the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    420\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault method for `with_structured_output` as of langchain-openai==0.3. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    423\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://platform.openai.com/docs/guides/structured-outputs#supported-schemas\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[1;32m    424\u001b[0m     )\n\u001b[1;32m    425\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message)\n\u001b[0;32m--> 426\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    428\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_openai/chat_models/base.py:1151\u001b[0m, in \u001b[0;36mBaseChatOpenAI._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1148\u001b[0m payload\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1150\u001b[0m     raw_response \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m-> 1151\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwith_raw_response\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1152\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpayload\u001b[49m\n\u001b[1;32m   1153\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1154\u001b[0m     )\n\u001b[1;32m   1155\u001b[0m     response \u001b[38;5;241m=\u001b[39m raw_response\u001b[38;5;241m.\u001b[39mparse()\n\u001b[1;32m   1156\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m openai\u001b[38;5;241m.\u001b[39mBadRequestError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_legacy_response.py:364\u001b[0m, in \u001b[0;36mto_raw_response_wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    360\u001b[0m extra_headers[RAW_RESPONSE_HEADER] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    362\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextra_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m extra_headers\n\u001b[0;32m--> 364\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cast(LegacyAPIResponse[R], \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py:183\u001b[0m, in \u001b[0;36mCompletions.parse\u001b[0;34m(self, messages, model, audio, response_format, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, safety_identifier, seed, service_tier, stop, store, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mparser\u001b[39m(raw_completion: ChatCompletion) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ParsedChatCompletion[ResponseFormatT]:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _parse_chat_completion(\n\u001b[1;32m    178\u001b[0m         response_format\u001b[38;5;241m=\u001b[39mresponse_format,\n\u001b[1;32m    179\u001b[0m         chat_completion\u001b[38;5;241m=\u001b[39mraw_completion,\n\u001b[1;32m    180\u001b[0m         input_tools\u001b[38;5;241m=\u001b[39mchat_completion_tools,\n\u001b[1;32m    181\u001b[0m     )\n\u001b[0;32m--> 183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maudio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_completion_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodalities\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprompt_cache_key\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreasoning_effort\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m_type_to_response_format\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msafety_identifier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mverbosity\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweb_search_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpost_parser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# we turn the `ChatCompletion` instance into a `ParsedChatCompletion`\u001b[39;49;00m\n\u001b[1;32m    232\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# in the `parser` function above\u001b[39;49;00m\n\u001b[1;32m    233\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mType\u001b[49m\u001b[43m[\u001b[49m\u001b[43mParsedChatCompletion\u001b[49m\u001b[43m[\u001b[49m\u001b[43mResponseFormatT\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_base_client.py:1259\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1245\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1246\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1247\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1254\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1255\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1256\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1257\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1258\u001b[0m     )\n\u001b[0;32m-> 1259\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_base_client.py:1047\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1044\u001b[0m             err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1046\u001b[0m         log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1047\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1051\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould not resolve response (should never happen)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': \"Invalid schema for response_format 'RouteResponse': In context=('properties', 'next'), array schema missing items.\", 'type': 'invalid_request_error', 'param': 'response_format', 'code': None}}"
     ]
    }
   ],
   "source": [
    "output = run_graph(\n",
    "    web_research_app,\n",
    "    \"https://finance.naver.com/news ì˜ ì£¼ìš” ë‰´ìŠ¤ ì •ë¦¬í•´ì„œ ì¶œë ¥í•´ì¤˜. ì¶œì²˜(URL) ë„ í•¨ê»˜ ì¶œë ¥í•´ì¤˜.\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìµœì¢… ê²°ê³¼ ì¶œë ¥\n",
    "print(output[\"messages\"][-1].content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "09. SQL ë°ì´í„°ë² ì´ìŠ¤ì™€ ìƒí˜¸ì‘ìš©í•˜ëŠ” ì—ì´ì „íŠ¸\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì‚¬ìš©í•˜ëŠ” ëª¨ë¸ëª…: gpt-4o\n"
     ]
    }
   ],
   "source": [
    "from langchain_teddynote.models import get_model_name, LLMs\n",
    "\n",
    "MODEL_NAME = get_model_name(LLMs.GPT4o)\n",
    "print(f\"ì‚¬ìš©í•˜ëŠ” ëª¨ë¸ëª…: {MODEL_NAME}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded and saved as Chinook.db\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://storage.googleapis.com/benchmarks-artifacts/chinook/Chinook.db\"\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    with open(\"Chinook.db\", \"wb\") as file:\n",
    "        file.write(response.content)\n",
    "    print(\"File downloaded and saved as Chinook.db\")\n",
    "else:\n",
    "    print(f\"Failed to download the file. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sqlite\n",
      "['Album', 'Artist', 'Customer', 'Employee', 'Genre', 'Invoice', 'InvoiceLine', 'MediaType', 'Playlist', 'PlaylistTrack', 'Track']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"[(1, 'AC/DC'), (2, 'Accept'), (3, 'Aerosmith'), (4, 'Alanis Morissette'), (5, 'Alice In Chains')]\""
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.utilities import SQLDatabase\n",
    "\n",
    "# SQLite ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ì—ì„œ SQLDatabase ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\n",
    "db = SQLDatabase.from_uri(\"sqlite:///Chinook.db\")\n",
    "\n",
    "# DB dialect ì¶œë ¥(sqlite)\n",
    "print(db.dialect)\n",
    "\n",
    "# ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ í…Œì´ë¸” ì´ë¦„ ëª©ë¡ ì¶œë ¥\n",
    "print(db.get_usable_table_names())\n",
    "\n",
    "# SQL ì¿¼ë¦¬ ì‹¤í–‰\n",
    "db.run(\"SELECT * FROM Artist LIMIT 5;\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "from langchain_core.messages import ToolMessage\n",
    "from langchain_core.runnables import RunnableLambda, RunnableWithFallbacks\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "\n",
    "# ì˜¤ë¥˜ ì²˜ë¦¬ í•¨ìˆ˜\n",
    "def handle_tool_error(state) -> dict:\n",
    "    # ì˜¤ë¥˜ ì •ë³´ ì¡°íšŒ\n",
    "    error = state.get(\"error\")\n",
    "    # ë„êµ¬ ì •ë³´ ì¡°íšŒ\n",
    "    tool_calls = state[\"messages\"][-1].tool_calls\n",
    "    # ToolMessage ë¡œ ë˜í•‘ í›„ ë°˜í™˜\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            ToolMessage(\n",
    "                content=f\"Here is the error: {repr(error)}\\n\\nPlease fix your mistakes.\",\n",
    "                tool_call_id=tc[\"id\"],\n",
    "            )\n",
    "            for tc in tool_calls\n",
    "        ]\n",
    "    }\n",
    "\n",
    "\n",
    "# ì˜¤ë¥˜ë¥¼ ì²˜ë¦¬í•˜ê³  ì—ì´ì „íŠ¸ì— ì˜¤ë¥˜ë¥¼ ì „ë‹¬í•˜ê¸° ìœ„í•œ ToolNode ìƒì„±\n",
    "def create_tool_node_with_fallback(tools: list) -> RunnableWithFallbacks[Any, dict]:\n",
    "    \"\"\"\n",
    "    Create a ToolNode with a fallback to handle errors and surface them to the agent.\n",
    "    \"\"\"\n",
    "    # ì˜¤ë¥˜ ë°œìƒ ì‹œ ëŒ€ì²´ ë™ì‘ì„ ì •ì˜í•˜ì—¬ ToolNodeì— ì¶”ê°€\n",
    "    return ToolNode(tools).with_fallbacks(\n",
    "        [RunnableLambda(handle_tool_error)], exception_key=\"error\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[QuerySQLDatabaseTool(description=\"Input to this tool is a detailed and correct SQL query, output is a result from the database. If the query is not correct, an error message will be returned. If an error is returned, rewrite the query, check the query, and try again. If you encounter an issue with Unknown column 'xxxx' in 'field list', use sql_db_schema to query the correct table fields.\", db=<langchain_community.utilities.sql_database.SQLDatabase object at 0x1391cb3a0>),\n",
       " InfoSQLDatabaseTool(description='Input to this tool is a comma-separated list of tables, output is the schema and sample rows for those tables. Be sure that the tables actually exist by calling sql_db_list_tables first! Example Input: table1, table2, table3', db=<langchain_community.utilities.sql_database.SQLDatabase object at 0x1391cb3a0>),\n",
       " ListSQLDatabaseTool(db=<langchain_community.utilities.sql_database.SQLDatabase object at 0x1391cb3a0>),\n",
       " QuerySQLCheckerTool(description='Use this tool to double check if your query is correct before executing it. Always use this tool before executing a query with sql_db_query!', db=<langchain_community.utilities.sql_database.SQLDatabase object at 0x1391cb3a0>, llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x12ed52770>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x12ed51330>, root_client=<openai.OpenAI object at 0x12ed52080>, root_async_client=<openai.AsyncOpenAI object at 0x12ed53c10>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')), llm_chain=LLMChain(verbose=False, prompt=PromptTemplate(input_variables=['dialect', 'query'], input_types={}, partial_variables={}, template='\\n{query}\\nDouble check the {dialect} query above for common mistakes, including:\\n- Using NOT IN with NULL values\\n- Using UNION when UNION ALL should have been used\\n- Using BETWEEN for exclusive ranges\\n- Data type mismatch in predicates\\n- Properly quoting identifiers\\n- Using the correct number of arguments for functions\\n- Casting to the correct data type\\n- Using the proper columns for joins\\n\\nIf there are any of the above mistakes, rewrite the query. If there are no mistakes, just reproduce the original query.\\n\\nOutput the final SQL query only.\\n\\nSQL Query: '), llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x12ed52770>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x12ed51330>, root_client=<openai.OpenAI object at 0x12ed52080>, root_async_client=<openai.AsyncOpenAI object at 0x12ed53c10>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')), output_parser=StrOutputParser(), llm_kwargs={}))]"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.agent_toolkits import SQLDatabaseToolkit\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# SQLDatabaseToolkit ìƒì„±\n",
    "toolkit = SQLDatabaseToolkit(db=db, llm=ChatOpenAI(model=MODEL_NAME))\n",
    "\n",
    "# SQLDatabaseToolkitì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ ëª©ë¡\n",
    "tools = toolkit.get_tools()\n",
    "tools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Album, Artist, Customer, Employee, Genre, Invoice, InvoiceLine, MediaType, Playlist, PlaylistTrack, Track\n",
      "\n",
      "CREATE TABLE \"Artist\" (\n",
      "\t\"ArtistId\" INTEGER NOT NULL, \n",
      "\t\"Name\" NVARCHAR(120), \n",
      "\tPRIMARY KEY (\"ArtistId\")\n",
      ")\n",
      "\n",
      "/*\n",
      "3 rows from Artist table:\n",
      "ArtistId\tName\n",
      "1\tAC/DC\n",
      "2\tAccept\n",
      "3\tAerosmith\n",
      "*/\n"
     ]
    }
   ],
   "source": [
    "# ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ í…Œì´ë¸”ì„ ë‚˜ì—´í•˜ëŠ” ë„êµ¬ ì„ íƒ\n",
    "list_tables_tool = next(tool for tool in tools if tool.name == \"sql_db_list_tables\")\n",
    "\n",
    "# íŠ¹ì • í…Œì´ë¸”ì˜ DDLì„ ê°€ì ¸ì˜¤ëŠ” ë„êµ¬ ì„ íƒ\n",
    "get_schema_tool = next(tool for tool in tools if tool.name == \"sql_db_schema\")\n",
    "\n",
    "# ë°ì´í„°ë² ì´ìŠ¤ì˜ ëª¨ë“  í…Œì´ë¸” ëª©ë¡ ì¶œë ¥\n",
    "print(list_tables_tool.invoke(\"\"))\n",
    "\n",
    "# Artist í…Œì´ë¸”ì˜ DDL ì •ë³´ ì¶œë ¥\n",
    "print(get_schema_tool.invoke(\"Artist\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "# Query ì‹¤í–‰ ë„êµ¬\n",
    "@tool\n",
    "def db_query_tool(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Run SQL queries against a database and return results\n",
    "    Returns an error message if the query is incorrect\n",
    "    If an error is returned, rewrite the query, check, and retry\n",
    "    \"\"\"\n",
    "    # ì¿¼ë¦¬ ì‹¤í–‰\n",
    "    result = db.run_no_throw(query)\n",
    "\n",
    "    # ì˜¤ë¥˜: ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ì˜¤ë¥˜ ë©”ì‹œì§€ ë°˜í™˜\n",
    "    if not result:\n",
    "        return \"Error: Query failed. Please rewrite your query and try again.\"\n",
    "    # ì •ìƒ: ì¿¼ë¦¬ ì‹¤í–‰ ê²°ê³¼ ë°˜í™˜\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 'AC/DC'), (2, 'Accept'), (3, 'Aerosmith'), (4, 'Alanis Morissette'), (5, 'Alice In Chains'), (6, 'AntÃ´nio Carlos Jobim'), (7, 'Apocalyptica'), (8, 'Audioslave'), (9, 'BackBeat'), (10, 'Billy Cobham')]\n"
     ]
    }
   ],
   "source": [
    "# Artist í…Œì´ë¸”ì—ì„œ ìƒìœ„ 10ê°œ í–‰ ì„ íƒ ë° ì‹¤í–‰ ê²°ê³¼ ì¶œë ¥\n",
    "print(db_query_tool.invoke(\"SELECT * FROM Artist LIMIT 10;\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: (sqlite3.OperationalError) near \"10\": syntax error\n",
      "[SQL: SELECT * FROM Artist LIMITS 10;]\n",
      "(Background on this error at: https://sqlalche.me/e/20/e3q8)\n"
     ]
    }
   ],
   "source": [
    "# Artist í…Œì´ë¸”ì—ì„œ ìƒìœ„ 10ê°œ í–‰ ì„ íƒ ë° ì‹¤í–‰ ê²°ê³¼ ì¶œë ¥\n",
    "print(db_query_tool.invoke(\"SELECT * FROM Artist LIMITS 10;\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# SQL ì¿¼ë¦¬ì˜ ì¼ë°˜ì ì¸ ì‹¤ìˆ˜ë¥¼ ì ê²€í•˜ê¸° ìœ„í•œ ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì •ì˜\n",
    "query_check_system = \"\"\"You are a SQL expert with a strong attention to detail.\n",
    "Double check the SQLite query for common mistakes, including:\n",
    "- Using NOT IN with NULL values\n",
    "- Using UNION when UNION ALL should have been used\n",
    "- Using BETWEEN for exclusive ranges\n",
    "- Data type mismatch in predicates\n",
    "- Properly quoting identifiers\n",
    "- Using the correct number of arguments for functions\n",
    "- Casting to the correct data type\n",
    "- Using the proper columns for joins\n",
    "\n",
    "If there are any of the above mistakes, rewrite the query. If there are no mistakes, just reproduce the original query.\n",
    "\n",
    "You will call the appropriate tool to execute the query after running this check.\"\"\"\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸ ìƒì„±\n",
    "query_check_prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", query_check_system), (\"placeholder\", \"{messages}\")]\n",
    ")\n",
    "\n",
    "# Query Checker ì²´ì¸ ìƒì„±\n",
    "query_check = query_check_prompt | ChatOpenAI(\n",
    "    model=MODEL_NAME, temperature=0\n",
    ").bind_tools([db_query_tool], tool_choice=\"db_query_tool\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'db_query_tool', 'args': {'query': 'SELECT * FROM Artist LIMIT 10;'}, 'id': 'call_Gde0XQ7hxjfhTVkpNi9VMW1t', 'type': 'tool_call'}\n"
     ]
    }
   ],
   "source": [
    "# ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ì‚¬ìš©í•˜ì—¬ ì¿¼ë¦¬ ì ê²€ ë…¸ë“œ ì‹¤í–‰\n",
    "response = query_check.invoke(\n",
    "    {\"messages\": [(\"user\", \"SELECT * FROM Artist LIMITS 10;\")]}\n",
    ")\n",
    "print(response.tool_calls[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Literal\n",
    "\n",
    "from langchain_core.messages import AIMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.graph.message import AnyMessage, add_messages\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "\n",
    "# ì—ì´ì „íŠ¸ì˜ ìƒíƒœ ì •ì˜\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], add_messages]\n",
    "\n",
    "\n",
    "# ìƒˆë¡œìš´ ê·¸ë˜í”„ ì •ì˜\n",
    "workflow = StateGraph(State)\n",
    "\n",
    "\n",
    "# ì²« ë²ˆì§¸ ë„êµ¬ í˜¸ì¶œì„ ìœ„í•œ ë…¸ë“œ ì¶”ê°€\n",
    "def first_tool_call(state: State) -> dict[str, list[AIMessage]]:\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            AIMessage(\n",
    "                content=\"\",\n",
    "                tool_calls=[\n",
    "                    {\n",
    "                        \"name\": \"sql_db_list_tables\",\n",
    "                        \"args\": {},\n",
    "                        \"id\": \"initial_tool_call_abc123\",\n",
    "                    }\n",
    "                ],\n",
    "            )\n",
    "        ]\n",
    "    }\n",
    "\n",
    "\n",
    "# ì¿¼ë¦¬ì˜ ì •í™•ì„±ì„ ëª¨ë¸ë¡œ ì ê²€í•˜ê¸° ìœ„í•œ í•¨ìˆ˜ ì •ì˜\n",
    "def model_check_query(state: State) -> dict[str, list[AIMessage]]:\n",
    "    \"\"\"\n",
    "    Use this tool to check that your query is correct before you run it\n",
    "    \"\"\"\n",
    "    return {\"messages\": [query_check.invoke({\"messages\": [state[\"messages\"][-1]]})]}\n",
    "\n",
    "\n",
    "# ì²« ë²ˆì§¸ ë„êµ¬ í˜¸ì¶œ ë…¸ë“œ ì¶”ê°€\n",
    "workflow.add_node(\"first_tool_call\", first_tool_call)\n",
    "\n",
    "# ì²« ë²ˆì§¸ ë‘ ë„êµ¬ë¥¼ ìœ„í•œ ë…¸ë“œ ì¶”ê°€\n",
    "workflow.add_node(\n",
    "    \"list_tables_tool\", create_tool_node_with_fallback([list_tables_tool])\n",
    ")\n",
    "workflow.add_node(\"get_schema_tool\", create_tool_node_with_fallback([get_schema_tool]))\n",
    "\n",
    "# ì§ˆë¬¸ê³¼ ì‚¬ìš© ê°€ëŠ¥í•œ í…Œì´ë¸”ì„ ê¸°ë°˜ìœ¼ë¡œ ê´€ë ¨ í…Œì´ë¸”ì„ ì„ íƒí•˜ëŠ” ëª¨ë¸ ë…¸ë“œ ì¶”ê°€\n",
    "model_get_schema = ChatOpenAI(model=MODEL_NAME, temperature=0).bind_tools(\n",
    "    [get_schema_tool]\n",
    ")\n",
    "workflow.add_node(\n",
    "    \"model_get_schema\",\n",
    "    lambda state: {\n",
    "        \"messages\": [model_get_schema.invoke(state[\"messages\"])],\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "# ìµœì¢… ìƒíƒœë¥¼ ë‚˜íƒ€ë‚´ëŠ” ë„êµ¬ ì„¤ëª…\n",
    "class SubmitFinalAnswer(BaseModel):\n",
    "    \"\"\"ì¿¼ë¦¬ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ìì—ê²Œ ìµœì¢… ë‹µë³€ ì œì¶œ\"\"\"\n",
    "\n",
    "    final_answer: str = Field(..., description=\"The final answer to the user\")\n",
    "\n",
    "\n",
    "# ì§ˆë¬¸ê³¼ ìŠ¤í‚¤ë§ˆë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì¿¼ë¦¬ë¥¼ ìƒì„±í•˜ê¸° ìœ„í•œ ëª¨ë¸ ë…¸ë“œ ì¶”ê°€\n",
    "QUERY_GEN_INSTRUCTION = \"\"\"You are a SQL expert with a strong attention to detail.\n",
    "\n",
    "You can define SQL queries, analyze queries results and interpretate query results to response an answer.\n",
    "\n",
    "Read the messages bellow and identify the user question, table schemas, query statement and query result, or error if they exist.\n",
    "\n",
    "1. If there's not any query result that make sense to answer the question, create a syntactically correct SQLite query to answer the user question. DO NOT make any DML statements (INSERT, UPDATE, DELETE, DROP etc.) to the database.\n",
    "\n",
    "2. If you create a query, response ONLY the query statement. For example, \"SELECT id, name FROM pets;\"\n",
    "\n",
    "3. If a query was already executed, but there was an error. Response with the same error message you found. For example: \"Error: Pets table doesn't exist\"\n",
    "\n",
    "4. If a query was already executed successfully interpretate the response and answer the question following this pattern: Answer: <<question answer>>. For example: \"Answer: There three cats registered as adopted\"\n",
    "\"\"\"\n",
    "\n",
    "query_gen_prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", QUERY_GEN_INSTRUCTION), (\"placeholder\", \"{messages}\")]\n",
    ")\n",
    "query_gen = query_gen_prompt | ChatOpenAI(model=MODEL_NAME, temperature=0).bind_tools(\n",
    "    [SubmitFinalAnswer, model_check_query]\n",
    ")\n",
    "\n",
    "\n",
    "# ì¡°ê±´ë¶€ ì—ì§€ ì •ì˜\n",
    "def should_continue(state: State) -> Literal[END, \"correct_query\", \"query_gen\"]:\n",
    "    messages = state[\"messages\"]\n",
    "\n",
    "    last_message = messages[-1]\n",
    "    if last_message.content.startswith(\"Answer:\"):\n",
    "        return END\n",
    "    if last_message.content.startswith(\"Error:\"):\n",
    "        return \"query_gen\"\n",
    "    else:\n",
    "        return \"correct_query\"\n",
    "\n",
    "\n",
    "# ì¿¼ë¦¬ ìƒì„± ë…¸ë“œ ì •ì˜\n",
    "def query_gen_node(state: State):\n",
    "    message = query_gen.invoke(state)\n",
    "\n",
    "    # LLMì´ ì˜ëª»ëœ ë„êµ¬ë¥¼ í˜¸ì¶œí•  ê²½ìš° ì˜¤ë¥˜ ë©”ì‹œì§€ë¥¼ ë°˜í™˜\n",
    "    tool_messages = []\n",
    "    message.pretty_print()\n",
    "    if message.tool_calls:\n",
    "        for tc in message.tool_calls:\n",
    "            if tc[\"name\"] != \"SubmitFinalAnswer\":\n",
    "                tool_messages.append(\n",
    "                    ToolMessage(\n",
    "                        content=f\"Error: The wrong tool was called: {tc['name']}. Please fix your mistakes. Remember to only call SubmitFinalAnswer to submit the final answer. Generated queries should be outputted WITHOUT a tool call.\",\n",
    "                        tool_call_id=tc[\"id\"],\n",
    "                    )\n",
    "                )\n",
    "    else:\n",
    "        tool_messages = []\n",
    "    return {\"messages\": [message] + tool_messages}\n",
    "\n",
    "\n",
    "# ì¿¼ë¦¬ ìƒì„± ë…¸ë“œ ì¶”ê°€\n",
    "workflow.add_node(\"query_gen\", query_gen_node)\n",
    "\n",
    "# ì¿¼ë¦¬ë¥¼ ì‹¤í–‰í•˜ê¸° ì „ì— ëª¨ë¸ë¡œ ì ê²€í•˜ëŠ” ë…¸ë“œ ì¶”ê°€\n",
    "workflow.add_node(\"correct_query\", model_check_query)\n",
    "\n",
    "# ì¿¼ë¦¬ë¥¼ ì‹¤í–‰í•˜ê¸° ìœ„í•œ ë…¸ë“œ ì¶”ê°€\n",
    "workflow.add_node(\"execute_query\", create_tool_node_with_fallback([db_query_tool]))\n",
    "\n",
    "# ë…¸ë“œ ê°„ì˜ ì—£ì§€ ì§€ì •\n",
    "workflow.add_edge(START, \"first_tool_call\")\n",
    "workflow.add_edge(\"first_tool_call\", \"list_tables_tool\")\n",
    "workflow.add_edge(\"list_tables_tool\", \"model_get_schema\")\n",
    "workflow.add_edge(\"model_get_schema\", \"get_schema_tool\")\n",
    "workflow.add_edge(\"get_schema_tool\", \"query_gen\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"query_gen\",\n",
    "    should_continue,\n",
    ")\n",
    "workflow.add_edge(\"correct_query\", \"execute_query\")\n",
    "workflow.add_edge(\"execute_query\", \"query_gen\")\n",
    "\n",
    "# ì‹¤í–‰ ê°€ëŠ¥í•œ ì›Œí¬í”Œë¡œìš°ë¡œ ì»´íŒŒì¼\n",
    "app = workflow.compile(checkpointer=MemorySaver())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_teddynote.graphs import visualize_graph\n",
    "\n",
    "visualize_graph(app, xray=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_teddynote.messages import random_uuid, invoke_graph, stream_graph\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langgraph.errors import GraphRecursionError\n",
    "\n",
    "\n",
    "def run_graph(\n",
    "    message: str, recursive_limit: int = 30, node_names=[], stream: bool = False\n",
    "):\n",
    "    # config ì„¤ì •(ì¬ê·€ ìµœëŒ€ íšŸìˆ˜, thread_id)\n",
    "    config = RunnableConfig(\n",
    "        recursion_limit=recursive_limit, configurable={\"thread_id\": random_uuid()}\n",
    "    )\n",
    "\n",
    "    # ì§ˆë¬¸ ì…ë ¥\n",
    "    inputs = {\n",
    "        \"messages\": [HumanMessage(content=message)],\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        if stream:\n",
    "            # ê·¸ë˜í”„ ì‹¤í–‰\n",
    "            stream_graph(app, inputs, config, node_names=node_names)\n",
    "        else:\n",
    "            invoke_graph(app, inputs, config, node_names=node_names)\n",
    "        output = app.get_state(config).values\n",
    "        return output\n",
    "    except GraphRecursionError as recursion_error:\n",
    "        print(f\"GraphRecursionError: {recursion_error}\")\n",
    "        output = app.get_state(config).values\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mfirst_tool_call\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  sql_db_list_tables (initial_tool_call_abc123)\n",
      " Call ID: initial_tool_call_abc123\n",
      "  Args:\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mlist_tables_tool\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: sql_db_list_tables\n",
      "\n",
      "Album, Artist, Customer, Employee, Genre, Invoice, InvoiceLine, MediaType, Playlist, PlaylistTrack, Track\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mmodel_get_schema\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  sql_db_schema (call_O3yTy4KRztcnxRsE6gPpuX7Y)\n",
      " Call ID: call_O3yTy4KRztcnxRsE6gPpuX7Y\n",
      "  Args:\n",
      "    table_names: Employee\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mget_schema_tool\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: sql_db_schema\n",
      "\n",
      "\n",
      "CREATE TABLE \"Employee\" (\n",
      "\t\"EmployeeId\" INTEGER NOT NULL, \n",
      "\t\"LastName\" NVARCHAR(20) NOT NULL, \n",
      "\t\"FirstName\" NVARCHAR(20) NOT NULL, \n",
      "\t\"Title\" NVARCHAR(30), \n",
      "\t\"ReportsTo\" INTEGER, \n",
      "\t\"BirthDate\" DATETIME, \n",
      "\t\"HireDate\" DATETIME, \n",
      "\t\"Address\" NVARCHAR(70), \n",
      "\t\"City\" NVARCHAR(40), \n",
      "\t\"State\" NVARCHAR(40), \n",
      "\t\"Country\" NVARCHAR(40), \n",
      "\t\"PostalCode\" NVARCHAR(10), \n",
      "\t\"Phone\" NVARCHAR(24), \n",
      "\t\"Fax\" NVARCHAR(24), \n",
      "\t\"Email\" NVARCHAR(60), \n",
      "\tPRIMARY KEY (\"EmployeeId\"), \n",
      "\tFOREIGN KEY(\"ReportsTo\") REFERENCES \"Employee\" (\"EmployeeId\")\n",
      ")\n",
      "\n",
      "/*\n",
      "3 rows from Employee table:\n",
      "EmployeeId\tLastName\tFirstName\tTitle\tReportsTo\tBirthDate\tHireDate\tAddress\tCity\tState\tCountry\tPostalCode\tPhone\tFax\tEmail\n",
      "1\tAdams\tAndrew\tGeneral Manager\tNone\t1962-02-18 00:00:00\t2002-08-14 00:00:00\t11120 Jasper Ave NW\tEdmonton\tAB\tCanada\tT5K 2N1\t+1 (780) 428-9482\t+1 (780) 428-3457\tandrew@chinookcorp.com\n",
      "2\tEdwards\tNancy\tSales Manager\t1\t1958-12-08 00:00:00\t2002-05-01 00:00:00\t825 8 Ave SW\tCalgary\tAB\tCanada\tT2P 2T3\t+1 (403) 262-3443\t+1 (403) 262-3322\tnancy@chinookcorp.com\n",
      "3\tPeacock\tJane\tSales Support Agent\t2\t1973-08-29 00:00:00\t2002-04-01 00:00:00\t1111 6 Ave SW\tCalgary\tAB\tCanada\tT2P 5M5\t+1 (403) 262-3443\t+1 (403) 262-6712\tjane@chinookcorp.com\n",
      "*/\n",
      "==================================================\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  model_check_query (call_PuoVwdB63oH0wZrarD5eYkrQ)\n",
      " Call ID: call_PuoVwdB63oH0wZrarD5eYkrQ\n",
      "  Args:\n",
      "    state: {'messages': [{'content': 'Andrew Adam ì§ì›ì˜ ì¸ì ì •ë³´ë¥¼ ëª¨ë‘ ì¡°íšŒí•´ì¤˜', 'type': 'human'}, {'content': 'Album, Artist, Customer, Employee, Genre, Invoice, InvoiceLine, MediaType, Playlist, PlaylistTrack, Track', 'type': 'function', 'name': 'sql_db_list_tables'}, {'content': '\\nCREATE TABLE \"Employee\" (\\n\\t\"EmployeeId\" INTEGER NOT NULL, \\n\\t\"LastName\" NVARCHAR(20) NOT NULL, \\n\\t\"FirstName\" NVARCHAR(20) NOT NULL, \\n\\t\"Title\" NVARCHAR(30), \\n\\t\"ReportsTo\" INTEGER, \\n\\t\"BirthDate\" DATETIME, \\n\\t\"HireDate\" DATETIME, \\n\\t\"Address\" NVARCHAR(70), \\n\\t\"City\" NVARCHAR(40), \\n\\t\"State\" NVARCHAR(40), \\n\\t\"Country\" NVARCHAR(40), \\n\\t\"PostalCode\" NVARCHAR(10), \\n\\t\"Phone\" NVARCHAR(24), \\n\\t\"Fax\" NVARCHAR(24), \\n\\t\"Email\" NVARCHAR(60), \\n\\tPRIMARY KEY (\"EmployeeId\"), \\n\\tFOREIGN KEY(\"ReportsTo\") REFERENCES \"Employee\" (\"EmployeeId\")\\n)\\n\\n/*\\n3 rows from Employee table:\\nEmployeeId\\tLastName\\tFirstName\\tTitle\\tReportsTo\\tBirthDate\\tHireDate\\tAddress\\tCity\\tState\\tCountry\\tPostalCode\\tPhone\\tFax\\tEmail\\n1\\tAdams\\tAndrew\\tGeneral Manager\\tNone\\t1962-02-18 00:00:00\\t2002-08-14 00:00:00\\t11120 Jasper Ave NW\\tEdmonton\\tAB\\tCanada\\tT5K 2N1\\t+1 (780) 428-9482\\t+1 (780) 428-3457\\tandrew@chinookcorp.com\\n2\\tEdwards\\tNancy\\tSales Manager\\t1\\t1958-12-08 00:00:00\\t2002-05-01 00:00:00\\t825 8 Ave SW\\tCalgary\\tAB\\tCanada\\tT2P 2T3\\t+1 (403) 262-3443\\t+1 (403) 262-3322\\tnancy@chinookcorp.com\\n3\\tPeacock\\tJane\\tSales Support Agent\\t2\\t1973-08-29 00:00:00\\t2002-04-01 00:00:00\\t1111 6 Ave SW\\tCalgary\\tAB\\tCanada\\tT2P 5M5\\t+1 (403) 262-3443\\t+1 (403) 262-6712\\tjane@chinookcorp.com\\n*/', 'type': 'function', 'name': 'sql_db_schema'}]}\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mquery_gen\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  model_check_query (call_PuoVwdB63oH0wZrarD5eYkrQ)\n",
      " Call ID: call_PuoVwdB63oH0wZrarD5eYkrQ\n",
      "  Args:\n",
      "    state: {'messages': [{'content': 'Andrew Adam ì§ì›ì˜ ì¸ì ì •ë³´ë¥¼ ëª¨ë‘ ì¡°íšŒí•´ì¤˜', 'type': 'human'}, {'content': 'Album, Artist, Customer, Employee, Genre, Invoice, InvoiceLine, MediaType, Playlist, PlaylistTrack, Track', 'type': 'function', 'name': 'sql_db_list_tables'}, {'content': '\\nCREATE TABLE \"Employee\" (\\n\\t\"EmployeeId\" INTEGER NOT NULL, \\n\\t\"LastName\" NVARCHAR(20) NOT NULL, \\n\\t\"FirstName\" NVARCHAR(20) NOT NULL, \\n\\t\"Title\" NVARCHAR(30), \\n\\t\"ReportsTo\" INTEGER, \\n\\t\"BirthDate\" DATETIME, \\n\\t\"HireDate\" DATETIME, \\n\\t\"Address\" NVARCHAR(70), \\n\\t\"City\" NVARCHAR(40), \\n\\t\"State\" NVARCHAR(40), \\n\\t\"Country\" NVARCHAR(40), \\n\\t\"PostalCode\" NVARCHAR(10), \\n\\t\"Phone\" NVARCHAR(24), \\n\\t\"Fax\" NVARCHAR(24), \\n\\t\"Email\" NVARCHAR(60), \\n\\tPRIMARY KEY (\"EmployeeId\"), \\n\\tFOREIGN KEY(\"ReportsTo\") REFERENCES \"Employee\" (\"EmployeeId\")\\n)\\n\\n/*\\n3 rows from Employee table:\\nEmployeeId\\tLastName\\tFirstName\\tTitle\\tReportsTo\\tBirthDate\\tHireDate\\tAddress\\tCity\\tState\\tCountry\\tPostalCode\\tPhone\\tFax\\tEmail\\n1\\tAdams\\tAndrew\\tGeneral Manager\\tNone\\t1962-02-18 00:00:00\\t2002-08-14 00:00:00\\t11120 Jasper Ave NW\\tEdmonton\\tAB\\tCanada\\tT5K 2N1\\t+1 (780) 428-9482\\t+1 (780) 428-3457\\tandrew@chinookcorp.com\\n2\\tEdwards\\tNancy\\tSales Manager\\t1\\t1958-12-08 00:00:00\\t2002-05-01 00:00:00\\t825 8 Ave SW\\tCalgary\\tAB\\tCanada\\tT2P 2T3\\t+1 (403) 262-3443\\t+1 (403) 262-3322\\tnancy@chinookcorp.com\\n3\\tPeacock\\tJane\\tSales Support Agent\\t2\\t1973-08-29 00:00:00\\t2002-04-01 00:00:00\\t1111 6 Ave SW\\tCalgary\\tAB\\tCanada\\tT2P 5M5\\t+1 (403) 262-3443\\t+1 (403) 262-6712\\tjane@chinookcorp.com\\n*/', 'type': 'function', 'name': 'sql_db_schema'}]}\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "\n",
      "Error: The wrong tool was called: model_check_query. Please fix your mistakes. Remember to only call SubmitFinalAnswer to submit the final answer. Generated queries should be outputted WITHOUT a tool call.\n",
      "==================================================\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "SELECT * FROM Employee WHERE FirstName = 'Andrew' AND LastName = 'Adams';\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mquery_gen\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "SELECT * FROM Employee WHERE FirstName = 'Andrew' AND LastName = 'Adams';\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mcorrect_query\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  db_query_tool (call_OEElD5MHC7UFGK0OmV4HTqmo)\n",
      " Call ID: call_OEElD5MHC7UFGK0OmV4HTqmo\n",
      "  Args:\n",
      "    query: SELECT * FROM Employee WHERE FirstName = 'Andrew' AND LastName = 'Adams';\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mexecute_query\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: db_query_tool\n",
      "\n",
      "[(1, 'Adams', 'Andrew', 'General Manager', None, '1962-02-18 00:00:00', '2002-08-14 00:00:00', '11120 Jasper Ave NW', 'Edmonton', 'AB', 'Canada', 'T5K 2N1', '+1 (780) 428-9482', '+1 (780) 428-3457', 'andrew@chinookcorp.com')]\n",
      "==================================================\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Answer: Andrew Adamsì˜ ì¸ì ì •ë³´ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n",
      "- ì§ì±…: General Manager\n",
      "- ìƒë…„ì›”ì¼: 1962-02-18\n",
      "- ê³ ìš©ì¼: 2002-08-14\n",
      "- ì£¼ì†Œ: 11120 Jasper Ave NW, Edmonton, AB, Canada, T5K 2N1\n",
      "- ì „í™”ë²ˆí˜¸: +1 (780) 428-9482\n",
      "- íŒ©ìŠ¤: +1 (780) 428-3457\n",
      "- ì´ë©”ì¼: andrew@chinookcorp.com\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mquery_gen\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Answer: Andrew Adamsì˜ ì¸ì ì •ë³´ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n",
      "- ì§ì±…: General Manager\n",
      "- ìƒë…„ì›”ì¼: 1962-02-18\n",
      "- ê³ ìš©ì¼: 2002-08-14\n",
      "- ì£¼ì†Œ: 11120 Jasper Ave NW, Edmonton, AB, Canada, T5K 2N1\n",
      "- ì „í™”ë²ˆí˜¸: +1 (780) 428-9482\n",
      "- íŒ©ìŠ¤: +1 (780) 428-3457\n",
      "- ì´ë©”ì¼: andrew@chinookcorp.com\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "output = run_graph(\n",
    "    \"Andrew Adam ì§ì›ì˜ ì¸ì ì •ë³´ë¥¼ ëª¨ë‘ ì¡°íšŒí•´ì¤˜\",\n",
    "    stream=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mfirst_tool_call\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  sql_db_list_tables (initial_tool_call_abc123)\n",
      " Call ID: initial_tool_call_abc123\n",
      "  Args:\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mlist_tables_tool\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: sql_db_list_tables\n",
      "\n",
      "Album, Artist, Customer, Employee, Genre, Invoice, InvoiceLine, MediaType, Playlist, PlaylistTrack, Track\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mmodel_get_schema\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  sql_db_schema (call_JMeyAhbiQQ6Hi8GhO56ir2Ds)\n",
      " Call ID: call_JMeyAhbiQQ6Hi8GhO56ir2Ds\n",
      "  Args:\n",
      "    table_names: Customer, Invoice\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mget_schema_tool\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: sql_db_schema\n",
      "\n",
      "\n",
      "CREATE TABLE \"Customer\" (\n",
      "\t\"CustomerId\" INTEGER NOT NULL, \n",
      "\t\"FirstName\" NVARCHAR(40) NOT NULL, \n",
      "\t\"LastName\" NVARCHAR(20) NOT NULL, \n",
      "\t\"Company\" NVARCHAR(80), \n",
      "\t\"Address\" NVARCHAR(70), \n",
      "\t\"City\" NVARCHAR(40), \n",
      "\t\"State\" NVARCHAR(40), \n",
      "\t\"Country\" NVARCHAR(40), \n",
      "\t\"PostalCode\" NVARCHAR(10), \n",
      "\t\"Phone\" NVARCHAR(24), \n",
      "\t\"Fax\" NVARCHAR(24), \n",
      "\t\"Email\" NVARCHAR(60) NOT NULL, \n",
      "\t\"SupportRepId\" INTEGER, \n",
      "\tPRIMARY KEY (\"CustomerId\"), \n",
      "\tFOREIGN KEY(\"SupportRepId\") REFERENCES \"Employee\" (\"EmployeeId\")\n",
      ")\n",
      "\n",
      "/*\n",
      "3 rows from Customer table:\n",
      "CustomerId\tFirstName\tLastName\tCompany\tAddress\tCity\tState\tCountry\tPostalCode\tPhone\tFax\tEmail\tSupportRepId\n",
      "1\tLuÃ­s\tGonÃ§alves\tEmbraer - Empresa Brasileira de AeronÃ¡utica S.A.\tAv. Brigadeiro Faria Lima, 2170\tSÃ£o JosÃ© dos Campos\tSP\tBrazil\t12227-000\t+55 (12) 3923-5555\t+55 (12) 3923-5566\tluisg@embraer.com.br\t3\n",
      "2\tLeonie\tKÃ¶hler\tNone\tTheodor-Heuss-StraÃŸe 34\tStuttgart\tNone\tGermany\t70174\t+49 0711 2842222\tNone\tleonekohler@surfeu.de\t5\n",
      "3\tFranÃ§ois\tTremblay\tNone\t1498 rue BÃ©langer\tMontrÃ©al\tQC\tCanada\tH2G 1A7\t+1 (514) 721-4711\tNone\tftremblay@gmail.com\t3\n",
      "*/\n",
      "\n",
      "\n",
      "CREATE TABLE \"Invoice\" (\n",
      "\t\"InvoiceId\" INTEGER NOT NULL, \n",
      "\t\"CustomerId\" INTEGER NOT NULL, \n",
      "\t\"InvoiceDate\" DATETIME NOT NULL, \n",
      "\t\"BillingAddress\" NVARCHAR(70), \n",
      "\t\"BillingCity\" NVARCHAR(40), \n",
      "\t\"BillingState\" NVARCHAR(40), \n",
      "\t\"BillingCountry\" NVARCHAR(40), \n",
      "\t\"BillingPostalCode\" NVARCHAR(10), \n",
      "\t\"Total\" NUMERIC(10, 2) NOT NULL, \n",
      "\tPRIMARY KEY (\"InvoiceId\"), \n",
      "\tFOREIGN KEY(\"CustomerId\") REFERENCES \"Customer\" (\"CustomerId\")\n",
      ")\n",
      "\n",
      "/*\n",
      "3 rows from Invoice table:\n",
      "InvoiceId\tCustomerId\tInvoiceDate\tBillingAddress\tBillingCity\tBillingState\tBillingCountry\tBillingPostalCode\tTotal\n",
      "1\t2\t2009-01-01 00:00:00\tTheodor-Heuss-StraÃŸe 34\tStuttgart\tNone\tGermany\t70174\t1.98\n",
      "2\t4\t2009-01-02 00:00:00\tUllevÃ¥lsveien 14\tOslo\tNone\tNorway\t0171\t3.96\n",
      "3\t8\t2009-01-03 00:00:00\tGrÃ©trystraat 63\tBrussels\tNone\tBelgium\t1000\t5.94\n",
      "*/\n",
      "==================================================\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  model_check_query (call_jrHV3YKiROan9fx812HrLnka)\n",
      " Call ID: call_jrHV3YKiROan9fx812HrLnka\n",
      "  Args:\n",
      "    state: {'messages': [{'content': '2009ë…„ë„ì— ì–´ëŠ êµ­ê°€ì˜ ê³ ê°ì´ ê°€ì¥ ë§ì´ ì§€ì¶œí–ˆì„ê¹Œìš”? ê·¸ë¦¬ê³  ì–¼ë§ˆë¥¼ ì§€ì¶œí–ˆì„ê¹Œìš”? í•œê¸€ë¡œ ë‹µë³€í•˜ì„¸ìš”.', 'type': 'human'}, {'content': 'Album, Artist, Customer, Employee, Genre, Invoice, InvoiceLine, MediaType, Playlist, PlaylistTrack, Track', 'type': 'function', 'name': 'sql_db_list_tables'}, {'content': '{\"table_names\":\"Customer, Invoice\"}', 'type': 'function', 'name': 'sql_db_schema'}, {'content': '\\nCREATE TABLE \"Customer\" (\\n\\t\"CustomerId\" INTEGER NOT NULL, \\n\\t\"FirstName\" NVARCHAR(40) NOT NULL, \\n\\t\"LastName\" NVARCHAR(20) NOT NULL, \\n\\t\"Company\" NVARCHAR(80), \\n\\t\"Address\" NVARCHAR(70), \\n\\t\"City\" NVARCHAR(40), \\n\\t\"State\" NVARCHAR(40), \\n\\t\"Country\" NVARCHAR(40), \\n\\t\"PostalCode\" NVARCHAR(10), \\n\\t\"Phone\" NVARCHAR(24), \\n\\t\"Fax\" NVARCHAR(24), \\n\\t\"Email\" NVARCHAR(60) NOT NULL, \\n\\t\"SupportRepId\" INTEGER, \\n\\tPRIMARY KEY (\"CustomerId\"), \\n\\tFOREIGN KEY(\"SupportRepId\") REFERENCES \"Employee\" (\"EmployeeId\")\\n)\\n\\n/*\\n3 rows from Customer table:\\nCustomerId\\tFirstName\\tLastName\\tCompany\\tAddress\\tCity\\tState\\tCountry\\tPostalCode\\tPhone\\tFax\\tEmail\\tSupportRepId\\n1\\tLuÃ­s\\tGonÃ§alves\\tEmbraer - Empresa Brasileira de AeronÃ¡utica S.A.\\tAv. Brigadeiro Faria Lima, 2170\\tSÃ£o JosÃ© dos Campos\\tSP\\tBrazil\\t12227-000\\t+55 (12) 3923-5555\\t+55 (12) 3923-5566\\tluisg@embraer.com.br\\t3\\n2\\tLeonie\\tKÃ¶hler\\tNone\\tTheodor-Heuss-StraÃŸe 34\\tStuttgart\\tNone\\tGermany\\t70174\\t+49 0711 2842222\\tNone\\tleonekohler@surfeu.de\\t5\\n3\\tFranÃ§ois\\tTremblay\\tNone\\t1498 rue BÃ©langer\\tMontrÃ©al\\tQC\\tCanada\\tH2G 1A7\\t+1 (514) 721-4711\\tNone\\tftremblay@gmail.com\\t3\\n*/\\n\\n\\nCREATE TABLE \"Invoice\" (\\n\\t\"InvoiceId\" INTEGER NOT NULL, \\n\\t\"CustomerId\" INTEGER NOT NULL, \\n\\t\"InvoiceDate\" DATETIME NOT NULL, \\n\\t\"BillingAddress\" NVARCHAR(70), \\n\\t\"BillingCity\" NVARCHAR(40), \\n\\t\"BillingState\" NVARCHAR(40), \\n\\t\"BillingCountry\" NVARCHAR(40), \\n\\t\"BillingPostalCode\" NVARCHAR(10), \\n\\t\"Total\" NUMERIC(10, 2) NOT NULL, \\n\\tPRIMARY KEY (\"InvoiceId\"), \\n\\tFOREIGN KEY(\"CustomerId\") REFERENCES \"Customer\" (\"CustomerId\")\\n)\\n\\n/*\\n3 rows from Invoice table:\\nInvoiceId\\tCustomerId\\tInvoiceDate\\tBillingAddress\\tBillingCity\\tBillingState\\tBillingCountry\\tBillingPostalCode\\tTotal\\n1\\t2\\t2009-01-01 00:00:00\\tTheodor-Heuss-StraÃŸe 34\\tStuttgart\\tNone\\tGermany\\t70174\\t1.98\\n2\\t4\\t2009-01-02 00:00:00\\tUllevÃ¥lsveien 14\\tOslo\\tNone\\tNorway\\t0171\\t3.96\\n3\\t8\\t2009-01-03 00:00:00\\tGrÃ©trystraat 63\\tBrussels\\tNone\\tBelgium\\t1000\\t5.94\\n*/', 'type': 'function', 'name': 'sql_db_schema'}]}\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mquery_gen\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  model_check_query (call_jrHV3YKiROan9fx812HrLnka)\n",
      " Call ID: call_jrHV3YKiROan9fx812HrLnka\n",
      "  Args:\n",
      "    state: {'messages': [{'content': '2009ë…„ë„ì— ì–´ëŠ êµ­ê°€ì˜ ê³ ê°ì´ ê°€ì¥ ë§ì´ ì§€ì¶œí–ˆì„ê¹Œìš”? ê·¸ë¦¬ê³  ì–¼ë§ˆë¥¼ ì§€ì¶œí–ˆì„ê¹Œìš”? í•œê¸€ë¡œ ë‹µë³€í•˜ì„¸ìš”.', 'type': 'human'}, {'content': 'Album, Artist, Customer, Employee, Genre, Invoice, InvoiceLine, MediaType, Playlist, PlaylistTrack, Track', 'type': 'function', 'name': 'sql_db_list_tables'}, {'content': '{\"table_names\":\"Customer, Invoice\"}', 'type': 'function', 'name': 'sql_db_schema'}, {'content': '\\nCREATE TABLE \"Customer\" (\\n\\t\"CustomerId\" INTEGER NOT NULL, \\n\\t\"FirstName\" NVARCHAR(40) NOT NULL, \\n\\t\"LastName\" NVARCHAR(20) NOT NULL, \\n\\t\"Company\" NVARCHAR(80), \\n\\t\"Address\" NVARCHAR(70), \\n\\t\"City\" NVARCHAR(40), \\n\\t\"State\" NVARCHAR(40), \\n\\t\"Country\" NVARCHAR(40), \\n\\t\"PostalCode\" NVARCHAR(10), \\n\\t\"Phone\" NVARCHAR(24), \\n\\t\"Fax\" NVARCHAR(24), \\n\\t\"Email\" NVARCHAR(60) NOT NULL, \\n\\t\"SupportRepId\" INTEGER, \\n\\tPRIMARY KEY (\"CustomerId\"), \\n\\tFOREIGN KEY(\"SupportRepId\") REFERENCES \"Employee\" (\"EmployeeId\")\\n)\\n\\n/*\\n3 rows from Customer table:\\nCustomerId\\tFirstName\\tLastName\\tCompany\\tAddress\\tCity\\tState\\tCountry\\tPostalCode\\tPhone\\tFax\\tEmail\\tSupportRepId\\n1\\tLuÃ­s\\tGonÃ§alves\\tEmbraer - Empresa Brasileira de AeronÃ¡utica S.A.\\tAv. Brigadeiro Faria Lima, 2170\\tSÃ£o JosÃ© dos Campos\\tSP\\tBrazil\\t12227-000\\t+55 (12) 3923-5555\\t+55 (12) 3923-5566\\tluisg@embraer.com.br\\t3\\n2\\tLeonie\\tKÃ¶hler\\tNone\\tTheodor-Heuss-StraÃŸe 34\\tStuttgart\\tNone\\tGermany\\t70174\\t+49 0711 2842222\\tNone\\tleonekohler@surfeu.de\\t5\\n3\\tFranÃ§ois\\tTremblay\\tNone\\t1498 rue BÃ©langer\\tMontrÃ©al\\tQC\\tCanada\\tH2G 1A7\\t+1 (514) 721-4711\\tNone\\tftremblay@gmail.com\\t3\\n*/\\n\\n\\nCREATE TABLE \"Invoice\" (\\n\\t\"InvoiceId\" INTEGER NOT NULL, \\n\\t\"CustomerId\" INTEGER NOT NULL, \\n\\t\"InvoiceDate\" DATETIME NOT NULL, \\n\\t\"BillingAddress\" NVARCHAR(70), \\n\\t\"BillingCity\" NVARCHAR(40), \\n\\t\"BillingState\" NVARCHAR(40), \\n\\t\"BillingCountry\" NVARCHAR(40), \\n\\t\"BillingPostalCode\" NVARCHAR(10), \\n\\t\"Total\" NUMERIC(10, 2) NOT NULL, \\n\\tPRIMARY KEY (\"InvoiceId\"), \\n\\tFOREIGN KEY(\"CustomerId\") REFERENCES \"Customer\" (\"CustomerId\")\\n)\\n\\n/*\\n3 rows from Invoice table:\\nInvoiceId\\tCustomerId\\tInvoiceDate\\tBillingAddress\\tBillingCity\\tBillingState\\tBillingCountry\\tBillingPostalCode\\tTotal\\n1\\t2\\t2009-01-01 00:00:00\\tTheodor-Heuss-StraÃŸe 34\\tStuttgart\\tNone\\tGermany\\t70174\\t1.98\\n2\\t4\\t2009-01-02 00:00:00\\tUllevÃ¥lsveien 14\\tOslo\\tNone\\tNorway\\t0171\\t3.96\\n3\\t8\\t2009-01-03 00:00:00\\tGrÃ©trystraat 63\\tBrussels\\tNone\\tBelgium\\t1000\\t5.94\\n*/', 'type': 'function', 'name': 'sql_db_schema'}]}\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "\n",
      "Error: The wrong tool was called: model_check_query. Please fix your mistakes. Remember to only call SubmitFinalAnswer to submit the final answer. Generated queries should be outputted WITHOUT a tool call.\n",
      "==================================================\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "```sql\n",
      "SELECT c.Country, SUM(i.Total) AS TotalSpent\n",
      "FROM Customer c\n",
      "JOIN Invoice i ON c.CustomerId = i.CustomerId\n",
      "WHERE strftime('%Y', i.InvoiceDate) = '2009'\n",
      "GROUP BY c.Country\n",
      "ORDER BY TotalSpent DESC\n",
      "LIMIT 1;\n",
      "```\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mquery_gen\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "```sql\n",
      "SELECT c.Country, SUM(i.Total) AS TotalSpent\n",
      "FROM Customer c\n",
      "JOIN Invoice i ON c.CustomerId = i.CustomerId\n",
      "WHERE strftime('%Y', i.InvoiceDate) = '2009'\n",
      "GROUP BY c.Country\n",
      "ORDER BY TotalSpent DESC\n",
      "LIMIT 1;\n",
      "```\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mcorrect_query\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  db_query_tool (call_XEefsOTcslitTwO71dnvvmJ0)\n",
      " Call ID: call_XEefsOTcslitTwO71dnvvmJ0\n",
      "  Args:\n",
      "    query: SELECT c.Country, SUM(i.Total) AS TotalSpent\n",
      "FROM Customer c\n",
      "JOIN Invoice i ON c.CustomerId = i.CustomerId\n",
      "WHERE strftime('%Y', i.InvoiceDate) = '2009'\n",
      "GROUP BY c.Country\n",
      "ORDER BY TotalSpent DESC\n",
      "LIMIT 1;\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mexecute_query\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: db_query_tool\n",
      "\n",
      "[('USA', 103.95)]\n",
      "==================================================\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Answer: 2009ë…„ë„ì— ë¯¸êµ­(USA) ê³ ê°ì´ ê°€ì¥ ë§ì´ ì§€ì¶œí–ˆìœ¼ë©°, ì´ ì§€ì¶œì•¡ì€ 103.95ì…ë‹ˆë‹¤.\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mquery_gen\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Answer: 2009ë…„ë„ì— ë¯¸êµ­(USA) ê³ ê°ì´ ê°€ì¥ ë§ì´ ì§€ì¶œí–ˆìœ¼ë©°, ì´ ì§€ì¶œì•¡ì€ 103.95ì…ë‹ˆë‹¤.\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "output = run_graph(\n",
    "    \"2009ë…„ë„ì— ì–´ëŠ êµ­ê°€ì˜ ê³ ê°ì´ ê°€ì¥ ë§ì´ ì§€ì¶œí–ˆì„ê¹Œìš”? ê·¸ë¦¬ê³  ì–¼ë§ˆë¥¼ ì§€ì¶œí–ˆì„ê¹Œìš”? í•œê¸€ë¡œ ë‹µë³€í•˜ì„¸ìš”.\",\n",
    "    stream=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "# í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”\n",
    "client = Client()\n",
    "\n",
    "# ë°ì´í„°ì…‹ ìƒì„± ë° ì—…ë¡œë“œ\n",
    "examples = [\n",
    "    (\n",
    "        \"Which country's customers spent the most? And how much did they spend?\",\n",
    "        \"The country whose customers spent the most is the USA, with a total spending of 523.06.\",\n",
    "    ),\n",
    "    (\n",
    "        \"What was the most purchased track of 2013?\",\n",
    "        \"The most purchased track of 2013 was Hot Girl.\",\n",
    "    ),\n",
    "    (\n",
    "        \"How many albums does the artist Led Zeppelin have?\",\n",
    "        \"Led Zeppelin has 14 albums\",\n",
    "    ),\n",
    "    (\n",
    "        \"What is the total price for the album â€œBig Onesâ€?\",\n",
    "        \"The total price for the album 'Big Ones' is 14.85\",\n",
    "    ),\n",
    "    (\n",
    "        \"Which sales agent made the most in sales in 2009?\",\n",
    "        \"Steve Johnson made the most sales in 2009\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "dataset_name = \"SQL Agent Response\"\n",
    "\n",
    "if not client.has_dataset(dataset_name=dataset_name):\n",
    "    dataset = client.create_dataset(dataset_name=dataset_name)\n",
    "    inputs, outputs = zip(\n",
    "        *[({\"input\": text}, {\"output\": label}) for text, label in examples]\n",
    "    )\n",
    "    client.create_examples(inputs=inputs, outputs=outputs, dataset_id=dataset.id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì—ì´ì „íŠ¸ì˜ SQL ì¿¼ë¦¬ ì‘ë‹µì„ ì˜ˆì¸¡í•˜ê¸° ìœ„í•œ í•¨ìˆ˜ ì •ì˜\n",
    "def predict_sql_agent_answer(example: dict):\n",
    "    \"\"\"Use this for answer evaluation\"\"\"\n",
    "    config = RunnableConfig(configurable={\"thread_id\": random_uuid()})\n",
    "\n",
    "    inputs = {\n",
    "        \"messages\": [HumanMessage(content=example[\"input\"])],\n",
    "    }\n",
    "    # ê·¸ë˜í”„ë¥¼ ì‹¤í–‰í•˜ì—¬ ë©”ì‹œì§€ ê²°ê³¼ ì¡°íšŒ\n",
    "    messages = app.invoke(inputs, config)\n",
    "    answer = messages[\"messages\"][-1].content\n",
    "    # ê²°ê³¼ ë°˜í™˜\n",
    "    return {\"response\": answer}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Grade prompt\n",
    "grade_prompt_answer_accuracy = hub.pull(\"langchain-ai/rag-answer-vs-reference\")\n",
    "\n",
    "\n",
    "# ë‹µë³€ í‰ê°€ì LLM-as-judge ì •ì˜\n",
    "def answer_evaluator(run, example) -> dict:\n",
    "    # input: ì§ˆë¬¸\n",
    "    input_question = example.inputs[\"input\"]\n",
    "    # output: ì°¸ì¡° ë‹µë³€\n",
    "    reference = example.outputs[\"output\"]\n",
    "    # ì˜ˆì¸¡ ë‹µë³€\n",
    "    prediction = run.outputs[\"response\"]\n",
    "\n",
    "    # LLM í‰ê°€ì ì´ˆê¸°í™”\n",
    "    llm = ChatOpenAI(model=MODEL_NAME, temperature=0)\n",
    "    answer_grader = grade_prompt_answer_accuracy | llm\n",
    "\n",
    "    # í‰ê°€ì ì‹¤í–‰\n",
    "    score = answer_grader.invoke(\n",
    "        {\n",
    "            \"question\": input_question,\n",
    "            \"correct_answer\": reference,\n",
    "            \"student_answer\": prediction,\n",
    "        }\n",
    "    )\n",
    "    score = score[\"Score\"]\n",
    "\n",
    "    # ì ìˆ˜ ë°˜í™˜\n",
    "    return {\"key\": \"answer_v_reference_score\", \"score\": score}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'sql-agent-eval-b55dacd7' at:\n",
      "https://smith.langchain.com/o/53db61eb-d386-4c02-9de8-5b28476b0edc/datasets/4c01e9c9-dfee-427a-8251-cebd69bbc095/compare?selectedSessions=f1513783-462d-43a7-8642-be3dfb0a3e9b\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad37ab8e1ca9441fa02d8dde0b0b0cd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  model_check_query (call_BVj9xlvpkd5bAp6j6YFo1ZOF)\n",
      " Call ID: call_BVj9xlvpkd5bAp6j6YFo1ZOF\n",
      "  Args:\n",
      "    state: {'messages': [{'content': 'What was the most purchased track of 2013?', 'type': 'human'}, {'content': 'CREATE TABLE \"Invoice\" (\\n\\t\"InvoiceId\" INTEGER NOT NULL, \\n\\t\"CustomerId\" INTEGER NOT NULL, \\n\\t\"InvoiceDate\" DATETIME NOT NULL, \\n\\t\"BillingAddress\" NVARCHAR(70), \\n\\t\"BillingCity\" NVARCHAR(40), \\n\\t\"BillingState\" NVARCHAR(40), \\n\\t\"BillingCountry\" NVARCHAR(40), \\n\\t\"BillingPostalCode\" NVARCHAR(10), \\n\\t\"Total\" NUMERIC(10, 2) NOT NULL, \\n\\tPRIMARY KEY (\"InvoiceId\"), \\n\\tFOREIGN KEY(\"CustomerId\") REFERENCES \"Customer\" (\"CustomerId\")\\n)\\n\\n/*\\n3 rows from Invoice table:\\nInvoiceId\\tCustomerId\\tInvoiceDate\\tBillingAddress\\tBillingCity\\tBillingState\\tBillingCountry\\tBillingPostalCode\\tTotal\\n1\\t2\\t2009-01-01 00:00:00\\tTheodor-Heuss-StraÃŸe 34\\tStuttgart\\tNone\\tGermany\\t70174\\t1.98\\n2\\t4\\t2009-01-02 00:00:00\\tUllevÃ¥lsveien 14\\tOslo\\tNone\\tNorway\\t0171\\t3.96\\n3\\t8\\t2009-01-03 00:00:00\\tGrÃ©trystraat 63\\tBrussels\\tNone\\tBelgium\\t1000\\t5.94\\n]', 'type': 'ai'}, {'content': 'CREATE TABLE \"InvoiceLine\" (\\n\\t\"InvoiceLineId\" INTEGER NOT NULL, \\n\\t\"InvoiceId\" INTEGER NOT NULL, \\n\\t\"TrackId\" INTEGER NOT NULL, \\n\\t\"UnitPrice\" NUMERIC(10, 2) NOT NULL, \\n\\t\"Quantity\" INTEGER NOT NULL, \\n\\tPRIMARY KEY (\"InvoiceLineId\"), \\n\\tFOREIGN KEY(\"TrackId\") REFERENCES \"Track\" (\"TrackId\"), \\n\\tFOREIGN KEY(\"InvoiceId\") REFERENCES \"Invoice\" (\"InvoiceId\")\\n)\\n\\n/*\\n3 rows from InvoiceLine table:\\nInvoiceLineId\\tInvoiceId\\tTrackId\\tUnitPrice\\tQuantity\\n1\\t1\\t2\\t0.99\\t1\\n2\\t1\\t4\\t0.99\\t1\\n3\\t2\\t6\\t0.99\\t1\\n]', 'type': 'ai'}, {'content': 'CREATE TABLE \"Track\" (\\n\\t\"TrackId\" INTEGER NOT NULL, \\n\\t\"Name\" NVARCHAR(200) NOT NULL, \\n\\t\"AlbumId\" INTEGER, \\n\\t\"MediaTypeId\" INTEGER NOT NULL, \\n\\t\"GenreId\" INTEGER, \\n\\t\"Composer\" NVARCHAR(220), \\n\\t\"Milliseconds\" INTEGER NOT NULL, \\n\\t\"Bytes\" INTEGER, \\n\\t\"UnitPrice\" NUMERIC(10, 2) NOT NULL, \\n\\tPRIMARY KEY (\"TrackId\"), \\n\\tFOREIGN KEY(\"MediaTypeId\") REFERENCES \"MediaType\" (\"MediaTypeId\"), \\n\\tFOREIGN KEY(\"GenreId\") REFERENCES \"Genre\" (\"GenreId\"), \\n\\tFOREIGN KEY(\"AlbumId\") REFERENCES \"Album\" (\"AlbumId\")\\n)\\n\\n/*\\n3 rows from Track table:\\nTrackId\\tName\\tAlbumId\\tMediaTypeId\\tGenreId\\tComposer\\tMilliseconds\\tBytes\\tUnitPrice\\n1\\tFor Those About To Rock (We Salute You)\\t1\\t1\\t1\\tAngus Young, Malcolm Young, Brian Johnson\\t343719\\t11170334\\t0.99\\n2\\tBalls to the Wall\\t2\\t2\\t1\\tNone\\t342562\\t5510424\\t0.99\\n3\\tFast As a Shark\\t3\\t2\\t1\\tF. Baltes, S. Kaufman, U. Dirkscneider & W. Hoffman\\t230619\\t3990994\\t0.99\\n]', 'type': 'ai'}]}\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "```sql\n",
      "SELECT \n",
      "    t.Name, \n",
      "    SUM(il.Quantity) AS TotalPurchases\n",
      "FROM \n",
      "    Track t\n",
      "JOIN \n",
      "    InvoiceLine il ON t.TrackId = il.TrackId\n",
      "JOIN \n",
      "    Invoice i ON il.InvoiceId = i.InvoiceId\n",
      "WHERE \n",
      "    strftime('%Y', i.InvoiceDate) = '2013'\n",
      "GROUP BY \n",
      "    t.TrackId\n",
      "ORDER BY \n",
      "    TotalPurchases DESC\n",
      "LIMIT 1;\n",
      "```\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Answer: The most purchased track of 2013 was \"Hot Girl\" with 1 purchase.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  model_check_query (call_tk3zfYh3Kc0VeCJnNOYNpAn5)\n",
      " Call ID: call_tk3zfYh3Kc0VeCJnNOYNpAn5\n",
      "  Args:\n",
      "    state: {'messages': [{'content': \"Which country's customers spent the most? And how much did they spend?\", 'type': 'human'}, {'content': 'SELECT Customer.Country, SUM(Invoice.Total) as TotalSpent FROM Customer JOIN Invoice ON Customer.CustomerId = Invoice.CustomerId GROUP BY Customer.Country ORDER BY TotalSpent DESC LIMIT 1;', 'type': 'ai'}]}\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "SELECT Customer.Country, SUM(Invoice.Total) as TotalSpent FROM Customer JOIN Invoice ON Customer.CustomerId = Invoice.CustomerId GROUP BY Customer.Country ORDER BY TotalSpent DESC LIMIT 1;\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Answer: The country's customers who spent the most are from the USA, with a total spending of 523.06.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  model_check_query (call_TQ16mXLb3XDpunzOBcdNBBAp)\n",
      " Call ID: call_TQ16mXLb3XDpunzOBcdNBBAp\n",
      "  Args:\n",
      "    state: {'messages': [{'content': 'How many albums does the artist Led Zeppelin have?', 'type': 'human'}, {'content': 'Album, Artist, Customer, Employee, Genre, Invoice, InvoiceLine, MediaType, Playlist, PlaylistTrack, Track', 'type': 'function', 'name': 'sql_db_list_tables'}, {'content': '\\nCREATE TABLE \"Album\" (\\n\\t\"AlbumId\" INTEGER NOT NULL, \\n\\t\"Title\" NVARCHAR(160) NOT NULL, \\n\\t\"ArtistId\" INTEGER NOT NULL, \\n\\tPRIMARY KEY (\"AlbumId\"), \\n\\tFOREIGN KEY(\"ArtistId\") REFERENCES \"Artist\" (\"ArtistId\")\\n)\\n\\n/*\\n3 rows from Album table:\\nAlbumId\\tTitle\\tArtistId\\n1\\tFor Those About To Rock We Salute You\\t1\\n2\\tBalls to the Wall\\t2\\n3\\tRestless and Wild\\t2\\n*/\\n\\n\\nCREATE TABLE \"Artist\" (\\n\\t\"ArtistId\" INTEGER NOT NULL, \\n\\t\"Name\" NVARCHAR(120), \\n\\tPRIMARY KEY (\"ArtistId\")\\n)\\n\\n/*\\n3 rows from Artist table:\\nArtistId\\tName\\n1\\tAC/DC\\n2\\tAccept\\n3\\tAerosmith\\n*/', 'type': 'function', 'name': 'sql_db_schema'}]}\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "SELECT COUNT(*) \n",
      "FROM Album \n",
      "JOIN Artist ON Album.ArtistId = Artist.ArtistId \n",
      "WHERE Artist.Name = 'Led Zeppelin';\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Answer: Led Zeppelin has 14 albums.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "To determine which sales agent made the most in sales in 2009, we need to join the `Employee`, `Invoice`, and `InvoiceLine` tables. We will filter the invoices by the year 2009 and sum the total sales for each sales agent. Here's the query to achieve that:\n",
      "\n",
      "```sql\n",
      "SELECT \n",
      "    e.FirstName || ' ' || e.LastName AS SalesAgent,\n",
      "    SUM(i.Total) AS TotalSales\n",
      "FROM \n",
      "    Employee e\n",
      "JOIN \n",
      "    Customer c ON e.EmployeeId = c.SupportRepId\n",
      "JOIN \n",
      "    Invoice i ON c.CustomerId = i.CustomerId\n",
      "WHERE \n",
      "    strftime('%Y', i.InvoiceDate) = '2009'\n",
      "GROUP BY \n",
      "    e.EmployeeId\n",
      "ORDER BY \n",
      "    TotalSales DESC\n",
      "LIMIT 1;\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error running target function: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-Soffyuxth7vsUnobnzk0MGeR on tokens per min (TPM): Limit 30000, Used 28954, Requested 1073. Please try again in 54ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/evaluation/_runner.py\", line 1924, in _forward\n",
      "    fn(*args, langsmith_extra=langsmith_extra)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/run_helpers.py\", line 703, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"/var/folders/sj/3r49bs6178v7nfhcxh7lpzd80000gn/T/ipykernel_43129/2799409917.py\", line 10, in predict_sql_agent_answer\n",
      "    messages = app.invoke(inputs, config)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/main.py\", line 3026, in invoke\n",
      "    for chunk in self.stream(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/main.py\", line 2647, in stream\n",
      "    for _ in runner.tick(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/_runner.py\", line 162, in tick\n",
      "    run_with_retry(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/_retry.py\", line 42, in run_with_retry\n",
      "    return task.proc.invoke(task.input, config)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/_internal/_runnable.py\", line 657, in invoke\n",
      "    input = context.run(step.invoke, input, config, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/_internal/_runnable.py\", line 401, in invoke\n",
      "    ret = self.func(*args, **kwargs)\n",
      "  File \"/var/folders/sj/3r49bs6178v7nfhcxh7lpzd80000gn/T/ipykernel_43129/266772150.py\", line 116, in query_gen_node\n",
      "    message = query_gen.invoke(state)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3082, in invoke\n",
      "    input_ = context.run(step.invoke, input_, config)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 5495, in invoke\n",
      "    return self.bound.invoke(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 393, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 1019, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 837, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 1085, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_openai/chat_models/base.py\", line 1183, in _generate\n",
      "    raise e\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_openai/chat_models/base.py\", line 1178, in _generate\n",
      "    raw_response = self.client.with_raw_response.create(**payload)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_legacy_response.py\", line 364, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], func(*args, **kwargs))\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1147, in create\n",
      "    return self._post(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-Soffyuxth7vsUnobnzk0MGeR on tokens per min (TPM): Limit 30000, Used 28954, Requested 1073. Please try again in 54ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running evaluator <DynamicRunEvaluator answer_evaluator> on run da51e121-9705-4207-bada-8ead922c3367: KeyError('response')\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/evaluation/_runner.py\", line 1620, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/evaluation/evaluator.py\", line 351, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/run_helpers.py\", line 703, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"/var/folders/sj/3r49bs6178v7nfhcxh7lpzd80000gn/T/ipykernel_43129/316870680.py\", line 15, in answer_evaluator\n",
      "    prediction = run.outputs[\"response\"]\n",
      "KeyError: 'response'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  model_check_query (call_OHFP0VpCv4485aDnVHL91kiS)\n",
      " Call ID: call_OHFP0VpCv4485aDnVHL91kiS\n",
      "  Args:\n",
      "    state: {'messages': [{'content': 'What is the total price for the album â€œBig Onesâ€?', 'type': 'human'}, {'content': 'Album, Artist, Customer, Employee, Genre, Invoice, InvoiceLine, MediaType, Playlist, PlaylistTrack, Track', 'type': 'function', 'name': 'sql_db_list_tables'}, {'content': 'CREATE TABLE \"Album\" (\\n\\t\"AlbumId\" INTEGER NOT NULL, \\n\\t\"Title\" NVARCHAR(160) NOT NULL, \\n\\t\"ArtistId\" INTEGER NOT NULL, \\n\\tPRIMARY KEY (\"AlbumId\"), \\n\\tFOREIGN KEY(\"ArtistId\") REFERENCES \"Artist\" (\"ArtistId\")\\n)\\n\\n/*\\n3 rows from Album table:\\nAlbumId\\tTitle\\tArtistId\\n1\\tFor Those About To Rock We Salute You\\t1\\n2\\tBalls to the Wall\\t2\\n3\\tRestless and Wild\\t2\\n*/', 'type': 'function', 'name': 'sql_db_schema'}, {'content': 'CREATE TABLE \"InvoiceLine\" (\\n\\t\"InvoiceLineId\" INTEGER NOT NULL, \\n\\t\"InvoiceId\" INTEGER NOT NULL, \\n\\t\"TrackId\" INTEGER NOT NULL, \\n\\t\"UnitPrice\" NUMERIC(10, 2) NOT NULL, \\n\\t\"Quantity\" INTEGER NOT NULL, \\n\\tPRIMARY KEY (\"InvoiceLineId\"), \\n\\tFOREIGN KEY(\"TrackId\") REFERENCES \"Track\" (\"TrackId\"), \\n\\tFOREIGN KEY(\"InvoiceId\") REFERENCES \"Invoice\" (\"InvoiceId\")\\n)\\n\\n/*\\n3 rows from InvoiceLine table:\\nInvoiceLineId\\tInvoiceId\\tTrackId\\tUnitPrice\\tQuantity\\n1\\t1\\t2\\t0.99\\t1\\n2\\t1\\t4\\t0.99\\t1\\n3\\t2\\t6\\t0.99\\t1\\n*/', 'type': 'function', 'name': 'sql_db_schema'}]}\n",
      "  model_check_query (call_sQEFPg8QnNFNiG76jfUDYzha)\n",
      " Call ID: call_sQEFPg8QnNFNiG76jfUDYzha\n",
      "  Args:\n",
      "    state: {'messages': [{'content': 'What is the total price for the album â€œBig Onesâ€?', 'type': 'human'}, {'content': 'Album, Artist, Customer, Employee, Genre, Invoice, InvoiceLine, MediaType, Playlist, PlaylistTrack, Track', 'type': 'function', 'name': 'sql_db_list_tables'}, {'content': 'CREATE TABLE \"Album\" (\\n\\t\"AlbumId\" INTEGER NOT NULL, \\n\\t\"Title\" NVARCHAR(160) NOT NULL, \\n\\t\"ArtistId\" INTEGER NOT NULL, \\n\\tPRIMARY KEY (\"AlbumId\"), \\n\\tFOREIGN KEY(\"ArtistId\") REFERENCES \"Artist\" (\"ArtistId\")\\n)\\n\\n/*\\n3 rows from Album table:\\nAlbumId\\tTitle\\tArtistId\\n1\\tFor Those About To Rock We Salute You\\t1\\n2\\tBalls to the Wall\\t2\\n3\\tRestless and Wild\\t2\\n*/', 'type': 'function', 'name': 'sql_db_schema'}, {'content': 'CREATE TABLE \"InvoiceLine\" (\\n\\t\"InvoiceLineId\" INTEGER NOT NULL, \\n\\t\"InvoiceId\" INTEGER NOT NULL, \\n\\t\"TrackId\" INTEGER NOT NULL, \\n\\t\"UnitPrice\" NUMERIC(10, 2) NOT NULL, \\n\\t\"Quantity\" INTEGER NOT NULL, \\n\\tPRIMARY KEY (\"InvoiceLineId\"), \\n\\tFOREIGN KEY(\"TrackId\") REFERENCES \"Track\" (\"TrackId\"), \\n\\tFOREIGN KEY(\"InvoiceId\") REFERENCES \"Invoice\" (\"InvoiceId\")\\n)\\n\\n/*\\n3 rows from InvoiceLine table:\\nInvoiceLineId\\tInvoiceId\\tTrackId\\tUnitPrice\\tQuantity\\n1\\t1\\t2\\t0.99\\t1\\n2\\t1\\t4\\t0.99\\t1\\n3\\t2\\t6\\t0.99\\t1\\n*/', 'type': 'function', 'name': 'sql_db_schema'}]}\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "To find the total price for the album \"Big Ones\", we need to join the Album, Track, and InvoiceLine tables. Here's the query to achieve that:\n",
      "\n",
      "```sql\n",
      "SELECT SUM(InvoiceLine.UnitPrice * InvoiceLine.Quantity) AS TotalPrice\n",
      "FROM Album\n",
      "JOIN Track ON Album.AlbumId = Track.AlbumId\n",
      "JOIN InvoiceLine ON Track.TrackId = InvoiceLine.TrackId\n",
      "WHERE Album.Title = 'Big Ones';\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error running target function: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-Soffyuxth7vsUnobnzk0MGeR on tokens per min (TPM): Limit 30000, Used 30000, Requested 706. Please try again in 1.412s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/evaluation/_runner.py\", line 1924, in _forward\n",
      "    fn(*args, langsmith_extra=langsmith_extra)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/run_helpers.py\", line 703, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"/var/folders/sj/3r49bs6178v7nfhcxh7lpzd80000gn/T/ipykernel_43129/2799409917.py\", line 10, in predict_sql_agent_answer\n",
      "    messages = app.invoke(inputs, config)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/main.py\", line 3026, in invoke\n",
      "    for chunk in self.stream(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/main.py\", line 2647, in stream\n",
      "    for _ in runner.tick(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/_runner.py\", line 162, in tick\n",
      "    run_with_retry(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/_retry.py\", line 42, in run_with_retry\n",
      "    return task.proc.invoke(task.input, config)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/_internal/_runnable.py\", line 657, in invoke\n",
      "    input = context.run(step.invoke, input, config, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/_internal/_runnable.py\", line 401, in invoke\n",
      "    ret = self.func(*args, **kwargs)\n",
      "  File \"/var/folders/sj/3r49bs6178v7nfhcxh7lpzd80000gn/T/ipykernel_43129/266772150.py\", line 116, in query_gen_node\n",
      "    message = query_gen.invoke(state)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3082, in invoke\n",
      "    input_ = context.run(step.invoke, input_, config)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 5495, in invoke\n",
      "    return self.bound.invoke(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 393, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 1019, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 837, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 1085, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_openai/chat_models/base.py\", line 1183, in _generate\n",
      "    raise e\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_openai/chat_models/base.py\", line 1178, in _generate\n",
      "    raw_response = self.client.with_raw_response.create(**payload)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_legacy_response.py\", line 364, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], func(*args, **kwargs))\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1147, in create\n",
      "    return self._post(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-Soffyuxth7vsUnobnzk0MGeR on tokens per min (TPM): Limit 30000, Used 30000, Requested 706. Please try again in 1.412s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running evaluator <DynamicRunEvaluator answer_evaluator> on run 643b50b9-f8ef-47d4-9251-b3aef268cb0b: KeyError('response')\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/evaluation/_runner.py\", line 1620, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/evaluation/evaluator.py\", line 351, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/run_helpers.py\", line 703, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"/var/folders/sj/3r49bs6178v7nfhcxh7lpzd80000gn/T/ipykernel_43129/316870680.py\", line 15, in answer_evaluator\n",
      "    prediction = run.outputs[\"response\"]\n",
      "KeyError: 'response'\n",
      "Error running target function: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-Soffyuxth7vsUnobnzk0MGeR on tokens per min (TPM): Limit 30000, Used 30000, Requested 41. Please try again in 82ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/evaluation/_runner.py\", line 1924, in _forward\n",
      "    fn(*args, langsmith_extra=langsmith_extra)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/run_helpers.py\", line 703, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"/var/folders/sj/3r49bs6178v7nfhcxh7lpzd80000gn/T/ipykernel_43129/2799409917.py\", line 10, in predict_sql_agent_answer\n",
      "    messages = app.invoke(inputs, config)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/main.py\", line 3026, in invoke\n",
      "    for chunk in self.stream(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/main.py\", line 2647, in stream\n",
      "    for _ in runner.tick(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/_runner.py\", line 162, in tick\n",
      "    run_with_retry(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/_retry.py\", line 42, in run_with_retry\n",
      "    return task.proc.invoke(task.input, config)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/_internal/_runnable.py\", line 657, in invoke\n",
      "    input = context.run(step.invoke, input, config, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/_internal/_runnable.py\", line 401, in invoke\n",
      "    ret = self.func(*args, **kwargs)\n",
      "  File \"/var/folders/sj/3r49bs6178v7nfhcxh7lpzd80000gn/T/ipykernel_43129/266772150.py\", line 65, in <lambda>\n",
      "    \"messages\": [model_get_schema.invoke(state[\"messages\"])],\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 5495, in invoke\n",
      "    return self.bound.invoke(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 393, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 1019, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 837, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 1085, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_openai/chat_models/base.py\", line 1183, in _generate\n",
      "    raise e\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_openai/chat_models/base.py\", line 1178, in _generate\n",
      "    raw_response = self.client.with_raw_response.create(**payload)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_legacy_response.py\", line 364, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], func(*args, **kwargs))\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1147, in create\n",
      "    return self._post(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-Soffyuxth7vsUnobnzk0MGeR on tokens per min (TPM): Limit 30000, Used 30000, Requested 41. Please try again in 82ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running evaluator <DynamicRunEvaluator answer_evaluator> on run dfc83b27-63ba-40a2-ac36-32a593c30323: KeyError('response')\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/evaluation/_runner.py\", line 1620, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/evaluation/evaluator.py\", line 351, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/run_helpers.py\", line 703, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"/var/folders/sj/3r49bs6178v7nfhcxh7lpzd80000gn/T/ipykernel_43129/316870680.py\", line 15, in answer_evaluator\n",
      "    prediction = run.outputs[\"response\"]\n",
      "KeyError: 'response'\n",
      "Error running target function: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-Soffyuxth7vsUnobnzk0MGeR on tokens per min (TPM): Limit 30000, Used 30000, Requested 48. Please try again in 96ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/evaluation/_runner.py\", line 1924, in _forward\n",
      "    fn(*args, langsmith_extra=langsmith_extra)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/run_helpers.py\", line 703, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"/var/folders/sj/3r49bs6178v7nfhcxh7lpzd80000gn/T/ipykernel_43129/2799409917.py\", line 10, in predict_sql_agent_answer\n",
      "    messages = app.invoke(inputs, config)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/main.py\", line 3026, in invoke\n",
      "    for chunk in self.stream(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/main.py\", line 2647, in stream\n",
      "    for _ in runner.tick(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/_runner.py\", line 162, in tick\n",
      "    run_with_retry(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/_retry.py\", line 42, in run_with_retry\n",
      "    return task.proc.invoke(task.input, config)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/_internal/_runnable.py\", line 657, in invoke\n",
      "    input = context.run(step.invoke, input, config, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/_internal/_runnable.py\", line 401, in invoke\n",
      "    ret = self.func(*args, **kwargs)\n",
      "  File \"/var/folders/sj/3r49bs6178v7nfhcxh7lpzd80000gn/T/ipykernel_43129/266772150.py\", line 65, in <lambda>\n",
      "    \"messages\": [model_get_schema.invoke(state[\"messages\"])],\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 5495, in invoke\n",
      "    return self.bound.invoke(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 393, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 1019, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 837, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 1085, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_openai/chat_models/base.py\", line 1183, in _generate\n",
      "    raise e\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_openai/chat_models/base.py\", line 1178, in _generate\n",
      "    raw_response = self.client.with_raw_response.create(**payload)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_legacy_response.py\", line 364, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], func(*args, **kwargs))\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1147, in create\n",
      "    return self._post(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-Soffyuxth7vsUnobnzk0MGeR on tokens per min (TPM): Limit 30000, Used 30000, Requested 48. Please try again in 96ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running evaluator <DynamicRunEvaluator answer_evaluator> on run 4088f72c-55a2-4d4a-b859-8fcf0f319732: KeyError('response')\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/evaluation/_runner.py\", line 1620, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/evaluation/evaluator.py\", line 351, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/run_helpers.py\", line 703, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"/var/folders/sj/3r49bs6178v7nfhcxh7lpzd80000gn/T/ipykernel_43129/316870680.py\", line 15, in answer_evaluator\n",
      "    prediction = run.outputs[\"response\"]\n",
      "KeyError: 'response'\n",
      "Error running target function: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-Soffyuxth7vsUnobnzk0MGeR on tokens per min (TPM): Limit 30000, Used 29979, Requested 43. Please try again in 44ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/evaluation/_runner.py\", line 1924, in _forward\n",
      "    fn(*args, langsmith_extra=langsmith_extra)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/run_helpers.py\", line 703, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"/var/folders/sj/3r49bs6178v7nfhcxh7lpzd80000gn/T/ipykernel_43129/2799409917.py\", line 10, in predict_sql_agent_answer\n",
      "    messages = app.invoke(inputs, config)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/main.py\", line 3026, in invoke\n",
      "    for chunk in self.stream(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/main.py\", line 2647, in stream\n",
      "    for _ in runner.tick(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/_runner.py\", line 162, in tick\n",
      "    run_with_retry(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/_retry.py\", line 42, in run_with_retry\n",
      "    return task.proc.invoke(task.input, config)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/_internal/_runnable.py\", line 657, in invoke\n",
      "    input = context.run(step.invoke, input, config, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/_internal/_runnable.py\", line 401, in invoke\n",
      "    ret = self.func(*args, **kwargs)\n",
      "  File \"/var/folders/sj/3r49bs6178v7nfhcxh7lpzd80000gn/T/ipykernel_43129/266772150.py\", line 65, in <lambda>\n",
      "    \"messages\": [model_get_schema.invoke(state[\"messages\"])],\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 5495, in invoke\n",
      "    return self.bound.invoke(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 393, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 1019, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 837, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 1085, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_openai/chat_models/base.py\", line 1183, in _generate\n",
      "    raise e\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_openai/chat_models/base.py\", line 1178, in _generate\n",
      "    raw_response = self.client.with_raw_response.create(**payload)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_legacy_response.py\", line 364, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], func(*args, **kwargs))\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1147, in create\n",
      "    return self._post(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-Soffyuxth7vsUnobnzk0MGeR on tokens per min (TPM): Limit 30000, Used 29979, Requested 43. Please try again in 44ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running evaluator <DynamicRunEvaluator answer_evaluator> on run 625bfa86-60e8-4952-84bf-4117fd3a08ba: KeyError('response')\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/evaluation/_runner.py\", line 1620, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/evaluation/evaluator.py\", line 351, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/run_helpers.py\", line 703, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"/var/folders/sj/3r49bs6178v7nfhcxh7lpzd80000gn/T/ipykernel_43129/316870680.py\", line 15, in answer_evaluator\n",
      "    prediction = run.outputs[\"response\"]\n",
      "KeyError: 'response'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  model_check_query (call_YrcMwHj4LaZWfUQiTPvf9jOI)\n",
      " Call ID: call_YrcMwHj4LaZWfUQiTPvf9jOI\n",
      "  Args:\n",
      "    state: {'messages': [{'content': 'Which sales agent made the most in sales in 2009?', 'type': 'human'}, {'content': 'Album, Artist, Customer, Employee, Genre, Invoice, InvoiceLine, MediaType, Playlist, PlaylistTrack, Track', 'type': 'function', 'name': 'sql_db_list_tables', 'id': 'b1b0b3b3-0b0b-4b0b-9b0b-0b0b0b0b0b0b'}, {'content': 'parallel', 'type': 'tool_use', 'tool_calls': [{'name': 'functions.sql_db_schema', 'args': '{\"table_names\":\"Employee\"}', 'id': 'b1b0b3b3-0b0b-4b0b-9b0b-0b0b0b0b0b0b'}, {'name': 'functions.sql_db_schema', 'args': '{\"table_names\":\"Invoice\"}', 'id': 'b1b0b3b3-0b0b-4b0b-9b0b-0b0b0b0b0b0b'}], 'id': 'b1b0b3b3-0b0b-4b0b-9b0b-0b0b0b0b0b0b'}, {'content': '\\nCREATE TABLE \"Employee\" (\\n\\t\"EmployeeId\" INTEGER NOT NULL, \\n\\t\"LastName\" NVARCHAR(20) NOT NULL, \\n\\t\"FirstName\" NVARCHAR(20) NOT NULL, \\n\\t\"Title\" NVARCHAR(30), \\n\\t\"ReportsTo\" INTEGER, \\n\\t\"BirthDate\" DATETIME, \\n\\t\"HireDate\" DATETIME, \\n\\t\"Address\" NVARCHAR(70), \\n\\t\"City\" NVARCHAR(40), \\n\\t\"State\" NVARCHAR(40), \\n\\t\"Country\" NVARCHAR(40), \\n\\t\"PostalCode\" NVARCHAR(10), \\n\\t\"Phone\" NVARCHAR(24), \\n\\t\"Fax\" NVARCHAR(24), \\n\\t\"Email\" NVARCHAR(60), \\n\\tPRIMARY KEY (\"EmployeeId\"), \\n\\tFOREIGN KEY(\"ReportsTo\") REFERENCES \"Employee\" (\"EmployeeId\")\\n)\\n\\n/*\\n3 rows from Employee table:\\nEmployeeId\\tLastName\\tFirstName\\tTitle\\tReportsTo\\tBirthDate\\tHireDate\\tAddress\\tCity\\tState\\tCountry\\tPostalCode\\tPhone\\tFax\\tEmail\\n1\\tAdams\\tAndrew\\tGeneral Manager\\tNone\\t1962-02-18 00:00:00\\t2002-08-14 00:00:00\\t11120 Jasper Ave NW\\tEdmonton\\tAB\\tCanada\\tT5K 2N1\\t+1 (780) 428-9482\\t+1 (780) 428-3457\\tandrew@chinookcorp.com\\n2\\tEdwards\\tNancy\\tSales Manager\\t1\\t1958-12-08 00:00:00\\t2002-05-01 00:00:00\\t825 8 Ave SW\\tCalgary\\tAB\\tCanada\\tT2P 2T3\\t+1 (403) 262-3443\\t+1 (403) 262-3322\\tnancy@chinookcorp.com\\n3\\tPeacock\\tJane\\tSales Support Agent\\t2\\t1973-08-29 00:00:00\\t2002-04-01 00:00:00\\t1111 6 Ave SW\\tCalgary\\tAB\\tCanada\\tT2P 5M5\\t+1 (403) 262-3443\\t+1 (403) 262-6712\\tjane@chinookcorp.com\\n*/', 'type': 'function', 'name': 'sql_db_schema', 'id': 'b1b0b3b3-0b0b-4b0b-9b0b-0b0b0b0b0b0'}, {'content': '\\nCREATE TABLE \"Invoice\" (\\n\\t\"InvoiceId\" INTEGER NOT NULL, \\n\\t\"CustomerId\" INTEGER NOT NULL, \\n\\t\"InvoiceDate\" DATETIME NOT NULL, \\n\\t\"BillingAddress\" NVARCHAR(70), \\n\\t\"BillingCity\" NVARCHAR(40), \\n\\t\"BillingState\" NVARCHAR(40), \\n\\t\"BillingCountry\" NVARCHAR(40), \\n\\t\"BillingPostalCode\" NVARCHAR(10), \\n\\t\"Total\" NUMERIC(10, 2) NOT NULL, \\n\\tPRIMARY KEY (\"InvoiceId\"), \\n\\tFOREIGN KEY(\"CustomerId\") REFERENCES \"Customer\" (\"CustomerId\")\\n)\\n\\n/*\\n3 rows from Invoice table:\\nInvoiceId\\tCustomerId\\tInvoiceDate\\tBillingAddress\\tBillingCity\\tBillingState\\tBillingCountry\\tBillingPostalCode\\tTotal\\n1\\t2\\t2009-01-01 00:00:00\\tTheodor-Heuss-StraÃŸe 34\\tStuttgart\\tNone\\tGermany\\t70174\\t1.98\\n2\\t4\\t2009-01-02 00:00:00\\tUllevÃ¥lsveien 14\\tOslo\\tNone\\tNorway\\t0171\\t3.96\\n3\\t8\\t2009-01-03 00:00:00\\tGrÃ©trystraat 63\\tBrussels\\tNone\\tBelgium\\t1000\\t5.94\\n*/', 'type': 'function', 'name': 'sql_db_schema', 'id': 'b1b0b3b3-0b0b-4b0b-9b0b-0b0b0b0b0b0'}]}\n",
      "    query: SELECT e.FirstName, e.LastName, SUM(i.Total) as TotalSales FROM Employee e JOIN Customer c ON e.EmployeeId = c.SupportRepId JOIN Invoice i ON c.CustomerId = i.CustomerId WHERE strftime('%Y', i.InvoiceDate) = '2009' GROUP BY e.EmployeeId ORDER BY TotalSales DESC LIMIT 1;\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "SELECT e.FirstName, e.LastName, SUM(i.Total) as TotalSales\n",
      "FROM Employee e\n",
      "JOIN Customer c ON e.EmployeeId = c.SupportRepId\n",
      "JOIN Invoice i ON c.CustomerId = i.CustomerId\n",
      "WHERE strftime('%Y', i.InvoiceDate) = '2009'\n",
      "GROUP BY e.EmployeeId\n",
      "ORDER BY TotalSales DESC\n",
      "LIMIT 1;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error running target function: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-Soffyuxth7vsUnobnzk0MGeR on tokens per min (TPM): Limit 30000, Used 30000, Requested 229. Please try again in 458ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/evaluation/_runner.py\", line 1924, in _forward\n",
      "    fn(*args, langsmith_extra=langsmith_extra)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/run_helpers.py\", line 703, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"/var/folders/sj/3r49bs6178v7nfhcxh7lpzd80000gn/T/ipykernel_43129/2799409917.py\", line 10, in predict_sql_agent_answer\n",
      "    messages = app.invoke(inputs, config)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/main.py\", line 3026, in invoke\n",
      "    for chunk in self.stream(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/main.py\", line 2647, in stream\n",
      "    for _ in runner.tick(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/_runner.py\", line 162, in tick\n",
      "    run_with_retry(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/_retry.py\", line 42, in run_with_retry\n",
      "    return task.proc.invoke(task.input, config)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/_internal/_runnable.py\", line 657, in invoke\n",
      "    input = context.run(step.invoke, input, config, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/_internal/_runnable.py\", line 401, in invoke\n",
      "    ret = self.func(*args, **kwargs)\n",
      "  File \"/var/folders/sj/3r49bs6178v7nfhcxh7lpzd80000gn/T/ipykernel_43129/266772150.py\", line 46, in model_check_query\n",
      "    return {\"messages\": [query_check.invoke({\"messages\": [state[\"messages\"][-1]]})]}\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3082, in invoke\n",
      "    input_ = context.run(step.invoke, input_, config)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 5495, in invoke\n",
      "    return self.bound.invoke(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 393, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 1019, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 837, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 1085, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_openai/chat_models/base.py\", line 1183, in _generate\n",
      "    raise e\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_openai/chat_models/base.py\", line 1178, in _generate\n",
      "    raw_response = self.client.with_raw_response.create(**payload)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_legacy_response.py\", line 364, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], func(*args, **kwargs))\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1147, in create\n",
      "    return self._post(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-Soffyuxth7vsUnobnzk0MGeR on tokens per min (TPM): Limit 30000, Used 30000, Requested 229. Please try again in 458ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running evaluator <DynamicRunEvaluator answer_evaluator> on run 94efafea-3ca4-4da2-9dcc-67f46e2d87ce: KeyError('response')\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/evaluation/_runner.py\", line 1620, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/evaluation/evaluator.py\", line 351, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/run_helpers.py\", line 703, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"/var/folders/sj/3r49bs6178v7nfhcxh7lpzd80000gn/T/ipykernel_43129/316870680.py\", line 15, in answer_evaluator\n",
      "    prediction = run.outputs[\"response\"]\n",
      "KeyError: 'response'\n",
      "Error running target function: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-Soffyuxth7vsUnobnzk0MGeR on tokens per min (TPM): Limit 30000, Used 30000, Requested 44. Please try again in 88ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/evaluation/_runner.py\", line 1924, in _forward\n",
      "    fn(*args, langsmith_extra=langsmith_extra)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/run_helpers.py\", line 703, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"/var/folders/sj/3r49bs6178v7nfhcxh7lpzd80000gn/T/ipykernel_43129/2799409917.py\", line 10, in predict_sql_agent_answer\n",
      "    messages = app.invoke(inputs, config)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/main.py\", line 3026, in invoke\n",
      "    for chunk in self.stream(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/main.py\", line 2647, in stream\n",
      "    for _ in runner.tick(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/_runner.py\", line 162, in tick\n",
      "    run_with_retry(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/_retry.py\", line 42, in run_with_retry\n",
      "    return task.proc.invoke(task.input, config)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/_internal/_runnable.py\", line 657, in invoke\n",
      "    input = context.run(step.invoke, input, config, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/_internal/_runnable.py\", line 401, in invoke\n",
      "    ret = self.func(*args, **kwargs)\n",
      "  File \"/var/folders/sj/3r49bs6178v7nfhcxh7lpzd80000gn/T/ipykernel_43129/266772150.py\", line 65, in <lambda>\n",
      "    \"messages\": [model_get_schema.invoke(state[\"messages\"])],\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 5495, in invoke\n",
      "    return self.bound.invoke(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 393, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 1019, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 837, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 1085, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_openai/chat_models/base.py\", line 1183, in _generate\n",
      "    raise e\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_openai/chat_models/base.py\", line 1178, in _generate\n",
      "    raw_response = self.client.with_raw_response.create(**payload)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_legacy_response.py\", line 364, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], func(*args, **kwargs))\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1147, in create\n",
      "    return self._post(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-Soffyuxth7vsUnobnzk0MGeR on tokens per min (TPM): Limit 30000, Used 30000, Requested 44. Please try again in 88ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running evaluator <DynamicRunEvaluator answer_evaluator> on run e2debb50-3f49-4afb-8b6c-cf1841105bf7: KeyError('response')\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/evaluation/_runner.py\", line 1620, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/evaluation/evaluator.py\", line 351, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/run_helpers.py\", line 703, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"/var/folders/sj/3r49bs6178v7nfhcxh7lpzd80000gn/T/ipykernel_43129/316870680.py\", line 15, in answer_evaluator\n",
      "    prediction = run.outputs[\"response\"]\n",
      "KeyError: 'response'\n",
      "Error running target function: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-Soffyuxth7vsUnobnzk0MGeR on tokens per min (TPM): Limit 30000, Used 30000, Requested 41. Please try again in 82ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/evaluation/_runner.py\", line 1924, in _forward\n",
      "    fn(*args, langsmith_extra=langsmith_extra)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/run_helpers.py\", line 703, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"/var/folders/sj/3r49bs6178v7nfhcxh7lpzd80000gn/T/ipykernel_43129/2799409917.py\", line 10, in predict_sql_agent_answer\n",
      "    messages = app.invoke(inputs, config)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/main.py\", line 3026, in invoke\n",
      "    for chunk in self.stream(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/main.py\", line 2647, in stream\n",
      "    for _ in runner.tick(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/_runner.py\", line 162, in tick\n",
      "    run_with_retry(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/_retry.py\", line 42, in run_with_retry\n",
      "    return task.proc.invoke(task.input, config)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/_internal/_runnable.py\", line 657, in invoke\n",
      "    input = context.run(step.invoke, input, config, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/_internal/_runnable.py\", line 401, in invoke\n",
      "    ret = self.func(*args, **kwargs)\n",
      "  File \"/var/folders/sj/3r49bs6178v7nfhcxh7lpzd80000gn/T/ipykernel_43129/266772150.py\", line 65, in <lambda>\n",
      "    \"messages\": [model_get_schema.invoke(state[\"messages\"])],\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 5495, in invoke\n",
      "    return self.bound.invoke(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 393, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 1019, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 837, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 1085, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_openai/chat_models/base.py\", line 1183, in _generate\n",
      "    raise e\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_openai/chat_models/base.py\", line 1178, in _generate\n",
      "    raw_response = self.client.with_raw_response.create(**payload)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_legacy_response.py\", line 364, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], func(*args, **kwargs))\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1147, in create\n",
      "    return self._post(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-Soffyuxth7vsUnobnzk0MGeR on tokens per min (TPM): Limit 30000, Used 30000, Requested 41. Please try again in 82ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running evaluator <DynamicRunEvaluator answer_evaluator> on run 9daf147d-e828-4a07-b626-92e435e16157: KeyError('response')\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/evaluation/_runner.py\", line 1620, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/evaluation/evaluator.py\", line 351, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/run_helpers.py\", line 703, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"/var/folders/sj/3r49bs6178v7nfhcxh7lpzd80000gn/T/ipykernel_43129/316870680.py\", line 15, in answer_evaluator\n",
      "    prediction = run.outputs[\"response\"]\n",
      "KeyError: 'response'\n",
      "Error running target function: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-Soffyuxth7vsUnobnzk0MGeR on tokens per min (TPM): Limit 30000, Used 30000, Requested 48. Please try again in 96ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/evaluation/_runner.py\", line 1924, in _forward\n",
      "    fn(*args, langsmith_extra=langsmith_extra)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/run_helpers.py\", line 703, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"/var/folders/sj/3r49bs6178v7nfhcxh7lpzd80000gn/T/ipykernel_43129/2799409917.py\", line 10, in predict_sql_agent_answer\n",
      "    messages = app.invoke(inputs, config)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/main.py\", line 3026, in invoke\n",
      "    for chunk in self.stream(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/main.py\", line 2647, in stream\n",
      "    for _ in runner.tick(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/_runner.py\", line 162, in tick\n",
      "    run_with_retry(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/_retry.py\", line 42, in run_with_retry\n",
      "    return task.proc.invoke(task.input, config)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/_internal/_runnable.py\", line 657, in invoke\n",
      "    input = context.run(step.invoke, input, config, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/_internal/_runnable.py\", line 401, in invoke\n",
      "    ret = self.func(*args, **kwargs)\n",
      "  File \"/var/folders/sj/3r49bs6178v7nfhcxh7lpzd80000gn/T/ipykernel_43129/266772150.py\", line 65, in <lambda>\n",
      "    \"messages\": [model_get_schema.invoke(state[\"messages\"])],\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 5495, in invoke\n",
      "    return self.bound.invoke(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 393, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 1019, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 837, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 1085, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_openai/chat_models/base.py\", line 1183, in _generate\n",
      "    raise e\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_openai/chat_models/base.py\", line 1178, in _generate\n",
      "    raw_response = self.client.with_raw_response.create(**payload)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_legacy_response.py\", line 364, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], func(*args, **kwargs))\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1147, in create\n",
      "    return self._post(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-Soffyuxth7vsUnobnzk0MGeR on tokens per min (TPM): Limit 30000, Used 30000, Requested 48. Please try again in 96ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running evaluator <DynamicRunEvaluator answer_evaluator> on run 198465a6-314e-4da1-9909-a955d09e158f: KeyError('response')\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/evaluation/_runner.py\", line 1620, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/evaluation/evaluator.py\", line 351, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/run_helpers.py\", line 703, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"/var/folders/sj/3r49bs6178v7nfhcxh7lpzd80000gn/T/ipykernel_43129/316870680.py\", line 15, in answer_evaluator\n",
      "    prediction = run.outputs[\"response\"]\n",
      "KeyError: 'response'\n",
      "Error running target function: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-Soffyuxth7vsUnobnzk0MGeR on tokens per min (TPM): Limit 30000, Used 30000, Requested 43. Please try again in 86ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/evaluation/_runner.py\", line 1924, in _forward\n",
      "    fn(*args, langsmith_extra=langsmith_extra)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/run_helpers.py\", line 703, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"/var/folders/sj/3r49bs6178v7nfhcxh7lpzd80000gn/T/ipykernel_43129/2799409917.py\", line 10, in predict_sql_agent_answer\n",
      "    messages = app.invoke(inputs, config)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/main.py\", line 3026, in invoke\n",
      "    for chunk in self.stream(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/main.py\", line 2647, in stream\n",
      "    for _ in runner.tick(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/_runner.py\", line 162, in tick\n",
      "    run_with_retry(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/_retry.py\", line 42, in run_with_retry\n",
      "    return task.proc.invoke(task.input, config)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/_internal/_runnable.py\", line 657, in invoke\n",
      "    input = context.run(step.invoke, input, config, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/_internal/_runnable.py\", line 401, in invoke\n",
      "    ret = self.func(*args, **kwargs)\n",
      "  File \"/var/folders/sj/3r49bs6178v7nfhcxh7lpzd80000gn/T/ipykernel_43129/266772150.py\", line 65, in <lambda>\n",
      "    \"messages\": [model_get_schema.invoke(state[\"messages\"])],\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 5495, in invoke\n",
      "    return self.bound.invoke(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 393, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 1019, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 837, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 1085, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_openai/chat_models/base.py\", line 1183, in _generate\n",
      "    raise e\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_openai/chat_models/base.py\", line 1178, in _generate\n",
      "    raw_response = self.client.with_raw_response.create(**payload)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_legacy_response.py\", line 364, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], func(*args, **kwargs))\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1147, in create\n",
      "    return self._post(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-Soffyuxth7vsUnobnzk0MGeR on tokens per min (TPM): Limit 30000, Used 30000, Requested 43. Please try again in 86ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running evaluator <DynamicRunEvaluator answer_evaluator> on run fe89b6bf-abb3-403e-a2f1-5d336bb7fdde: KeyError('response')\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/evaluation/_runner.py\", line 1620, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/evaluation/evaluator.py\", line 351, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/run_helpers.py\", line 703, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"/var/folders/sj/3r49bs6178v7nfhcxh7lpzd80000gn/T/ipykernel_43129/316870680.py\", line 15, in answer_evaluator\n",
      "    prediction = run.outputs[\"response\"]\n",
      "KeyError: 'response'\n",
      "Error running target function: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-Soffyuxth7vsUnobnzk0MGeR on tokens per min (TPM): Limit 30000, Used 30000, Requested 43. Please try again in 86ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/evaluation/_runner.py\", line 1924, in _forward\n",
      "    fn(*args, langsmith_extra=langsmith_extra)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/run_helpers.py\", line 703, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"/var/folders/sj/3r49bs6178v7nfhcxh7lpzd80000gn/T/ipykernel_43129/2799409917.py\", line 10, in predict_sql_agent_answer\n",
      "    messages = app.invoke(inputs, config)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/main.py\", line 3026, in invoke\n",
      "    for chunk in self.stream(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/main.py\", line 2647, in stream\n",
      "    for _ in runner.tick(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/_runner.py\", line 162, in tick\n",
      "    run_with_retry(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/_retry.py\", line 42, in run_with_retry\n",
      "    return task.proc.invoke(task.input, config)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/_internal/_runnable.py\", line 657, in invoke\n",
      "    input = context.run(step.invoke, input, config, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/_internal/_runnable.py\", line 401, in invoke\n",
      "    ret = self.func(*args, **kwargs)\n",
      "  File \"/var/folders/sj/3r49bs6178v7nfhcxh7lpzd80000gn/T/ipykernel_43129/266772150.py\", line 65, in <lambda>\n",
      "    \"messages\": [model_get_schema.invoke(state[\"messages\"])],\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 5495, in invoke\n",
      "    return self.bound.invoke(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 393, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 1019, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 837, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 1085, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_openai/chat_models/base.py\", line 1183, in _generate\n",
      "    raise e\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_openai/chat_models/base.py\", line 1178, in _generate\n",
      "    raw_response = self.client.with_raw_response.create(**payload)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_legacy_response.py\", line 364, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], func(*args, **kwargs))\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1147, in create\n",
      "    return self._post(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-Soffyuxth7vsUnobnzk0MGeR on tokens per min (TPM): Limit 30000, Used 30000, Requested 43. Please try again in 86ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running evaluator <DynamicRunEvaluator answer_evaluator> on run 8e773812-02df-40ff-be20-f7fedacf6ebb: KeyError('response')\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/evaluation/_runner.py\", line 1620, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/evaluation/evaluator.py\", line 351, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/run_helpers.py\", line 703, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"/var/folders/sj/3r49bs6178v7nfhcxh7lpzd80000gn/T/ipykernel_43129/316870680.py\", line 15, in answer_evaluator\n",
      "    prediction = run.outputs[\"response\"]\n",
      "KeyError: 'response'\n",
      "Error running target function: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-Soffyuxth7vsUnobnzk0MGeR on tokens per min (TPM): Limit 30000, Used 30000, Requested 44. Please try again in 88ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/evaluation/_runner.py\", line 1924, in _forward\n",
      "    fn(*args, langsmith_extra=langsmith_extra)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/run_helpers.py\", line 703, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"/var/folders/sj/3r49bs6178v7nfhcxh7lpzd80000gn/T/ipykernel_43129/2799409917.py\", line 10, in predict_sql_agent_answer\n",
      "    messages = app.invoke(inputs, config)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/main.py\", line 3026, in invoke\n",
      "    for chunk in self.stream(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/main.py\", line 2647, in stream\n",
      "    for _ in runner.tick(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/_runner.py\", line 162, in tick\n",
      "    run_with_retry(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/_retry.py\", line 42, in run_with_retry\n",
      "    return task.proc.invoke(task.input, config)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/_internal/_runnable.py\", line 657, in invoke\n",
      "    input = context.run(step.invoke, input, config, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/_internal/_runnable.py\", line 401, in invoke\n",
      "    ret = self.func(*args, **kwargs)\n",
      "  File \"/var/folders/sj/3r49bs6178v7nfhcxh7lpzd80000gn/T/ipykernel_43129/266772150.py\", line 65, in <lambda>\n",
      "    \"messages\": [model_get_schema.invoke(state[\"messages\"])],\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 5495, in invoke\n",
      "    return self.bound.invoke(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 393, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 1019, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 837, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 1085, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_openai/chat_models/base.py\", line 1183, in _generate\n",
      "    raise e\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_openai/chat_models/base.py\", line 1178, in _generate\n",
      "    raw_response = self.client.with_raw_response.create(**payload)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_legacy_response.py\", line 364, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], func(*args, **kwargs))\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1147, in create\n",
      "    return self._post(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-Soffyuxth7vsUnobnzk0MGeR on tokens per min (TPM): Limit 30000, Used 30000, Requested 44. Please try again in 88ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running evaluator <DynamicRunEvaluator answer_evaluator> on run 78983cfa-c42f-4f24-962f-8d6cf57637e8: KeyError('response')\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/evaluation/_runner.py\", line 1620, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/evaluation/evaluator.py\", line 351, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/run_helpers.py\", line 703, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"/var/folders/sj/3r49bs6178v7nfhcxh7lpzd80000gn/T/ipykernel_43129/316870680.py\", line 15, in answer_evaluator\n",
      "    prediction = run.outputs[\"response\"]\n",
      "KeyError: 'response'\n"
     ]
    }
   ],
   "source": [
    "from langsmith.evaluation import evaluate\n",
    "\n",
    "# í‰ê°€ìš© ë°ì´í„°ì…‹ ì´ë¦„\n",
    "dataset_name = \"SQL Agent Response\"\n",
    "\n",
    "try:\n",
    "    # í‰ê°€ ì§„í–‰\n",
    "    experiment_results = evaluate(\n",
    "        predict_sql_agent_answer,  # í‰ê°€ì‹œ í™œìš©í•  ì˜ˆì¸¡ í•¨ìˆ˜\n",
    "        data=dataset_name,  # í‰ê°€ìš© ë°ì´í„°ì…‹ ì´ë¦„\n",
    "        evaluators=[answer_evaluator],  # í‰ê°€ì ëª©ë¡\n",
    "        num_repetitions=3,  # ì‹¤í—˜ ë°˜ë³µ íšŸìˆ˜ ì„¤ì •\n",
    "        experiment_prefix=\"sql-agent-eval\",\n",
    "        metadata={\"version\": \"chinook db, sql-agent-eval: gpt-4o\"},  # ì‹¤í—˜ ë©”íƒ€ë°ì´í„°\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. STORM ê°œë…ì„ ë„ì…í•œ ì—°êµ¬ë¥¼ ìœ„í•œ ë©€í‹° ì—ì´ì „íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote.models import get_model_name, LLMs\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# ìµœì‹  ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°\n",
    "GPT4o = get_model_name(LLMs.GPT4o)\n",
    "\n",
    "# ëª¨ë¸ ì´ˆê¸°í™”\n",
    "llm = ChatOpenAI(model=GPT4o)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from typing_extensions import TypedDict\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_teddynote.graphs import visualize_graph\n",
    "\n",
    "\n",
    "# ë¶„ì„ê°€ì˜ ì†ì„±ê³¼ ë©”íƒ€ë°ì´í„°ë¥¼ ì •ì˜í•˜ëŠ” í´ë˜ìŠ¤\n",
    "class Analyst(BaseModel):\n",
    "    # ì£¼ìš” ì†Œì† ì •ë³´\n",
    "    affiliation: str = Field(\n",
    "        description=\"Primary affiliation of the analyst.\",\n",
    "    )\n",
    "    # ì´ë¦„\n",
    "    name: str = Field(description=\"Name of the analyst.\")\n",
    "\n",
    "    # ì—­í• \n",
    "    role: str = Field(\n",
    "        description=\"Role of the analyst in the context of the topic.\",\n",
    "    )\n",
    "    # ì¤‘ì , ìš°ë ¤ ì‚¬í•­ ë° ë™ê¸°ì— ëŒ€í•œ ì„¤ëª…\n",
    "    description: str = Field(\n",
    "        description=\"Description of the analyst focus, concerns, and motives.\",\n",
    "    )\n",
    "\n",
    "    # ë¶„ì„ê°€ì˜ ì¸ì  ì •ë³´ë¥¼ ë¬¸ìì—´ë¡œ ë°˜í™˜í•˜ëŠ” ì†ì„±\n",
    "    @property\n",
    "    def persona(self) -> str:\n",
    "        return f\"Name: {self.name}\\nRole: {self.role}\\nAffiliation: {self.affiliation}\\nDescription: {self.description}\\n\"\n",
    "\n",
    "\n",
    "# ë¶„ì„ê°€ë“¤ì˜ ì§‘í•©\n",
    "class Perspectives(BaseModel):\n",
    "    # ë¶„ì„ê°€ ëª©ë¡\n",
    "    analysts: List[Analyst] = Field(\n",
    "        description=\"Comprehensive list of analysts with their roles and affiliations.\",\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìƒíƒœ ì •ì˜\n",
    "class GenerateAnalystsState(TypedDict):\n",
    "    # ì—°êµ¬ ì£¼ì œ\n",
    "    topic: str\n",
    "    # ìƒì„±í•  ë¶„ì„ê°€ì˜ ìµœëŒ€ ìˆ˜\n",
    "    max_analysts: int\n",
    "    # ì‚¬ëŒ í”¼ë“œë°±\n",
    "    human_analyst_feedback: str\n",
    "    # ë¶„ì„ê°€ ëª©ë¡\n",
    "    analysts: List[Analyst]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "# ë¶„ì„ê°€ ìƒì„± í”„ë¡¬í”„íŠ¸\n",
    "analyst_instructions = \"\"\"You are tasked with creating a set of AI analyst personas. \n",
    "\n",
    "Follow these instructions carefully:\n",
    "1. First, review the research topic:\n",
    "\n",
    "{topic}\n",
    "\n",
    "2. Examine any editorial feedback that has been optionally provided to guide creation of the analysts: \n",
    "\n",
    "{human_analyst_feedback}\n",
    "\n",
    "3. Determine the most interesting themes based upon documents and / or feedback above.\n",
    "\n",
    "4. Pick the top {max_analysts} themes.\n",
    "\n",
    "5. Assign one analyst to each theme.\"\"\"\n",
    "\n",
    "\n",
    "# ë¶„ì„ê°€ ìƒì„± ë…¸ë“œ\n",
    "def create_analysts(state: GenerateAnalystsState):\n",
    "    \"\"\"ë¶„ì„ê°€ í˜ë¥´ì†Œë‚˜ë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜\"\"\"\n",
    "\n",
    "    topic = state[\"topic\"]\n",
    "    max_analysts = state[\"max_analysts\"]\n",
    "    human_analyst_feedback = state.get(\"human_analyst_feedback\", \"\")\n",
    "\n",
    "    # LLMì— êµ¬ì¡°í™”ëœ ì¶œë ¥ í˜•ì‹ì„ ì ìš©\n",
    "    structured_llm = llm.with_structured_output(Perspectives)\n",
    "\n",
    "    # ë¶„ì„ê°€ ìƒì„±ì„ ìœ„í•œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±\n",
    "    system_message = analyst_instructions.format(\n",
    "        topic=topic,\n",
    "        human_analyst_feedback=human_analyst_feedback,\n",
    "        max_analysts=max_analysts,\n",
    "    )\n",
    "\n",
    "    # LLMì„ í˜¸ì¶œí•˜ì—¬ ë¶„ì„ê°€ í˜ë¥´ì†Œë‚˜ ìƒì„±\n",
    "    analysts = structured_llm.invoke(\n",
    "        [SystemMessage(content=system_message)]\n",
    "        + [HumanMessage(content=\"Generate the set of analysts.\")]\n",
    "    )\n",
    "\n",
    "    # ìƒì„±ëœ ë¶„ì„ê°€ ëª©ë¡ì„ ìƒíƒœì— ì €ì¥\n",
    "    return {\"analysts\": analysts.analysts}\n",
    "\n",
    "\n",
    "# ì‚¬ìš©ì í”¼ë“œë°± ë…¸ë“œ(ìƒíƒœ ì—…ë°ì´íŠ¸ë¥¼ ì§„í–‰í•  ì˜ˆì •ì´ë¯€ë¡œ, ë‚´ìš©ì€ ë¹„ì›Œ ë‘ì–´ë„ ë¬´ë°©)\n",
    "def human_feedback(state: GenerateAnalystsState):\n",
    "    \"\"\"ì‚¬ìš©ì í”¼ë“œë°±ì„ ë°›ê¸° ìœ„í•œ ì¤‘ë‹¨ì  ë…¸ë“œ\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "# ì¸ê°„ í”¼ë“œë°± ì—¬ë¶€ì— ë”°ë¼ ì›Œí¬í”Œë¡œìš°ì˜ ë‹¤ìŒ ë‹¨ê³„ë¥¼ ê²°ì •í•˜ëŠ” í•¨ìˆ˜\n",
    "def should_continue(state: GenerateAnalystsState):\n",
    "    \"\"\"ì›Œí¬í”Œë¡œìš°ì˜ ë‹¤ìŒ ë‹¨ê³„ë¥¼ ê²°ì •í•˜ëŠ” í•¨ìˆ˜\"\"\"\n",
    "\n",
    "    human_analyst_feedback = state.get(\"human_analyst_feedback\", None)\n",
    "    if human_analyst_feedback:\n",
    "        return \"create_analysts\"\n",
    "\n",
    "    return END\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langgraph.graph import START, END, StateGraph\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "# ê·¸ë˜í”„ ìƒì„±\n",
    "builder = StateGraph(GenerateAnalystsState)\n",
    "\n",
    "# ë…¸ë“œ ì¶”ê°€\n",
    "builder.add_node(\"create_analysts\", create_analysts)\n",
    "builder.add_node(\"human_feedback\", human_feedback)\n",
    "\n",
    "# ì—£ì§€ ì—°ê²°\n",
    "builder.add_edge(START, \"create_analysts\")\n",
    "builder.add_edge(\"create_analysts\", \"human_feedback\")\n",
    "\n",
    "# ì¡°ê±´ë¶€ ì—£ì§€ ì¶”ê°€: ì‚¬ëŒ í”¼ë“œë°±ì´ ìˆì„ ê²½ìš° ë‹¤ì‹œ ë¶„ì„ê°€ ìƒì„± ë…¸ë“œë¡œ ëŒì•„ê°‘ë‹ˆë‹¤.\n",
    "builder.add_conditional_edges(\n",
    "    \"human_feedback\", should_continue, [\"create_analysts\", END]\n",
    ")\n",
    "\n",
    "# ë©”ëª¨ë¦¬ ìƒì„±\n",
    "memory = MemorySaver()\n",
    "\n",
    "# ê·¸ë˜í”„ ì»´íŒŒì¼(ì¤‘ë‹¨ì  ì„¤ì •)\n",
    "graph = builder.compile(interrupt_before=[\"human_feedback\"], checkpointer=memory)\n",
    "\n",
    "# ê·¸ë˜í”„ ì‹œê°í™”\n",
    "visualize_graph(graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mcreate_analysts\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "affiliation='AI Research Lab' name='Dr. Emily Tan' role='AI Systems Researcher' description='Dr. Tan focuses on the architectural and performance improvements of AI models. Her major interest is in how Modular RAG can improve flexibility and scalability compared to Naive RAG, particularly in large-scale AI systems. She analyzes how the modular approach allows for more efficient resource management and adaptability in various production environments.'\n",
      "affiliation='Tech Industry Observer' name='Michael Chen' role='Tech Industry Analyst' description=\"Michael observes trends and innovations in AI technologies as they apply to industry use. He studies the commercial advantages that Modular RAG offers over Naive RAG, such as ease of integration into existing workflows and the potential for cost reduction. Michael's interest lies in understanding how these features can lead to increased adoption in business settings.\"\n",
      "affiliation='Enterprise Software Solutions' name='Laura MartÃ­nez' role='Enterprise Solutions Architect' description='Laura provides insights into how companies transition AI models into production environments. She evaluates the practical advantages of using Modular RAG over Naive RAG in real-world applications, such as increased reliability, improved maintenance workflows, and reduced downtime. Her focus is on how enterprise clients can leverage these advantages to enhance their AI deployment strategies effectively.'\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36m__interrupt__\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_teddynote.messages import random_uuid, invoke_graph\n",
    "\n",
    "config = RunnableConfig(\n",
    "    recursion_limit=10,\n",
    "    configurable={\"thread_id\": random_uuid()},\n",
    ")\n",
    "\n",
    "# ë¶„ì„ê°€ ìˆ˜ ì„¤ì •\n",
    "max_analysts = 3\n",
    "\n",
    "# ì—°êµ¬ ì£¼ì œ ì„¤ì •\n",
    "topic = \"Modular RAG ê°€ ê¸°ì¡´ì˜ Naive RAG ì™€ ì–´ë–¤ ì°¨ì´ê°€ ìˆëŠ”ì§€ì™€ production level ì—ì„œ ì‚¬ìš©í•˜ëŠ” ì´ì \"\n",
    "\n",
    "# ì…ë ¥ ë°ì´í„° ì„¤ì •\n",
    "inputs = {\n",
    "    \"topic\": topic,\n",
    "    \"max_analysts\": max_analysts,\n",
    "}\n",
    "\n",
    "# ê·¸ë˜í”„ ì‹¤í–‰\n",
    "invoke_graph(graph, inputs, config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('human_feedback',)\n"
     ]
    }
   ],
   "source": [
    "# ê·¸ë˜í”„ì˜ í˜„ì¬ ìƒíƒœ ê°€ì ¸ì˜¤ê¸°\n",
    "state = graph.get_state(config)\n",
    "\n",
    "# ë‹¤ìŒ ì‹¤í–‰í•  ë…¸ë“œ í™•ì¸\n",
    "print(state.next)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'configurable': {'thread_id': '0f224407-101d-45c7-b3fc-89073ff26589',\n",
       "  'checkpoint_ns': '',\n",
       "  'checkpoint_id': '1f083d30-5f58-60cc-8002-d5d9fef4e090'}}"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ê·¸ë˜í”„ ìƒíƒœë¥¼ ì—…ë°ì´íŠ¸í•˜ì—¬ human_feedback ë…¸ë“œì˜ ì—­í•  ìˆ˜í–‰\n",
    "graph.update_state(\n",
    "    config,\n",
    "    {\n",
    "        \"human_analyst_feedback\": \"Add in someone named Teddy Lee from a startup to add an entrepreneur perspective\"\n",
    "    },\n",
    "    as_node=\"human_feedback\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mcreate_analysts\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "affiliation='Independent Researcher' name='Dr. Hannah Kim' role='Academic Specialist' description='Dr. Kim is an academic researcher with a focus on machine learning frameworks and their applications in real-world scenarios. She is particularly interested in comparing Modular RAG and Naive RAG in terms of flexibility and efficiency for handling complex data requirements in production settings.'\n",
      "affiliation='TechVision Analytics' name='Rahul Patel' role='Data Engineer' description='Rahul works as a data engineer who has extensive experience implementing and optimizing retrieval-augmented generation systems in various production environments. His interest lies in the practical aspects of how Modular RAG can enhance scalability and robustness compared to Naive RAG for enterprise-level applications.'\n",
      "affiliation='Startup Founder' name='Teddy Lee' role='Entrepreneurial Thinker' description='Teddy is a co-founder of an AI-driven startup looking to leverage cutting-edge technologies to innovate new services. He focuses on the entrepreneurial benefits of using Modular RAG over Naive RAG, such as cost-effectiveness and faster deployment capabilities that are crucial for startup survival and competition in fast-moving markets.'\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36m__interrupt__\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# ì´ì–´ì„œ ì§„í–‰\n",
    "invoke_graph(graph, None, config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'configurable': {'thread_id': '0f224407-101d-45c7-b3fc-89073ff26589',\n",
       "  'checkpoint_ns': '',\n",
       "  'checkpoint_id': '1f083d30-e5ca-6b00-8004-7965721ba1e3'}}"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ì¶”ê°€ í”¼ë“œë°±ì´ ì—†ì„ ê²½ìš° None ê°’ì„ í• ë‹¹í•˜ì—¬ ìƒíƒœ ì—…ë°ì´íŠ¸\n",
    "human_feedback_input = None\n",
    "\n",
    "# ê·¸ë˜í”„ ìƒíƒœë¥¼ ì—…ë°ì´íŠ¸í•˜ì—¬ human_feedback ë…¸ë“œì˜ ì—­í•  ìˆ˜í–‰\n",
    "graph.update_state(\n",
    "    config, {\"human_analyst_feedback\": human_feedback_input}, as_node=\"human_feedback\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì´ì–´ì„œ ì§„í–‰\n",
    "invoke_graph(graph, None, config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìƒì„±ëœ ë¶„ì„ê°€ ìˆ˜: 3\n",
      "================================\n",
      "Name: Dr. Hannah Kim\n",
      "Role: Academic Specialist\n",
      "Affiliation: Independent Researcher\n",
      "Description: Dr. Kim is an academic researcher with a focus on machine learning frameworks and their applications in real-world scenarios. She is particularly interested in comparing Modular RAG and Naive RAG in terms of flexibility and efficiency for handling complex data requirements in production settings.\n",
      "\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Name: Rahul Patel\n",
      "Role: Data Engineer\n",
      "Affiliation: TechVision Analytics\n",
      "Description: Rahul works as a data engineer who has extensive experience implementing and optimizing retrieval-augmented generation systems in various production environments. His interest lies in the practical aspects of how Modular RAG can enhance scalability and robustness compared to Naive RAG for enterprise-level applications.\n",
      "\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Name: Teddy Lee\n",
      "Role: Entrepreneurial Thinker\n",
      "Affiliation: Startup Founder\n",
      "Description: Teddy is a co-founder of an AI-driven startup looking to leverage cutting-edge technologies to innovate new services. He focuses on the entrepreneurial benefits of using Modular RAG over Naive RAG, such as cost-effectiveness and faster deployment capabilities that are crucial for startup survival and competition in fast-moving markets.\n",
      "\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n"
     ]
    }
   ],
   "source": [
    "# ê·¸ë˜í”„ì˜ ìµœì¢… ìƒíƒœ ê°€ì ¸ì˜¤ê¸°\n",
    "final_state = graph.get_state(config)\n",
    "\n",
    "# ìµœì¢… ìƒíƒœì—ì„œ ìƒì„±ëœ ë¶„ì„ê°€ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°\n",
    "analysts = final_state.values.get(\"analysts\")\n",
    "\n",
    "# ìƒì„±ëœ ë¶„ì„ê°€ ìˆ˜ ì¶œë ¥\n",
    "print(f\"ìƒì„±ëœ ë¶„ì„ê°€ ìˆ˜: {len(analysts)}\", end=\"\\n================================\\n\")\n",
    "\n",
    "# ê° ë¶„ì„ê°€ì˜ í˜ë¥´ì†Œë‚˜ ì¶œë ¥\n",
    "for analyst in analysts:\n",
    "    print(analyst.persona)\n",
    "    print(\"- \" * 30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "()\n"
     ]
    }
   ],
   "source": [
    "# ê·¸ë˜í”„ì˜ ë‹¤ìŒ ì‹¤í–‰í•  ë…¸ë“œ ìƒíƒœ ê°€ì ¸ì˜¤ê¸°\n",
    "print(final_state.next)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import Annotated\n",
    "from langgraph.graph import MessagesState\n",
    "\n",
    "\n",
    "# ì¸í„°ë·° ìƒíƒœ ì •ì˜\n",
    "class InterviewState(MessagesState):\n",
    "    # ëŒ€í™” í„´ìˆ˜\n",
    "    max_num_turns: int\n",
    "    # ì†ŒìŠ¤ ë¬¸ì„œë¥¼ í¬í•¨í•˜ëŠ” ì»¨í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸\n",
    "    context: Annotated[list, operator.add]\n",
    "    # ì§€ì •ëœ ë¶„ì„ê°€\n",
    "    analyst: Analyst\n",
    "    # ì¸í„°ë·° ë‚´ìš©ì„ ì €ì¥í•˜ëŠ” ë¬¸ìì—´\n",
    "    interview: str\n",
    "    # ë³´ê³ ì„œ ì„¹ì…˜ ë¦¬ìŠ¤íŠ¸\n",
    "    sections: list\n",
    "\n",
    "\n",
    "# ê²€ìƒ‰ ì¿¼ë¦¬ ë°ì´í„° í´ë˜ìŠ¤ ì •ì˜\n",
    "class SearchQuery(BaseModel):\n",
    "    search_query: str = Field(None, description=\"Search query for retrieval.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_instructions = \"\"\"You are an analyst tasked with interviewing an expert to learn about a specific topic. \n",
    "\n",
    "Your goal is boil down to interesting and specific insights related to your topic.\n",
    "\n",
    "1. Interesting: Insights that people will find surprising or non-obvious.\n",
    "\n",
    "2. Specific: Insights that avoid generalities and include specific examples from the expert.\n",
    "\n",
    "Here is your topic of focus and set of goals: {goals}\n",
    "\n",
    "Begin by introducing yourself using a name that fits your persona, and then ask your question.\n",
    "\n",
    "Continue to ask questions to drill down and refine your understanding of the topic.\n",
    "\n",
    "When you are satisfied with your understanding, complete the interview with: \"Thank you so much for your help!\"\n",
    "\n",
    "Remember to stay in character throughout your response, reflecting the persona and goals provided to you.\"\"\"\n",
    "\n",
    "\n",
    "# ì§ˆë¬¸ì„ ìƒì„±í•˜ëŠ” ë…¸ë“œ ì •ì˜\n",
    "def generate_question(state: InterviewState):\n",
    "    # ìƒíƒœì—ì„œ ë¶„ì„ê°€ì™€ ë©”ì‹œì§€ ê°€ì ¸ì˜¤ê¸°\n",
    "    analyst = state[\"analyst\"]\n",
    "    messages = state[\"messages\"]\n",
    "\n",
    "    # ì§ˆë¬¸ ìƒì„±\n",
    "    # ë¶„ì„ê°€ì˜ ëª©í‘œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‹œìŠ¤í…œ ë©”ì‹œì§€ ìƒì„±\n",
    "    system_message = question_instructions.format(goals=analyst.persona)\n",
    "    # LLMì„ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ ìƒì„±\n",
    "    question = llm.invoke([SystemMessage(content=system_message)] + messages)\n",
    "\n",
    "    # ìƒíƒœì— ë©”ì‹œì§€ ê¸°ë¡\n",
    "    return {\"messages\": [question]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì›¹ ê²€ìƒ‰ ë„êµ¬ ì´ˆê¸°í™”\n",
    "from langchain_teddynote.tools.tavily import TavilySearch\n",
    "\n",
    "# ì›¹ ê²€ìƒ‰ì„ ìœ„í•œ TavilySearch ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\n",
    "tavily_search = TavilySearch(max_results=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'Published': '2024-07-26', 'Title': 'Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks', 'Authors': 'Yunfan Gao, Yun Xiong, Meng Wang, Haofen Wang', 'Summary': 'Retrieval-augmented Generation (RAG) has markedly enhanced the capabilities\\nof Large Language Models (LLMs) in tackling knowledge-intensive tasks. The\\nincreasing demands of application scenarios have driven the evolution of RAG,\\nleading to the integration of advanced retrievers, LLMs and other complementary\\ntechnologies, which in turn has amplified the intricacy of RAG systems.\\nHowever, the rapid advancements are outpacing the foundational RAG paradigm,\\nwith many methods struggling to be unified under the process of\\n\"retrieve-then-generate\". In this context, this paper examines the limitations\\nof the existing RAG paradigm and introduces the modular RAG framework. By\\ndecomposing complex RAG systems into independent modules and specialized\\noperators, it facilitates a highly reconfigurable framework. Modular RAG\\ntranscends the traditional linear architecture, embracing a more advanced\\ndesign that integrates routing, scheduling, and fusion mechanisms. Drawing on\\nextensive research, this paper further identifies prevalent RAG\\npatterns-linear, conditional, branching, and looping-and offers a comprehensive\\nanalysis of their respective implementation nuances. Modular RAG presents\\ninnovative opportunities for the conceptualization and deployment of RAG\\nsystems. Finally, the paper explores the potential emergence of new operators\\nand paradigms, establishing a solid theoretical foundation and a practical\\nroadmap for the continued evolution and practical deployment of RAG\\ntechnologies.', 'entry_id': 'http://arxiv.org/abs/2407.21059v1', 'published_first_time': '2024-07-26', 'comment': None, 'journal_ref': None, 'doi': None, 'primary_category': 'cs.CL', 'categories': ['cs.CL', 'cs.AI', 'cs.IR'], 'links': ['http://arxiv.org/abs/2407.21059v1', 'http://arxiv.org/pdf/2407.21059v1']}, page_content='1\\nModular RAG: Transforming RAG Systems into\\nLEGO-like Reconfigurable Frameworks\\nYunfan Gao, Yun Xiong, Meng Wang, Haofen Wang\\nAbstractâ€”Retrieval-augmented\\nGeneration\\n(RAG)\\nhas\\nmarkedly enhanced the capabilities of Large Language Models\\n(LLMs) in tackling knowledge-intensive tasks. The increasing\\ndemands of application scenarios have driven the evolution\\nof RAG, leading to the integration of advanced retrievers,\\nLLMs and other complementary technologies, which in turn\\nhas amplified the intricacy of RAG systems. However, the rapid\\nadvancements are outpacing the foundational RAG paradigm,\\nwith many methods struggling to be unified under the process\\nof â€œretrieve-then-generateâ€. In this context, this paper examines\\nthe limitations of the existing RAG paradigm and introduces\\nthe modular RAG framework. By decomposing complex RAG\\nsystems into independent modules and specialized operators, it\\nfacilitates a highly reconfigurable framework. Modular RAG\\ntranscends the traditional linear architecture, embracing a\\nmore advanced design that integrates routing, scheduling, and\\nfusion mechanisms. Drawing on extensive research, this paper\\nfurther identifies prevalent RAG patternsâ€”linear, conditional,\\nbranching, and loopingâ€”and offers a comprehensive analysis\\nof their respective implementation nuances. Modular RAG\\npresents\\ninnovative\\nopportunities\\nfor\\nthe\\nconceptualization\\nand deployment of RAG systems. Finally, the paper explores\\nthe potential emergence of new operators and paradigms,\\nestablishing a solid theoretical foundation and a practical\\nroadmap for the continued evolution and practical deployment\\nof RAG technologies.\\nIndex Termsâ€”Retrieval-augmented generation, large language\\nmodel, modular system, information retrieval\\nI. INTRODUCTION\\nL\\nARGE Language Models (LLMs) have demonstrated\\nremarkable capabilities, yet they still face numerous\\nchallenges, such as hallucination and the lag in information up-\\ndates [1]. Retrieval-augmented Generation (RAG), by access-\\ning external knowledge bases, provides LLMs with important\\ncontextual information, significantly enhancing their perfor-\\nmance on knowledge-intensive tasks [2]. Currently, RAG, as\\nan enhancement method, has been widely applied in various\\npractical application scenarios, including knowledge question\\nanswering, recommendation systems, customer service, and\\npersonal assistants. [3]â€“[6]\\nDuring the nascent stages of RAG , its core framework is\\nconstituted by indexing, retrieval, and generation, a paradigm\\nreferred to as Naive RAG [7]. However, as the complexity\\nof tasks and the demands of applications have escalated, the\\nYunfan Gao is with Shanghai Research Institute for Intelligent Autonomous\\nSystems, Tongji University, Shanghai, 201210, China.\\nYun Xiong is with Shanghai Key Laboratory of Data Science, School of\\nComputer Science, Fudan University, Shanghai, 200438, China.\\nMeng Wang and Haofen Wang are with College of Design and Innovation,\\nTongji University, Shanghai, 20092, China. (Corresponding author: Haofen\\nWang. E-mail: carter.whfcarter@gmail.com)\\nlimitations of Naive RAG have become increasingly apparent.\\nAs depicted in Figure 1, it predominantly hinges on the\\nstraightforward similarity of chunks, result in poor perfor-\\nmance when confronted with complex queries and chunks with\\nsubstantial variability. The primary challenges of Naive RAG\\ninclude: 1) Shallow Understanding of Queries. The semantic\\nsimilarity between a query and document chunk is not always\\nhighly consistent. Relying solely on similarity calculations\\nfor retrieval lacks an in-depth exploration of the relationship\\nbetween the query and the document [8]. 2) Retrieval Re-\\ndundancy and Noise. Feeding all retrieved chunks directly\\ninto LLMs is not always beneficial. Research indicates that\\nan excess of redundant and noisy information may interfere\\nwith the LLMâ€™s identification of key information, thereby\\nincreasing the risk of generating erroneous and hallucinated\\nresponses. [9]\\nTo overcome the aforementioned limitations, '), Document(metadata={'Published': '2024-03-27', 'Title': 'Retrieval-Augmented Generation for Large Language Models: A Survey', 'Authors': 'Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, Haofen Wang', 'Summary': \"Large Language Models (LLMs) showcase impressive capabilities but encounter\\nchallenges like hallucination, outdated knowledge, and non-transparent,\\nuntraceable reasoning processes. Retrieval-Augmented Generation (RAG) has\\nemerged as a promising solution by incorporating knowledge from external\\ndatabases. This enhances the accuracy and credibility of the generation,\\nparticularly for knowledge-intensive tasks, and allows for continuous knowledge\\nupdates and integration of domain-specific information. RAG synergistically\\nmerges LLMs' intrinsic knowledge with the vast, dynamic repositories of\\nexternal databases. This comprehensive review paper offers a detailed\\nexamination of the progression of RAG paradigms, encompassing the Naive RAG,\\nthe Advanced RAG, and the Modular RAG. It meticulously scrutinizes the\\ntripartite foundation of RAG frameworks, which includes the retrieval, the\\ngeneration and the augmentation techniques. The paper highlights the\\nstate-of-the-art technologies embedded in each of these critical components,\\nproviding a profound understanding of the advancements in RAG systems.\\nFurthermore, this paper introduces up-to-date evaluation framework and\\nbenchmark. At the end, this article delineates the challenges currently faced\\nand points out prospective avenues for research and development.\", 'entry_id': 'http://arxiv.org/abs/2312.10997v5', 'published_first_time': '2023-12-18', 'comment': 'Ongoing Work', 'journal_ref': None, 'doi': None, 'primary_category': 'cs.CL', 'categories': ['cs.CL', 'cs.AI'], 'links': ['http://arxiv.org/abs/2312.10997v5', 'http://arxiv.org/pdf/2312.10997v5']}, page_content='1\\nRetrieval-Augmented Generation for Large\\nLanguage Models: A Survey\\nYunfan Gaoa, Yun Xiongb, Xinyu Gaob, Kangxiang Jiab, Jinliu Panb, Yuxi Bic, Yi Daia, Jiawei Suna, Meng\\nWangc, and Haofen Wang a,c\\naShanghai Research Institute for Intelligent Autonomous Systems, Tongji University\\nbShanghai Key Laboratory of Data Science, School of Computer Science, Fudan University\\ncCollege of Design and Innovation, Tongji University\\nAbstractâ€”Large Language Models (LLMs) showcase impres-\\nsive capabilities but encounter challenges like hallucination,\\noutdated knowledge, and non-transparent, untraceable reasoning\\nprocesses. Retrieval-Augmented Generation (RAG) has emerged\\nas a promising solution by incorporating knowledge from external\\ndatabases. This enhances the accuracy and credibility of the\\ngeneration, particularly for knowledge-intensive tasks, and allows\\nfor continuous knowledge updates and integration of domain-\\nspecific information. RAG synergistically merges LLMsâ€™ intrin-\\nsic knowledge with the vast, dynamic repositories of external\\ndatabases. This comprehensive review paper offers a detailed\\nexamination of the progression of RAG paradigms, encompassing\\nthe Naive RAG, the Advanced RAG, and the Modular RAG.\\nIt meticulously scrutinizes the tripartite foundation of RAG\\nframeworks, which includes the retrieval, the generation and the\\naugmentation techniques. The paper highlights the state-of-the-\\nart technologies embedded in each of these critical components,\\nproviding a profound understanding of the advancements in RAG\\nsystems. Furthermore, this paper introduces up-to-date evalua-\\ntion framework and benchmark. At the end, this article delineates\\nthe challenges currently faced and points out prospective avenues\\nfor research and development 1.\\nIndex Termsâ€”Large language model, retrieval-augmented gen-\\neration, natural language processing, information retrieval\\nI. INTRODUCTION\\nL\\nARGE language models (LLMs) have achieved remark-\\nable success, though they still face significant limitations,\\nespecially in domain-specific or knowledge-intensive tasks [1],\\nnotably producing â€œhallucinationsâ€ [2] when handling queries\\nbeyond their training data or requiring current information. To\\novercome challenges, Retrieval-Augmented Generation (RAG)\\nenhances LLMs by retrieving relevant document chunks from\\nexternal knowledge base through semantic similarity calcu-\\nlation. By referencing external knowledge, RAG effectively\\nreduces the problem of generating factually incorrect content.\\nIts integration into LLMs has resulted in widespread adoption,\\nestablishing RAG as a key technology in advancing chatbots\\nand enhancing the suitability of LLMs for real-world applica-\\ntions.\\nRAG technology has rapidly developed in recent years, and\\nthe technology tree summarizing related research is shown\\nCorresponding Author.Email:haofen.wang@tongji.edu.cn\\n1Resources\\nare\\navailable\\nat\\nhttps://github.com/Tongji-KGLLM/\\nRAG-Survey\\nin Figure 1. The development trajectory of RAG in the era\\nof large models exhibits several distinct stage characteristics.\\nInitially, RAGâ€™s inception coincided with the rise of the\\nTransformer architecture, focusing on enhancing language\\nmodels by incorporating additional knowledge through Pre-\\nTraining Models (PTM). This early stage was characterized\\nby foundational work aimed at refining pre-training techniques\\n[3]â€“[5].The subsequent arrival of ChatGPT [6] marked a\\npivotal moment, with LLM demonstrating powerful in context\\nlearning (ICL) capabilities. RAG research shifted towards\\nproviding better information for LLMs to answer more com-\\nplex and knowledge-intensive tasks during the inference stage,\\nleading to rapid development in RAG studies. As research\\nprogressed, the enhancement of RAG was no longer limited\\nto the inference stage but began to incorporate more with LLM\\nfine-tuning techniques.\\nThe burgeoning field of RAG has experienced swift growth,\\nyet it has not been accompanied by a systematic synthesis that\\ncould clarify its broader trajectory. Thi'), Document(metadata={'Published': '2025-02-04', 'Title': 'Plan*RAG: Efficient Test-Time Planning for Retrieval Augmented Generation', 'Authors': 'Prakhar Verma, Sukruta Prakash Midigeshi, Gaurav Sinha, Arno Solin, Nagarajan Natarajan, Amit Sharma', 'Summary': \"We introduce Plan*RAG, a novel framework that enables structured multi-hop\\nreasoning in retrieval-augmented generation (RAG) through test-time reasoning\\nplan generation. While existing approaches such as ReAct maintain reasoning\\nchains within the language model's context window, we observe that this often\\nleads to plan fragmentation and execution failures. Our key insight is that by\\nisolating the reasoning plan as a directed acyclic graph (DAG) outside the LM's\\nworking memory, we can enable (1) systematic exploration of reasoning paths,\\n(2) atomic subqueries enabling precise retrievals and grounding, and (3)\\nefficiency through parallel execution and bounded context window utilization.\\nMoreover, Plan*RAG's modular design allows it to be integrated with existing\\nRAG methods, thus providing a practical solution to improve current RAG\\nsystems. On standard multi-hop reasoning benchmarks, Plan*RAG consistently\\nachieves improvements over recently proposed methods such as RQ-RAG and\\nSelf-RAG, while maintaining comparable computational costs.\", 'entry_id': 'http://arxiv.org/abs/2410.20753v2', 'published_first_time': '2024-10-28', 'comment': '19 pages, preprint', 'journal_ref': None, 'doi': None, 'primary_category': 'cs.CL', 'categories': ['cs.CL', 'cs.LG'], 'links': ['http://arxiv.org/abs/2410.20753v2', 'http://arxiv.org/pdf/2410.20753v2']}, page_content='Planâˆ—RAG: Efficient Test-Time Planning for Retrieval Augmented Generation\\nPrakhar Verma â€  1 Sukruta Prakash Midigeshi 2 Gaurav Sinha 2 Arno Solin 1\\nNagarajan Natarajan 2 Amit Sharma 2\\nAbstract\\nWe introduce Planâˆ—RAG, a novel framework\\nthat enables structured multi-hop reasoning in\\nretrieval-augmented generation (RAG) through\\ntest-time reasoning plan generation. While exist-\\ning approaches such as ReAct maintain reason-\\ning chains within the language modelâ€™s context\\nwindow, we observe that this often leads to plan\\nfragmentation and execution failures. Our key\\ninsight is that by isolating the reasoning plan as\\na directed acyclic graph (DAG) outside the LMâ€™s\\nworking memory, we can enable (1) systematic\\nexploration of reasoning paths, (2) atomic sub-\\nqueries enabling precise retrievals and ground-\\ning, and (3) efficiency through parallel execution\\nand bounded context window utilization. More-\\nover, Planâˆ—RAGâ€™s modular design allows it to\\nbe integrated with existing RAG methods, thus\\nproviding a practical solution to improve current\\nRAG systems. On standard multi-hop reasoning\\nbenchmarks, Planâˆ—RAG consistently achieves im-\\nprovements over recently proposed methods such\\nas RQ-RAG and Self-RAG, while maintaining\\ncomparable computational costs.\\n1. Introduction\\nRetrieval-Augmented Generation (RAG, Lewis et al., 2020;\\nPetroni et al., 2020; Guu et al., 2020) has emerged as a\\npromising approach for grounding language model (LM)\\nresponses in external knowledge. However, RAG systems\\nstruggle with multi-hop queries that require reasoning across\\nmultiple retrieved documents (Tang & Yang, 2024; Wei\\net al., 2022). A key challenge lies in the initial retrieval step,\\nwhich often fails to retrieve sufficient relevant documents\\ndue to the queryâ€™s lack of full contextual information (Ma\\net al., 2023). This limitation has been highlighted in recent\\nsurveys (Torfi et al., 2020; Zhao et al., 2023) as a funda-\\nâ€  Work done during an internship with Microsoft Research.\\n1Aalto University, Finland 2Microsoft Research. Correspondence\\nto: <prakhar.verma@aalto.fi>, <amshar@microsoft.com>.\\nRAG\\nSelf-RAG\\nReAct\\n20\\n25\\n30\\n35\\n40\\n45\\nTest-time planning improves RAG\\n25.51\\n34.09\\n33.15\\n31.12\\n37.31\\n40.44\\nAccuracy (%)\\nVanilla\\nWith Planâˆ—\\nFigure 1. Planâˆ—RAG improves performance on the HotpotQA\\nbenchmark substantially compared to various existing RAG meth-\\nods, demonstrating the value of externalizing planning as a directed\\nacyclic graph (DAG) outside of the LLMâ€™s context.\\nmental barrier to reliable AI systems, particularly given the\\nwidespread deployment of large language models (Brown\\net al., 2020) across critical domains. Consider the query:\\nâ€œRumble Fish was a novel by the author of the coming-of-age\\nnovel published in what year by Viking Press?â€ Answering\\nthis requires an iterative retrieval process: identifying the\\nRumble Fishâ€™s author, connecting to their coming-of-age\\nnovel, and determining its publication year. Single-step\\nretrieval in RAG systems often fails in such cases, as it\\nmay retrieve documents about Rumble Fishâ€™s author and\\nViking Press without recognizing the intermediate factâ€”\\nthe authorâ€™s coming-of-age novelâ€”must first be established.\\nFurthermore, Leng et al. (2024); Shuster et al. (2021) demon-\\nstrate that even when relevant documents are retrieved, LMs\\nstruggle to reason across them due to fixed context win-\\ndows, leading to information loss and broken reasoning\\nchain. These limitations pose risks in critical domains such\\nas healthcare and finance (Pal et al., 2023; Zhao et al., 2024),\\nwhere accurate multi-step reasoning is essential.\\nRecent research has attempted to address these limita-\\ntions through structured reasoning frameworks. Chain-of-\\nThought (CoT) prompting (Wei et al., 2022) and systematic\\nquery decomposition (Patel et al., 2022) have introduced\\nexplicit reasoning steps, enabling more granular thought\\nprocesses and targeted retrievals. Building upon these foun-\\ndations, Yao et al. (2023) proposed ReActâ€”a framework\\nthat creates a reasoning chain ')]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.retrievers import ArxivRetriever\n",
    "\n",
    "# Arxiv ê²€ìƒ‰ì„ ìœ„í•œ ArxivRetriever ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\n",
    "arxiv_retriever = ArxivRetriever(\n",
    "    load_max_docs=3,\n",
    "    load_all_available_meta=True,\n",
    "    get_full_documents=True,\n",
    ")\n",
    "\n",
    "# ê²€ìƒ‰ ê²°ê³¼ ì¶œë ¥\n",
    "arxiv_search_results = arxiv_retriever.invoke(\"Modular RAG vs Naive RAG\")\n",
    "print(arxiv_search_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Published': '2024-07-26',\n",
       " 'Title': 'Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks',\n",
       " 'Authors': 'Yunfan Gao, Yun Xiong, Meng Wang, Haofen Wang',\n",
       " 'Summary': 'Retrieval-augmented Generation (RAG) has markedly enhanced the capabilities\\nof Large Language Models (LLMs) in tackling knowledge-intensive tasks. The\\nincreasing demands of application scenarios have driven the evolution of RAG,\\nleading to the integration of advanced retrievers, LLMs and other complementary\\ntechnologies, which in turn has amplified the intricacy of RAG systems.\\nHowever, the rapid advancements are outpacing the foundational RAG paradigm,\\nwith many methods struggling to be unified under the process of\\n\"retrieve-then-generate\". In this context, this paper examines the limitations\\nof the existing RAG paradigm and introduces the modular RAG framework. By\\ndecomposing complex RAG systems into independent modules and specialized\\noperators, it facilitates a highly reconfigurable framework. Modular RAG\\ntranscends the traditional linear architecture, embracing a more advanced\\ndesign that integrates routing, scheduling, and fusion mechanisms. Drawing on\\nextensive research, this paper further identifies prevalent RAG\\npatterns-linear, conditional, branching, and looping-and offers a comprehensive\\nanalysis of their respective implementation nuances. Modular RAG presents\\ninnovative opportunities for the conceptualization and deployment of RAG\\nsystems. Finally, the paper explores the potential emergence of new operators\\nand paradigms, establishing a solid theoretical foundation and a practical\\nroadmap for the continued evolution and practical deployment of RAG\\ntechnologies.',\n",
       " 'entry_id': 'http://arxiv.org/abs/2407.21059v1',\n",
       " 'published_first_time': '2024-07-26',\n",
       " 'comment': None,\n",
       " 'journal_ref': None,\n",
       " 'doi': None,\n",
       " 'primary_category': 'cs.CL',\n",
       " 'categories': ['cs.CL', 'cs.AI', 'cs.IR'],\n",
       " 'links': ['http://arxiv.org/abs/2407.21059v1',\n",
       "  'http://arxiv.org/pdf/2407.21059v1']}"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Arxiv ë©”íƒ€ë°ì´í„° ì¶œë ¥\n",
    "arxiv_search_results[0].metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Modular RAG: Transforming RAG Systems into\n",
      "LEGO-like Reconfigurable Frameworks\n",
      "Yunfan Gao, Yun Xiong, Meng Wang, Haofen Wang\n",
      "Abstractâ€”Retrieval-augmented\n",
      "Generation\n",
      "(RAG)\n",
      "has\n",
      "markedly enhanced the capabilities of Large Language Models\n",
      "(LLMs) in tackling knowledge-intensive tasks. The increasing\n",
      "demands of application scenarios have driven the evolution\n",
      "of RAG, leading to the integration of advanced retrievers,\n",
      "LLMs and other complementary technologies, which in turn\n",
      "has amplified the intricacy of RAG systems. However, the rapid\n",
      "advancements are outpacing the foundational RAG paradigm,\n",
      "with many methods struggling to be unified under the process\n",
      "of â€œretrieve-then-generateâ€. In this context, this paper examines\n",
      "the limitations of the existing RAG paradigm and introduces\n",
      "the modular RAG framework. By decomposing complex RAG\n",
      "systems into independent modules and specialized operators, it\n",
      "facilitates a highly reconfigurable framework. Modular RAG\n",
      "transcends the traditional linear architecture, embracing a\n",
      "more advanced design that integrates routing, scheduling, and\n",
      "fusion mechanisms. Drawing on extensive research, this paper\n",
      "further identifies prevalent RAG patternsâ€”linear, conditional,\n",
      "branching, and loopingâ€”and offers a comprehensive analysis\n",
      "of their respective implementation nuances. Modular RAG\n",
      "presents\n",
      "innovative\n",
      "opportunities\n",
      "for\n",
      "the\n",
      "conceptualization\n",
      "and deployment of RAG systems. Finally, the paper explores\n",
      "the potential emergence of new operators and paradigms,\n",
      "establishing a solid theoretical foundation and a practical\n",
      "roadmap for the continued evolution and practical deployment\n",
      "of RAG technologies.\n",
      "Index Termsâ€”Retrieval-augmented generation, large language\n",
      "model, modular system, information retrieval\n",
      "I. INTRODUCTION\n",
      "L\n",
      "ARGE Language Models (LLMs) have demonstrated\n",
      "remarkable capabilities, yet they still face numerous\n",
      "challenges, such as hallucination and the lag in information up-\n",
      "dates [1]. Retrieval-augmented Generation (RAG), by access-\n",
      "ing external knowledge bases, provides LLMs with important\n",
      "contextual information, significantly enhancing their perfor-\n",
      "mance on knowledge-intensive tasks [2]. Currently, RAG, as\n",
      "an enhancement method, has been widely applied in various\n",
      "practical application scenarios, including knowledge question\n",
      "answering, recommendation systems, customer service, and\n",
      "personal assistants. [3]â€“[6]\n",
      "During the nascent stages of RAG , its core framework is\n",
      "constituted by indexing, retrieval, and generation, a paradigm\n",
      "referred to as Naive RAG [7]. However, as the complexity\n",
      "of tasks and the demands of applications have escalated, the\n",
      "Yunfan Gao is with Shanghai Research Institute for Intelligent Autonomous\n",
      "Systems, Tongji University, Shanghai, 201210, China.\n",
      "Yun Xiong is with Shanghai Key Laboratory of Data Science, School of\n",
      "Computer Science, Fudan University, Shanghai, 200438, China.\n",
      "Meng Wang and Haofen Wang are with College of Design and Innovation,\n",
      "Tongji University, Shanghai, 20092, China. (Corresponding author: Haofen\n",
      "Wang. E-mail: carter.whfcarter@gmail.com)\n",
      "limitations of Naive RAG have become increasingly apparent.\n",
      "As depicted in Figure 1, it predominantly hinges on the\n",
      "straightforward similarity of chunks, result in poor perfor-\n",
      "mance when confronted with complex queries and chunks with\n",
      "substantial variability. The primary challenges of Naive RAG\n",
      "include: 1) Shallow Understanding of Queries. The semantic\n",
      "similarity between a query and document chunk is not always\n",
      "highly consistent. Relying solely on similarity calculations\n",
      "for retrieval lacks an in-depth exploration of the relationship\n",
      "between the query and the document [8]. 2) Retrieval Re-\n",
      "dundancy and Noise. Feeding all retrieved chunks directly\n",
      "into LLMs is not always beneficial. Research indicates that\n",
      "an excess of redundant and noisy information may interfere\n",
      "with the LLMâ€™s identification of key information, thereby\n",
      "increasing the risk of generating erroneous and hallucinated\n",
      "responses. [9]\n",
      "To overcome the aforementioned limitations, \n"
     ]
    }
   ],
   "source": [
    "# Arxiv ë‚´ìš© ì¶œë ¥\n",
    "print(arxiv_search_results[0].page_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¬¸ì„œ ê²€ìƒ‰ ê²°ê³¼ë¥¼ í¬ë§·íŒ…\n",
    "formatted_search_docs = \"\\n\\n---\\n\\n\".join(\n",
    "    [\n",
    "        f'<Document source=\"{doc.metadata[\"entry_id\"]}\" date=\"{doc.metadata.get(\"Published\", \"\")}\" authors=\"{doc.metadata.get(\"Authors\", \"\")}\"/>\\n<Title>\\n{doc.metadata[\"Title\"]}\\n</Title>\\n\\n<Summary>\\n{doc.metadata[\"Summary\"]}\\n</Summary>\\n\\n<Content>\\n{doc.page_content}\\n</Content>\\n</Document>'\n",
    "        for doc in arxiv_search_results\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Document source=\"http://arxiv.org/abs/2407.21059v1\" date=\"2024-07-26\" authors=\"Yunfan Gao, Yun Xiong, Meng Wang, Haofen Wang\"/>\n",
      "<Title>\n",
      "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks\n",
      "</Title>\n",
      "\n",
      "<Summary>\n",
      "Retrieval-augmented Generation (RAG) has markedly enhanced the capabilities\n",
      "of Large Language Models (LLMs) in tackling knowledge-intensive tasks. The\n",
      "increasing demands of application scenarios have driven the evolution of RAG,\n",
      "leading to the integration of advanced retrievers, LLMs and other complementary\n",
      "technologies, which in turn has amplified the intricacy of RAG systems.\n",
      "However, the rapid advancements are outpacing the foundational RAG paradigm,\n",
      "with many methods struggling to be unified under the process of\n",
      "\"retrieve-then-generate\". In this context, this paper examines the limitations\n",
      "of the existing RAG paradigm and introduces the modular RAG framework. By\n",
      "decomposing complex RAG systems into independent modules and specialized\n",
      "operators, it facilitates a highly reconfigurable framework. Modular RAG\n",
      "transcends the traditional linear architecture, embracing a more advanced\n",
      "design that integrates routing, scheduling, and fusion mechanisms. Drawing on\n",
      "extensive research, this paper further identifies prevalent RAG\n",
      "patterns-linear, conditional, branching, and looping-and offers a comprehensive\n",
      "analysis of their respective implementation nuances. Modular RAG presents\n",
      "innovative opportunities for the conceptualization and deployment of RAG\n",
      "systems. Finally, the paper explores the potential emergence of new operators\n",
      "and paradigms, establishing a solid theoretical foundation and a practical\n",
      "roadmap for the continued evolution and practical deployment of RAG\n",
      "technologies.\n",
      "</Summary>\n",
      "\n",
      "<Content>\n",
      "1\n",
      "Modular RAG: Transforming RAG Systems into\n",
      "LEGO-like Reconfigurable Frameworks\n",
      "Yunfan Gao, Yun Xiong, Meng Wang, Haofen Wang\n",
      "Abstractâ€”Retrieval-augmented\n",
      "Generation\n",
      "(RAG)\n",
      "has\n",
      "markedly enhanced the capabilities of Large Language Models\n",
      "(LLMs) in tackling knowledge-intensive tasks. The increasing\n",
      "demands of application scenarios have driven the evolution\n",
      "of RAG, leading to the integration of advanced retrievers,\n",
      "LLMs and other complementary technologies, which in turn\n",
      "has amplified the intricacy of RAG systems. However, the rapid\n",
      "advancements are outpacing the foundational RAG paradigm,\n",
      "with many methods struggling to be unified under the process\n",
      "of â€œretrieve-then-generateâ€. In this context, this paper examines\n",
      "the limitations of the existing RAG paradigm and introduces\n",
      "the modular RAG framework. By decomposing complex RAG\n",
      "systems into independent modules and specialized operators, it\n",
      "facilitates a highly reconfigurable framework. Modular RAG\n",
      "transcends the traditional linear architecture, embracing a\n",
      "more advanced design that integrates routing, scheduling, and\n",
      "fusion mechanisms. Drawing on extensive research, this paper\n",
      "further identifies prevalent RAG patternsâ€”linear, conditional,\n",
      "branching, and loopingâ€”and offers a comprehensive analysis\n",
      "of their respective implementation nuances. Modular RAG\n",
      "presents\n",
      "innovative\n",
      "opportunities\n",
      "for\n",
      "the\n",
      "conceptualization\n",
      "and deployment of RAG systems. Finally, the paper explores\n",
      "the potential emergence of new operators and paradigms,\n",
      "establishing a solid theoretical foundation and a practical\n",
      "roadmap for the continued evolution and practical deployment\n",
      "of RAG technologies.\n",
      "Index Termsâ€”Retrieval-augmented generation, large language\n",
      "model, modular system, information retrieval\n",
      "I. INTRODUCTION\n",
      "L\n",
      "ARGE Language Models (LLMs) have demonstrated\n",
      "remarkable capabilities, yet they still face numerous\n",
      "challenges, such as hallucination and the lag in information up-\n",
      "dates [1]. Retrieval-augmented Generation (RAG), by access-\n",
      "ing external knowledge bases, provides LLMs with important\n",
      "contextual information, significantly enhancing their perfor-\n",
      "mance on knowledge-intensive tasks [2]. Currently, RAG, as\n",
      "an enhancement method, has been widely applied in various\n",
      "practical application scenarios, including knowledge question\n",
      "answering, recommendation systems, customer service, and\n",
      "personal assistants. [3]â€“[6]\n",
      "During the nascent stages of RAG , its core framework is\n",
      "constituted by indexing, retrieval, and generation, a paradigm\n",
      "referred to as Naive RAG [7]. However, as the complexity\n",
      "of tasks and the demands of applications have escalated, the\n",
      "Yunfan Gao is with Shanghai Research Institute for Intelligent Autonomous\n",
      "Systems, Tongji University, Shanghai, 201210, China.\n",
      "Yun Xiong is with Shanghai Key Laboratory of Data Science, School of\n",
      "Computer Science, Fudan University, Shanghai, 200438, China.\n",
      "Meng Wang and Haofen Wang are with College of Design and Innovation,\n",
      "Tongji University, Shanghai, 20092, China. (Corresponding author: Haofen\n",
      "Wang. E-mail: carter.whfcarter@gmail.com)\n",
      "limitations of Naive RAG have become increasingly apparent.\n",
      "As depicted in Figure 1, it predominantly hinges on the\n",
      "straightforward similarity of chunks, result in poor perfor-\n",
      "mance when confronted with complex queries and chunks with\n",
      "substantial variability. The primary challenges of Naive RAG\n",
      "include: 1) Shallow Understanding of Queries. The semantic\n",
      "similarity between a query and document chunk is not always\n",
      "highly consistent. Relying solely on similarity calculations\n",
      "for retrieval lacks an in-depth exploration of the relationship\n",
      "between the query and the document [8]. 2) Retrieval Re-\n",
      "dundancy and Noise. Feeding all retrieved chunks directly\n",
      "into LLMs is not always beneficial. Research indicates that\n",
      "an excess of redundant and noisy information may interfere\n",
      "with the LLMâ€™s identification of key information, thereby\n",
      "increasing the risk of generating erroneous and hallucinated\n",
      "responses. [9]\n",
      "To overcome the aforementioned limitations, \n",
      "</Content>\n",
      "</Document>\n",
      "\n",
      "---\n",
      "\n",
      "<Document source=\"http://arxiv.org/abs/2312.10997v5\" date=\"2024-03-27\" authors=\"Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, Haofen Wang\"/>\n",
      "<Title>\n",
      "Retrieval-Augmented Generation for Large Language Models: A Survey\n",
      "</Title>\n",
      "\n",
      "<Summary>\n",
      "Large Language Models (LLMs) showcase impressive capabilities but encounter\n",
      "challenges like hallucination, outdated knowledge, and non-transparent,\n",
      "untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has\n",
      "emerged as a promising solution by incorporating knowledge from external\n",
      "databases. This enhances the accuracy and credibility of the generation,\n",
      "particularly for knowledge-intensive tasks, and allows for continuous knowledge\n",
      "updates and integration of domain-specific information. RAG synergistically\n",
      "merges LLMs' intrinsic knowledge with the vast, dynamic repositories of\n",
      "external databases. This comprehensive review paper offers a detailed\n",
      "examination of the progression of RAG paradigms, encompassing the Naive RAG,\n",
      "the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the\n",
      "tripartite foundation of RAG frameworks, which includes the retrieval, the\n",
      "generation and the augmentation techniques. The paper highlights the\n",
      "state-of-the-art technologies embedded in each of these critical components,\n",
      "providing a profound understanding of the advancements in RAG systems.\n",
      "Furthermore, this paper introduces up-to-date evaluation framework and\n",
      "benchmark. At the end, this article delineates the challenges currently faced\n",
      "and points out prospective avenues for research and development.\n",
      "</Summary>\n",
      "\n",
      "<Content>\n",
      "1\n",
      "Retrieval-Augmented Generation for Large\n",
      "Language Models: A Survey\n",
      "Yunfan Gaoa, Yun Xiongb, Xinyu Gaob, Kangxiang Jiab, Jinliu Panb, Yuxi Bic, Yi Daia, Jiawei Suna, Meng\n",
      "Wangc, and Haofen Wang a,c\n",
      "aShanghai Research Institute for Intelligent Autonomous Systems, Tongji University\n",
      "bShanghai Key Laboratory of Data Science, School of Computer Science, Fudan University\n",
      "cCollege of Design and Innovation, Tongji University\n",
      "Abstractâ€”Large Language Models (LLMs) showcase impres-\n",
      "sive capabilities but encounter challenges like hallucination,\n",
      "outdated knowledge, and non-transparent, untraceable reasoning\n",
      "processes. Retrieval-Augmented Generation (RAG) has emerged\n",
      "as a promising solution by incorporating knowledge from external\n",
      "databases. This enhances the accuracy and credibility of the\n",
      "generation, particularly for knowledge-intensive tasks, and allows\n",
      "for continuous knowledge updates and integration of domain-\n",
      "specific information. RAG synergistically merges LLMsâ€™ intrin-\n",
      "sic knowledge with the vast, dynamic repositories of external\n",
      "databases. This comprehensive review paper offers a detailed\n",
      "examination of the progression of RAG paradigms, encompassing\n",
      "the Naive RAG, the Advanced RAG, and the Modular RAG.\n",
      "It meticulously scrutinizes the tripartite foundation of RAG\n",
      "frameworks, which includes the retrieval, the generation and the\n",
      "augmentation techniques. The paper highlights the state-of-the-\n",
      "art technologies embedded in each of these critical components,\n",
      "providing a profound understanding of the advancements in RAG\n",
      "systems. Furthermore, this paper introduces up-to-date evalua-\n",
      "tion framework and benchmark. At the end, this article delineates\n",
      "the challenges currently faced and points out prospective avenues\n",
      "for research and development 1.\n",
      "Index Termsâ€”Large language model, retrieval-augmented gen-\n",
      "eration, natural language processing, information retrieval\n",
      "I. INTRODUCTION\n",
      "L\n",
      "ARGE language models (LLMs) have achieved remark-\n",
      "able success, though they still face significant limitations,\n",
      "especially in domain-specific or knowledge-intensive tasks [1],\n",
      "notably producing â€œhallucinationsâ€ [2] when handling queries\n",
      "beyond their training data or requiring current information. To\n",
      "overcome challenges, Retrieval-Augmented Generation (RAG)\n",
      "enhances LLMs by retrieving relevant document chunks from\n",
      "external knowledge base through semantic similarity calcu-\n",
      "lation. By referencing external knowledge, RAG effectively\n",
      "reduces the problem of generating factually incorrect content.\n",
      "Its integration into LLMs has resulted in widespread adoption,\n",
      "establishing RAG as a key technology in advancing chatbots\n",
      "and enhancing the suitability of LLMs for real-world applica-\n",
      "tions.\n",
      "RAG technology has rapidly developed in recent years, and\n",
      "the technology tree summarizing related research is shown\n",
      "Corresponding Author.Email:haofen.wang@tongji.edu.cn\n",
      "1Resources\n",
      "are\n",
      "available\n",
      "at\n",
      "https://github.com/Tongji-KGLLM/\n",
      "RAG-Survey\n",
      "in Figure 1. The development trajectory of RAG in the era\n",
      "of large models exhibits several distinct stage characteristics.\n",
      "Initially, RAGâ€™s inception coincided with the rise of the\n",
      "Transformer architecture, focusing on enhancing language\n",
      "models by incorporating additional knowledge through Pre-\n",
      "Training Models (PTM). This early stage was characterized\n",
      "by foundational work aimed at refining pre-training techniques\n",
      "[3]â€“[5].The subsequent arrival of ChatGPT [6] marked a\n",
      "pivotal moment, with LLM demonstrating powerful in context\n",
      "learning (ICL) capabilities. RAG research shifted towards\n",
      "providing better information for LLMs to answer more com-\n",
      "plex and knowledge-intensive tasks during the inference stage,\n",
      "leading to rapid development in RAG studies. As research\n",
      "progressed, the enhancement of RAG was no longer limited\n",
      "to the inference stage but began to incorporate more with LLM\n",
      "fine-tuning techniques.\n",
      "The burgeoning field of RAG has experienced swift growth,\n",
      "yet it has not been accompanied by a systematic synthesis that\n",
      "could clarify its broader trajectory. Thi\n",
      "</Content>\n",
      "</Document>\n",
      "\n",
      "---\n",
      "\n",
      "<Document source=\"http://arxiv.org/abs/2410.20753v2\" date=\"2025-02-04\" authors=\"Prakhar Verma, Sukruta Prakash Midigeshi, Gaurav Sinha, Arno Solin, Nagarajan Natarajan, Amit Sharma\"/>\n",
      "<Title>\n",
      "Plan*RAG: Efficient Test-Time Planning for Retrieval Augmented Generation\n",
      "</Title>\n",
      "\n",
      "<Summary>\n",
      "We introduce Plan*RAG, a novel framework that enables structured multi-hop\n",
      "reasoning in retrieval-augmented generation (RAG) through test-time reasoning\n",
      "plan generation. While existing approaches such as ReAct maintain reasoning\n",
      "chains within the language model's context window, we observe that this often\n",
      "leads to plan fragmentation and execution failures. Our key insight is that by\n",
      "isolating the reasoning plan as a directed acyclic graph (DAG) outside the LM's\n",
      "working memory, we can enable (1) systematic exploration of reasoning paths,\n",
      "(2) atomic subqueries enabling precise retrievals and grounding, and (3)\n",
      "efficiency through parallel execution and bounded context window utilization.\n",
      "Moreover, Plan*RAG's modular design allows it to be integrated with existing\n",
      "RAG methods, thus providing a practical solution to improve current RAG\n",
      "systems. On standard multi-hop reasoning benchmarks, Plan*RAG consistently\n",
      "achieves improvements over recently proposed methods such as RQ-RAG and\n",
      "Self-RAG, while maintaining comparable computational costs.\n",
      "</Summary>\n",
      "\n",
      "<Content>\n",
      "Planâˆ—RAG: Efficient Test-Time Planning for Retrieval Augmented Generation\n",
      "Prakhar Verma â€  1 Sukruta Prakash Midigeshi 2 Gaurav Sinha 2 Arno Solin 1\n",
      "Nagarajan Natarajan 2 Amit Sharma 2\n",
      "Abstract\n",
      "We introduce Planâˆ—RAG, a novel framework\n",
      "that enables structured multi-hop reasoning in\n",
      "retrieval-augmented generation (RAG) through\n",
      "test-time reasoning plan generation. While exist-\n",
      "ing approaches such as ReAct maintain reason-\n",
      "ing chains within the language modelâ€™s context\n",
      "window, we observe that this often leads to plan\n",
      "fragmentation and execution failures. Our key\n",
      "insight is that by isolating the reasoning plan as\n",
      "a directed acyclic graph (DAG) outside the LMâ€™s\n",
      "working memory, we can enable (1) systematic\n",
      "exploration of reasoning paths, (2) atomic sub-\n",
      "queries enabling precise retrievals and ground-\n",
      "ing, and (3) efficiency through parallel execution\n",
      "and bounded context window utilization. More-\n",
      "over, Planâˆ—RAGâ€™s modular design allows it to\n",
      "be integrated with existing RAG methods, thus\n",
      "providing a practical solution to improve current\n",
      "RAG systems. On standard multi-hop reasoning\n",
      "benchmarks, Planâˆ—RAG consistently achieves im-\n",
      "provements over recently proposed methods such\n",
      "as RQ-RAG and Self-RAG, while maintaining\n",
      "comparable computational costs.\n",
      "1. Introduction\n",
      "Retrieval-Augmented Generation (RAG, Lewis et al., 2020;\n",
      "Petroni et al., 2020; Guu et al., 2020) has emerged as a\n",
      "promising approach for grounding language model (LM)\n",
      "responses in external knowledge. However, RAG systems\n",
      "struggle with multi-hop queries that require reasoning across\n",
      "multiple retrieved documents (Tang & Yang, 2024; Wei\n",
      "et al., 2022). A key challenge lies in the initial retrieval step,\n",
      "which often fails to retrieve sufficient relevant documents\n",
      "due to the queryâ€™s lack of full contextual information (Ma\n",
      "et al., 2023). This limitation has been highlighted in recent\n",
      "surveys (Torfi et al., 2020; Zhao et al., 2023) as a funda-\n",
      "â€  Work done during an internship with Microsoft Research.\n",
      "1Aalto University, Finland 2Microsoft Research. Correspondence\n",
      "to: <prakhar.verma@aalto.fi>, <amshar@microsoft.com>.\n",
      "RAG\n",
      "Self-RAG\n",
      "ReAct\n",
      "20\n",
      "25\n",
      "30\n",
      "35\n",
      "40\n",
      "45\n",
      "Test-time planning improves RAG\n",
      "25.51\n",
      "34.09\n",
      "33.15\n",
      "31.12\n",
      "37.31\n",
      "40.44\n",
      "Accuracy (%)\n",
      "Vanilla\n",
      "With Planâˆ—\n",
      "Figure 1. Planâˆ—RAG improves performance on the HotpotQA\n",
      "benchmark substantially compared to various existing RAG meth-\n",
      "ods, demonstrating the value of externalizing planning as a directed\n",
      "acyclic graph (DAG) outside of the LLMâ€™s context.\n",
      "mental barrier to reliable AI systems, particularly given the\n",
      "widespread deployment of large language models (Brown\n",
      "et al., 2020) across critical domains. Consider the query:\n",
      "â€œRumble Fish was a novel by the author of the coming-of-age\n",
      "novel published in what year by Viking Press?â€ Answering\n",
      "this requires an iterative retrieval process: identifying the\n",
      "Rumble Fishâ€™s author, connecting to their coming-of-age\n",
      "novel, and determining its publication year. Single-step\n",
      "retrieval in RAG systems often fails in such cases, as it\n",
      "may retrieve documents about Rumble Fishâ€™s author and\n",
      "Viking Press without recognizing the intermediate factâ€”\n",
      "the authorâ€™s coming-of-age novelâ€”must first be established.\n",
      "Furthermore, Leng et al. (2024); Shuster et al. (2021) demon-\n",
      "strate that even when relevant documents are retrieved, LMs\n",
      "struggle to reason across them due to fixed context win-\n",
      "dows, leading to information loss and broken reasoning\n",
      "chain. These limitations pose risks in critical domains such\n",
      "as healthcare and finance (Pal et al., 2023; Zhao et al., 2024),\n",
      "where accurate multi-step reasoning is essential.\n",
      "Recent research has attempted to address these limita-\n",
      "tions through structured reasoning frameworks. Chain-of-\n",
      "Thought (CoT) prompting (Wei et al., 2022) and systematic\n",
      "query decomposition (Patel et al., 2022) have introduced\n",
      "explicit reasoning steps, enabling more granular thought\n",
      "processes and targeted retrievals. Building upon these foun-\n",
      "dations, Yao et al. (2023) proposed ReActâ€”a framework\n",
      "that creates a reasoning chain \n",
      "</Content>\n",
      "</Document>\n"
     ]
    }
   ],
   "source": [
    "print(formatted_search_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import get_buffer_string\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "# ê²€ìƒ‰ ì¿¼ë¦¬ ì‘ì„±\n",
    "search_instructions = SystemMessage(\n",
    "    content=f\"\"\"You will be given a conversation between an analyst and an expert. \n",
    "\n",
    "Your goal is to generate a well-structured query for use in retrieval and / or web-search related to the conversation.\n",
    "\n",
    "First, analyze the full conversation.\n",
    "\n",
    "Pay particular attention to the final question posed by the analyst.\n",
    "\n",
    "Convert this final question into a well-structured web search query\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "# ì›¹ ê²€ìƒ‰ ìˆ˜í–‰ í•¨ìˆ˜ ì •ì˜\n",
    "def search_web(state: InterviewState):\n",
    "    \"\"\"ì›¹ ê²€ìƒ‰ì„ í†µí•œ ë¬¸ì„œ ê²€ìƒ‰\"\"\"\n",
    "\n",
    "    # ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±\n",
    "    structured_llm = llm.with_structured_output(SearchQuery)\n",
    "    search_query = structured_llm.invoke([search_instructions] + state[\"messages\"])\n",
    "\n",
    "    # ê²€ìƒ‰ ìˆ˜í–‰\n",
    "    search_docs = tavily_search.invoke(search_query.search_query)\n",
    "\n",
    "    # ê²€ìƒ‰ ê²°ê³¼ í˜•ì‹ ì§€ì •\n",
    "    formatted_search_docs = \"\\n\\n---\\n\\n\".join(\n",
    "        [\n",
    "            f'<Document href=\"{doc[\"url\"]}\"/>\\n{doc[\"content\"]}\\n</Document>'\n",
    "            for doc in search_docs\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return {\"context\": [formatted_search_docs]}\n",
    "\n",
    "\n",
    "# Arxiv ê²€ìƒ‰ ë…¸ë“œ ìƒì„±\n",
    "def search_arxiv(state: InterviewState):\n",
    "    \"\"\"Arxiv ê²€ìƒ‰ ë…¸ë“œ\"\"\"\n",
    "\n",
    "    # ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±\n",
    "    structured_llm = llm.with_structured_output(SearchQuery)\n",
    "    search_query = structured_llm.invoke([search_instructions] + state[\"messages\"])\n",
    "\n",
    "    try:\n",
    "        # ê²€ìƒ‰ ìˆ˜í–‰\n",
    "        arxiv_search_results = arxiv_retriever.invoke(\n",
    "            search_query.search_query,\n",
    "            load_max_docs=2,\n",
    "            load_all_available_meta=True,\n",
    "            get_full_documents=True,\n",
    "        )\n",
    "\n",
    "        # ê²€ìƒ‰ ê²°ê³¼ í˜•ì‹ ì§€ì •\n",
    "        formatted_search_docs = \"\\n\\n---\\n\\n\".join(\n",
    "            [\n",
    "                f'<Document source=\"{doc.metadata[\"entry_id\"]}\" date=\"{doc.metadata.get(\"Published\", \"\")}\" authors=\"{doc.metadata.get(\"Authors\", \"\")}\"/>\\n<Title>\\n{doc.metadata[\"Title\"]}\\n</Title>\\n\\n<Summary>\\n{doc.metadata[\"Summary\"]}\\n</Summary>\\n\\n<Content>\\n{doc.page_content}\\n</Content>\\n</Document>'\n",
    "                for doc in arxiv_search_results\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        return {\"context\": [formatted_search_docs]}\n",
    "    except Exception as e:\n",
    "        print(f\"Arxiv ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}\")\n",
    "        return {\n",
    "            \"context\": [\"<Error>Arxiv ê²€ìƒ‰ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜¤ëŠ”ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.</Error>\"]\n",
    "        }\n",
    "\n",
    "\n",
    "answer_instructions = \"\"\"You are an expert being interviewed by an analyst.\n",
    "\n",
    "Here is analyst area of focus: {goals}. \n",
    "\n",
    "You goal is to answer a question posed by the interviewer.\n",
    "\n",
    "To answer question, use this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "When answering questions, follow these guidelines:\n",
    "\n",
    "1. Use only the information provided in the context. \n",
    "\n",
    "2. Do not introduce external information or make assumptions beyond what is explicitly stated in the context.\n",
    "\n",
    "3. The context contain sources at the topic of each individual document.\n",
    "\n",
    "4. Include these sources your answer next to any relevant statements. For example, for source # 1 use [1]. \n",
    "\n",
    "5. List your sources in order at the bottom of your answer. [1] Source 1, [2] Source 2, etc\n",
    "\n",
    "6. If the source is: <Document source=\"assistant/docs/llama3_1.pdf\" page=\"7\"/>' then just list: \n",
    "\n",
    "[1] assistant/docs/llama3_1.pdf, page 7 \n",
    "\n",
    "And skip the addition of the brackets as well as the Document source preamble in your citation.\"\"\"\n",
    "\n",
    "\n",
    "# ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„± í•¨ìˆ˜ ì •ì˜\n",
    "def generate_answer(state: InterviewState):\n",
    "    \"\"\"ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„± ë…¸ë“œ\"\"\"\n",
    "\n",
    "    # ìƒíƒœì—ì„œ ë¶„ì„ê°€ì™€ ë©”ì‹œì§€ ê°€ì ¸ì˜¤ê¸°\n",
    "    analyst = state[\"analyst\"]\n",
    "    messages = state[\"messages\"]\n",
    "    context = state[\"context\"]\n",
    "\n",
    "    # ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„±\n",
    "    system_message = answer_instructions.format(goals=analyst.persona, context=context)\n",
    "    answer = llm.invoke([SystemMessage(content=system_message)] + messages)\n",
    "\n",
    "    # ë©”ì‹œì§€ë¥¼ ì „ë¬¸ê°€ì˜ ë‹µë³€ìœ¼ë¡œ ëª…ëª…\n",
    "    answer.name = \"expert\"\n",
    "\n",
    "    # ìƒíƒœì— ë©”ì‹œì§€ ì¶”ê°€\n",
    "    return {\"messages\": [answer]}\n",
    "\n",
    "\n",
    "# ì¸í„°ë·° ì €ì¥ í•¨ìˆ˜ ì •ì˜\n",
    "def save_interview(state: InterviewState):\n",
    "    \"\"\"ì¸í„°ë·° ì €ì¥\"\"\"\n",
    "\n",
    "    # ë©”ì‹œì§€ ê°€ì ¸ì˜¤ê¸°\n",
    "    messages = state[\"messages\"]\n",
    "\n",
    "    # ì¸í„°ë·°ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜\n",
    "    interview = get_buffer_string(messages)\n",
    "\n",
    "    # ì¸í„°ë·° í‚¤ì— ì €ì¥\n",
    "    return {\"interview\": interview}\n",
    "\n",
    "\n",
    "# ë©”ì‹œì§€ ë¼ìš°íŒ… í•¨ìˆ˜ ì •ì˜\n",
    "def route_messages(state: InterviewState, name: str = \"expert\"):\n",
    "    \"\"\"ì§ˆë¬¸ê³¼ ë‹µë³€ ì‚¬ì´ì˜ ë¼ìš°íŒ…\"\"\"\n",
    "\n",
    "    # ë©”ì‹œì§€ ê°€ì ¸ì˜¤ê¸°\n",
    "    messages = state[\"messages\"]\n",
    "    max_num_turns = state.get(\"max_num_turns\", 2)\n",
    "\n",
    "    # ì „ë¬¸ê°€ì˜ ë‹µë³€ ìˆ˜ í™•ì¸\n",
    "    num_responses = len(\n",
    "        [m for m in messages if isinstance(m, AIMessage) and m.name == name]\n",
    "    )\n",
    "\n",
    "    # ì „ë¬¸ê°€ê°€ ìµœëŒ€ í„´ ìˆ˜ ì´ìƒ ë‹µë³€í•œ ê²½ìš° ì¢…ë£Œ\n",
    "    if num_responses >= max_num_turns:\n",
    "        return \"save_interview\"\n",
    "\n",
    "    # ì´ ë¼ìš°í„°ëŠ” ê° ì§ˆë¬¸-ë‹µë³€ ìŒ í›„ì— ì‹¤í–‰ë¨\n",
    "    # ë…¼ì˜ ì¢…ë£Œë¥¼ ì‹ í˜¸í•˜ëŠ” ë§ˆì§€ë§‰ ì§ˆë¬¸ ê°€ì ¸ì˜¤ê¸°\n",
    "    last_question = messages[-2]\n",
    "\n",
    "    if \"Thank you so much for your help\" in last_question.content:\n",
    "        return \"save_interview\"\n",
    "    return \"ask_question\"\n",
    "\n",
    "\n",
    "# ì„¸ì…˜ ì‘ì„± ì§€ì‹œì‚¬í•­\n",
    "section_writer_instructions = \"\"\"You are an expert technical writer. \n",
    "\n",
    "Your task is to create a detailed and comprehensive section of a report, thoroughly analyzing a set of source documents.\n",
    "This involves extracting key insights, elaborating on relevant points, and providing in-depth explanations to ensure clarity and understanding. Your writing should include necessary context, supporting evidence, and examples to enhance the reader's comprehension. Maintain a logical and well-organized structure, ensuring that all critical aspects are covered in detail and presented in a professional tone.\n",
    "\n",
    "Please follow these instructions:\n",
    "1. Analyze the content of the source documents: \n",
    "- The name of each source document is at the start of the document, with the <Document tag.\n",
    "\n",
    "2. Create a report structure using markdown formatting:\n",
    "- Use ## for the section title\n",
    "- Use ### for sub-section headers\n",
    "\n",
    "3. Write the report following this structure:\n",
    "a. Title (## header)\n",
    "b. Summary (### header)\n",
    "c. Comprehensive analysis (### header)\n",
    "d. Sources (### header)\n",
    "\n",
    "4. Make your title engaging based upon the focus area of the analyst: \n",
    "{focus}\n",
    "\n",
    "5. For the summary section:\n",
    "- Set up summary with general background / context related to the focus area of the analyst\n",
    "- Emphasize what is novel, interesting, or surprising about insights gathered from the interview\n",
    "- Create a numbered list of source documents, as you use them\n",
    "- Do not mention the names of interviewers or experts\n",
    "- Aim for approximately 400 words maximum\n",
    "- Use numbered sources in your report (e.g., [1], [2]) based on information from source documents\n",
    "\n",
    "6. For the Comprehensive analysis section:\n",
    "- Provide a detailed examination of the information from the source documents.\n",
    "- Break down complex ideas into digestible segments, ensuring a logical flow of ideas.\n",
    "- Use sub-sections where necessary to cover multiple perspectives or dimensions of the analysis.\n",
    "- Support your analysis with data, direct quotes, and examples from the source documents.\n",
    "- Clearly explain the relevance of each point to the overall focus of the report.\n",
    "- Use bullet points or numbered lists for clarity when presenting multiple related ideas.\n",
    "- Ensure the tone remains professional and objective, avoiding bias or unsupported opinions.\n",
    "- Aim for at least 800 words to ensure the analysis is thorough.\n",
    "\n",
    "7. In the Sources section:\n",
    "- Include all sources used in your report\n",
    "- Provide full links to relevant websites or specific document paths\n",
    "- Separate each source by a newline. Use two spaces at the end of each line to create a newline in Markdown.\n",
    "- It will look like:\n",
    "\n",
    "### Sources\n",
    "[1] Link or Document name\n",
    "[2] Link or Document name\n",
    "\n",
    "8. Be sure to combine sources. For example this is not correct:\n",
    "\n",
    "[3] https://ai.meta.com/blog/meta-llama-3-1/\n",
    "[4] https://ai.meta.com/blog/meta-llama-3-1/\n",
    "\n",
    "There should be no redundant sources. It should simply be:\n",
    "\n",
    "[3] https://ai.meta.com/blog/meta-llama-3-1/\n",
    "\n",
    "9. Final review:\n",
    "- Ensure the report follows the required structure\n",
    "- Include no preamble before the title of the report\n",
    "- Check that all guidelines have been followed\"\"\"\n",
    "\n",
    "\n",
    "# ì„¹ì…˜ ì‘ì„± í•¨ìˆ˜ ì •ì˜\n",
    "def write_section(state: InterviewState):\n",
    "    \"\"\"ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„± ë…¸ë“œ\"\"\"\n",
    "\n",
    "    # ìƒíƒœì—ì„œ ì»¨í…ìŠ¤íŠ¸, ë¶„ì„ê°€ ê°€ì ¸ì˜¤ê¸°\n",
    "    context = state[\"context\"]\n",
    "    analyst = state[\"analyst\"]\n",
    "\n",
    "    # ì„¹ì…˜ ì‘ì„±ì„ ìœ„í•œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì •ì˜\n",
    "    system_message = section_writer_instructions.format(focus=analyst.description)\n",
    "    section = llm.invoke(\n",
    "        [SystemMessage(content=system_message)]\n",
    "        + [HumanMessage(content=f\"Use this source to write your section: {context}\")]\n",
    "    )\n",
    "\n",
    "    # ìƒíƒœì— ì„¹ì…˜ ì¶”ê°€\n",
    "    return {\"sections\": [section.content]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# ë…¸ë“œ ë° ì—£ì§€ ì¶”ê°€\n",
    "interview_builder = StateGraph(InterviewState)\n",
    "interview_builder.add_node(\"ask_question\", generate_question)\n",
    "interview_builder.add_node(\"search_web\", search_web)\n",
    "interview_builder.add_node(\"search_arxiv\", search_arxiv)\n",
    "interview_builder.add_node(\"answer_question\", generate_answer)\n",
    "interview_builder.add_node(\"save_interview\", save_interview)\n",
    "interview_builder.add_node(\"write_section\", write_section)\n",
    "\n",
    "# íë¦„ ì„¤ì •\n",
    "interview_builder.add_edge(START, \"ask_question\")\n",
    "interview_builder.add_edge(\"ask_question\", \"search_web\")\n",
    "interview_builder.add_edge(\"ask_question\", \"search_arxiv\")\n",
    "interview_builder.add_edge(\"search_web\", \"answer_question\")\n",
    "interview_builder.add_edge(\"search_arxiv\", \"answer_question\")\n",
    "interview_builder.add_conditional_edges(\n",
    "    \"answer_question\", route_messages, [\"ask_question\", \"save_interview\"]\n",
    ")\n",
    "interview_builder.add_edge(\"save_interview\", \"write_section\")\n",
    "interview_builder.add_edge(\"write_section\", END)\n",
    "\n",
    "# ì¸í„°ë·° ê·¸ë˜í”„ ìƒì„±\n",
    "memory = MemorySaver()\n",
    "interview_graph = interview_builder.compile(checkpointer=memory).with_config(\n",
    "    run_name=\"Conduct Interviews\"\n",
    ")\n",
    "\n",
    "# ê·¸ë˜í”„ ì‹œê°í™”\n",
    "visualize_graph(interview_graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Analyst(affiliation='Independent Researcher', name='Dr. Hannah Kim', role='Academic Specialist', description='Dr. Kim is an academic researcher with a focus on machine learning frameworks and their applications in real-world scenarios. She is particularly interested in comparing Modular RAG and Naive RAG in terms of flexibility and efficiency for handling complex data requirements in production settings.')"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ë¶„ì„ê°€ ëª©ë¡ì—ì„œ ì²« ë²ˆì§¸ ë¶„ì„ê°€ ì„ íƒ\n",
    "analysts[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mask_question\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello, Dr. Kim. My name is Alex Thompson, and I'm an analyst deeply interested in the practical applications of machine learning frameworks, particularly Modular RAG and Naive RAG. I'm eager to understand the differences between these two frameworks, especially in terms of their flexibility and efficiency in handling complex data requirements in production settings. Could you please elaborate on these aspects for us?\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36msearch_arxiv\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "<Document source=\"http://arxiv.org/abs/2508.05650v1\" date=\"2025-07-26\" authors=\"Jiaxuan Liang, Shide Zhou, Kailong Wang\"/>\n",
      "<Title>\n",
      "OmniBench-RAG: A Multi-Domain Evaluation Platform for Retrieval-Augmented Generation Tools\n",
      "</Title>\n",
      "\n",
      "<Summary>\n",
      "While Retrieval Augmented Generation (RAG) is now widely adopted to enhance\n",
      "LLMs, evaluating its true performance benefits in a reproducible and\n",
      "interpretable way remains a major hurdle. Existing methods often fall short:\n",
      "they lack domain coverage, employ coarse metrics that miss sub document\n",
      "precision, and fail to capture computational trade offs. Most critically, they\n",
      "provide no standardized framework for comparing RAG effectiveness across\n",
      "different models and domains.\n",
      "  We introduce OmniBench RAG, a novel automated platform for multi domain\n",
      "evaluation of RAG systems. The platform quantifies performance gains across\n",
      "accuracy and efficiency dimensions, spanning nine knowledge fields including\n",
      "culture, geography, and health. We introduce two standardized metrics:\n",
      "Improvements (accuracy gains) and Transformation (efficiency differences\n",
      "between pre RAG and post RAG models), enabling reproducible comparisons across\n",
      "models and tasks. The platform features dynamic test generation, modular\n",
      "evaluation pipelines, and automated knowledge base construction. Our evaluation\n",
      "reveals striking variability in RAG effectiveness, from significant gains in\n",
      "culture to declines in mathematics, highlighting the critical importance of\n",
      "systematic, domain aware assessment. A demonstration video is available at:\n",
      "https://www.youtube.com/watch?v=BZx83QFcTCI. Code and datasets:\n",
      "https://github.com/Garnett-Liang/Omnibench-RAG.\n",
      "</Summary>\n",
      "\n",
      "<Content>\n",
      "OmniBench-RAG: A Multi-Domain Evaluation\n",
      "Platform for Retrieval-Augmented Generation Tools\n",
      "Jiaxuan Liang*, Shide Zhou*, and Kailong Wangâ€ \n",
      "Huazhong University of Science and Technology\n",
      "{liangjx, shidez, wangkl}@hust.edu.cn\n",
      "Abstractâ€”While Retrieval Augmented Generation (RAG) is\n",
      "now widely adopted to enhance LLMs, evaluating its true\n",
      "performance benefits in a reproducible and interpretable way\n",
      "remains a major hurdle. Existing methods often fall short: they\n",
      "lack domain coverage, employ coarse metrics that miss sub\n",
      "document precision, and fail to capture computational trade\n",
      "offs. Most critically, they provide no standardized framework\n",
      "for comparing RAG effectiveness across different models and\n",
      "domains.\n",
      "We introduce OmniBench RAG, a novel automated platform\n",
      "for multi domain evaluation of RAG systems. The platform\n",
      "quantifies performance gains across accuracy and efficiency\n",
      "dimensions, spanning nine knowledge fields including culture,\n",
      "geography, and health. We introduce two standardized metrics:\n",
      "Improvements (accuracy gains) and Transformation (efficiency\n",
      "differences between pre RAG and post RAG models), enabling\n",
      "reproducible comparisons across models and tasks. The platform\n",
      "features dynamic test generation, modular evaluation pipelines,\n",
      "and automated knowledge base construction. Our evaluation\n",
      "reveals striking variability in RAG effectiveness, from significant\n",
      "gains in culture to declines in mathematics, highlighting the\n",
      "critical importance of systematic, domain aware assessment. A\n",
      "demonstration video is available at: https://www.youtube.com/\n",
      "watch?v=BZx83QFcTCI. Code and datasets: https://github.com/\n",
      "Garnett-Liang/Omnibench-RAG.\n",
      "I. INTRODUCTION\n",
      "Retrieval-Augmented Generation (RAG) is a key technique\n",
      "for enhancing Large Language Models (LLMs) [1], [2]. By\n",
      "grounding model responses in external, verifiable knowledge,\n",
      "RAG promises to mitigate hallucinations [3], improve factual\n",
      "accuracy [4], and provide up-to-date information [5]. However,\n",
      "the true effectiveness of RAG is far from uniform. Recent\n",
      "studies [6], [7] reveal a significant disparity: while RAG\n",
      "can boost the accuracy of smaller models like Llama-3.2-3B-\n",
      "Instruct by as much as 38.12%, its impact on state-of-the-art\n",
      "models such as GPT-4o, which excel with extended context\n",
      "windows, is often less pronounced. This variability, which\n",
      "depends not only on the modelâ€™s scale but also heavily on\n",
      "the knowledge domain, underscores a critical challenge: the\n",
      "lack of a systematic platform to quantify the value of RAG\n",
      "across these diverse contexts.\n",
      "Current RAG evaluation approaches suffer from fundamen-\n",
      "tal limitations that impede reproducible and comprehensive as-\n",
      "sessment: First, they lack automated multi-domain evalua-\n",
      "tion capabilities and rely on non-deterministic components.\n",
      "*Jiaxuan Liang and Shide Zhou are co-first authors.\n",
      "â€ Kailong Wang is the corresponding author.\n",
      "Existing benchmarks typically require manual configuration\n",
      "for each knowledge domain and fail to provide unified assess-\n",
      "ment across diverse fields like finance, healthcare, or culture,\n",
      "making cross-domain performance analysis labor-intensive and\n",
      "inconsistent. Moreover, key metrics in leading frameworks\n",
      "(e.g., LLM-based scoring in Ragas [8]) inadvertently introduce\n",
      "randomness due to reliance on large language models in the\n",
      "evaluation loop, undermining result reproducibility. Second,\n",
      "they employ static datasets and coarse-grained metrics.\n",
      "Most frameworks rely on fixed benchmarks and document-\n",
      "level retrieval metrics (e.g., MRR@k[9]), missing the critical\n",
      "sub-document precision needed to assess whether models\n",
      "extract specific facts accurately. They also lack the ability to\n",
      "dynamically generate test cases that probe complex reasoning\n",
      "patterns. Third, they fail to capture the computational\n",
      "trade-offs inherent in RAG systems. Without automated\n",
      "profiling of resource utilization and efficiency metrics, practi-\n",
      "tioners cannot make informed decisions about the cost-benefit\n",
      "trade-offs of deploying RAG in production environments.\n",
      "Our \n",
      "</Content>\n",
      "</Document>\n",
      "\n",
      "---\n",
      "\n",
      "<Document source=\"http://arxiv.org/abs/2508.13828v1\" date=\"2025-08-19\" authors=\"Yifei Chen, Guanting Dong, Yutao Zhu, Zhicheng Dou\"/>\n",
      "<Title>\n",
      "Revisiting RAG Ensemble: A Theoretical and Mechanistic Analysis of Multi-RAG System Collaboration\n",
      "</Title>\n",
      "\n",
      "<Summary>\n",
      "Retrieval-Augmented Generation (RAG) technology has been widely applied in\n",
      "recent years. However, despite the emergence of various RAG frameworks, a\n",
      "single RAG framework still cannot adapt well to a broad range of downstream\n",
      "tasks. Therefore, how to leverage the advantages of multiple RAG systems has\n",
      "become an area worth exploring. To address this issue, we have conducted a\n",
      "comprehensive and systematic investigation into ensemble methods based on RAG\n",
      "systems. Specifically, we have analyzed the RAG ensemble framework from both\n",
      "theoretical and mechanistic analysis perspectives. From the theoretical\n",
      "analysis, we provide the first explanation of the RAG ensemble framework from\n",
      "the perspective of information entropy. In terms of mechanism analysis, we have\n",
      "explored the RAG ensemble framework from both the pipeline and module levels.\n",
      "We carefully select four different pipelines (Branching, Iterative, Loop, and\n",
      "Agentic) and three different modules (Generator, Retriever, and Reranker) to\n",
      "solve seven different research questions. The experiments show that aggregating\n",
      "multiple RAG systems is both generalizable and robust, whether at the pipeline\n",
      "level or the module level. Our work lays the foundation for similar research on\n",
      "the multi-RAG system ensemble.\n",
      "</Summary>\n",
      "\n",
      "<Content>\n",
      "Revisiting RAG Ensemble: A Theoretical and Mechanistic\n",
      "Analysis of Multi-RAG System Collaboration\n",
      "Yifei Chen\n",
      "zhangboguodong@ruc.edu.cn\n",
      "Gaoling School of Artificial Intelligence, Renmin\n",
      "University of China\n",
      "Haidian Qu, Beijing Shi, China\n",
      "Guanting Dong\n",
      "Gaoling School of Artificial Intelligence, Renmin\n",
      "University of China\n",
      "Haidian Qu, Beijing Shi, China\n",
      "Yutao Zhu\n",
      "Gaoling School of Artificial Intelligence, Renmin\n",
      "University of China\n",
      "Haidian Qu, Beijing Shi, China\n",
      "Zhicheng Dou\n",
      "Gaoling School of Artificial Intelligence, Renmin\n",
      "University of China\n",
      "Haidian Qu, Beijing Shi, China\n",
      "Abstract\n",
      "Retrieval-Augmented Generation (RAG) technology has been widely\n",
      "applied in recent years. However, despite the emergence of various\n",
      "RAG frameworks, a single RAG framework still cannot adapt well\n",
      "to a broad range of downstream tasks. Therefore, how to leverage\n",
      "the advantages of multiple RAG systems has become an area worth\n",
      "exploring. To address this issue, we have conducted a comprehen-\n",
      "sive and systematic investigation into ensemble methods based on\n",
      "RAG systems. Specifically, we have analyzed the RAG ensemble\n",
      "framework from both theoretical and mechanistic analysis perspec-\n",
      "tives. From the theoretical analysis, we provide the first explanation\n",
      "of the RAG ensemble framework from the perspective of informa-\n",
      "tion entropy. In terms of mechanism analysis, we have explored\n",
      "the RAG ensemble framework from both the pipeline and module\n",
      "levels. We carefully select four different pipelines (Branching, Iter-\n",
      "ative, Loop, and Agentic) and three different modules (Generator,\n",
      "Retriever, and Reranker) to solve seven different research questions.\n",
      "The experiments show that aggregating multiple RAG systems is\n",
      "both generalizable and robust, whether at the pipeline level or the\n",
      "module level. Our work lays the foundation for similar research on\n",
      "the multi-RAG system ensemble.\n",
      "CCS Concepts\n",
      "â€¢ Information systems â†’Information integration.\n",
      "Keywords\n",
      "Retrieval-Augmented Generation, Pipeline Ensemble, Module En-\n",
      "semble, Model Preference\n",
      "ACM Reference Format:\n",
      "Yifei Chen, Guanting Dong, Yutao Zhu, and Zhicheng Dou. 2025. Revisiting\n",
      "RAG Ensemble: A Theoretical and Mechanistic Analysis of Multi-RAG\n",
      "Permission to make digital or hard copies of all or part of this work for personal or\n",
      "classroom use is granted without fee provided that copies are not made or distributed\n",
      "for profit or commercial advantage and that copies bear this notice and the full citation\n",
      "on the first page. Copyrights for components of this work owned by others than the\n",
      "author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\n",
      "republish, to post on servers or to redistribute to lists, requires prior specific permission\n",
      "and/or a fee. Request permissions from permissions@acm.org.\n",
      "CIKMâ€™25, Seoul, Korea\n",
      "Â© 2025 Copyright held by the owner/author(s). Publication rights licensed to ACM.\n",
      "ACM ISBN 978-1-4503-XXXX-X/2018/06\n",
      "https://doi.org/XXXXXXX.XXXXXXX\n",
      "System Collaboration. In Proceedings of CIKMâ€™25. ACM, New York, NY, USA,\n",
      "13 pages. https://doi.org/XXXXXXX.XXXXXXX\n",
      "1\n",
      "Introduction\n",
      "The emergence of Large Language Models (LLMs) has profoundly\n",
      "revolutionized many real-world tasks that rely on natural lan-\n",
      "guage [4, 45, 70]. However, when dealing with knowledge-intensive\n",
      "tasks, LLMs relying solely on their parametric knowledge often suf-\n",
      "fer from factual inconsistencies or hallucinations. To address these\n",
      "limitations, Retrieval-Augmented Generation (RAG) methods have\n",
      "been proposed, augmenting LLMs with dynamically retrieved exter-\n",
      "nal knowledge. This integration enhances response accuracy and\n",
      "reliability by grounding outputs in verifiable information sources.\n",
      "As research in this field advances, more and more RAG methods\n",
      "have been proposed. Component Module RAG inserts various mod-\n",
      "ules into the standard pipeline to better complete the retrieval task.\n",
      "For instance, the LongLLMLingua and RECOMP methods refine\n",
      "the retrieved knowledge with a refiner, and the SKR and Adaptive\n",
      "RAG methods distinguish the diffi\n",
      "</Content>\n",
      "</Document>\n",
      "\n",
      "---\n",
      "\n",
      "<Document source=\"http://arxiv.org/abs/2507.18515v1\" date=\"2025-07-24\" authors=\"Zezhou Yang, Ting Peng, Cuiyun Gao, Chaozheng Wang, Hailiang Huang, Yuetang Deng\"/>\n",
      "<Title>\n",
      "A Deep Dive into Retrieval-Augmented Generation for Code Completion: Experience on WeChat\n",
      "</Title>\n",
      "\n",
      "<Summary>\n",
      "Code completion, a crucial task in software engineering that enhances\n",
      "developer productivity, has seen substantial improvements with the rapid\n",
      "advancement of large language models (LLMs). In recent years,\n",
      "retrieval-augmented generation (RAG) has emerged as a promising method to\n",
      "enhance the code completion capabilities of LLMs, which leverages relevant\n",
      "context from codebases without requiring model retraining. While existing\n",
      "studies have demonstrated the effectiveness of RAG on public repositories and\n",
      "benchmarks, the potential distribution shift between open-source and\n",
      "closed-source codebases presents unique challenges that remain unexplored. To\n",
      "mitigate the gap, we conduct an empirical study to investigate the performance\n",
      "of widely-used RAG methods for code completion in the industrial-scale codebase\n",
      "of WeChat, one of the largest proprietary software systems. Specifically, we\n",
      "extensively explore two main types of RAG methods, namely identifier-based RAG\n",
      "and similarity-based RAG, across 26 open-source LLMs ranging from 0.5B to 671B\n",
      "parameters. For a more comprehensive analysis, we employ different retrieval\n",
      "techniques for similarity-based RAG, including lexical and semantic retrieval.\n",
      "Based on 1,669 internal repositories, we achieve several key findings: (1) both\n",
      "RAG methods demonstrate effectiveness in closed-source repositories, with\n",
      "similarity-based RAG showing superior performance, (2) the effectiveness of\n",
      "similarity-based RAG improves with more advanced retrieval techniques, where\n",
      "BM25 (lexical retrieval) and GTE-Qwen (semantic retrieval) achieve superior\n",
      "performance, and (3) the combination of lexical and semantic retrieval\n",
      "techniques yields optimal results, demonstrating complementary strengths.\n",
      "Furthermore, we conduct a developer survey to validate the practical utility of\n",
      "RAG methods in real-world development environments.\n",
      "</Summary>\n",
      "\n",
      "<Content>\n",
      "A Deep Dive into Retrieval-Augmented Generation\n",
      "for Code Completion: Experience on WeChat\n",
      "Zezhou Yang\n",
      "Tencent\n",
      "Guangzhou, China\n",
      "zezhouyang@tencent.com\n",
      "Ting Peng\n",
      "Tencent\n",
      "Guangzhou, China\n",
      "sakurapeng@tencent.com\n",
      "Cuiyun Gaoâˆ—\n",
      "The Chinese University of Hong Kong\n",
      "Hong Kong, China\n",
      "cuiyungao@outlook.com\n",
      "Chaozheng Wang\n",
      "The Chinese University of Hong Kong\n",
      "Hong Kong, China\n",
      "adf111178@gmail.com\n",
      "Hailiang Huang\n",
      "Tencent\n",
      "Guangzhou, China\n",
      "eraserhuang@tencent.com\n",
      "Yuetang Deng\n",
      "Tencent\n",
      "Guangzhou, China\n",
      "yuetangdeng@tencent.com\n",
      "Abstractâ€”Code completion, a crucial task in software engi-\n",
      "neering that enhances developer productivity, has seen substantial\n",
      "improvements with the rapid advancement of large language\n",
      "models (LLMs). In recent years, retrieval-augmented generation\n",
      "(RAG) has emerged as a promising method to enhance the code\n",
      "completion capabilities of LLMs, which leverages relevant con-\n",
      "text from codebases without requiring model retraining. While\n",
      "existing studies have demonstrated the effectiveness of RAG on\n",
      "public repositories and benchmarks, the potential distribution\n",
      "shift between open-source and closed-source codebases presents\n",
      "unique challenges that remain unexplored. To mitigate the gap,\n",
      "we conduct an empirical study to investigate the performance of\n",
      "widely-used RAG methods for code completion in the industrial-\n",
      "scale codebase of WeChat, one of the largest proprietary software\n",
      "systems. Specifically, we extensively explore two main types\n",
      "of RAG methods, namely identifier-based RAG and similarity-\n",
      "based RAG, across 26 open-source LLMs ranging from 0.5B\n",
      "to 671B parameters. For a more comprehensive analysis, we\n",
      "employ different retrieval techniques for similarity-based RAG,\n",
      "including lexical and semantic retrieval. Based on 1,669 internal\n",
      "repositories, we achieve several key findings: (1) both RAG\n",
      "methods demonstrate effectiveness in closed-source repositories,\n",
      "with similarity-based RAG showing superior performance, (2)\n",
      "the effectiveness of similarity-based RAG improves with more\n",
      "advanced retrieval techniques, where BM25 (lexical retrieval)\n",
      "and GTE-Qwen (semantic retrieval) achieve superior perfor-\n",
      "mance, and (3) the combination of lexical and semantic retrieval\n",
      "techniques yields optimal results, demonstrating complemen-\n",
      "tary strengths. Furthermore, we conduct a developer survey\n",
      "to validate the practical utility of RAG methods in real-world\n",
      "development environments.\n",
      "Index Termsâ€”large language model, retrieval-augmented gen-\n",
      "eration, code completion\n",
      "I. INTRODUCTION\n",
      "Code completion, which automatically predicts and sug-\n",
      "gests code fragments based on the surrounding programming\n",
      "context, has evolved from simple token-level suggestions to\n",
      "âˆ—Cuiyun Gao is the corresponding author.\n",
      "This research is supported by National Key R&D Program of China\n",
      "(No. 2022YFB3103900), National Natural Science Foundation of China\n",
      "under project (No. 62472126), Natural Science Foundation of Guangdong\n",
      "Province (Project No. 2023A1515011959), Shenzhen-Hong Kong Jointly\n",
      "Funded Project (Category A, No. SGDX20230116091246007), and Shenzhen\n",
      "Basic Research (General Project No. JCYJ20220531095214031).\n",
      "generating entire code blocks [1], [2]. Studies have demon-\n",
      "strated that code completion tools substantially enhance de-\n",
      "veloper productivity in real-world software development [3],\n",
      "[4]. Notably, 87% of professional developers report significant\n",
      "improvements in their coding efficiency when utilizing code\n",
      "completion tools in industrial settings [5]. Recent advances\n",
      "in large language models (LLMs) have further transformed\n",
      "various software engineering tasks [6]â€“[10], demonstrating\n",
      "unprecedented capabilities in code understanding and gen-\n",
      "eration. These models have achieved particularly impressive\n",
      "performance in code completion tasks [11]â€“[13].\n",
      "To enhance LLMsâ€™ performance on domain-specific tasks,\n",
      "researchers have explored Retrieval-Augmented Generation\n",
      "(RAG), which augments model inference by retrieving and in-\n",
      "corporating relevant context from the target codebase without\n",
      "requiring param\n",
      "</Content>\n",
      "</Document>\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36msearch_web\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "<Document href=\"https://rabiloo.com/blog/the-3-types-of-rag-models-naive-rag-modular-rag-and-advanced-rag\"/>\n",
      "Building AI systems requires choosing the right tools for the job, and Retrieval-Augmented Generation (RAG) offers various models, each designed to serve different needs. Essentially, a RAG model retrieves information from external databases, knowledge bases, or the web and then uses that data to generate relevant, meaningful responses. Naive RAG begins by indexing the data source for quick retrieval of relevant information. Modular RAG builds on the basic principles of Naive RAG by breaking down the retrieval and generation processes into separate, specialized modules. Unlike Naive RAG or Modular RAG, this stage is more precise and contextually aware, ensuring that the system retrieves only the most relevant and contextually appropriate data. The generation module in Advanced RAG then processes the refined data using sophisticated models like T5 or GPT-3.\n",
      "</Document>\n",
      "\n",
      "---\n",
      "\n",
      "<Document href=\"https://medium.com/@drjulija/what-are-naive-rag-advanced-rag-modular-rag-paradigms-edff410c202e\"/>\n",
      "I share the differences between Naive RAG, Advanced RAG and Modular RAG frameworks. Retrieval-Augmented Generation (RAG) framework is designed to help mitigate these challenges. RAG enhances LLMsâ€™ knowledge base with additional, domain-specific data. RAG systems can be split into 3 categories: See the comparison between all three paradigms of RAG â€” Naive RAG, Advanced RAG and Modular RAG below.\n",
      "</Document>\n",
      "\n",
      "---\n",
      "\n",
      "<Document href=\"https://blog.searce.com/unlocking-the-power-of-llm-rag-discover-naive-advanced-and-modular-retrieval-augmented-6922b353d8d3\"/>\n",
      "Aug 5, 2024Â·Modular RAG takes the concept further by enabling the customization and modularization of the retrieval process. This allows for tailored\n",
      "</Document>\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36manswer_question\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: expert\n",
      "\n",
      "Certainly, Dr. Kim. The primary difference between Modular RAG and Naive RAG lies in their structure and approach to information retrieval and generation.\n",
      "\n",
      "Naive RAG starts by indexing data sources to enable quick retrieval of relevant information. It maintains a straightforward approach where the retrieval and generation processes occur in a single, undivided pipeline. This simplicity can sometimes limit its flexibility and precision, particularly in handling complex or specialized data requirements in production environments [4].\n",
      "\n",
      "Modular RAG, on the other hand, builds upon Naive RAG by compartmentalizing the retrieval and generation processes into distinct, specialized modules. This modularity allows for greater flexibility as each module can be independently optimized or replaced to better suit specific data characteristics or processing needs. For example, different retrieval techniques can be employed within the same system, depending on the data type and retrieval demands. This separation allows for more precise and context-aware data retrieval, which enhances the overall responsiveness and relevance of the generated outputs [4][5].\n",
      "\n",
      "In production settings, using Modular RAG offers several benefits:\n",
      "\n",
      "1. **Customization**: By separating the retrieval and generation processes, organizations can tailor each component to better align with their data and application requirements, ensuring higher performance and efficiency.\n",
      "\n",
      "2. **Scalability**: Modular systems can be easily scaled by adapting or swapping individual modules without overhauling the entire architecture, which is particularly useful as data complexity grows.\n",
      "\n",
      "3. **Improved Accuracy**: With specialized modules, Modular RAG systems can achieve better accuracy in retrieving relevant data and generating coherent outputs, as each module is tuned to handle specific tasks optimally.\n",
      "\n",
      "These features make Modular RAG a more robust choice for handling complex and dynamic data requirements in production environments compared to Naive RAG [4].\n",
      "\n",
      "Sources:\n",
      "[4] medium.com/@drjulija/what-are-naive-rag-advanced-rag-modular-rag-paradigms-edff410c202e\n",
      "[5] blog.searce.com/unlocking-the-power-of-llm-rag-discover-naive-advanced-and-modular-retrieval-augmented-6922b353d8d3\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mask_question\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Thank you for that detailed explanation, Dr. Kim. Could you please provide a specific example or case study where Modular RAG was effectively implemented in a production setting, highlighting the challenges faced and how the modular approach was beneficial?\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36msearch_web\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "<Document href=\"https://rabiloo.com/blog/the-3-types-of-rag-models-naive-rag-modular-rag-and-advanced-rag\"/>\n",
      "Building AI systems requires choosing the right tools for the job, and Retrieval-Augmented Generation (RAG) offers various models, each designed to serve different needs. Essentially, a RAG model retrieves information from external databases, knowledge bases, or the web and then uses that data to generate relevant, meaningful responses. Naive RAG begins by indexing the data source for quick retrieval of relevant information. Modular RAG builds on the basic principles of Naive RAG by breaking down the retrieval and generation processes into separate, specialized modules. Unlike Naive RAG or Modular RAG, this stage is more precise and contextually aware, ensuring that the system retrieves only the most relevant and contextually appropriate data. The generation module in Advanced RAG then processes the refined data using sophisticated models like T5 or GPT-3.\n",
      "</Document>\n",
      "\n",
      "---\n",
      "\n",
      "<Document href=\"https://adasci.org/how-does-modular-rag-improve-upon-naive-rag/\"/>\n",
      "Naive RAG, the initial implementation of Retrieval-Augmented Generation, operates on a straightforward principle: retrieve relevant documents from an external knowledge base and use these documents to inform the generative process. The retrieval process in Naive RAG is relatively static and lacks flexibility, often leading to inefficiencies and suboptimal integration with the generative model. By adopting a modular architecture, this approach addresses the limitations of Naive RAG, offering enhanced flexibility, scalability, and efficiency. Unlike Naive RAG, which operates as a monolithic entity, Modular RAG breaks down the retrieval and generation processes into distinct, interchangeable modules. * *Seamless Integration*: Generative models in Modular RAG are designed to seamlessly integrate with various retrieval modules, enhancing the coherence and relevance of generated responses.\n",
      "</Document>\n",
      "\n",
      "---\n",
      "\n",
      "<Document href=\"https://www.superteams.ai/blog/how-to-implement-naive-rag-advanced-rag-and-modular-rag\"/>\n",
      "Naive RAG is a paradigm that combines information retrieval with natural language generation to produce responses to queries or prompts. In Naive RAG, retrieval is typically performed using retrieval models that rank the indexed data based on its relevance to the input query. These models generate text based on the input query and the retrieved context, aiming to produce coherent and contextually relevant responses. Advanced RAG models may fine-tune embeddings to capture task-specific semantics or domain knowledge, thereby improving the quality of retrieved information and generated responses. Dynamic embedding techniques enable RAG models to adaptively adjust embeddings during inference based on the context of the query or retrieved information.\n",
      "</Document>\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36msearch_arxiv\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "<Document source=\"http://arxiv.org/abs/2502.11228v2\" date=\"2025-05-22\" authors=\"Mohammad Reza Rezaei, Adji Bousso Dieng\"/>\n",
      "<Title>\n",
      "Vendi-RAG: Adaptively Trading-Off Diversity And Quality Significantly Improves Retrieval Augmented Generation With LLMs\n",
      "</Title>\n",
      "\n",
      "<Summary>\n",
      "Retrieval-augmented generation (RAG) enhances large language models (LLMs)\n",
      "for domain-specific question-answering (QA) tasks by leveraging external\n",
      "knowledge sources. However, traditional RAG systems primarily focus on\n",
      "relevance-based retrieval and often struggle with redundancy, especially when\n",
      "reasoning requires connecting information from multiple sources. This paper\n",
      "introduces Vendi-RAG, a framework based on an iterative process that jointly\n",
      "optimizes retrieval diversity and answer quality. This joint optimization leads\n",
      "to significantly higher accuracy for multi-hop QA tasks. Vendi-RAG leverages\n",
      "the Vendi Score (VS), a flexible similarity-based diversity metric, to promote\n",
      "semantic diversity in document retrieval. It then uses an LLM judge that\n",
      "evaluates candidate answers, generated after a reasoning step, and outputs a\n",
      "score that the retriever uses to balance relevance and diversity among the\n",
      "retrieved documents during each iteration. Experiments on three challenging\n",
      "datasets -- HotpotQA, MuSiQue, and 2WikiMultiHopQA -- demonstrate Vendi-RAG's\n",
      "effectiveness in multi-hop reasoning tasks. The framework achieves significant\n",
      "accuracy improvements over traditional single-step and multi-step RAG\n",
      "approaches, with accuracy increases reaching up to +4.2% on HotpotQA, +4.1% on\n",
      "2WikiMultiHopQA, and +1.3% on MuSiQue compared to Adaptive-RAG, the current\n",
      "best baseline. The benefits of Vendi-RAG are even more pronounced as the number\n",
      "of retrieved documents increases. Finally, we evaluated Vendi-RAG across\n",
      "different LLM backbones, including GPT-3.5, GPT-4, and GPT-4o-mini, and\n",
      "observed consistent improvements, demonstrating that the framework's advantages\n",
      "are model-agnostic.\n",
      "</Summary>\n",
      "\n",
      "<Content>\n",
      "Vendi-RAG: Adaptively Trading-Off Diversity And\n",
      "Quality Significantly Improves Retrieval\n",
      "Augmented Generation With LLMs\n",
      "Mohammad R. Rezaei1, 3 and Adji Bousso Dieng2, 3\n",
      "1Institute of Biomedical Engineering, University of Toronto\n",
      "2Department of Computer Science, Princeton University\n",
      "3Vertaix\n",
      "May 26, 2025\n",
      "Abstract\n",
      "Retrieval-augmented generation (RAG) enhances large language models (LLMs)\n",
      "for domain-specific question-answering (QA) tasks by leveraging external\n",
      "knowledge sources. However, traditional RAG systems primarily focus on\n",
      "relevance-based retrieval and often struggle with redundancy, especially when\n",
      "reasoning requires connecting information from multiple sources. This paper\n",
      "introduces Vendi-RAG, a framework based on an iterative process that jointly\n",
      "optimizes retrieval diversity and answer quality. This joint optimization leads\n",
      "to significantly higher accuracy for multi-hop QA tasks. Vendi-RAG leverages\n",
      "the Vendi Score (VS), a flexible similarity-based diversity metric, to promote\n",
      "semantic diversity in document retrieval. It then uses an LLM judge that eval-\n",
      "uates candidate answers, generated after a reasoning step, and outputs a\n",
      "score that the retriever uses to balance relevance and diversity among the\n",
      "retrieved documents during each iteration. Experiments on three challenging\n",
      "datasetsâ€”HotpotQA, MuSiQue, and 2WikiMultiHopQAâ€”demonstrate Vendi-\n",
      "RAGâ€™s effectiveness in multi-hop reasoning tasks. The framework achieves sig-\n",
      "nificant accuracy improvements over traditional single-step or multi-step RAG\n",
      "approaches, with accuracy increases reaching +4.2% on HotpotQA, +4.1% on\n",
      "2WikiMultiHopQA, and +1.3% on MuSiQue compared to Adaptive-RAG, the\n",
      "current best baseline. The benefits of Vendi-RAG are even more pronounced as\n",
      "the number of retrieved documents increases. Finally, we evaluated Vendi-RAG\n",
      "across different LLM backbones, including GPT-3.5, GPT-4, and GPT-4o-mini,\n",
      "and observed consistent improvements, demonstrating that the frameworkâ€™s\n",
      "advantages are model-agnostic.\n",
      "Keywords: RAG, LLMs, Question Answering, NLP, Diversity, Vendi Scoring\n",
      "1\n",
      "Introduction\n",
      "Retrieval-augmented generation (RAG) has emerged as a transformative framework\n",
      "for enhancing the performance of large language models (LLMs) in domain-specific\n",
      "tasks such as question-answering (QA). By retrieving relevant information from\n",
      "external sources beyond the training set, RAG enables LLMs to answer specialized\n",
      "1\n",
      "arXiv:2502.11228v2  [cs.CL]  22 May 2025\n",
      "Retrieval\n",
      "Query\n",
      "Answer\n",
      "LLM Judge \n",
      "Quality Check\n",
      "Rewritten  \n",
      "Query\n",
      "Final \n",
      "Answer\n",
      "Vendi Score\n",
      "Vendi Score\n",
      "Diversity  \n",
      "weight s\n",
      "Decision Block \n",
      "s > threshold Thr \n",
      "Iteration < N\n",
      "â€¦\n",
      "Reasoning\n",
      "Figure 1: The process begins with an initial retrieval step, where a diverse set of\n",
      "documents is retrieved using the Vendi Score, ensuring broad semantic coverage.\n",
      "Next, leveraging a reasoning step to construct a coherent path to the final answer,\n",
      "the LLM generates an answer, which then undergoes quality assessment by an LLM\n",
      "judge. Based on the answer quality, the retriever is adjusted to balance diversity and\n",
      "relevance: high-quality answers limit the emphasis on diversity, while low-quality\n",
      "answers prompt the retriever to prioritize diversity more heavily. This adjustment\n",
      "is controlled by an adaptive parameter, s, which is updated over iterations. The\n",
      "process continues until the answer quality reaches an optimal threshold, denoted\n",
      "by Thr. Finally, the highest-quality responses and documents are selected, ensuring\n",
      "both diversity and accuracy.\n",
      "queries more effectively Achiam et al. (2023); Team et al. (2023); Jiang et al. (2024).\n",
      "This approach has been particularly successful in single-hop QA, where a question\n",
      "can be answered using information from a single document Raiaan et al. (2024);\n",
      "Kwiatkowski et al. (2019). For instance, answering a question such as \"Who wrote\n",
      "the novel Frankenstein?\" only requires retrieving relevant information from a single\n",
      "document containing this fact.\n",
      "However, multi-hop QA introduces significantly greater complexit\n",
      "</Content>\n",
      "</Document>\n",
      "\n",
      "---\n",
      "\n",
      "<Document source=\"http://arxiv.org/abs/2408.05933v1\" date=\"2024-08-12\" authors=\"Fei Liu, Zejun Kang, Xing Han\"/>\n",
      "<Title>\n",
      "Optimizing RAG Techniques for Automotive Industry PDF Chatbots: A Case Study with Locally Deployed Ollama Models\n",
      "</Title>\n",
      "\n",
      "<Summary>\n",
      "With the growing demand for offline PDF chatbots in automotive industrial\n",
      "production environments, optimizing the deployment of large language models\n",
      "(LLMs) in local, low-performance settings has become increasingly important.\n",
      "This study focuses on enhancing Retrieval-Augmented Generation (RAG) techniques\n",
      "for processing complex automotive industry documents using locally deployed\n",
      "Ollama models. Based on the Langchain framework, we propose a multi-dimensional\n",
      "optimization approach for Ollama's local RAG implementation. Our method\n",
      "addresses key challenges in automotive document processing, including\n",
      "multi-column layouts and technical specifications. We introduce improvements in\n",
      "PDF processing, retrieval mechanisms, and context compression, tailored to the\n",
      "unique characteristics of automotive industry documents. Additionally, we\n",
      "design custom classes supporting embedding pipelines and an agent supporting\n",
      "self-RAG based on LangGraph best practices. To evaluate our approach, we\n",
      "constructed a proprietary dataset comprising typical automotive industry\n",
      "documents, including technical reports and corporate regulations. We compared\n",
      "our optimized RAG model and self-RAG agent against a naive RAG baseline across\n",
      "three datasets: our automotive industry dataset, QReCC, and CoQA. Results\n",
      "demonstrate significant improvements in context precision, context recall,\n",
      "answer relevancy, and faithfulness, with particularly notable performance on\n",
      "the automotive industry dataset. Our optimization scheme provides an effective\n",
      "solution for deploying local RAG systems in the automotive sector, addressing\n",
      "the specific needs of PDF chatbots in industrial production environments. This\n",
      "research has important implications for advancing information processing and\n",
      "intelligent production in the automotive industry.\n",
      "</Summary>\n",
      "\n",
      "<Content>\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Optimizing RAG Techniques for Automotive Industry PDF Chatbots: A Case Study with \n",
      "Locally Deployed Ollama Models \n",
      "Optimizing RAG Techniques Based on Locally Deployed Ollama Models \n",
      "A Case Study with Locally Deployed Ollama Models \n",
      "Fei Liu * \n",
      "China Automotive Technology & Research Center, liufei@catarc.ac.cn \n",
      "Zejun Kang \n",
      "China Automotive Technology & Research Center, kangzejun@catarc.ac.cn \n",
      "Xing Han \n",
      "China Automotive Technology & Research Center, hanxing@catarc.ac.cn \n",
      "With the growing demand for offline PDF chatbots in automotive industrial production environments, optimizing the deployment \n",
      "of large language models (LLMs) in local, low-performance settings has become increasingly important. This study focuses on \n",
      "enhancing Retrieval-Augmented Generation (RAG) techniques for processing complex automotive industry documents using \n",
      "locally deployed Ollama models. \n",
      "Based on the Langchain framework, we propose a multi-dimensional optimization approach for Ollama's local RAG \n",
      "implementation. Our method addresses key challenges in automotive document processing, including multi-column layouts and \n",
      "technical specifications. We introduce improvements in PDF processing, retrieval mechanisms, and context compression, tailored \n",
      "to the unique characteristics of automotive industry documents. Additionally, we design custom classes supporting embedding \n",
      "pipelines and an agent supporting self-RAG based on LangGraph best practices. \n",
      "To evaluate our approach, we constructed a proprietary dataset comprising typical automotive industry documents, including \n",
      "technical reports and corporate regulations. We compared our optimized RAG model and self-RAG agent against a naive RAG \n",
      "baseline across three datasets: our automotive industry dataset, QReCC, and CoQA. Results demonstrate significant improvements \n",
      "in context precision, context recall, answer relevancy, and faithfulness, with particularly notable performance on the automotive \n",
      "industry dataset. \n",
      "Our optimization scheme provides an effective solution for deploying local RAG systems in the automotive sector, addressing the \n",
      "specific needs of PDF chatbots in industrial production environments. This research has important implications for advancing \n",
      "information processing and intelligent production in the automotive industry. \n",
      " \n",
      "* Place the footnote text for the author (if applicable) here.  \n",
      "CCS CONCEPTS â€¢ Computing methodologies â€¢ Artificial intelligence â€¢ Natural language processing â€¢ Natural language \n",
      "generation \n",
      " \n",
      "Additional Keywords and Phrases: Automotive Industry, Langchain, self-rag, PDF Processing, RAG, Ollama \n",
      "1 INTRODUCTION \n",
      "1.1 Research Background \n",
      "The automotive industry is undergoing a significant digital transformation, with an increasing reliance on complex \n",
      "technical documentation for various processes [1]. This shift encompasses design, manufacturing, and quality \n",
      "control, all of which now heavily depend on efficient information management systems [2]. The growing volume of \n",
      "technical documents, often in PDF format, has created a pressing need for advanced information retrieval and \n",
      "question-answering capabilities in industrial settings [3]. \n",
      "Large Language Models (LLMs) have emerged as powerful tools in natural language processing, demonstrating \n",
      "remarkable abilities in tasks such as document understanding and question answering [4]. These models have \n",
      "shown potential in handling the complex, domain-specific language often found in automotive documentation. \n",
      "However, the application of LLMs in industrial environments presents unique challenges, particularly in terms of \n",
      "computational resources and data privacy [5]. \n",
      "Among the various techniques developed to enhance LLM performance, Retrieval-Augmented Generation (RAG) \n",
      "has gained significant attention [6]. RAG combines the generative capabilities of LLMs with external knowledge \n",
      "retrieval, allowing for more accurate and contextually relevant responses. This approach, initially proposed by \n",
      "Lew\n",
      "</Content>\n",
      "</Document>\n",
      "\n",
      "---\n",
      "\n",
      "<Document source=\"http://arxiv.org/abs/2409.12682v1\" date=\"2024-09-19\" authors=\"Jiho Shin, Reem Aleithan, Hadi Hemmati, Song Wang\"/>\n",
      "<Title>\n",
      "Retrieval-Augmented Test Generation: How Far Are We?\n",
      "</Title>\n",
      "\n",
      "<Summary>\n",
      "Retrieval Augmented Generation (RAG) has shown notable advancements in\n",
      "software engineering tasks. Despite its potential, RAG's application in unit\n",
      "test generation remains under-explored. To bridge this gap, we take the\n",
      "initiative to investigate the efficacy of RAG-based LLMs in test generation. As\n",
      "RAGs can leverage various knowledge sources to enhance their performance, we\n",
      "also explore the impact of different sources of RAGs' knowledge bases on unit\n",
      "test generation to provide insights into their practical benefits and\n",
      "limitations. Specifically, we examine RAG built upon three types of domain\n",
      "knowledge: 1) API documentation, 2) GitHub issues, and 3) StackOverflow Q&As.\n",
      "Each source offers essential knowledge for creating tests from different\n",
      "perspectives, i.e., API documentations provide official API usage guidelines,\n",
      "GitHub issues offer resolutions of issues related to the APIs from the library\n",
      "developers, and StackOverflow Q&As present community-driven solutions and best\n",
      "practices. For our experiment, we focus on five widely used and typical\n",
      "Python-based machine learning (ML) projects, i.e., TensorFlow, PyTorch,\n",
      "Scikit-learn, Google JAX, and XGBoost to build, train, and deploy complex\n",
      "neural networks efficiently. We conducted experiments using the top 10% most\n",
      "widely used APIs across these projects, involving a total of 188 APIs. We\n",
      "investigate the effectiveness of four state-of-the-art LLMs (open and\n",
      "closed-sourced), i.e., GPT-3.5-Turbo, GPT-4o, Mistral MoE 8x22B, and Llamma 3.1\n",
      "405B. Additionally, we compare three prompting strategies in generating unit\n",
      "test cases for the experimental APIs, i.e., zero-shot, a Basic RAG, and an\n",
      "API-level RAG on the three external sources. Finally, we compare the cost of\n",
      "different sources of knowledge used for the RAG.\n",
      "</Summary>\n",
      "\n",
      "<Content>\n",
      "Retrieval-Augmented Test Generation: How Far Are We?\n",
      "JIHO SHIN, York University, Canada\n",
      "REEM ALEITHAN, York University, Canada\n",
      "HADI HEMMATI, York University, Canada\n",
      "SONG WANG, York University, Canada\n",
      "Retrieval Augmented Generation (RAG) has shown notable advancements in software engineering tasks.\n",
      "Despite its potential, RAGâ€™s application in unit test generation remains under-explored. To bridge this gap, we\n",
      "take the initiative to investigate the efficacy of RAG-based LLMs in test generation. As RAGs can leverage\n",
      "various knowledge sources to enhance their performance, we also explore the impact of different sources of\n",
      "RAGsâ€™ knowledge bases on unit test generation to provide insights into their practical benefits and limitations.\n",
      "Specifically, we examine RAG built upon three types of domain knowledge: 1) API documentation, 2) GitHub\n",
      "issues, and 3) StackOverflow Q&As. Each source offers essential knowledge for creating tests from different\n",
      "perspectives, i.e., API documentations provide official API usage guidelines, GitHub issues offer resolutions of\n",
      "issues related to the APIs from the library developers, and StackOverflow Q&As present community-driven\n",
      "solutions and best practices. For our experiment, we focus on five widely used and typical Python-based\n",
      "machine learning (ML) projects, i.e., TensorFlow, PyTorch, Scikit-learn, Google JAX, and XGBoost to build,\n",
      "train, and deploy complex neural networks efficiently. We conducted experiments using the top 10% most\n",
      "widely used APIs across these projects, involving a total of 188 APIs.\n",
      "We investigate the effectiveness of four state-of-the-art LLMs (open and closed-sourced), i.e., GPT-3.5-Turbo,\n",
      "GPT-4o, Mistral MoE 8x22B, and Llamma 3.1 405B. Additionally, we compare three prompting strategies in\n",
      "generating unit test cases for the experimental APIs, i.e., zero-shot, a Basic RAG, and an API-level RAG on the\n",
      "three external sources. Finally, we compare the cost of different sources of knowledge used for the RAG.\n",
      "We conduct both qualitative and quantitative evaluations to investigate the generated test cases. For the\n",
      "quantitative analysis, we assess the syntactical and dynamic correctness of the generated tests. We observe\n",
      "that RAG does not improve the syntactical or dynamic correctness of unit test cases. However, using Basic\n",
      "RAG could improve the line coverage by an average of 8.94% and API-level RAG by 9.68%. We investigate\n",
      "the token cost of different RAGs with different prompting strategies. We find that RAGs using GitHub issue\n",
      "documents have the highest token cost. We also find that using limiting the number of test cases for cost\n",
      "efficiency significantly reduces the cost. Finally, we perform a manual analysis over a subset of the generated\n",
      "tests to evaluate how different strategies impact the software under test in more depth. We find that RAG helps\n",
      "cover unique lines by providing unique states of programs that cannot be generated directly by LLMs. Our\n",
      "study suggests that RAG has great potential in improving unit testsâ€™ line coverage when the right documents\n",
      "with unique examples of program states are given. Proposing new retrieval techniques that can search these\n",
      "documents will further improve RAG-based unit test generation for future work.\n",
      "CCS Concepts: â€¢ Software and its engineering â†’Software maintenance tools; Automatic programming; â€¢\n",
      "Computing methodologies â†’Natural language generation; Machine translation; â€¢ General and reference\n",
      "â†’Surveys and overviews.\n",
      "Authorsâ€™ Contact Information: Jiho Shin, jihoshin@yorku.ca, York University, Toronto, ON, Canada; Reem Aleithan,\n",
      "reem1100@yorku.ca, York University, Toronto, ON, Canada; Hadi Hemmati, hemmati@yorku.ca, York University, Toronto,\n",
      "ON, Canada; Song Wang, wangsong@yorku.ca, York University, Toronto, ON, Canada.\n",
      "Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee\n",
      "provided that copies are not made or distributed for profit or commercial advantage and that copies bear \n",
      "</Content>\n",
      "</Document>\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36manswer_question\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: expert\n",
      "\n",
      "Certainly. Let's consider a case study in the automotive industry, as described in a research article on optimizing RAG techniques for PDF chatbots. The context involved processing complex automotive industry documents using locally deployed models. Here, Modular RAG was utilized to efficiently handle multi-column layouts and technical specifications typical of automotive documents [6].\n",
      "\n",
      "Challenges in this scenario included the need to process diverse document types and the requirement for high context precision and recall. Traditional methods, like Naive RAG, might struggle with these complexities due to their monolithic design, which limits the ability to tailor retrieval processes to meet specific document processing needs.\n",
      "\n",
      "By implementing a Modular RAG system, the retrieval mechanisms could be separately optimized to handle the specific challenges posed by automotive document structures. Custom modules for PDF processing and context compression were designed, allowing for tailored embedding pipelines and retrieval strategies. This modular approach resulted in significant improvements in context precision, recall, and answer relevancy, particularly when applied to the proprietary automotive industry dataset [6].\n",
      "\n",
      "The benefits of using Modular RAG in this production setting included enhanced flexibility to adapt retrieval processes to different document types and improved scalability and accuracy in generating contextually relevant responses. This case study demonstrates how Modular RAG can be effectively deployed to address complex data requirements in real-world scenarios, showcasing its advantage over more static frameworks like Naive RAG.\n",
      "\n",
      "[6] arxiv.org/abs/2408.05933v1\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mask_question\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "That's fascinating, Dr. Kim. It's clear that Modular RAG offers tangible benefits in complex environments like the automotive industry. One last question: are there any specific challenges or limitations associated with implementing Modular RAG that organizations should be aware of?\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36msearch_arxiv\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "<Document source=\"http://arxiv.org/abs/2508.05650v1\" date=\"2025-07-26\" authors=\"Jiaxuan Liang, Shide Zhou, Kailong Wang\"/>\n",
      "<Title>\n",
      "OmniBench-RAG: A Multi-Domain Evaluation Platform for Retrieval-Augmented Generation Tools\n",
      "</Title>\n",
      "\n",
      "<Summary>\n",
      "While Retrieval Augmented Generation (RAG) is now widely adopted to enhance\n",
      "LLMs, evaluating its true performance benefits in a reproducible and\n",
      "interpretable way remains a major hurdle. Existing methods often fall short:\n",
      "they lack domain coverage, employ coarse metrics that miss sub document\n",
      "precision, and fail to capture computational trade offs. Most critically, they\n",
      "provide no standardized framework for comparing RAG effectiveness across\n",
      "different models and domains.\n",
      "  We introduce OmniBench RAG, a novel automated platform for multi domain\n",
      "evaluation of RAG systems. The platform quantifies performance gains across\n",
      "accuracy and efficiency dimensions, spanning nine knowledge fields including\n",
      "culture, geography, and health. We introduce two standardized metrics:\n",
      "Improvements (accuracy gains) and Transformation (efficiency differences\n",
      "between pre RAG and post RAG models), enabling reproducible comparisons across\n",
      "models and tasks. The platform features dynamic test generation, modular\n",
      "evaluation pipelines, and automated knowledge base construction. Our evaluation\n",
      "reveals striking variability in RAG effectiveness, from significant gains in\n",
      "culture to declines in mathematics, highlighting the critical importance of\n",
      "systematic, domain aware assessment. A demonstration video is available at:\n",
      "https://www.youtube.com/watch?v=BZx83QFcTCI. Code and datasets:\n",
      "https://github.com/Garnett-Liang/Omnibench-RAG.\n",
      "</Summary>\n",
      "\n",
      "<Content>\n",
      "OmniBench-RAG: A Multi-Domain Evaluation\n",
      "Platform for Retrieval-Augmented Generation Tools\n",
      "Jiaxuan Liang*, Shide Zhou*, and Kailong Wangâ€ \n",
      "Huazhong University of Science and Technology\n",
      "{liangjx, shidez, wangkl}@hust.edu.cn\n",
      "Abstractâ€”While Retrieval Augmented Generation (RAG) is\n",
      "now widely adopted to enhance LLMs, evaluating its true\n",
      "performance benefits in a reproducible and interpretable way\n",
      "remains a major hurdle. Existing methods often fall short: they\n",
      "lack domain coverage, employ coarse metrics that miss sub\n",
      "document precision, and fail to capture computational trade\n",
      "offs. Most critically, they provide no standardized framework\n",
      "for comparing RAG effectiveness across different models and\n",
      "domains.\n",
      "We introduce OmniBench RAG, a novel automated platform\n",
      "for multi domain evaluation of RAG systems. The platform\n",
      "quantifies performance gains across accuracy and efficiency\n",
      "dimensions, spanning nine knowledge fields including culture,\n",
      "geography, and health. We introduce two standardized metrics:\n",
      "Improvements (accuracy gains) and Transformation (efficiency\n",
      "differences between pre RAG and post RAG models), enabling\n",
      "reproducible comparisons across models and tasks. The platform\n",
      "features dynamic test generation, modular evaluation pipelines,\n",
      "and automated knowledge base construction. Our evaluation\n",
      "reveals striking variability in RAG effectiveness, from significant\n",
      "gains in culture to declines in mathematics, highlighting the\n",
      "critical importance of systematic, domain aware assessment. A\n",
      "demonstration video is available at: https://www.youtube.com/\n",
      "watch?v=BZx83QFcTCI. Code and datasets: https://github.com/\n",
      "Garnett-Liang/Omnibench-RAG.\n",
      "I. INTRODUCTION\n",
      "Retrieval-Augmented Generation (RAG) is a key technique\n",
      "for enhancing Large Language Models (LLMs) [1], [2]. By\n",
      "grounding model responses in external, verifiable knowledge,\n",
      "RAG promises to mitigate hallucinations [3], improve factual\n",
      "accuracy [4], and provide up-to-date information [5]. However,\n",
      "the true effectiveness of RAG is far from uniform. Recent\n",
      "studies [6], [7] reveal a significant disparity: while RAG\n",
      "can boost the accuracy of smaller models like Llama-3.2-3B-\n",
      "Instruct by as much as 38.12%, its impact on state-of-the-art\n",
      "models such as GPT-4o, which excel with extended context\n",
      "windows, is often less pronounced. This variability, which\n",
      "depends not only on the modelâ€™s scale but also heavily on\n",
      "the knowledge domain, underscores a critical challenge: the\n",
      "lack of a systematic platform to quantify the value of RAG\n",
      "across these diverse contexts.\n",
      "Current RAG evaluation approaches suffer from fundamen-\n",
      "tal limitations that impede reproducible and comprehensive as-\n",
      "sessment: First, they lack automated multi-domain evalua-\n",
      "tion capabilities and rely on non-deterministic components.\n",
      "*Jiaxuan Liang and Shide Zhou are co-first authors.\n",
      "â€ Kailong Wang is the corresponding author.\n",
      "Existing benchmarks typically require manual configuration\n",
      "for each knowledge domain and fail to provide unified assess-\n",
      "ment across diverse fields like finance, healthcare, or culture,\n",
      "making cross-domain performance analysis labor-intensive and\n",
      "inconsistent. Moreover, key metrics in leading frameworks\n",
      "(e.g., LLM-based scoring in Ragas [8]) inadvertently introduce\n",
      "randomness due to reliance on large language models in the\n",
      "evaluation loop, undermining result reproducibility. Second,\n",
      "they employ static datasets and coarse-grained metrics.\n",
      "Most frameworks rely on fixed benchmarks and document-\n",
      "level retrieval metrics (e.g., MRR@k[9]), missing the critical\n",
      "sub-document precision needed to assess whether models\n",
      "extract specific facts accurately. They also lack the ability to\n",
      "dynamically generate test cases that probe complex reasoning\n",
      "patterns. Third, they fail to capture the computational\n",
      "trade-offs inherent in RAG systems. Without automated\n",
      "profiling of resource utilization and efficiency metrics, practi-\n",
      "tioners cannot make informed decisions about the cost-benefit\n",
      "trade-offs of deploying RAG in production environments.\n",
      "Our \n",
      "</Content>\n",
      "</Document>\n",
      "\n",
      "---\n",
      "\n",
      "<Document source=\"http://arxiv.org/abs/2407.21059v1\" date=\"2024-07-26\" authors=\"Yunfan Gao, Yun Xiong, Meng Wang, Haofen Wang\"/>\n",
      "<Title>\n",
      "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks\n",
      "</Title>\n",
      "\n",
      "<Summary>\n",
      "Retrieval-augmented Generation (RAG) has markedly enhanced the capabilities\n",
      "of Large Language Models (LLMs) in tackling knowledge-intensive tasks. The\n",
      "increasing demands of application scenarios have driven the evolution of RAG,\n",
      "leading to the integration of advanced retrievers, LLMs and other complementary\n",
      "technologies, which in turn has amplified the intricacy of RAG systems.\n",
      "However, the rapid advancements are outpacing the foundational RAG paradigm,\n",
      "with many methods struggling to be unified under the process of\n",
      "\"retrieve-then-generate\". In this context, this paper examines the limitations\n",
      "of the existing RAG paradigm and introduces the modular RAG framework. By\n",
      "decomposing complex RAG systems into independent modules and specialized\n",
      "operators, it facilitates a highly reconfigurable framework. Modular RAG\n",
      "transcends the traditional linear architecture, embracing a more advanced\n",
      "design that integrates routing, scheduling, and fusion mechanisms. Drawing on\n",
      "extensive research, this paper further identifies prevalent RAG\n",
      "patterns-linear, conditional, branching, and looping-and offers a comprehensive\n",
      "analysis of their respective implementation nuances. Modular RAG presents\n",
      "innovative opportunities for the conceptualization and deployment of RAG\n",
      "systems. Finally, the paper explores the potential emergence of new operators\n",
      "and paradigms, establishing a solid theoretical foundation and a practical\n",
      "roadmap for the continued evolution and practical deployment of RAG\n",
      "technologies.\n",
      "</Summary>\n",
      "\n",
      "<Content>\n",
      "1\n",
      "Modular RAG: Transforming RAG Systems into\n",
      "LEGO-like Reconfigurable Frameworks\n",
      "Yunfan Gao, Yun Xiong, Meng Wang, Haofen Wang\n",
      "Abstractâ€”Retrieval-augmented\n",
      "Generation\n",
      "(RAG)\n",
      "has\n",
      "markedly enhanced the capabilities of Large Language Models\n",
      "(LLMs) in tackling knowledge-intensive tasks. The increasing\n",
      "demands of application scenarios have driven the evolution\n",
      "of RAG, leading to the integration of advanced retrievers,\n",
      "LLMs and other complementary technologies, which in turn\n",
      "has amplified the intricacy of RAG systems. However, the rapid\n",
      "advancements are outpacing the foundational RAG paradigm,\n",
      "with many methods struggling to be unified under the process\n",
      "of â€œretrieve-then-generateâ€. In this context, this paper examines\n",
      "the limitations of the existing RAG paradigm and introduces\n",
      "the modular RAG framework. By decomposing complex RAG\n",
      "systems into independent modules and specialized operators, it\n",
      "facilitates a highly reconfigurable framework. Modular RAG\n",
      "transcends the traditional linear architecture, embracing a\n",
      "more advanced design that integrates routing, scheduling, and\n",
      "fusion mechanisms. Drawing on extensive research, this paper\n",
      "further identifies prevalent RAG patternsâ€”linear, conditional,\n",
      "branching, and loopingâ€”and offers a comprehensive analysis\n",
      "of their respective implementation nuances. Modular RAG\n",
      "presents\n",
      "innovative\n",
      "opportunities\n",
      "for\n",
      "the\n",
      "conceptualization\n",
      "and deployment of RAG systems. Finally, the paper explores\n",
      "the potential emergence of new operators and paradigms,\n",
      "establishing a solid theoretical foundation and a practical\n",
      "roadmap for the continued evolution and practical deployment\n",
      "of RAG technologies.\n",
      "Index Termsâ€”Retrieval-augmented generation, large language\n",
      "model, modular system, information retrieval\n",
      "I. INTRODUCTION\n",
      "L\n",
      "ARGE Language Models (LLMs) have demonstrated\n",
      "remarkable capabilities, yet they still face numerous\n",
      "challenges, such as hallucination and the lag in information up-\n",
      "dates [1]. Retrieval-augmented Generation (RAG), by access-\n",
      "ing external knowledge bases, provides LLMs with important\n",
      "contextual information, significantly enhancing their perfor-\n",
      "mance on knowledge-intensive tasks [2]. Currently, RAG, as\n",
      "an enhancement method, has been widely applied in various\n",
      "practical application scenarios, including knowledge question\n",
      "answering, recommendation systems, customer service, and\n",
      "personal assistants. [3]â€“[6]\n",
      "During the nascent stages of RAG , its core framework is\n",
      "constituted by indexing, retrieval, and generation, a paradigm\n",
      "referred to as Naive RAG [7]. However, as the complexity\n",
      "of tasks and the demands of applications have escalated, the\n",
      "Yunfan Gao is with Shanghai Research Institute for Intelligent Autonomous\n",
      "Systems, Tongji University, Shanghai, 201210, China.\n",
      "Yun Xiong is with Shanghai Key Laboratory of Data Science, School of\n",
      "Computer Science, Fudan University, Shanghai, 200438, China.\n",
      "Meng Wang and Haofen Wang are with College of Design and Innovation,\n",
      "Tongji University, Shanghai, 20092, China. (Corresponding author: Haofen\n",
      "Wang. E-mail: carter.whfcarter@gmail.com)\n",
      "limitations of Naive RAG have become increasingly apparent.\n",
      "As depicted in Figure 1, it predominantly hinges on the\n",
      "straightforward similarity of chunks, result in poor perfor-\n",
      "mance when confronted with complex queries and chunks with\n",
      "substantial variability. The primary challenges of Naive RAG\n",
      "include: 1) Shallow Understanding of Queries. The semantic\n",
      "similarity between a query and document chunk is not always\n",
      "highly consistent. Relying solely on similarity calculations\n",
      "for retrieval lacks an in-depth exploration of the relationship\n",
      "between the query and the document [8]. 2) Retrieval Re-\n",
      "dundancy and Noise. Feeding all retrieved chunks directly\n",
      "into LLMs is not always beneficial. Research indicates that\n",
      "an excess of redundant and noisy information may interfere\n",
      "with the LLMâ€™s identification of key information, thereby\n",
      "increasing the risk of generating erroneous and hallucinated\n",
      "responses. [9]\n",
      "To overcome the aforementioned limitations, \n",
      "</Content>\n",
      "</Document>\n",
      "\n",
      "---\n",
      "\n",
      "<Document source=\"http://arxiv.org/abs/2403.09040v3\" date=\"2025-07-16\" authors=\"Jennifer Hsia, Afreen Shaikh, Zhiruo Wang, Graham Neubig\"/>\n",
      "<Title>\n",
      "RAGGED: Towards Informed Design of Scalable and Stable RAG Systems\n",
      "</Title>\n",
      "\n",
      "<Summary>\n",
      "Retrieval-augmented generation (RAG) enhances language models by integrating\n",
      "external knowledge, but its effectiveness is highly dependent on system\n",
      "configuration. Improper retrieval settings can degrade performance, making RAG\n",
      "less reliable than closed-book generation. In this work, we introduce RAGGED, a\n",
      "framework for systematically evaluating RAG systems across diverse\n",
      "retriever-reader configurations, retrieval depths, and datasets. Our analysis\n",
      "reveals that reader robustness to noise is the key determinant of RAG stability\n",
      "and scalability. Some readers benefit from increased retrieval depth, while\n",
      "others degrade due to their sensitivity to distracting content. Through\n",
      "large-scale experiments on open-domain, multi-hop, and specialized-domain\n",
      "datasets, we show that retrievers, rerankers, and prompts influence performance\n",
      "but do not fundamentally alter these reader-driven trends. By providing a\n",
      "principled framework and new metrics to assess RAG stability and scalability,\n",
      "RAGGED enables systematic evaluation of retrieval-augmented generation systems,\n",
      "guiding future research on optimizing retrieval depth and model robustness.\n",
      "</Summary>\n",
      "\n",
      "<Content>\n",
      "RAGGED: Towards Informed Design of Scalable and Stable RAG Systems\n",
      "Jennifer Hsia * 1 Afreen Shaikh * 2 Zhiruo Wang 2 Graham Neubig 2\n",
      "Abstract\n",
      "Retrieval-augmented generation (RAG) enhances\n",
      "language models by integrating external knowl-\n",
      "edge, but its effectiveness is highly dependent on\n",
      "system configuration. Improper retrieval settings\n",
      "can degrade performance, making RAG less reli-\n",
      "able than closed-book generation. In this work,\n",
      "we introduce RAGGED, a framework for system-\n",
      "atically evaluating RAG systems across diverse\n",
      "retriever-reader configurations, retrieval depths,\n",
      "and datasets. Our analysis reveals that reader ro-\n",
      "bustness to noise is the key determinant of RAG\n",
      "stability and scalability. Some readers benefit\n",
      "from increased retrieval depth, while others de-\n",
      "grade due to their sensitivity to distracting content.\n",
      "Through large-scale experiments on open-domain,\n",
      "multi-hop, and specialized-domain datasets, we\n",
      "show that retrievers, rerankers, and prompts in-\n",
      "fluence performance but do not fundamentally\n",
      "alter these reader-driven trends. By providing a\n",
      "principled framework and new metrics to assess\n",
      "RAG stability and scalability, RAGGED enables\n",
      "systematic evaluation of retrieval-augmented gen-\n",
      "eration systems, guiding future research on opti-\n",
      "mizing retrieval depth and model robustness. 1\n",
      "1. Introduction\n",
      "Retrieval-augmented generation (RAG) (Chen et al., 2017;\n",
      "Lewis et al., 2020) enhances large language models (LLMs)\n",
      "by retrieving relevant external contexts, enabling more spe-\n",
      "cific and factually grounded responses. However, despite\n",
      "its promise, RAGâ€™s effectiveness is not guaranteed. In fact,\n",
      "improper configurations can degrade model performance,\n",
      "leading to outputs that are worse than closed-book genera-\n",
      "*Equal contribution 1Machine Learning Department, Carnegie\n",
      "Mellon University, Pittsburgh, USA 2The Language Technologies\n",
      "Institute, Carnegie Mellon University, Pittsburgh, USA. Correspon-\n",
      "dence to: Jennifer Hsia <jhsia2@andrew.cmu.edu>.\n",
      "Proceedings of the 42 nd International Conference on Machine\n",
      "Learning, Vancouver, Canada. PMLR 267, 2025. Copyright 2025\n",
      "by the author(s).\n",
      "1Code and data for the RAGGED framework are available at\n",
      "https://github.com/neulab/ragged\n",
      "â€¦\n",
      "Reader\n",
      "Retriever\n",
      "â€¦ \n",
      "Documents\n",
      "R1: When does RAG \n",
      "improve over closed-book \n",
      "generation? Â§4\n",
      "R2: Impact of \n",
      "increasing context \n",
      "documents. Â§5\n",
      "R4: Impact of \n",
      "retriever choice Â§7\n",
      "R3: Robustness to \n",
      "irrelevant information Â§6\n",
      "Figure 1: Roadmap of what our framework RAGGED anal-\n",
      "yses across the RAG pipeline.\n",
      "tion. Understanding when and why RAG helps or harms is\n",
      "critical for optimizing system design.\n",
      "Most prior work evaluates RAG under controlled conditions\n",
      "and curated contexts (Liu et al., 2023; Cuconasu et al., 2024),\n",
      "which fail to reflect real-world retrieval challenges. In prac-\n",
      "tice, retrieved contexts contain both relevant and irrelevant\n",
      "information, making the reader modelâ€™s ability to filter noise\n",
      "a critical factor in RAG success. Additionally, prior studies\n",
      "provide conflicting findings on retrieval depth (k)â€” while\n",
      "some suggest increasing k improves performance (Izacard\n",
      "& Grave, 2021), others observe diminishing returns (Liu\n",
      "et al., 2023) or even degradation at high k (Cuconasu et al.,\n",
      "2024; Jiang et al., 2024). This lack of consensus leaves\n",
      "practitioners without clear guidance on how to configure\n",
      "RAG systems for different tasks.\n",
      "To address these challenges, we introduce RAGGED\n",
      "(Retrieval-Augmented Generation Generalized Evalu-\n",
      "ation Device), a framework for systematically evaluating\n",
      "RAG performance across retrieval depths, model architec-\n",
      "tures, and retrieval conditions. Unlike prior work, which\n",
      "often relies on synthetic or manual retrieval modifications,\n",
      "RAGGED assesses models under realistic retrieval scenar-\n",
      "ios â€” analyzing performance on naturally retrieved top-k\n",
      "contexts rather than manually curated, oracle-aware con-\n",
      "texts.\n",
      "Our study reveals that reader robustness to noise is the pri-\n",
      "mary factor driving RAG stability and scalability, rather\n",
      "than retriever quality al\n",
      "</Content>\n",
      "</Document>\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36msearch_web\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "<Document href=\"https://adasci.org/how-does-modular-rag-improve-upon-naive-rag/\"/>\n",
      "Naive RAG, the initial implementation of Retrieval-Augmented Generation, operates on a straightforward principle: retrieve relevant documents from an external knowledge base and use these documents to inform the generative process. The retrieval process in Naive RAG is relatively static and lacks flexibility, often leading to inefficiencies and suboptimal integration with the generative model. By adopting a modular architecture, this approach addresses the limitations of Naive RAG, offering enhanced flexibility, scalability, and efficiency. Unlike Naive RAG, which operates as a monolithic entity, Modular RAG breaks down the retrieval and generation processes into distinct, interchangeable modules. * *Seamless Integration*: Generative models in Modular RAG are designed to seamlessly integrate with various retrieval modules, enhancing the coherence and relevance of generated responses.\n",
      "</Document>\n",
      "\n",
      "---\n",
      "\n",
      "<Document href=\"https://rabiloo.com/blog/the-3-types-of-rag-models-naive-rag-modular-rag-and-advanced-rag\"/>\n",
      "Building AI systems requires choosing the right tools for the job, and Retrieval-Augmented Generation (RAG) offers various models, each designed to serve different needs. Essentially, a RAG model retrieves information from external databases, knowledge bases, or the web and then uses that data to generate relevant, meaningful responses. Naive RAG begins by indexing the data source for quick retrieval of relevant information. Modular RAG builds on the basic principles of Naive RAG by breaking down the retrieval and generation processes into separate, specialized modules. Unlike Naive RAG or Modular RAG, this stage is more precise and contextually aware, ensuring that the system retrieves only the most relevant and contextually appropriate data. The generation module in Advanced RAG then processes the refined data using sophisticated models like T5 or GPT-3.\n",
      "</Document>\n",
      "\n",
      "---\n",
      "\n",
      "<Document href=\"https://medium.com/@drjulija/what-are-naive-rag-advanced-rag-modular-rag-paradigms-edff410c202e\"/>\n",
      "I share the differences between Naive RAG, Advanced RAG and Modular RAG frameworks. Retrieval-Augmented Generation (RAG) framework is designed to help mitigate these challenges. RAG enhances LLMsâ€™ knowledge base with additional, domain-specific data. RAG systems can be split into 3 categories: See the comparison between all three paradigms of RAG â€” Naive RAG, Advanced RAG and Modular RAG below.\n",
      "</Document>\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36manswer_question\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: expert\n",
      "\n",
      "Yes, there are specific challenges and limitations associated with implementing Modular RAG that organizations should consider:\n",
      "\n",
      "1. **Integration Complexity**: While Modular RAG provides flexibility through its separate modules, integrating these components can sometimes be more complex than using a monolithic system like Naive RAG. Organizations must carefully design each module and ensure seamless interaction between them to maintain the system's overall efficiency and effectiveness [5].\n",
      "\n",
      "2. **Resource Intensive**: The modular approach might require more computational resources, especially if multiple or sophisticated modules are used. This can increase the cost of deployment, particularly in environments with limited infrastructure capabilities [5].\n",
      "\n",
      "3. **Maintenance Overhead**: With several independent modules, maintaining and updating the system can be more demanding. Each module might need individual attention to ensure it keeps up with evolving data requirements and technology advancements [4].\n",
      "\n",
      "4. **Configuration Challenges**: Determining the optimal configuration of different modules can be challenging. The effectiveness of Modular RAG systems can heavily depend on selecting the right modules and fine-tuning them to work in harmony, which may require skilled personnel [5].\n",
      "\n",
      "Despite these challenges, the benefits of adaptability, precision, and scalability often outweigh the downsides for many organizations with complex data processing needs.\n",
      "\n",
      "Sources:\n",
      "[4] medium.com/@drjulija/what-are-naive-rag-advanced-rag-modular-rag-paradigms-edff410c202e\n",
      "[5] adasci.org/how-does-modular-rag-improve-upon-naive-rag\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mask_question\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Thank you so much for your help, Dr. Kim!\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36msearch_arxiv\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "<Document source=\"http://arxiv.org/abs/2502.11228v2\" date=\"2025-05-22\" authors=\"Mohammad Reza Rezaei, Adji Bousso Dieng\"/>\n",
      "<Title>\n",
      "Vendi-RAG: Adaptively Trading-Off Diversity And Quality Significantly Improves Retrieval Augmented Generation With LLMs\n",
      "</Title>\n",
      "\n",
      "<Summary>\n",
      "Retrieval-augmented generation (RAG) enhances large language models (LLMs)\n",
      "for domain-specific question-answering (QA) tasks by leveraging external\n",
      "knowledge sources. However, traditional RAG systems primarily focus on\n",
      "relevance-based retrieval and often struggle with redundancy, especially when\n",
      "reasoning requires connecting information from multiple sources. This paper\n",
      "introduces Vendi-RAG, a framework based on an iterative process that jointly\n",
      "optimizes retrieval diversity and answer quality. This joint optimization leads\n",
      "to significantly higher accuracy for multi-hop QA tasks. Vendi-RAG leverages\n",
      "the Vendi Score (VS), a flexible similarity-based diversity metric, to promote\n",
      "semantic diversity in document retrieval. It then uses an LLM judge that\n",
      "evaluates candidate answers, generated after a reasoning step, and outputs a\n",
      "score that the retriever uses to balance relevance and diversity among the\n",
      "retrieved documents during each iteration. Experiments on three challenging\n",
      "datasets -- HotpotQA, MuSiQue, and 2WikiMultiHopQA -- demonstrate Vendi-RAG's\n",
      "effectiveness in multi-hop reasoning tasks. The framework achieves significant\n",
      "accuracy improvements over traditional single-step and multi-step RAG\n",
      "approaches, with accuracy increases reaching up to +4.2% on HotpotQA, +4.1% on\n",
      "2WikiMultiHopQA, and +1.3% on MuSiQue compared to Adaptive-RAG, the current\n",
      "best baseline. The benefits of Vendi-RAG are even more pronounced as the number\n",
      "of retrieved documents increases. Finally, we evaluated Vendi-RAG across\n",
      "different LLM backbones, including GPT-3.5, GPT-4, and GPT-4o-mini, and\n",
      "observed consistent improvements, demonstrating that the framework's advantages\n",
      "are model-agnostic.\n",
      "</Summary>\n",
      "\n",
      "<Content>\n",
      "Vendi-RAG: Adaptively Trading-Off Diversity And\n",
      "Quality Significantly Improves Retrieval\n",
      "Augmented Generation With LLMs\n",
      "Mohammad R. Rezaei1, 3 and Adji Bousso Dieng2, 3\n",
      "1Institute of Biomedical Engineering, University of Toronto\n",
      "2Department of Computer Science, Princeton University\n",
      "3Vertaix\n",
      "May 26, 2025\n",
      "Abstract\n",
      "Retrieval-augmented generation (RAG) enhances large language models (LLMs)\n",
      "for domain-specific question-answering (QA) tasks by leveraging external\n",
      "knowledge sources. However, traditional RAG systems primarily focus on\n",
      "relevance-based retrieval and often struggle with redundancy, especially when\n",
      "reasoning requires connecting information from multiple sources. This paper\n",
      "introduces Vendi-RAG, a framework based on an iterative process that jointly\n",
      "optimizes retrieval diversity and answer quality. This joint optimization leads\n",
      "to significantly higher accuracy for multi-hop QA tasks. Vendi-RAG leverages\n",
      "the Vendi Score (VS), a flexible similarity-based diversity metric, to promote\n",
      "semantic diversity in document retrieval. It then uses an LLM judge that eval-\n",
      "uates candidate answers, generated after a reasoning step, and outputs a\n",
      "score that the retriever uses to balance relevance and diversity among the\n",
      "retrieved documents during each iteration. Experiments on three challenging\n",
      "datasetsâ€”HotpotQA, MuSiQue, and 2WikiMultiHopQAâ€”demonstrate Vendi-\n",
      "RAGâ€™s effectiveness in multi-hop reasoning tasks. The framework achieves sig-\n",
      "nificant accuracy improvements over traditional single-step or multi-step RAG\n",
      "approaches, with accuracy increases reaching +4.2% on HotpotQA, +4.1% on\n",
      "2WikiMultiHopQA, and +1.3% on MuSiQue compared to Adaptive-RAG, the\n",
      "current best baseline. The benefits of Vendi-RAG are even more pronounced as\n",
      "the number of retrieved documents increases. Finally, we evaluated Vendi-RAG\n",
      "across different LLM backbones, including GPT-3.5, GPT-4, and GPT-4o-mini,\n",
      "and observed consistent improvements, demonstrating that the frameworkâ€™s\n",
      "advantages are model-agnostic.\n",
      "Keywords: RAG, LLMs, Question Answering, NLP, Diversity, Vendi Scoring\n",
      "1\n",
      "Introduction\n",
      "Retrieval-augmented generation (RAG) has emerged as a transformative framework\n",
      "for enhancing the performance of large language models (LLMs) in domain-specific\n",
      "tasks such as question-answering (QA). By retrieving relevant information from\n",
      "external sources beyond the training set, RAG enables LLMs to answer specialized\n",
      "1\n",
      "arXiv:2502.11228v2  [cs.CL]  22 May 2025\n",
      "Retrieval\n",
      "Query\n",
      "Answer\n",
      "LLM Judge \n",
      "Quality Check\n",
      "Rewritten  \n",
      "Query\n",
      "Final \n",
      "Answer\n",
      "Vendi Score\n",
      "Vendi Score\n",
      "Diversity  \n",
      "weight s\n",
      "Decision Block \n",
      "s > threshold Thr \n",
      "Iteration < N\n",
      "â€¦\n",
      "Reasoning\n",
      "Figure 1: The process begins with an initial retrieval step, where a diverse set of\n",
      "documents is retrieved using the Vendi Score, ensuring broad semantic coverage.\n",
      "Next, leveraging a reasoning step to construct a coherent path to the final answer,\n",
      "the LLM generates an answer, which then undergoes quality assessment by an LLM\n",
      "judge. Based on the answer quality, the retriever is adjusted to balance diversity and\n",
      "relevance: high-quality answers limit the emphasis on diversity, while low-quality\n",
      "answers prompt the retriever to prioritize diversity more heavily. This adjustment\n",
      "is controlled by an adaptive parameter, s, which is updated over iterations. The\n",
      "process continues until the answer quality reaches an optimal threshold, denoted\n",
      "by Thr. Finally, the highest-quality responses and documents are selected, ensuring\n",
      "both diversity and accuracy.\n",
      "queries more effectively Achiam et al. (2023); Team et al. (2023); Jiang et al. (2024).\n",
      "This approach has been particularly successful in single-hop QA, where a question\n",
      "can be answered using information from a single document Raiaan et al. (2024);\n",
      "Kwiatkowski et al. (2019). For instance, answering a question such as \"Who wrote\n",
      "the novel Frankenstein?\" only requires retrieving relevant information from a single\n",
      "document containing this fact.\n",
      "However, multi-hop QA introduces significantly greater complexit\n",
      "</Content>\n",
      "</Document>\n",
      "\n",
      "---\n",
      "\n",
      "<Document source=\"http://arxiv.org/abs/2508.13828v1\" date=\"2025-08-19\" authors=\"Yifei Chen, Guanting Dong, Yutao Zhu, Zhicheng Dou\"/>\n",
      "<Title>\n",
      "Revisiting RAG Ensemble: A Theoretical and Mechanistic Analysis of Multi-RAG System Collaboration\n",
      "</Title>\n",
      "\n",
      "<Summary>\n",
      "Retrieval-Augmented Generation (RAG) technology has been widely applied in\n",
      "recent years. However, despite the emergence of various RAG frameworks, a\n",
      "single RAG framework still cannot adapt well to a broad range of downstream\n",
      "tasks. Therefore, how to leverage the advantages of multiple RAG systems has\n",
      "become an area worth exploring. To address this issue, we have conducted a\n",
      "comprehensive and systematic investigation into ensemble methods based on RAG\n",
      "systems. Specifically, we have analyzed the RAG ensemble framework from both\n",
      "theoretical and mechanistic analysis perspectives. From the theoretical\n",
      "analysis, we provide the first explanation of the RAG ensemble framework from\n",
      "the perspective of information entropy. In terms of mechanism analysis, we have\n",
      "explored the RAG ensemble framework from both the pipeline and module levels.\n",
      "We carefully select four different pipelines (Branching, Iterative, Loop, and\n",
      "Agentic) and three different modules (Generator, Retriever, and Reranker) to\n",
      "solve seven different research questions. The experiments show that aggregating\n",
      "multiple RAG systems is both generalizable and robust, whether at the pipeline\n",
      "level or the module level. Our work lays the foundation for similar research on\n",
      "the multi-RAG system ensemble.\n",
      "</Summary>\n",
      "\n",
      "<Content>\n",
      "Revisiting RAG Ensemble: A Theoretical and Mechanistic\n",
      "Analysis of Multi-RAG System Collaboration\n",
      "Yifei Chen\n",
      "zhangboguodong@ruc.edu.cn\n",
      "Gaoling School of Artificial Intelligence, Renmin\n",
      "University of China\n",
      "Haidian Qu, Beijing Shi, China\n",
      "Guanting Dong\n",
      "Gaoling School of Artificial Intelligence, Renmin\n",
      "University of China\n",
      "Haidian Qu, Beijing Shi, China\n",
      "Yutao Zhu\n",
      "Gaoling School of Artificial Intelligence, Renmin\n",
      "University of China\n",
      "Haidian Qu, Beijing Shi, China\n",
      "Zhicheng Dou\n",
      "Gaoling School of Artificial Intelligence, Renmin\n",
      "University of China\n",
      "Haidian Qu, Beijing Shi, China\n",
      "Abstract\n",
      "Retrieval-Augmented Generation (RAG) technology has been widely\n",
      "applied in recent years. However, despite the emergence of various\n",
      "RAG frameworks, a single RAG framework still cannot adapt well\n",
      "to a broad range of downstream tasks. Therefore, how to leverage\n",
      "the advantages of multiple RAG systems has become an area worth\n",
      "exploring. To address this issue, we have conducted a comprehen-\n",
      "sive and systematic investigation into ensemble methods based on\n",
      "RAG systems. Specifically, we have analyzed the RAG ensemble\n",
      "framework from both theoretical and mechanistic analysis perspec-\n",
      "tives. From the theoretical analysis, we provide the first explanation\n",
      "of the RAG ensemble framework from the perspective of informa-\n",
      "tion entropy. In terms of mechanism analysis, we have explored\n",
      "the RAG ensemble framework from both the pipeline and module\n",
      "levels. We carefully select four different pipelines (Branching, Iter-\n",
      "ative, Loop, and Agentic) and three different modules (Generator,\n",
      "Retriever, and Reranker) to solve seven different research questions.\n",
      "The experiments show that aggregating multiple RAG systems is\n",
      "both generalizable and robust, whether at the pipeline level or the\n",
      "module level. Our work lays the foundation for similar research on\n",
      "the multi-RAG system ensemble.\n",
      "CCS Concepts\n",
      "â€¢ Information systems â†’Information integration.\n",
      "Keywords\n",
      "Retrieval-Augmented Generation, Pipeline Ensemble, Module En-\n",
      "semble, Model Preference\n",
      "ACM Reference Format:\n",
      "Yifei Chen, Guanting Dong, Yutao Zhu, and Zhicheng Dou. 2025. Revisiting\n",
      "RAG Ensemble: A Theoretical and Mechanistic Analysis of Multi-RAG\n",
      "Permission to make digital or hard copies of all or part of this work for personal or\n",
      "classroom use is granted without fee provided that copies are not made or distributed\n",
      "for profit or commercial advantage and that copies bear this notice and the full citation\n",
      "on the first page. Copyrights for components of this work owned by others than the\n",
      "author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\n",
      "republish, to post on servers or to redistribute to lists, requires prior specific permission\n",
      "and/or a fee. Request permissions from permissions@acm.org.\n",
      "CIKMâ€™25, Seoul, Korea\n",
      "Â© 2025 Copyright held by the owner/author(s). Publication rights licensed to ACM.\n",
      "ACM ISBN 978-1-4503-XXXX-X/2018/06\n",
      "https://doi.org/XXXXXXX.XXXXXXX\n",
      "System Collaboration. In Proceedings of CIKMâ€™25. ACM, New York, NY, USA,\n",
      "13 pages. https://doi.org/XXXXXXX.XXXXXXX\n",
      "1\n",
      "Introduction\n",
      "The emergence of Large Language Models (LLMs) has profoundly\n",
      "revolutionized many real-world tasks that rely on natural lan-\n",
      "guage [4, 45, 70]. However, when dealing with knowledge-intensive\n",
      "tasks, LLMs relying solely on their parametric knowledge often suf-\n",
      "fer from factual inconsistencies or hallucinations. To address these\n",
      "limitations, Retrieval-Augmented Generation (RAG) methods have\n",
      "been proposed, augmenting LLMs with dynamically retrieved exter-\n",
      "nal knowledge. This integration enhances response accuracy and\n",
      "reliability by grounding outputs in verifiable information sources.\n",
      "As research in this field advances, more and more RAG methods\n",
      "have been proposed. Component Module RAG inserts various mod-\n",
      "ules into the standard pipeline to better complete the retrieval task.\n",
      "For instance, the LongLLMLingua and RECOMP methods refine\n",
      "the retrieved knowledge with a refiner, and the SKR and Adaptive\n",
      "RAG methods distinguish the diffi\n",
      "</Content>\n",
      "</Document>\n",
      "\n",
      "---\n",
      "\n",
      "<Document source=\"http://arxiv.org/abs/2410.12812v1\" date=\"2024-10-01\" authors=\"Sarah Packowski, Inge Halilovic, Jenifer Schlotfeldt, Trish Smith\"/>\n",
      "<Title>\n",
      "Optimizing and Evaluating Enterprise Retrieval-Augmented Generation (RAG): A Content Design Perspective\n",
      "</Title>\n",
      "\n",
      "<Summary>\n",
      "Retrieval-augmented generation (RAG) is a popular technique for using large\n",
      "language models (LLMs) to build customer-support, question-answering solutions.\n",
      "In this paper, we share our team's practical experience building and\n",
      "maintaining enterprise-scale RAG solutions that answer users' questions about\n",
      "our software based on product documentation. Our experience has not always\n",
      "matched the most common patterns in the RAG literature. This paper focuses on\n",
      "solution strategies that are modular and model-agnostic. For example, our\n",
      "experience over the past few years - using different search methods and LLMs,\n",
      "and many knowledge base collections - has been that simple changes to the way\n",
      "we create knowledge base content can have a huge impact on our RAG solutions'\n",
      "success. In this paper, we also discuss how we monitor and evaluate results.\n",
      "Common RAG benchmark evaluation techniques have not been useful for evaluating\n",
      "responses to novel user questions, so we have found a flexible, \"human in the\n",
      "lead\" approach is required.\n",
      "</Summary>\n",
      "\n",
      "<Content>\n",
      "Optimizing and Evaluating Enterprise Retrieval-Augmented\n",
      "Generation (RAG): A Content Design Perspective\n",
      "Sarah Packowski\n",
      "spackows@ca.ibm.com\n",
      "IBM\n",
      "Canada\n",
      "Inge Halilovic\n",
      "ingeh@us.ibm.com\n",
      "IBM\n",
      "United States\n",
      "Jenifer Schlotfeldt\n",
      "jschlot@us.ibm.com\n",
      "IBM\n",
      "United States\n",
      "Trish Smith\n",
      "smith@ca.ibm.com\n",
      "IBM\n",
      "Canada\n",
      "ABSTRACT\n",
      "Retrieval-augmented generation (RAG) is a popular technique for\n",
      "using large language models (LLMs) to build customer-support,\n",
      "question-answering solutions. In this paper, we share our teamâ€™s\n",
      "practical experience building and maintaining enterprise-scale RAG\n",
      "solutions that answer usersâ€™ questions about our software based on\n",
      "product documentation. Our experience has not always matched\n",
      "the most common patterns in the RAG literature. This paper focuses\n",
      "on solution strategies that are modular and model-agnostic. For\n",
      "example, our experience over the past few years - using different\n",
      "search methods and LLMs, and many knowledge base collections -\n",
      "has been that simple changes to the way we create knowledge base\n",
      "content can have a huge impact on our RAG solutionsâ€™ success. In\n",
      "this paper, we also discuss how we monitor and evaluate results.\n",
      "Common RAG benchmark evaluation techniques have not been\n",
      "useful for evaluating responses to novel user questions, so we have\n",
      "found a flexible, \"human in the lead\" approach is required.\n",
      "CCS CONCEPTS\n",
      "â€¢ Computing methodologies â†’Artificial intelligence; Natu-\n",
      "ral language generation; â€¢ Applied computing â†’Document\n",
      "management and text processing.\n",
      "KEYWORDS\n",
      "Retrieval-augmented generation, RAG, Large language models\n",
      "ACM Reference Format:\n",
      "Sarah Packowski, Inge Halilovic, Jenifer Schlotfeldt, and Trish Smith. 2024.\n",
      "Optimizing and Evaluating Enterprise Retrieval-Augmented Generation\n",
      "(RAG): A Content Design Perspective. In Proceedings of 8th International\n",
      "Conference on Advances in Artificial Intelligence (ICAAI â€™24). ACM, New York,\n",
      "NY, USA, 6 pages.\n",
      "Permission to make digital or hard copies of part or all of this work for personal or\n",
      "classroom use is granted without fee provided that copies are not made or distributed\n",
      "for profit or commercial advantage and that copies bear this notice and the full citation\n",
      "on the first page. Copyrights for third-party components of this work must be honored.\n",
      "For all other uses, contact the owner/author(s).\n",
      "ICAAI â€™24, October 2024, London, UK\n",
      "Â© 2024 Copyright held by the owner/author(s).\n",
      "ACM ISBN 979-8-4007-1801-4/24/10.\n",
      "1\n",
      "INTRODUCTION\n",
      "Retrieval-augmented generation (RAG) is an effective way to use\n",
      "large language models (LLMs) to answer questions while avoiding\n",
      "hallucinations and factual inaccuracy[12, 20, 46]. Basic RAG is sim-\n",
      "ple: 1) search a knowledge base for relevant content; 2) compose a\n",
      "prompt grounded in the retrieved content; and 3) prompt an LLM to\n",
      "generate output. For the retrieval step, one approach dominates the\n",
      "literature: 1) segment content text into chunks; 2) index vectorized\n",
      "chunks for search in a vector database; and 3) when generating\n",
      "answers, ground prompts in a subset of retrieved chunks[13]. Our\n",
      "RAG solutions donâ€™t always use vector databases for search.\n",
      "Wikipedia has long been influenced by and had an influence on\n",
      "scientific research [21, 41]. With respect to RAG, Wikipedia is a dom-\n",
      "inant source of knowledge base content for training data and bench-\n",
      "marks, including: 2WikiMultiHopQA, AmbigQA, ASQA, DART,\n",
      "FEVER, HotpotQA, KILT, MuSiQue, Natural Questions, NoMIRACL,\n",
      "PopQA, SQuAD, StrategyQA, SuperGLUE, TriviaQA, WikiAsp, Wik-\n",
      "iBio, WikiEval, and Wizard of Wikipedia[8, 9, 14â€“16, 18, 22, 23, 25,\n",
      "28, 29, 31, 34, 39, 40, 42â€“44, 48]. The knowledge base for our teamâ€™s\n",
      "RAG solutions is our own product documentation, which is struc-\n",
      "tured differently from Wikipedia articles.\n",
      "Using common benchmarks to test your RAG implementation\n",
      "involves these steps: 1) index the given knowledge base content in\n",
      "your retriever component; 2) prompt your solution to answer the\n",
      "given questions; and 3) compare generated answers to expected\n",
      "answers, using methods such as exact match, cos\n",
      "</Content>\n",
      "</Document>\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36msearch_web\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "<Document href=\"https://www.linkedin.com/pulse/rag-architectural-review-strategic-outlook-2025-bal%C3%A1zs-feh%C3%A9r-bwzpf\"/>\n",
      "Jul 24, 2025Â·In a naive RAG pipeline, the flow is fixed: the user's query is used to retrieve documents, which are then passed to the LLM for generation. An\n",
      "</Document>\n",
      "\n",
      "---\n",
      "\n",
      "<Document href=\"https://medium.com/aingineer/a-comprehensive-guide-to-implementing-modular-rag-for-scalable-ai-systems-3fb47c46dc8e\"/>\n",
      "In the rapidly evolving landscape of AI, Modular RAG (Retrieval-Augmented Generation) has emerged as a transformative approach to building robust, scalable, and adaptable AI systems. By decoupling retrieval, reasoning, and generation into independent modules, Modular RAG empowers engineering leaders, architects, and senior engineers to design systems that are not only efficient but also flexible enough to meet the dynamic demands of modern enterprises. Modular RAG is an architectural paradigm that separates the core components of Retrieval-Augmented Generation into three independent modules: For instance, retrieval mechanisms can be powered by vector search engines like Elasticsearch, while the reasoning module may use transformer-based models for contextual understanding. Implement mechanisms to fine-tune modules for domain-specific requirements, such as using custom embeddings for retrieval or training bespoke generation models.\n",
      "</Document>\n",
      "\n",
      "---\n",
      "\n",
      "<Document href=\"https://dextralabs.com/blog/rag-pipeline-explained-diagram-implementation/\"/>\n",
      "**Retrieval-Augmented Generation (RAG)** is changing how we build and use large language models. _**RAG in Action:**A question-answering process with 3 steps â€” (1) Indexing: split and store document chunks as vectors; (2) Retrieval: fetch top-k relevant chunks; (3) Generation: use retrieved chunks with the question to generate the final answer via LLM._ _A system-level RAG pipeline showing how queries pass through indexing, retrieval, and generation stages â€” ideal for illustrating how external knowledge integrates with LLMs to produce accurate responses._ A **RAG stack** refers to the full set of tools used to build a RAG system â€” including the retriever (like FAISS or Pinecone), vector store, LLM (like GPT), prompt layer, and sometimes rerankers or query rewriters.\n",
      "</Document>\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36manswer_question\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: expert\n",
      "\n",
      "You're welcome! If you have any more questions in the future or need further assistance, feel free to reach out. Have a great day and good luck with your research on RAG systems!\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36msave_interview\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "\u001b[1;32minterview\u001b[0m:\n",
      "Human: So you said you were writing an article on Modular RAG ê°€ ê¸°ì¡´ì˜ Naive RAG ì™€ ì–´ë–¤ ì°¨ì´ê°€ ìˆëŠ”ì§€ì™€ production level ì—ì„œ ì‚¬ìš©í•˜ëŠ” ì´ì ?\n",
      "AI: Hello, Dr. Kim. My name is Alex Thompson, and I'm an analyst deeply interested in the practical applications of machine learning frameworks, particularly Modular RAG and Naive RAG. I'm eager to understand the differences between these two frameworks, especially in terms of their flexibility and efficiency in handling complex data requirements in production settings. Could you please elaborate on these aspects for us?\n",
      "AI: Certainly, Dr. Kim. The primary difference between Modular RAG and Naive RAG lies in their structure and approach to information retrieval and generation.\n",
      "\n",
      "Naive RAG starts by indexing data sources to enable quick retrieval of relevant information. It maintains a straightforward approach where the retrieval and generation processes occur in a single, undivided pipeline. This simplicity can sometimes limit its flexibility and precision, particularly in handling complex or specialized data requirements in production environments [4].\n",
      "\n",
      "Modular RAG, on the other hand, builds upon Naive RAG by compartmentalizing the retrieval and generation processes into distinct, specialized modules. This modularity allows for greater flexibility as each module can be independently optimized or replaced to better suit specific data characteristics or processing needs. For example, different retrieval techniques can be employed within the same system, depending on the data type and retrieval demands. This separation allows for more precise and context-aware data retrieval, which enhances the overall responsiveness and relevance of the generated outputs [4][5].\n",
      "\n",
      "In production settings, using Modular RAG offers several benefits:\n",
      "\n",
      "1. **Customization**: By separating the retrieval and generation processes, organizations can tailor each component to better align with their data and application requirements, ensuring higher performance and efficiency.\n",
      "\n",
      "2. **Scalability**: Modular systems can be easily scaled by adapting or swapping individual modules without overhauling the entire architecture, which is particularly useful as data complexity grows.\n",
      "\n",
      "3. **Improved Accuracy**: With specialized modules, Modular RAG systems can achieve better accuracy in retrieving relevant data and generating coherent outputs, as each module is tuned to handle specific tasks optimally.\n",
      "\n",
      "These features make Modular RAG a more robust choice for handling complex and dynamic data requirements in production environments compared to Naive RAG [4].\n",
      "\n",
      "Sources:\n",
      "[4] medium.com/@drjulija/what-are-naive-rag-advanced-rag-modular-rag-paradigms-edff410c202e\n",
      "[5] blog.searce.com/unlocking-the-power-of-llm-rag-discover-naive-advanced-and-modular-retrieval-augmented-6922b353d8d3\n",
      "AI: Thank you for that detailed explanation, Dr. Kim. Could you please provide a specific example or case study where Modular RAG was effectively implemented in a production setting, highlighting the challenges faced and how the modular approach was beneficial?\n",
      "AI: Certainly. Let's consider a case study in the automotive industry, as described in a research article on optimizing RAG techniques for PDF chatbots. The context involved processing complex automotive industry documents using locally deployed models. Here, Modular RAG was utilized to efficiently handle multi-column layouts and technical specifications typical of automotive documents [6].\n",
      "\n",
      "Challenges in this scenario included the need to process diverse document types and the requirement for high context precision and recall. Traditional methods, like Naive RAG, might struggle with these complexities due to their monolithic design, which limits the ability to tailor retrieval processes to meet specific document processing needs.\n",
      "\n",
      "By implementing a Modular RAG system, the retrieval mechanisms could be separately optimized to handle the specific challenges posed by automotive document structures. Custom modules for PDF processing and context compression were designed, allowing for tailored embedding pipelines and retrieval strategies. This modular approach resulted in significant improvements in context precision, recall, and answer relevancy, particularly when applied to the proprietary automotive industry dataset [6].\n",
      "\n",
      "The benefits of using Modular RAG in this production setting included enhanced flexibility to adapt retrieval processes to different document types and improved scalability and accuracy in generating contextually relevant responses. This case study demonstrates how Modular RAG can be effectively deployed to address complex data requirements in real-world scenarios, showcasing its advantage over more static frameworks like Naive RAG.\n",
      "\n",
      "[6] arxiv.org/abs/2408.05933v1\n",
      "AI: That's fascinating, Dr. Kim. It's clear that Modular RAG offers tangible benefits in complex environments like the automotive industry. One last question: are there any specific challenges or limitations associated with implementing Modular RAG that organizations should be aware of?\n",
      "AI: Yes, there are specific challenges and limitations associated with implementing Modular RAG that organizations should consider:\n",
      "\n",
      "1. **Integration Complexity**: While Modular RAG provides flexibility through its separate modules, integrating these components can sometimes be more complex than using a monolithic system like Naive RAG. Organizations must carefully design each module and ensure seamless interaction between them to maintain the system's overall efficiency and effectiveness [5].\n",
      "\n",
      "2. **Resource Intensive**: The modular approach might require more computational resources, especially if multiple or sophisticated modules are used. This can increase the cost of deployment, particularly in environments with limited infrastructure capabilities [5].\n",
      "\n",
      "3. **Maintenance Overhead**: With several independent modules, maintaining and updating the system can be more demanding. Each module might need individual attention to ensure it keeps up with evolving data requirements and technology advancements [4].\n",
      "\n",
      "4. **Configuration Challenges**: Determining the optimal configuration of different modules can be challenging. The effectiveness of Modular RAG systems can heavily depend on selecting the right modules and fine-tuning them to work in harmony, which may require skilled personnel [5].\n",
      "\n",
      "Despite these challenges, the benefits of adaptability, precision, and scalability often outweigh the downsides for many organizations with complex data processing needs.\n",
      "\n",
      "Sources:\n",
      "[4] medium.com/@drjulija/what-are-naive-rag-advanced-rag-modular-rag-paradigms-edff410c202e\n",
      "[5] adasci.org/how-does-modular-rag-improve-upon-naive-rag\n",
      "AI: Thank you so much for your help, Dr. Kim!\n",
      "AI: You're welcome! If you have any more questions in the future or need further assistance, feel free to reach out. Have a great day and good luck with your research on RAG systems!\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mwrite_section\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "## Evaluating the Flexibility and Efficiency of Modular RAG vs. Naive RAG\n",
      "\n",
      "### Summary\n",
      "\n",
      "In the rapidly advancing field of machine learning, Retrieval-Augmented Generation (RAG) frameworks have become pivotal in enhancing the capabilities of Large Language Models (LLMs), especially in complex data scenarios. Dr. Kim, an academic researcher with a special focus on machine learning frameworks, delves into a critical comparison of Modular RAG and Naive RAG, analyzing their flexibility and efficiency in production settings. The insights gathered offer novel perspectives on the adaptability and performance of these frameworks across different domains.\n",
      "\n",
      "RAG is designed to mitigate the typical issues faced by LLMs, such as hallucinations and factual inaccuracies, by grounding responses in retrievable external knowledge [1]. The conventional Naive RAG functions through a simple mechanism of retrieving and leveraging relevant documents for generation tasks. However, as highlighted in recent evaluations, Naive RAG's lack of flexibility often leads to inefficiencies, especially in adapting to diverse domain requirements [2]. This has paved the way for Modular RAG, which breaks down the RAG process into separable modules, offering greater adaptability and precision [3].\n",
      "\n",
      "In their comprehensive evaluation, Liang and colleagues introduce OmniBench RAG, a platform that further exemplifies how Modular RAG can be leveraged to assess RAG systems across multiple domains with precision [4]. On the other hand, frameworks such as Vendi-RAG demonstrate how dynamic integration and iterative processes can enhance RAGâ€™s performance by balancing retrieval diversity and answer quality [5]. \n",
      "\n",
      "Emerging research emphasizes the need for a modular approach, which is agile not just in technical performance but also in handling various complexities inherent in real-world scenarios. Such findings underscore the critical role of modular architectures in optimizing RAG's effectiveness when applied to intricate, domain-specific applications.\n",
      "\n",
      "Sources:\n",
      "1. [2] https://medium.com/@drjulija/what-are-naive-rag-advanced-rag-modular-rag-paradigms-edff410c202e\n",
      "2. [2] https://adasci.org/how-does-modular-rag-improve-upon-naive-rag/\n",
      "3. [2] https://blog.searce.com/unlocking-the-power-of-llm-rag-discover-naive-advanced-and-modular-retrieval-augmented-6922b353d8d3\n",
      "4. [1] http://arxiv.org/abs/2508.05650v1\n",
      "5. [4] http://arxiv.org/abs/2502.11228v2\n",
      "\n",
      "### Comprehensive Analysis\n",
      "\n",
      "#### 1. Introduction to RAG Frameworks\n",
      "\n",
      "Retrieval-Augmented Generation (RAG) frameworks represent a fusion of information retrieval and natural language generation, enhancing LLMs' competence by accessing external information sources. This aspect is crucial for Dr. Kimâ€™s interest since RAG systems address foundational challenges, such as large context window limitations and the need for real-time factual accuracy [4]. While Naive RAG follows a monolithic structure, Modular RAG emerges as a flexible paradigm by decomposing the processes into specialized, interchangeable modules [2].\n",
      "\n",
      "#### 2. Naive RAG: Strengths and Limitations\n",
      "\n",
      "The Naive RAG framework begins with straightforward indexing and retrieval, relying heavily on the relevance of documents to input queries [2]. It functions efficiently in environments requiring a basic level of retrieval and generation without complex modular demands. However, its static nature becomes a liability in dynamic production settings where rapid adaptability is required:\n",
      "\n",
      "- **Efficiency Concerns**: Naive RAG lacks agility, making it less efficient in handling diverse data requirements. As tasks become complex, the framework's inefficiencies in integrating retrieval and generation processes lead to increased latency and possible incorrect outputs [3].\n",
      "\n",
      "- **Integration Challenges**: The monolithic framework restricts seamless integration with advanced retrieval systems, hindering its effectiveness in scaling across different domains [3].\n",
      "\n",
      "#### 3. Modular RAG: Advancements and Benefits\n",
      "\n",
      "The Modular RAG framework represents a significant shift towards configurable and scalable AI systems. By segregating retrieval, reasoning, and generation into distinct components, it addresses many of the efficiency challenges associated with Naive RAG [2]:\n",
      "\n",
      "- **Enhanced Flexibility**: Modular RAG's ability to interchange modules allows it to tailor components specifically to domain requirements, ensuring a higher degree of accuracy and relevance in generated responses [2].\n",
      "\n",
      "- **Scalable Architecture**: The frameworkâ€™s architecture supports a variety of retrieval mechanisms, such as Elasticsearch, enhancing its efficacy in diverse production environments [6].\n",
      "\n",
      "- **Superior Performance Metrics**: Tools like OmniBench RAG illustrate how Modular RAG's architecture can be utilized for systematic performance evaluations across domains, highlighting significant gains in areas like cultural data and efficiency drops in domains like mathematics [4].\n",
      "\n",
      "#### 4. Comparative Evaluations and Domain Applications\n",
      "\n",
      "By leveraging advanced platform evaluations like OmniBench RAG, the nuanced differences in RAG systems can be quantified. The introduction of standardized metrics â€“ Improvements and Transformation â€“ allows for reproducible comparisons, providing deep insights into the respective capabilities of Modular versus Naive RAG:\n",
      "\n",
      "- **Domain-Specific Evaluation**: Modular RAG demonstrates significant improvements in knowledge-intensive and domain-specific tasks due to its modular nature, allowing for precise evaluations and scalability [4].\n",
      "\n",
      "- **Iterative Improvements**: Frameworks like Vendi-RAG that incorporate iterative methods further demonstrate the adaptability and improved performance of Modular RAG under strenuous conditions [5].\n",
      "\n",
      "In conclusion, the comprehensive analysis underscores Modular RAG's superiority in managing complex, evolving data requirements in production settings, ultimately advocating for its broader adoption in scenarios demanding high adaptability and efficiency. Dr. Kim would likely find Modular RAG to be the favorable choice for its robustness, agility, and precision across varying domains.\n",
      "\n",
      "### Sources\n",
      "[1] http://arxiv.org/abs/2508.05650v1  \n",
      "[2] https://medium.com/@drjulija/what-are-naive-rag-advanced-rag-modular-rag-paradigms-edff410c202e  \n",
      "[3] https://adasci.org/how-does-modular-rag-improve-upon-naive-rag/  \n",
      "[4] http://arxiv.org/abs/2502.11228v2  \n",
      "[5] https://blog.searce.com/unlocking-the-power-of-llm-rag-discover-naive-advanced-and-modular-retrieval-augmented-6922b353d8d3  \n",
      "[6] https://dextralabs.com/blog/rag-pipeline-explained-diagram-implementation/  \n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "# ì£¼ì œ ì„¤ì •\n",
    "topic = \"Modular RAG ê°€ ê¸°ì¡´ì˜ Naive RAG ì™€ ì–´ë–¤ ì°¨ì´ê°€ ìˆëŠ”ì§€ì™€ production level ì—ì„œ ì‚¬ìš©í•˜ëŠ” ì´ì \"\n",
    "\n",
    "# ì¸í„°ë·° ì‹œì‘ ë©”ì‹œì§€ ìƒì„±\n",
    "messages = [HumanMessage(f\"So you said you were writing an article on {topic}?\")]\n",
    "\n",
    "# ìŠ¤ë ˆë“œ ID ì„¤ì •\n",
    "config = RunnableConfig(\n",
    "    recursion_limit=100,\n",
    "    configurable={\"thread_id\": random_uuid()},\n",
    ")\n",
    "\n",
    "# ê·¸ë˜í”„ ì‹¤í–‰\n",
    "invoke_graph(\n",
    "    interview_graph,\n",
    "    {\"analyst\": analysts[0], \"messages\": messages, \"max_num_turns\": 5},\n",
    "    config,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Evaluating the Flexibility and Efficiency of Modular RAG vs. Naive RAG\n",
       "\n",
       "### Summary\n",
       "\n",
       "In the rapidly advancing field of machine learning, Retrieval-Augmented Generation (RAG) frameworks have become pivotal in enhancing the capabilities of Large Language Models (LLMs), especially in complex data scenarios. Dr. Kim, an academic researcher with a special focus on machine learning frameworks, delves into a critical comparison of Modular RAG and Naive RAG, analyzing their flexibility and efficiency in production settings. The insights gathered offer novel perspectives on the adaptability and performance of these frameworks across different domains.\n",
       "\n",
       "RAG is designed to mitigate the typical issues faced by LLMs, such as hallucinations and factual inaccuracies, by grounding responses in retrievable external knowledge [1]. The conventional Naive RAG functions through a simple mechanism of retrieving and leveraging relevant documents for generation tasks. However, as highlighted in recent evaluations, Naive RAG's lack of flexibility often leads to inefficiencies, especially in adapting to diverse domain requirements [2]. This has paved the way for Modular RAG, which breaks down the RAG process into separable modules, offering greater adaptability and precision [3].\n",
       "\n",
       "In their comprehensive evaluation, Liang and colleagues introduce OmniBench RAG, a platform that further exemplifies how Modular RAG can be leveraged to assess RAG systems across multiple domains with precision [4]. On the other hand, frameworks such as Vendi-RAG demonstrate how dynamic integration and iterative processes can enhance RAGâ€™s performance by balancing retrieval diversity and answer quality [5]. \n",
       "\n",
       "Emerging research emphasizes the need for a modular approach, which is agile not just in technical performance but also in handling various complexities inherent in real-world scenarios. Such findings underscore the critical role of modular architectures in optimizing RAG's effectiveness when applied to intricate, domain-specific applications.\n",
       "\n",
       "Sources:\n",
       "1. [2] https://medium.com/@drjulija/what-are-naive-rag-advanced-rag-modular-rag-paradigms-edff410c202e\n",
       "2. [2] https://adasci.org/how-does-modular-rag-improve-upon-naive-rag/\n",
       "3. [2] https://blog.searce.com/unlocking-the-power-of-llm-rag-discover-naive-advanced-and-modular-retrieval-augmented-6922b353d8d3\n",
       "4. [1] http://arxiv.org/abs/2508.05650v1\n",
       "5. [4] http://arxiv.org/abs/2502.11228v2\n",
       "\n",
       "### Comprehensive Analysis\n",
       "\n",
       "#### 1. Introduction to RAG Frameworks\n",
       "\n",
       "Retrieval-Augmented Generation (RAG) frameworks represent a fusion of information retrieval and natural language generation, enhancing LLMs' competence by accessing external information sources. This aspect is crucial for Dr. Kimâ€™s interest since RAG systems address foundational challenges, such as large context window limitations and the need for real-time factual accuracy [4]. While Naive RAG follows a monolithic structure, Modular RAG emerges as a flexible paradigm by decomposing the processes into specialized, interchangeable modules [2].\n",
       "\n",
       "#### 2. Naive RAG: Strengths and Limitations\n",
       "\n",
       "The Naive RAG framework begins with straightforward indexing and retrieval, relying heavily on the relevance of documents to input queries [2]. It functions efficiently in environments requiring a basic level of retrieval and generation without complex modular demands. However, its static nature becomes a liability in dynamic production settings where rapid adaptability is required:\n",
       "\n",
       "- **Efficiency Concerns**: Naive RAG lacks agility, making it less efficient in handling diverse data requirements. As tasks become complex, the framework's inefficiencies in integrating retrieval and generation processes lead to increased latency and possible incorrect outputs [3].\n",
       "\n",
       "- **Integration Challenges**: The monolithic framework restricts seamless integration with advanced retrieval systems, hindering its effectiveness in scaling across different domains [3].\n",
       "\n",
       "#### 3. Modular RAG: Advancements and Benefits\n",
       "\n",
       "The Modular RAG framework represents a significant shift towards configurable and scalable AI systems. By segregating retrieval, reasoning, and generation into distinct components, it addresses many of the efficiency challenges associated with Naive RAG [2]:\n",
       "\n",
       "- **Enhanced Flexibility**: Modular RAG's ability to interchange modules allows it to tailor components specifically to domain requirements, ensuring a higher degree of accuracy and relevance in generated responses [2].\n",
       "\n",
       "- **Scalable Architecture**: The frameworkâ€™s architecture supports a variety of retrieval mechanisms, such as Elasticsearch, enhancing its efficacy in diverse production environments [6].\n",
       "\n",
       "- **Superior Performance Metrics**: Tools like OmniBench RAG illustrate how Modular RAG's architecture can be utilized for systematic performance evaluations across domains, highlighting significant gains in areas like cultural data and efficiency drops in domains like mathematics [4].\n",
       "\n",
       "#### 4. Comparative Evaluations and Domain Applications\n",
       "\n",
       "By leveraging advanced platform evaluations like OmniBench RAG, the nuanced differences in RAG systems can be quantified. The introduction of standardized metrics â€“ Improvements and Transformation â€“ allows for reproducible comparisons, providing deep insights into the respective capabilities of Modular versus Naive RAG:\n",
       "\n",
       "- **Domain-Specific Evaluation**: Modular RAG demonstrates significant improvements in knowledge-intensive and domain-specific tasks due to its modular nature, allowing for precise evaluations and scalability [4].\n",
       "\n",
       "- **Iterative Improvements**: Frameworks like Vendi-RAG that incorporate iterative methods further demonstrate the adaptability and improved performance of Modular RAG under strenuous conditions [5].\n",
       "\n",
       "In conclusion, the comprehensive analysis underscores Modular RAG's superiority in managing complex, evolving data requirements in production settings, ultimately advocating for its broader adoption in scenarios demanding high adaptability and efficiency. Dr. Kim would likely find Modular RAG to be the favorable choice for its robustness, agility, and precision across varying domains.\n",
       "\n",
       "### Sources\n",
       "[1] http://arxiv.org/abs/2508.05650v1  \n",
       "[2] https://medium.com/@drjulija/what-are-naive-rag-advanced-rag-modular-rag-paradigms-edff410c202e  \n",
       "[3] https://adasci.org/how-does-modular-rag-improve-upon-naive-rag/  \n",
       "[4] http://arxiv.org/abs/2502.11228v2  \n",
       "[5] https://blog.searce.com/unlocking-the-power-of-llm-rag-discover-naive-advanced-and-modular-retrieval-augmented-6922b353d8d3  \n",
       "[6] https://dextralabs.com/blog/rag-pipeline-explained-diagram-implementation/  "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ì™„ì„±ëœ ì¸í„°ë·° ì„¹ì…˜ ì¶œë ¥\n",
    "Markdown(interview_graph.get_state(config).values[\"sections\"][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import List, Annotated\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "\n",
    "# ResearchGraphState ìƒíƒœ ì •ì˜\n",
    "class ResearchGraphState(TypedDict):\n",
    "    # ì—°êµ¬ ì£¼ì œ\n",
    "    topic: str\n",
    "    # ìƒì„±í•  ë¶„ì„ê°€ì˜ ìµœëŒ€ ìˆ˜\n",
    "    max_analysts: int\n",
    "    # ì¸ê°„ ë¶„ì„ê°€ì˜ í”¼ë“œë°±\n",
    "    human_analyst_feedback: str\n",
    "    # ì§ˆë¬¸ì„ í•˜ëŠ” ë¶„ì„ê°€ ëª©ë¡\n",
    "    analysts: List[Analyst]\n",
    "    # Send() API í‚¤ë¥¼ í¬í•¨í•˜ëŠ” ì„¹ì…˜ ë¦¬ìŠ¤íŠ¸\n",
    "    sections: Annotated[list, operator.add]\n",
    "    # ìµœì¢… ë³´ê³ ì„œì˜ ì„œë¡ \n",
    "    introduction: str\n",
    "    # ìµœì¢… ë³´ê³ ì„œì˜ ë³¸ë¬¸ ë‚´ìš©\n",
    "    content: str\n",
    "    # ìµœì¢… ë³´ê³ ì„œì˜ ê²°ë¡ \n",
    "    conclusion: str\n",
    "    # ìµœì¢… ë³´ê³ ì„œ\n",
    "    final_report: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sj/3r49bs6178v7nfhcxh7lpzd80000gn/T/ipykernel_43129/556175951.py:1: LangGraphDeprecatedSinceV10: Importing Send from langgraph.constants is deprecated. Please use 'from langgraph.types import Send' instead. Deprecated in LangGraph V1.0 to be removed in V2.0.\n",
      "  from langgraph.constants import Send\n"
     ]
    }
   ],
   "source": [
    "from langgraph.constants import Send\n",
    "\n",
    "\n",
    "# ëª¨ë“  ì¸í„°ë·°ë¥¼ ì‹œì‘\n",
    "def initiate_all_interviews(state: ResearchGraphState):\n",
    "    # ì‚¬ëŒì˜ í”¼ë“œë°± í™•ì¸\n",
    "    human_analyst_feedback = state.get(\"human_analyst_feedback\")\n",
    "\n",
    "    # ë§Œì•½, ì‚¬ëŒì˜ í”¼ë“œë°±ì´ ìˆìœ¼ë©´ ë¶„ì„ê°€ ìƒì„±ìœ¼ë¡œ ëŒì•„ê°€ê¸°\n",
    "    if human_analyst_feedback:\n",
    "        return \"create_analysts\"\n",
    "\n",
    "    # ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ Send() í•¨ìˆ˜ë¥¼ í†µí•´ ì¸í„°ë·° ë³‘ë ¬ë¡œ ì‹œì‘\n",
    "    else:\n",
    "        topic = state[\"topic\"]\n",
    "        return [\n",
    "            Send(\n",
    "                \"conduct_interview\",\n",
    "                {\n",
    "                    \"analyst\": analyst,\n",
    "                    \"messages\": [\n",
    "                        HumanMessage(\n",
    "                            content=f\"So you said you were writing an article on {topic}?\"\n",
    "                        )\n",
    "                    ],\n",
    "                },\n",
    "            )\n",
    "            for analyst in state[\"analysts\"]\n",
    "        ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë³´ê³ ì„œ ì‘ì„± ì§€ì‹œì‚¬í•­\n",
    "report_writer_instructions = \"\"\"You are a technical writer creating a report on this overall topic:\n",
    "\n",
    "{topic}\n",
    "\n",
    "You have a team of analysts. Each analyst has done two things:\n",
    "\n",
    "1. They conducted an interview with an expert on a specific sub-topic.\n",
    "2. They write up their finding into a memo.\n",
    "\n",
    "Your task:\n",
    "\n",
    "1. You will be given a collection of memos from your analysts.  \n",
    "2. Carefully review and analyze the insights from each memo.  \n",
    "3. Consolidate these insights into a detailed and comprehensive summary that integrates the central ideas from all the memos.  \n",
    "4. Organize the key points from each memo into the appropriate sections provided below, ensuring that each section is logical and well-structured.  \n",
    "5. Include all required sections in your report, using `### Section Name` as the header for each.  \n",
    "6. Aim for approximately 250 words per section, providing in-depth explanations, context, and supporting details.  \n",
    "\n",
    "**Sections to consider (including optional ones for greater depth):**\n",
    "\n",
    "- **Background**: Theoretical foundations, key concepts, and preliminary information necessary to understand the methodology and results.\n",
    "- **Related Work**: Overview of prior studies and how they compare or relate to the current research.\n",
    "- **Problem Definition**: A formal and precise definition of the research question or problem the paper aims to address.\n",
    "- **Methodology (or Methods)**: Detailed description of the methods, algorithms, models, data collection processes, or experimental setups used in the study.\n",
    "- **Implementation Details**: Practical details of how the methods or models were implemented, including software frameworks, computational resources, or parameter settings.\n",
    "- **Experiments**: Explanation of experimental protocols, datasets, evaluation metrics, procedures, and configurations employed to validate the methods.\n",
    "- **Results**: Presentation of experimental outcomes, often with statistical tables, graphs, figures, or qualitative analyses.\n",
    "\n",
    "To format your report:\n",
    "\n",
    "1. Use markdown formatting.\n",
    "2. Include no pre-amble for the report.\n",
    "3. Use no sub-heading.\n",
    "4. Start your report with a single title header: ## Insights\n",
    "5. Do not mention any analyst names in your report.\n",
    "6. Preserve any citations in the memos, which will be annotated in brackets, for example [1] or [2].\n",
    "7. Create a final, consolidated list of sources and add to a Sources section with the `## Sources` header.\n",
    "8. List your sources in order and do not repeat.\n",
    "\n",
    "[1] Source 1\n",
    "[2] Source 2\n",
    "\n",
    "Here are the memos from your analysts to build your report from:\n",
    "\n",
    "{context}\"\"\"\n",
    "\n",
    "\n",
    "# ë³´ê³ ì„œ ì‘ì„± í•¨ìˆ˜ ì •ì˜\n",
    "def write_report(state: ResearchGraphState):\n",
    "    # ëª¨ë“  ì„¹ì…˜ ê°€ì ¸ì˜¤ê¸°\n",
    "    sections = state[\"sections\"]\n",
    "    topic = state[\"topic\"]\n",
    "\n",
    "    # ëª¨ë“  ì„¹ì…˜ì„ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ ì—°ê²°\n",
    "    formatted_str_sections = \"\\n\\n\".join([f\"{section}\" for section in sections])\n",
    "\n",
    "    # ì„¹ì…˜ì„ ìš”ì•½í•˜ì—¬ ìµœì¢… ë³´ê³ ì„œ ì‘ì„±\n",
    "    system_message = report_writer_instructions.format(\n",
    "        topic=topic, context=formatted_str_sections\n",
    "    )\n",
    "    report = llm.invoke(\n",
    "        [SystemMessage(content=system_message)]\n",
    "        + [HumanMessage(content=f\"Write a report based upon these memos.\")]\n",
    "    )\n",
    "    return {\"content\": report.content}\n",
    "\n",
    "\n",
    "# ì„œë¡ ê³¼ ê²°ë¡  ì‘ì„± ì§€ì‹œì‚¬í•­\n",
    "intro_conclusion_instructions = \"\"\"You are a technical writer finishing a report on {topic}\n",
    "\n",
    "You will be given all of the sections of the report.\n",
    "\n",
    "You job is to write a crisp and compelling introduction or conclusion section.\n",
    "\n",
    "The user will instruct you whether to write the introduction or conclusion.\n",
    "\n",
    "Include no pre-amble for either section.\n",
    "\n",
    "Target around 200 words, crisply previewing (for introduction),  or recapping (for conclusion) all of the sections of the report.\n",
    "\n",
    "Use markdown formatting.\n",
    "\n",
    "For your introduction, create a compelling title and use the # header for the title.\n",
    "\n",
    "For your introduction, use ## Introduction as the section header.\n",
    "\n",
    "For your conclusion, use ## Conclusion as the section header.\n",
    "\n",
    "Here are the sections to reflect on for writing: {formatted_str_sections}\"\"\"\n",
    "\n",
    "\n",
    "# ì„œë¡  ì‘ì„± í•¨ìˆ˜ ì •ì˜\n",
    "def write_introduction(state: ResearchGraphState):\n",
    "    # ëª¨ë“  ì„¹ì…˜ ê°€ì ¸ì˜¤ê¸°\n",
    "    sections = state[\"sections\"]\n",
    "    topic = state[\"topic\"]\n",
    "\n",
    "    # ëª¨ë“  ì„¹ì…˜ì„ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ ì—°ê²°\n",
    "    formatted_str_sections = \"\\n\\n\".join([f\"{section}\" for section in sections])\n",
    "\n",
    "    # ì„¹ì…˜ì„ ìš”ì•½í•˜ì—¬ ì„œë¡  ì‘ì„±\n",
    "    instructions = intro_conclusion_instructions.format(\n",
    "        topic=topic, formatted_str_sections=formatted_str_sections\n",
    "    )\n",
    "    intro = llm.invoke(\n",
    "        [instructions] + [HumanMessage(content=f\"Write the report introduction\")]\n",
    "    )\n",
    "    return {\"introduction\": intro.content}\n",
    "\n",
    "\n",
    "# ê²°ë¡  ì‘ì„± í•¨ìˆ˜ ì •ì˜\n",
    "def write_conclusion(state: ResearchGraphState):\n",
    "    # ëª¨ë“  ì„¹ì…˜ ê°€ì ¸ì˜¤ê¸°\n",
    "    sections = state[\"sections\"]\n",
    "    topic = state[\"topic\"]\n",
    "\n",
    "    # ëª¨ë“  ì„¹ì…˜ì„ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ ì—°ê²°\n",
    "    formatted_str_sections = \"\\n\\n\".join([f\"{section}\" for section in sections])\n",
    "\n",
    "    # ì„¹ì…˜ì„ ìš”ì•½í•˜ì—¬ ê²°ë¡  ì‘ì„±\n",
    "    instructions = intro_conclusion_instructions.format(\n",
    "        topic=topic, formatted_str_sections=formatted_str_sections\n",
    "    )\n",
    "    conclusion = llm.invoke(\n",
    "        [instructions] + [HumanMessage(content=f\"Write the report conclusion\")]\n",
    "    )\n",
    "    return {\"conclusion\": conclusion.content}\n",
    "\n",
    "\n",
    "# ìµœì¢… ë³´ê³ ì„œ ì‘ì„± í•¨ìˆ˜ ì •ì˜\n",
    "def finalize_report(state: ResearchGraphState):\n",
    "    # ëª¨ë“  ì„¹ì…˜ì„ ëª¨ì•„ ìµœì¢… ë³´ê³ ì„œ ì‘ì„±\n",
    "    content = state[\"content\"]\n",
    "    if content.startswith(\"## Insights\"):\n",
    "        content = content.strip(\"## Insights\")\n",
    "    if \"## Sources\" in content:\n",
    "        try:\n",
    "            content, sources = content.split(\"\\n## Sources\\n\")\n",
    "        except:\n",
    "            sources = None\n",
    "    else:\n",
    "        sources = None\n",
    "\n",
    "    final_report = (\n",
    "        state[\"introduction\"]\n",
    "        + \"\\n\\n---\\n\\n## Main Idea\\n\\n\"\n",
    "        + content\n",
    "        + \"\\n\\n---\\n\\n\"\n",
    "        + state[\"conclusion\"]\n",
    "    )\n",
    "    if sources is not None:\n",
    "        final_report += \"\\n\\n## Sources\\n\" + sources\n",
    "    return {\"final_report\": final_report}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjkAAAKgCAIAAAD24PuxAAAAAXNSR0IArs4c6QAAIABJREFUeJzs3XdgU9XfBvBv0ozuvekubdNFC7TQsvcQFJQho4IiCqIiThRFGaKg4sAJOEFkiwyRvUFGWzqge++9m6SZ7x+XX+yLpQyb3pv2+fyVJjcnT1ro03POTcLTarUEAADAYXy2AwAAANwFugoAALgOXQUAAFyHrgIAAK5DVwEAANehqwAAgOuMVqxYwXYGAINR3iK9VFMmU6tuNNScqy6xExlbCUWnKovavHyyovB8dam9yNhSP5ePVxRcqC51EJtYCu54+VRlUVJDtauJmZhvxPY3D+DBCdgOAMB1LRr1X+X5Co1mgpNnfF1lsazJ1MhIplYqNOo6ZYupkeBOl6UatUKjrlMqTIzk+rgsU6sVGnW9osWYb3Sny1K1qkYhL5fLyJjeSL7obWa1yCfUQiBk+5sKcH94eC0wwJ20aNQCHv9IeX6tqiXKxtlGKGY70X+V3FDtaWrhLDY9XlH4iIs323EA7hW6CqBtJyuL95VkrQzsz3YQvdhVnFmjkC/zj+DxeGxnAbg7dBVAG9Ra7Y/5qY+5+rAdRI9qlC3OYtN8aUOIpR3bWQDuAl0FcLtrtRWOYhPz7rGp80dpzlC7Hj3NrdgOAtAenLMO8P98lBmv0mq6SVER0WQXn0JZY3ZTPdtBANqDeRXAP6qVLS1qVTc8vVvA51sKRNi5As7CvArgljplS1WLtBsWFRGpNJqn4k6wnQLgjtBVALesSrtmJTRmOwVr5nsF7S3JYjsFQNuwBghARHS9vqpZpQi0sGU7CJtEfL6FQMR2CoA2oKsAiIhkapVUrWI7BcvypA2NSuUwhx5sBwG4HdYAASi9qe5QWV7nP+6695f+/P2GB7jjzMlDkxJiOzyPl6nlgbKcDh8W4L9DVwHQ6aqizn+LPJlMenj/Lr+AoPu9Y9zVi7k5GQ9wx3sxycWnVC7Vx8gA/wW6CoD8zazDrR30NHh87N8frXlz6sQB/UOdRw2U7Nr2PRGdP3tsWD8fuVz2yqKYd15fQETpackfrXlz2sMDh0R4PTv3kUvnTzF3T7lxvX+o8/kzRx8aEfbs3Ec2fLJy0dNTtFrtsH4++3Zt6fC0IZZ2zsamHT4swH+E91kHoP42TnKNWh8jK5SKpUueGjpi/Kq1X7t7eMdevfDmy/N9fAMGDx0z79klB//YeejkdSLSaDTvvrHI0tr6zXfWNUubzp0++vKiWQeOxTm59MjJSieiQ3/sWrv++54BQaamZjeT4x2dXVev+1YfgUvlzacqi6b16KmPwQEeGLoKurtSuXRfSfbjbn76GLymsqKhvm7wsDFBIb2JaPioiTv2n/Pw9CWirMw03Toen8///NvfjE1MbGzticjbJ+Dgvu2ZmSlOLj0y01OIaN7CJQGSUObg9NTkQUNH6yMtEQn5Rkn1Vegq4Bp0FXR3lQpZtUKup8GdXHr0ix7y0Zq3qirKowYN7+Hm6e3jz9yUlZkyetwk5rJMJt2/99fE61fzcrNqqiuZK+0dnIgoKys1MmqIrqhKiwtlMqlfQIieAtsIxUNxHiBwD/aroLvzNrN6TG/TCB6Pt/azH8aMm7z9103TJg74/KN3ZTIpEUmlzSVFBT39g5kFwIVzJ584sv/R6XMPHI+/klw2b8HLPB7Py9uPiDJSk/tEROsGzMy4SURBIeF6CmzE4w2yc9XT4AAPDF0F3Z2FkcDTxFx/45uZWbz0+ordBy8uXf7RsSP731/+EhFlpt0kIn9JMBHFX72Ylpq0bNVnY8ZPFgqFRJSZnuLtG2BsbFJRXtJQX+frF6AbLSszzc7e0dLKWk9pG1SKTzOv62lwgAeGroLurlGlXJl6RR8ja7Xa0ycOKRQtzARr0pTZI0ZNKMzPI6KcrDSBQMBsXJWVFRORk/Ot2Ux5afGVS2eYGsvMSCEiP/9/VvxyMlO9ewbc+TH/qwqFXEN4fwDgHHQVdHcWAqFUrWpSKzt8ZGlz44q3Xvx03fLUlMTamqqjh/f9eWBXZPQQIqqpqeLzjeKvXqysKPPw6klEJ44cIKK01KRV7y6xsLRycHQhouyMNBMTU1c3D92YtXU1submuCsXWlr0ssfmKDKe5xmsj5EB/gu8xxIAVSpkSo1GH++wHnft0tpVrxfkZZuZW/oHBI2dOOXhyTMFAkF+XvabS+YV5Gdv2LSrb+SAHb9u2vT1J81NDX37DXzv/Q1ff7Hm6J+/z5qzoKqqoryseNMvB3QDnj11+MOVb2g06j+OxpqamnV4YBHfqPNfFg1wV+gqAGL2aZQaDdsp2Lc2Pfb94Oju+cEowGU4Zx2ANKR9NenC2pABdzpAq9Wufuelf18vb5Ebi9v+GBGhWPzWux93aMx/5OVmbvn+yzZvqqwqc7B3bvOmngFBs+YsbGfYuPpKZ2MzFBVwEOZVAEREP+SnWAnEw7v3S4tkapWj2FTExzY2cA66CuAWuUbdrOr4MywMhUqrEfKMbEVitoMAtAF/QAHcUqOQl8ib2U7Bmk8y4zU8/OUKHIWuArjF1djsZGVhWlMt20FYEFtb8ZRnkL2w7b03ANZhDRDg/7lSW+ZpainkdaM/4/Kkjb2t7Pk8HttBAO6oG/2HBLgX/W2cpSpVvrSR7SCd5GhFgVStRFEBx6GrAG7nY2b5V3leoayJ7SB6Z8TjEdFgvFktcB7WAAHallBf6WdmndZU62VqyXaWjnehutTcSDDS0Z3tIAD3BPMqgLaFWzmYCYTHygs+y0rgdZUlsnqlwojHu1xTVq+Uj0BRgeHAvArgLlIbawMtbKoU8o8y4rxMLR9385OpVcn11SI+P9zaoVmlTGyoNjcS9LKy1/flJpUy6T4vpzTWmBgJelnaFcmbdxZlOIpMX+oZ1qJR480pwLBgXgVwF4EWNkRkLzKe5xXc09zKTmQs4htlN9dnSesthSI+n5dYX/nfL5eXlf5y5vidjsmVNVgKRTwe3e/lXGlDjUJuKzJ2MTZ93jfspZ5hRISiAoODeRUAJ2RnZy9btmznzp1sBwHgIsyrAACA69BVAADAdegqAADgOnQVAABwHboKAAC4Dl0FAABch64CAACuQ1cBAADXoasAAIDr0FUAAMB16CoAAOA6dBUAAHAdugoAALgOXQUAAFyHrgIAAK5DVwEAANehqwAAgOvQVQAAwHXoKgAA4Dp0FQAAcB26CgAAuA5dBQAAXIeuAgAArkNXAXACj8ezs7NjOwUAR6GrADhBq9VWV1eznQKAo9BVAADAdegqAADgOnQVAABwHboKAAC4Dl0FAABch64CAACuQ1cBAADXoasAAIDr0FUAAMB16CoAAOA6dBUAAHAdugoAALgOXQUAAFyHrgIAAK5DVwEAANfxtFot2xkAuq/Zs2c3Njby+fyWlpbq6mpnZ2c+ny+Xy48cOcJ2NAAOwbwKgE3Tp0+vqqoqKiqqrKzUaDQlJSVFRUUWFhZs5wLgFnQVAJsmTZrk6enZ+hoejzdo0CD2EgFwEboKgGXTpk0Ti8W6Lz08PKZNm8ZqIgDOQVcBsOyxxx5zc3NjLvN4vOHDh7u6urIdCoBb0FUA7Js1axYztfLw8JgyZQrbcQA4B10FwL5Jkya5ubkxkyoXFxe24wBwjoDtAAD3rUWryW9uKG2RqjRqtrN0mIh5s+pOnHAeM+RkZSHbWTqMiZHAy9TS1diM7SBg8PD6KjAwh8vyj1bkS1UqH3PLRqWC7TjQHjOBMLWh1tXEbLFvmJuJOdtxwIChq8CQ7CvJvlBdOq1HT7aDwH2oUyl2FWa+F9TP0wSvG4MHhP0qMBhnKovPVpWgqAyOtUD0rHfws/Gn2A4CBgxdBQZjX2n2BGfPezgQuGiis9fP+alspwBDha4CwyBVq3KbGywEIraDwAOyERmnNNawnQIMFboKDEOFQuZuit0OA2YjNJZ3ofM2oZOhq8BAaKlZpWQ7BDw4jVbTgPM24UGhqwAAgOvQVQAAwHXoKgAA4Dp0FQAAcB26CgAAuA5dBQAAXIeuAgAArkNXAQAA16GrAACA69BVAADAdegqAADgOnQVQBe05LGRMdGS2LPH2A4C0DHQVQD/yavTxny29Hm2U+hFF35qYHDQVQAPLjslubyogO0UetGFnxoYIgHbAQD06NT+nWcP7C3OyxKbmPqF9J40d4F3YAgRbXh7ydVTR2a88HptRfmZg7tf/3STJDwyN+3G3s1f5qbfNOIL+gwZPmX+YgtrG2acY7u3nv9rf3FulpWtfWj/gVOfecnSxnbbhrV/bf+ZiOLOnYyJlry87uu+Q0a2M0g72hxfl3PW4qXWtvbHf99elJXRMyTsmXfW2Do4E5G0uWnfD18lXDpbWVrSw9MnatRDE2Ke5vP/3x+gR3dt2frZBw6ubp/tPcFcc/bQ3s1r3raxd9pw4IxKqTi6+9crJw4X52XbODiF9hs4cNwjfiHhbT61yycOn9q/Ky89VSgSBvWN6j1w2IAxE/XzcwO4HeZV0GVlJMX/uPa97JSkiKGjbR2cYs8e/+CFJ2sqy4hIIBQS0en9u07u2+7pH2RqZlGan/v+oicSLp3tM3hEz9Dwk7/veG/+dGlzExGdObB7y6drqspKHprxpImZ+al9O39Y+y4RhUQODOrbn4hcPL0nz1vk4uHdziDtuNP4RCQUCYno2pnj+376xt3XT61RJV+9uO2LtcytW9av+mv7z8amphNjnq4oKdr57fpju7feNvig8ZOEQlFlSVFxXjZzTfKVC0Q05OHHeDzer198uOOrj1tkspGPzvALCTux97f1ry2UNjf9+6ldPnH4q+WvFGWlR40aN3DsI7FnT3zz3msZSfF6+LkBtAHzKuiy9mz+kogmxMyf+fxrRLR83pTc1Jsn9+2c9uxLzOSjurzsw60HXDy9iWj715+0yGR9h4x6eukqIvptw7rD2386/+fvY6fPyU5NlvSOHDLh0SETHguOHLDm+TnXL55WqVRh0YNzUpNT4q64evpMfWZx+4O0k/NO4wsEAh6PT0TlBbmf7Dluambu5R/047r3ki5fJKIWmbSmolzSO/KJl5d5+gXy+fzfv/8q/vypcY/PbT24mYVVxPAxfx87lHDhdA8vX61Wy9x9yITHiCgj6ToRzX97jX9obyLq1X+wWq3RtPXUTuz9jYjGznhy0twFRNRn8PCS/BxLa1v9/xgBCF0FXZZKpUq7fpWIQvsNYK5Z/ePe244JiYxmioqIMhJiicgnKIT50ieoFxGlXY8dO30OUzwMG0dnItKo1dLGBmaZrrV2Bmkn6l3HDx80wtTMnIi8JcFEJGtuVKlUYhPTZV/98s8d7R2JqKGm+t/jD39k2t/HDsVfPDMhZn52SpK0qcE/rK9TD3cicnbzLMxK3/T+W30Hj7R37jFw/CPMA/2bs4cnER3Z8VN1abGdi+uAMQ9LwiPbeVIAHQtdBV2TrKlRo1YTkYmZxZ2OsXd21V1uqKshot3ffb77u891V1aUFBHRpWOHdn33WVVp8V0ftJ1B2nHX8Y1NTJkLImMT5oJGrdIaGe3Z+PnR3VvlUmn74wf17W/n7JqeECtrbky+fIGIBo1/hLlp5ouvNzXUpcZf/XPbD0S069tPQ/sPfH71p0ZGRrcNMuyRaQVZGWcP7jm1fxcR7dn4hX9Y32ff+ZDpPAB9Q1dB1yQ2ufVrXdrUcKdj+Px/fiObWlgS0YhHH48aMV53pcjEpLq85NsVr2u12pGPzeg/fFx1RenG1W/dacA7DdJOzvsav7W/jx3a/8tGodg4ZslbHr4B8RdPH9nxy50OHv3YzB3frI89eyrp6kWBSBw96tY5EY6u7m9/vSU37UZBVnr8+dNx505cPX10wIVTEUNH3zaCSGw8/63Vj8x9Nj8j9ca1S+f+/CM9IfbY7q1PLFl2L2kB/iOcWwFdk0hszKzvMTMJIvps6Qsx0ZKd365v8/ieQWFEVFdVGRQRFRQRJRCJSgvyBEJhaUGeVqslotkvLg2KiFLIW5jjtRoNEfF4PCJSyOXtD9JOzvbHb0dJXjYRufv4jXt8blBEVF1VBRFp7nCvIROnGAkEl44dyEyKjxw22sTMjIgUctmRnb/8uPZdb0nI0IlTXl731cjHZugmgrc9tcsn/tq46k2BUBQ5bMxTr69Y8M4HRFR5tykjQEfBvAq6rCnzF3+1/OXD23+qr62qKCrMvJFgY+80fsZTbR48ZlrMyT92xJ8/9f6iJ/xCw0/9sau5oX7J2g0+QWF8Pl+j0Wxas8yph0fchdM9fPyKczL3bN7w8JxnrO0diCjt+rVd334a2n/QnQbx8g+6U0hXL992xm/n2bn5+hNRbtqNbRvWKhUtsuYmHo9XXlywZ/MG5myI1ixtbPsMGnHtzDEiGjRuEnOlUGx87s99BZlptdWVnv6BTfW1fx87TESBffoR0W1PLT0h9vxff+RnpfUZOEytUV85cYSIAnv3e6CfDMB9M1qxYgXbGQDurk6pOFNVHGHjeO93cfPx85IEZ99MSrt+rbayvFfUoEUrP3Fw6UFEsWdPFGSl+waHhUUPYQ62sLbxCexVWVqckRiXkRjv4RsQs+StfsPHmpiZ8wVGzfX1OSnJ5tbWTy9d5ebjl3kjITftZtSo8YHh/eIvnq6rqkhPjAuLHhzYp1+bg7QTsv3xs28mts7ZWFfLnI836cmFbr7+ZYX5cmlzcXaWm6/fU6+vFIrEOSnJpQW542fMPbJzi7SpMWrUeFcvX+aBlAp53LmTNvZOT77+LjNn4vF4of0H1lSUx58/lXb9WkFWelj0kHlvruwZHE5Ezu5erZ/a6CmzWuSylNi/b1z7OyMxvoeP39Rnl4x49HFmqHshVatSG2snu/rc+08QQIfHrD8AcFyetHFF6pWF3iFsBzFIWq32i7cWx549PmPRqxOfaG+6pj+VLbK9Jdk/9x3FyqODocMaIIDeHdvza9aNxDZvCosaPHDcI3p99E/fWFRVVlKQmebg6jZmWoxeHwtAT9BVAHo3ZmrMmKmslURGUrxc2hwSOSBmyVu6s94BDAu6CqCL++7IZbYjAPxXOGcdAAC4Dl0FAABch64CAACuQ1cBAADXoasAAIDr0FUAAMB16CoAAOA6dBUAAHAdugoAALgOXQUAAFyHrgLDIOLzbYVitlPAg9MQuZmYsZ0CDBW6CgyDq7FZVnN9i0bNdhB4QCWyJisB/tqAB4SuAoMxytE9R1rPdgp4QKUt0sEOPdhOAYYKXQUGY7Fv2NnKkmJ5M9tB4L4drShwNzGPsnFiOwgYKnwuMBgShUbzfMLpMGsHEY/vKDbVaDVsJ4L2aIhXKm+qVbY4iU0X4DOd4T9AV4HhOVCam9JQ06JVl8mlbGd5cCqVsrq6xsmp7alGS4u8rKzc09Oz03N1JDcTMyuhuL+Nc6SNI9tZwLChqwDYcfjw4StXrqxcubLNW7du3bphw4YRI0asW7eu06MBcA72qwDYkZKSEhgYeKdbr1y5olarz50799NPP3VuLgAuQlcBsCM1NfVOXdXU1FRaWsrn85VK5fbt269cudLp6QC4BV0FwI52uioxMbG2tpa5XFNTs2bNmqamps5NB8At6CoAFmRnZ3t4eIhEojZvvXbtWl1dne7LoqKiJUuWdGI6AM5BVwGwIDU1VSKR3OnW5ORkHo+n+5LP5ycmJr777rudlQ6AcwRsBwDojlJTU4OCgu50a01NDY936xxdIyMje3t7CwuLVatWdW5GAA5BVwGwIDU1dezYsXe6taamxsnJ6c8//2xoaDAyMjIzw1u+QneH11cBsCAqKurChQsCwV3+WDxy5MiFCxfef//9zsoFwFHYrwLobFlZWV5eXnctKiLq1atXQ0NDp4QC4DR0FUBnS0lJaWezqjVXV9cNGzboPxEA16GrADpbWlpaOycB3iY/P7+5GW8tD90dugqgs937vIqI/vjjj99//13PiQC4Dl0F0NnS0tLaeSfA2wwYMKClpUXPiQC4DuesA3SqjIwMHx8fIyOjezw+MjIyMjJSz6EAuA7zKoBOdV+TKsbVq1fVarXeEgEYAHQVQKdq/6NA2vTLL7/ExcXpLRGAAUBXAXSqB5hXTZw4USaT6S0RgAHA+1YAdKp+/fpdvnyZz8efiQD3Af9hADpPenq6n5/f/RaVVCo9e/as3kIBGAB0FUDnaefzFdthamq6YsWKxsZG/YQCMADoKoDO82BdRUTPPvtsTU2NHhIBGAZ0FUDneeCumjlzpqenpx4SARgGdBVA53ngriooKDh37pweEgEYBnQVQCcpKioaNGhQ6w+nv3dKpfLPP//UQygAw4CuAugkbm5uly5derB3oNBqtSNGjNBDKADDgPcDBOg87u7uRUVFD7Dz1LNnz549e+onFIABwLwKoPO4ubkVFhY+wB3Pnj2bnp6uh0QAhgFdBdB53N3dH6yrtm7dKpfL9ZAIwDCgqwA6zwN31YgRI/z9/fWQCMAwoKsAOs8Dd9WsWbNMTEz0kAjAMKCrADrPg3VVZWXlzp079ZMIwDCgqwA6j5ubW3Fx8f3eKzEx8fr16/pJBGAY0FUAneoBTgV0cXGZOXOm3hIBGAC8vgqgUzHLgO7u7vd+l+DgYH0mAjAAmFcBdKoH2LL67bffioqK9JYIwACgqwA61QN01ZYtW3ASIHRz6CqATnW/XaVWq5977jk7Ozt9hgLgOnQVQKdyc3O7rwU9IyOjSZMm6TMRgAFAVwF0qvudV6WkpOzevVufiQAMALoKoFPx+XwXF5d7f5XV1atXy8vL9RwKgOt4Wq2W7QwA3cugQYPMzMwUCkV9fb2bm9uBAwfaOTg5OdnGxsbNza0TAwJwDl5fBdBJevfuzfsf3ZumDxkypP17hYaGdko6AE7DGiBAJ5k7d65AIGj9Gfa2trZDhw5t/15r165taGjQfzoATkNXAXSSJUuWBAQE6FbdNRqNnZ1dZGRkO3dRKBQHDhywtLTsrIwAHIWuAug8y5cvd3FxYS7zeLz+/fu3f7xSqfziiy86JRoAp6GrADqPv7//lClTxGIxEdnY2Nx1s8rMzKz9iRdAN4GuAuhUTz31VK9evdRqtZ2dXURERPsHnzlzpv2zBAG6CZwHCAZAQ9rKFlmNooV3Dwdz34xXX8pe0xQQFZXWWNv+kUdvJHh4eNz1MIPB47mbmJkZCdnOAYYHr68CrttRlHGoNE+l1ViLxJpu9s9V3tIiFAiMjIzYDtIx7MUmN+urJRa2czwkwZa2bMcBQ4KuAk77OiepqkU+zKGHmN9Ffl9Do0qxoyjjJd/evazwhrxwr9BVwF0bc29UKeQjHfCWDV3QptwbSwMiAsyt2Q4ChgHnVgBH5csac6QNKKquaopbz18L0thOAQYDXQUcldfcoNFq2E4B+mInNL5WW44fMNwjdBVwVGWLzElsxnYK0KNgK7siWSPbKcAwoKuAoxQadYtGxXYK0KNKuYxHXeNlCKB36CoAAOA6dBUAAHAdugoAALgOXQUAAFyHrgIAAK5DVwEAANehqwAAgOvQVQAAwHXoKgAA4Dp0FQAAcB26CgAAuA5dBV3HS4+OiImWXDtzjO0g9+3U/p1vzHgoJlqy89tPO2RARYs8JloSEy0pzssmou9WLY2Jlmz5dE2HDE5EFSWFzPhNDfUdNSZAOwRsBwDo7krycn5c+x4RDX14qm9QKNtxALgIXQXAssb6GiISGRs/s+x9trMAcBS6Crqg33/46u/jfzY3NvQZNOKpN1YYGRmlxF7+4MUnTcwsNp+4xhzz3tPTs1OSnnztvVFTZh7bs23L+tV9Bo8Y9/jcP376pig3K2LoqNkvLr1+6dz3H7xNRH2HjHjm7Q8FAgERlRXk7fn+y8yk+KaGOq+A4PEz5kQMHUNERTmZb85+2MTU/JNdR3Zv+iLu/ElTM/Ox058YM+2JdqLu/2Xj7u8+IyKFXB4TLRkyccqzb69pqq/btmFdemJsY11dSGT0I3Of9ZaEMMfnpt3Yu/nL3PSbRnxBnyHDp8xfbGFtw9x06ejB33/8urww39rO8cnX3/33Y/F4vEtHDx7/fXtBZqpfSO+F766ztncgImlz074fvkq4dLaytKSHp0/UqIcmxDzN5/OJSKPRbPti7Y2rFytKi23sHUL6DXhs3gvMvVq7dubYF28ttrKzX/XjbjtHlw76MQL8A/tV0NUc37v9r99+ljdLG2qqzxzYfWrfzrveRSgUElF5Yf7PH6+0c3RpqKk+tW/nV+++uuvb9RFDR8qlzRePHDz3514iUioVH73yzOXjf/oG9Ro6cWp6Quznby7OTb9JRAKRmIha5NLP33xBrVQ6OPcoLyrY8umawuyMdh46IKzP4IceJSIjgWDyvEV9Bg1TqVSrF84+f3ifs7tn9JiHkq9eXLlgdk7KDSIqzc99f9ETCZfO9hk8omdo+Mnfd7w3f7q0uYmICrPSv135RllBXs/gsKCIft+tevPfj5WWELt70xeefgF8vuDGtUub1rzFXL9l/aq/tv9sbGo6MebpipKind+uP7Z7K3PToa3fH921pba6atD4R5QtilP7dn629HmN5v99nG9xXvY3K94Qio3f+Gwzigr0BPMq6GpkTQ1fHDgjEok/ePHJjMS4G9cujp46q/278Ph85nfuuu1/9vDy5QsEZw/uSbx09rN9J+0cXVRK1eUTh1Pjr42Y9Hheeoqto7NTD49FKz8RCIVFORkpcVeuXzjtHRDM5/OYiUifwSMfnvOMUql4/fHxVaXFN65ecvf1v9NDS8Ijiej84X0CoXDqM4uJKOHS2eK8bAsb25c/+kYoFIVHD/30jUUHtmxcsvbLM4f2tshkfYeMenrpKiL6bcO6w9t/Ov/n72Onzzl9YI9Wq/X0D1q+8Tc+n7//l+92f/f5bY9VVVa8ftdRcyvrsOih619bmHT5Qk1lmZm5ZU1FuaR35BMvL/P0C+Tz+b9//1X8+VPjHp8rl0oPbtlERItWfBQ+YGio7PJ0AAAgAElEQVTDszWLJw/PTklKjbvi2MP91ne7uWn9qwuVLfKX133t6RfYET9AgDZgXgVdzfDJ003NzAVCYeSw0UTUUFtzj3d09vDq4eVLRO6+fkTk6unDzBLce/oTUVNdLRH5hYS/883WpV/8IBAKicjGwZGIGmqrW48zYOwEIhIKRR49A4iooe5eAzDSE2OJyMsvUCgUEVHPkHAiSr1+lYgyEmKJyCfo1nqgT1AvIkq7HktEuanJRBQWNYhZuxv+yPR/j9xv+FhzK2siCu03kLkmJyVZbGK67Ktf3vlmK9M0NvaORNRQU01EOSlJMmkTEYVEDiAiSxvbn88m/vp3WnBktJa0zAjfrHi9oqRwwNiH+w4ZeV9PE+C+YF4FXY2F1a39G6FQTEQatfoe7ygSGzMXBAIREYlNTG9dLzImIrVGwzTflk/fv3LyL61We6dxxCZm/xtQzMy07it/Q20tESVfvRgTLdFd2dxQL2tuZGpv93eft54zVZQUMfMbIjIxs2CuNLWw/PfIJmbmt56gUGhiai6TNkmbm7Va7Z6Nnx/dvVUuld52fHVFORGJTUyYYm5TZlI8EcWePV5fU21la3dfzxTg3qGroHvg8ZjdJt0VzC/3+7Vn4+eXTxy2c3KZ+eJSSyvrP375LiX2cocGJTMLS2YCN23BktbXC0RipoFGPPp41IjxuutFJia6HmpuamCubK6v+/fIuqesVLQwEyZzS6u/jx3a/8tGodg4ZslbHr4B8RdPH9nxy/+SWBBRi0ymUirvVFdPvLws+eqlhItntn+1buG7H3XctwHg/8EaIHQLJmZmRKRStDBnOpTk5ZTk5zzAOMxLa6NGPRQ1cpx/eERpXu4DzJza5xvci4iqykq9A0ODIqKcPb0KstPVGrVQKOoZFEZEdVWVQRFRQRFRApGotCCPaRHmRMHkKxeZCd+l44f+PXLSlQvNjfVElPj32VuPFRhakpdNRO4+fuMenxsUEVVXVaF7Rl6SoFt3vHyeabgXHh4SEy1J/Ps8j3jMTYPGT4pZ/Cafz7/w14Gsm4kd+H0AaA3zKugW3HtK7JxcqstLP3lt4aCxD188dsjN17+o3TP02uTm65eeGHfhyH6hWJyXnuLhH1BbVZ4Sd+XMwT1Bfft3SNS+g0c6uXmUFxUsnzclctjo6+dPF+VmjZ0+J7TfwDHTYk7+sSP+/Kn3Fz3hFxp+6o9dzQ31S9Zu8PIPGjxh8rE9v+ZnpKx+LsbBxfXm7bM9LRHJpc3Ln5wa0n/ApaOHmO0rKzt7N19/5lT4bRvWKhUtsuYmHo9XXlywZ/OGqc8sHj1l9vG9275duXTA2AkJl87VVVWE9hsYFj24oqTw1rhacvbwGvHo4yf2bv9x3XsfbPmjQ74JALfBvAq6BYFA8NyKj5zcPOqrK3PTb76w+lNXTx8iUqkU93Dvf0ycPT84YoBSoUy4eDY4ImrJh19GDB1dUVyYdv1ah0UVCl9fvyli6Oiq0uKDWzbLZNIZL7w+44XXiMjF0/vVj7/z69Un7fq1g1s2O7t5vrjmc+bVXd6SkHlvrhSbmGQkxqUnxr/4/mciY2MiUiuVRKRSKolo5OTHA8L6ntq3Uy5tDu03cN6bq4gocvjYqNET7Jxdr548aiQQvvj+F4/Nf1EkMj5/+A8imvva8mkLl/B4vJO/72isqxkyccriDzb8O/PUZ14yMTUvyEw7dxhdBXrBa2eLGIBFvxWm50obRjq4sx0E9OWbnOQ1wdHuJuZsBwEDgDVAAL07tufXrBtt7+WERQ0eOO6RTk8EYGDQVQB6N2ZqzJipMWynADBg2K8CAACuQ1cBAADXoasAAIDr0FUAAMB16CoAAOA6dBUAAHAdugoAALgOXQUAAFyHrgIAAK7D+1ZAF5Fy8KixVRsfMAidycnBwSywJ9spoAtCV0EXYWNt3SeqH9spujUekZlQlElKtoNAF4Sugi7Ce0C/BoER2ym6u2atmu0I0DWhq6CLkBvxSNuRn88LANyBcysAAIDr0FUAAMB16CoAAOA6dBUAAHAdugoAALgOXQUAAFyHrgIAAK5DVwEAANehqwAAgOvQVQAAwHXoKgAA4Dp0FQAAcB26CgAAuA5dBQAAXIeugu5uz+YNMdGSyyePtHNMQ13Nns0b5FJpJ+aiyyePxJ8/9QB3/PiVZ2KiJTWV5ff6QCcOPzumf0y05NC2Hx7g4QA6AboKuruoUQ8t+/Ln8Ogh7Ryz57vPj+7YYmxq2mmpqsqKN658o6Kk8AHum5GU4OTuaevgdC8HK1rk361aamJquuzLnwePn/wADwfQCXharZbtDABt+K0wPVfaMNLBXa+PolIqnx7R28XTZ+2vB3Z+u/7gls2zFi89umNLXU1lr6jBs196y8bO/ukRfXTHf38yPutmwv6fv8vPSBMZG/cZOGzW4jeNTU13fLP+0NbN0xa+/OevP0h6R4ycPOPjV58dPWV2StzlmsqKz34/sXBs/+CIAW99+SMR7fz204NbNi1Z+2Wv/oPmDe/dMzjMP6zvkR0/2zq5jJoya+Lsp7dtWPvX9p+ZRxw3Y27MS28xl//89fvtX39y21N459utkvBI3ZdVZcVLHh0ZMXS0Vqu5GXvZ2d3r4TnP9B8xjoiO7d56/vAfRbnZDs6uwydNHz/zyaQrFz5aMp+5o8jY+PuT8Ud3brl0/GBRTraltU2v6MFT5y+2srNXyGXzhvcOioiysLK+cvLI6p/2NDc2/PubcL/f/G9yktcER7ubmD/Qjw66F3wuMHRrhVnpapXKyz+QiErycogo4eKZZ95Zc+bAnssnDrv5+E1+cuHUBUv2bPx80PhJwx6ZWl6c//Erz1pY2z77zvtlBfk7vllvaWc/9ZnFpfm5RHT11NGYl5dZ29mX5GUTUezZExNi5lla22bfTCIiT38J86D5GSlE5C0JLivMJ6Ki3MwhEx97bsXHP65bseOrjweMmTB04pT4C6fLC/Pf3PCji4eXLq1faO/J8xbd9hTsnFxbf5l1M5mIctNvPrHkrZDIgb+sX/XrZx/2HzHu5O87tny6JiA84uV1X106enDbhrWefhI3H78Rk6af2r/r4TnP9h0y8sTebds2rB00ftLMRa9n3kzc/d1nlcVFb274sbQwj4jy0lIGjJ0wb+lKLVGb34RO+YlBN4Wugm4tN+MmEXkFBBFRXkaqiZnFy+u+MjGzaGpouHzisEhsLDYx1Wo0RBQWPUQSHrn18w/UKtXEmKcjho4hoiO7tqTGXaFnqDArncfjLVr5cQ8vXyK6fPwwEU2et2jko48T0f5fNhKRp9+trspNTzExs7Bzck2Nv0ZED895dsTkx4no9IHdKbGX66urPP2DqkqLXT19QiIHtE7rH9bXP6xv+88oL/0mEc1/a3Vov4FEtGfzFw111UR0av8uIprzytuefoH2Tq4X/tqfEnc5KCKqpUVGRP1GjHV0dVvz/FwTM4t5b6wQGZsERUTt++Gr7JRkIirITCOiXlGDnnztPSK60zcBQH/QVdCt5aWnMF3V3FhfXVYSHDHAxMxCN/Vh2oX57e/lH0REmUnXiejXzz/89fMPmRFcvXyb6usqSgp9AkOZoiKivIwUPp8/YMxE5sv8jFQi8pIEE1F1RWljbU1o/4G6DgiOiGYOa6yrJSIbe6f8zFS1SuXhH/gAzygn9Qafz5eERxCRUtEibWywsXdqkUmZZ/T2nEd1RxoJhESUn55qJBB4+gUmXT6vbJH3HTxcZGxCRAq5TKVU2jm7ElFuWgoRDRz3CHPHNr8J/+GHAHB36Cro1goy0ojIOyA4LSHu38t0XpIgZu1LbGLi4ulNRGq1moheW79RJBIzR1ra2uWm3ySinsFhzDUqpbIwK93N19/EzOzWo2SliU1MnN29iOjG1UvMIzKVRkTmVjZMURXnZDq5e1rbO1y/eJqIvP/VVfeyX5WXdtPYzEIoEhNRStxVrVYb0DtCo1ETkYun91Ovvac70tnTS6VUFudle0mC+Xy+SqkgIhNzC+bW9KR4Igru059ZUSQi/169mZva/CZ03M8EoA3oKui+NBpNbvpNF09vsYlpXquJFLNMZ2puaevgTER1NZXmVtYpsZf9w/p6+knyM1KKcrPcffz/+OkbpULx3qbf4s+fJiLvwOD/3femVqtl2oghlzYrFYprp4+qlMrfvvyIiDxvrTqmCYTCE3u39Rk4/NC27zUazdhpMURUX1tDRI0N9aX5uUxHMvqPGu8tCbntWbj7+usul+TlSJsahELRyX07HXu4/fzJSiIaPWWWiZmFvUuP2sqKpsZ6tUq145v1vfoPevrNVRnJ17Varbd/EBH5BPXiGxnFnz/Vf8Q4jUbzy/rVjq7uk59epNFo8jNSHF3dzSysmEdp85ugzx8UALoKurHi3Cy1SuXpH9TOMh0RhQ8YFnfuxKYP3v5s74lZi5eKjMV7N36hVCrsXXosWL5WKBIzPecdGMocn5t2k4hal8rjz712cMvGr5a/EhQR1Stq8N/HDrn5+FWWFksb6weMfbimvOyDF580t7KetnDJ6KkxRBTab+Dvmzcc2rrZ2d2zdVfZO/ewd+7RzjNKjb9KRFOfXXxy3/aCzDTHHu7Pr1rv36sPEb3+6aYdX3+yYdlLRBTYp9/kpxb+swQqCSYiWwent778ec+mL9Yunsc3Muo3fOycV9+xtLYtyslUyOW6JiaiNr8Jevj5APwD56wDR3XOOessunr66IZlLz3+3KsPz+mmpyXgnHW4d5hXQddRXph/4eiBf1+v0aj5fKPbrjQ1txg/48nOitaGgoxUIurh05PFDACGAl0FXYeTu+eU+S+yneJe5WWm3bbbBAB3gq4CYMdrn3zHdgQAg4H3AwQAAK5DVwEAANehqwAAgOvQVQAAwHXoKgAA4Dp0FQAAcB26CgAAuA5dBQAAXIeuAgAArkNXAQAA16GrAACA69BVAADAdegqAADgOnQVcJSFQGTMw+cAdGXOxqZGPB7bKcAwoKuAo5yNzfJljWynAH1RaDSpjTWuxmZsBwHDgK4Cjgq1steQlu0UoC9Fsqah9j3YTgEGA10FHGXM589w899SkMZ2EOh4MrVqe1HGa3592A4CBoOn1eJPV+CuhPqq9Znxwxzc7EUmlgIB/rEaNB7xKlpk9SrFkbK83/qNM+YbsZ0IDAa6CriuUNa0qygzpbGmQaVQd91/rlqNVqFUisUitoPokYephUaj7WvjONdDwnYWMDDoKgBOyM7OXrZs2c6dO9kOAsBF2K8C4AR7e/unnnqK7RQAHIV5FQAAcB3mVQCcUFVV9eOPP7KdAoCj0FUAnFBfX3/06FG2UwBwFLoKgBOwXwXQDuxXAQAA12FeBcAJ2K8CaAe6CoATsF8F0A50FQAnYL8KoB3YrwIAAK7DvAqAE7BfBdAOdBUAJ2C/CqAd6CoATsB+FUA7sF8FAABch3kVACdgvwqgHegqAE7AfhVAO9BVAJyA/SqAdmC/CgAAuA7zKgBOwH4VQDvQVQCcgP0qgHagqwA4wcHBYf78+WynAOAo7FcBAADXYV4FwAlVVVXff/892ykAOApdBcAJjY2N2dnZbKcA4Ch0FQAn2NnZjRgxgu0UAByF/SoAAOA6zKsAOAH7VQDtQFcBcEJ9ff3x48fZTgHAUegqAE7A66sA2oH9KgAA4DrMqwA4AftVAO1AVwFwAvarANqBrgLgBOxXAbQD+1UAAMB1mFcBcAL2qwDaga4C4ATsVwG0A10FwAnYrwJoB/arAACA6zCvAuAE7FcBtANdBcAJ2K8CaAfWAAHYtGDBgqamJh6Pp1Qqm5qarK2teTxeS0vL7t272Y4GwCECtgMAdGu9e/duvfRXXl5ORM7OzqyGAuAcrAECsGnq1KkeHh6tr9Fqtb169WIvEQAXoasA2GRvbz9mzBgej6e7xsXFZebMmayGAuAcdBUAy6ZPn+7u7q77MjQ0NDQ0lNVEAJyDrgJgma2t7ejRo5mplaOj4+zZs9lOBMA56CoA9k2fPp3ZtQoKCgoJCWE7DgDn4DxA4K5qRQufdw/HGT6+pfmgcWNqDhx4bE5MrbKF7Tidx0YoZjsCGAa8vgo4J7G+amdRZkJ9pYuxeYOqG/3i7m68TS1zmxsG2rk879tLyMMaD7QHXQXccqaqeEdhxnhnT3uRiRGve8yqurFmtbJcLt1amL4tcqwt5lhwZ+gq4JDTlUV7S7Jj3APYDgKd7b3UK38NnIS5FdwJ/m0Ah+xDUXVXs90Dvs5OYjsFcBe6Crgiq7m+SaVkOwWww0FscqW2jO0UwF3oKuCKUlmzr7k12ymAHTZCsa3IpEWjYTsIcBS6CrhCoVU3KhVspwDWZDfV4VwauBN0FQAAcB26CgAAuA5dBQAAXIeuAgAArkNXAQAA16GrAACA69BVAADAdegqAADgOnQVAABwHboKAAC4Dl0FAABch64CaI+iRR4TLYmJlhTnZXfOI363amlMtGTLp2s65+H+raKkkHnKTQ31bGUAuA26CqAzVJUVx0RL/tr+812PdPH0kfSOdHb3uJdhN615+5lRkR0R8B9CoVjSO1LSO9LISNCxIwM8MPxbBOgMV078dY9HTpq7YNLcBfdypEqpjD174r/laoONg+M732zt8GEB/gvMq8Cwndq/872np88f2ef5iYM+f/PF3NQbrW96e86j80f2WTC2/7vzpiVcOstcX5STGRMteWZkRH111fcfLn/uoQGvThtzbPc/v50vHT342uPjnhgQ+OLDQ5OvXGz9cO8veqL19Cjh0tmYaMmiCQOZLzUazdbPPlg6c8JTw8JfmTr6x4/eq6uqJKLlT03Z/vUnRLRtw9qYaImsubmdZ9R6DbCdqHHnTj45JFTaWC9rboyJlmzbsJaImurrNq5+65Wpo58ZFfnFW4tz027c9pQzEuOWPzVl9aIn5g4OiYmWXL94Rve4L08ZFRMtOfn7jn+vAf59/M9VC2fPH9n37TmP7v/lO7VaTUQLxvaPiZZUlhYTUWNdLXOX3Zu+YO7y1fJXYqIlSnzIC3QQdBUYsIyk+B/XvpedkhQxdLStg1Ps2eMfvPBkTWUZER3a9sOPa98rzE6PGDrav1efnNTkT15dwNSVQCQmoha59PM3X1ArlQ7OPcqLCrZ8uqYwO4OICrPSv135RllBXs/gsKCIft+tevPe8xza+v3RXVtqq6sGjX9E2aI4tW/nZ0uf12g0wx6Z7uDqRkSh/QdOnrdIIBLe44DtRHXx8B780KNEJBSKJs9bFBI5UKVSrV44+/zhfc7untFjHkq+enHlgtk5KTeYY4iopUX2zco3pE2N3gFB4QOGEVHi3+eYByrMzqgsKeLz+f1Hjrstw6Vjh75+99WCzNQxU2cLRaLd333+9buvElFg735ExNRhWsI1IuLz+VnJicy98jJSiAiriNBR0FVgwPZs/pKIJsTMX/juutU/7fUODJZJm07u2ymXSv/48Wsimrd05cJ317368bejp8wmor3ff0lEfD6PmQP1GTxywbtrl2/cZu/Sg4huXL1ERKcP7NFqtZ7+Qcs3/vbcex9PfOLpewwjl0oPbtlERItWfPT00lVrtuwTiMTZKUmpcVdGPvq4g6s7EfXqP3jqM4uZ5rgX7UR19fIZ+vBjTJ9NfWZxWPTgG1cvFudlW9jYvvzRN/PeWLloxccqRcuBLRuJiGfEJyKNWh3Yu9/63cdiXnqr/4ixRBR39iTzQPEXTjNVam51+0czH925hYimPLN4+nOvrPh+p7ck+OqpI4VZ6QFhfYkoJ5Xpqlgejzdg7MT0pDiNRtPcWF9WkOfg6sbn4zcMdAz8SwJDpVIq065fJaLQfgOYa1b/uPfXv9OmPftSWsI1uVRKRFGjJjA3RQ4fQ0S5qTdan9s2YOwEZs7h0TOAiBrqaogoNzWZiMKiBjG/Z4c/Mv0e8+SkJMmkTUQUEjmAiCxtbH8+m/jr32nBkdH//cm2GfU26YmxROTlF8h0Yc+QcCJKvX619TFMZxNRxNBRQrFxbVU5c35jwsUzRNR/5PjbxlS0yHNSk4nIN6gXc41vcC9mWEl4hK6rMhLjPP2DQiIHqhQteWkpzJWSsIj//sQBGJihg6Fqqq/TqNVEZGJmcdtNDXW1RCQ2MTE2NWWusbS1u3WvhnpmskJEYhMz5oJILGamL0Qka25qPaapheU95qmuKGceVCC81yW+e9dm1Ns01NYSUfLVizHREt2VzQ31suZG3Zf2rq63xjE2CY8ecu3MsYRLZy2sbDKTrxsJBJHDxt42ZmN9rVarJaJVC2a1vr68uHDUlNnGpqa5qTdkzc156Sljpj0h6R1BRBnJcS1yOREF9EZXQYdBV4GhMjG79etb2tRw201W1rZEpJDLFS1ykdiYiJrqa5mbLG1sdZfvMKw5ETX/b8zm+rrWt/J4PCJSKm6dMsAUG8PMwoKIWmQylVKpj7q6KzMLSyLyCwmftmBJ6+uZTS8Gn2+ku9x/5PhrZ44l/X2BuWPvgcN139J/xjS3Yi489cZKF3dP3fXW9o58Pl8SHplw6ezZQ3u0Wm1AeF975x7W9o6ZydfVKjURBfXtr7fnCt0O1gDBUIlNTF08vYko+fIF5prPlr4QEy3Z+e36gPAIEzMLrVZ79dRR5qa/j/9JRIF9+pmambc/rLckhIiSr1xk5hOXjh9qfSvTZHnpN5kv486d1N3kJQliLiRdPk9ESkXLCw8PiYmWJP59noh4xCMihULegd+BW8WpVDBRmdW5qrJS78DQoIgoZ0+vgux0tUZ9p+2xPoOGCcXGqfGXr50+SkRRox769zHGpqZuvv5EpJDLgiKigiKiGupqKsuKxaYmRMQsAx7dtZWIJL0jme9wemJ8bvpNO0cXR1f3Dnyy0M1hXgUGbMr8xV8tf/nw9p/qa6sqigozbyTY2DuNn/GUsanpY/Of3/bF2u8/fCcl/mpdVXnS5Qt8Pv/xRa/edczBEyYf2/NrfkbK6udiHFxcb8Zebn1r+MCh8edPXT19dOPqt2TNTfXVlbqbbB2cR0+ZfXzvtm9XLh0wdkLCpXN1VRWh/QaGRQ8mIhsHByI6e3Bvi1Q6auosWwfn//70bRyciEilaNn8wTuS8L4Dxjzs5OZRXlSwfN6UyGGjr58/XZSbNXb6nNB+A9u8u8jYpM+gYVdOHkn8+7xAJO4zeHibhz0088lN7y/77ct1uek3jU1NT/+xSyASf7r7GBH5h/UlosqSoh5evpbWtkTkF9r772OHiGjguIf/+xME0MG8CgxY1Kjxr3z8raOr+8UjB7NuJvaKGvTmlz9a2tgS0fgZTy5Y/qGltd25Q3uTLl/wlgS/u2l7z+Cwu47pLQmZ9+ZKsYlJRmJcemL8i+9/JjI2JiK1UklEwx6eNnzSdL6RUXpCrJuv35NvvEdEKqWKue/c15ZPW7iEx+Od/H1HY13NkIlTFn+wgblp7ONzTM0tK4oLD237QdvWbtMDcHR1Z05bP3dob9bNRIFQ+Pr6TRFDR1eVFh/cslkmk8544fUZL7zWzgiRw2+doR45bDSzWPpvQyY8NvulNx3dPC4dPXjmwJ4+g0e+u3GbjYMjEfkE9WIWGP3D+zIHB4T1uXUhHJtV0JF4zOoBAOtOVhYeLy+c7OrDdhBgx+q0a/uiJohwmju0BWuAAJ3t509WSpua2rxp/MwnvQOCOz0RANehqwA625Ovvcd2BAADg+k2AABwHboKAAC4Dl0FAABch64CAACuQ1cBAADXoasAAIDr0FUAAMB16CoAAOA6dBUAAHAdugoAALgOXQUAAFyH9wMElrW0tCQmJiYkJFysKfUYN4LtOMAaP3NrtiMAd6GrgAXl5eWJ/5OXlxcWFhYWFvbYsFH71TK2owE7qhXyGoUcHwgCd4Kugk6Snp6u6yciYvrpkUceCQgI0B1zKumclojHak5gRaVCZlVR99JLL0VERERERAQGBrKdCLgFn7UI+iKXyxNb8fLyCvsfJyenNu9yqaZ0W0H6Ex6STg8LbFKT9v3Ua4eiJly+fDk2NjY2NjY/P58prb59+7b+awa6LXQVdKSysjJdOeXn5zPNFB4eHhYWJhaL72WEuPrKjTnJY5087UXGYr6R/iMDm+pVimqFfEt+6r7oCSb8f5Z5pFJpbGxsXFxcbGxscXFxxP/07NmT1bzAGnQV/FdpaWm6fuLxeLrJ0wP/OZzeVLuzKDO+tsJaZFynVHR0Xq7SalUqlUAoZDtH5+lpblkiax5s5/qcT2g7hzU1NcX+T2VlJTPZ6tu3r6+vbyeGBZahq+C+yWSy1ot73t7ed13cezCNKmX32bvKzc1dtWrVTz/9xHaQzsQzF9zflnl9fT0z2YqLi6upqdGtE3p5eektJHACugruSWlpqa6cCgsLw1q5x8U9aF92dvayZct27tzJdhCDUVtby0y24uLimpqa+vbty/SWh4cH29Gg4+E8QLij1NRUXT8ZGRkxzfToo4/6+/uzHQ2AbGxsRo8ePXr0aCKqrq5memvr1q1yuZwprYiIiB49erAdEzoG5lXwD6lU2npxz9fXVzd5cnR0ZDtdF4d5VUepqKhgJltxcXEqlUrXWy4uLmxHgweHruruSkpKdOVUXFzcenFPJBKxna4bQVfpQ1lZme58Qj6frzsvo2M3VqEToKu6o5SUFF0/CYVCXTn5+fmxHa37QlfpW3FxMTPZio2NFYlEuvmWvb0929Hg7tBV3UJzc3PrxT0/Pz9dPzk4OLCdDghd1ckKCwt18y1zc3NmshUREWFra8t2NGgbuqrLKi4u1pVTaWlp68U9YXd6EY+hQFexJT8/nymt2NhYa2trprQiIiKsrKzYjgb/QFd1KTdv3tT1k1gs1pUTXu3PfegqLsjJydG9fsve3l7XWxYWFmxH6+7QVYZNt7iXkJCQmJgYEBCg6yeswhsWdBXXZGVl6eZbLi4uutcdm2dPx6IAACAASURBVJmZsR2tO0JXGZ7Wi3tlZWWtF/cE9/kuAMAd6Couy8jI0O1vubu7696f0NjYmO1o3QW6yjC0XtwzNjbWlRPeEq3LQFcZirS0NN37E/r6+urOg8cbuOgVuoqjmpqaWp+5J5FIdP1kZ2fHdjroeOgqQ3Tz5k3d/pa/v79unRArHB0OXcUhRUVFunIqLy/XlVN4eLiRET4do4tDVxm65ORk3fsThoSE6N6fkI8PO+4I6CqW3bhxQ9dPJiYmWNzrttBVXUliYqKut8LDw3XrhGznMmDoqs7W2NjYenEvMDAQi3uArurC4uPjmdKKj4/XTbZ69+7Ndi4Dg0XVzlBYWKgrp8rKSqaZFi5cGBYWhsU9gK6tT58+ffr0YS4zk61vvvkmKSlJN9kKCwtjO6MBQFfpS3Jysq6fzMzMmH6aOXOmj48P29EAgB3MyRdEpFarmZMyNmzYkJKSojspIyQkhO2MHIU1wA7T0NDQenEvODhYt7iHNxmDu8IaYLelVCp1m1tZWVm6NycMCgpiOxqHoKv+k4KCAl05VVdXt35ZLk7+gfuCrgIiamlp0b3oOC8vT/dm8AEBAWxHYxnWAO9bUlKSrp8sLCyYZpo9e7a3tzfb0QDAsInF4oEDBw4cOJD57FOmt1avXl1UVKTrre752T2YV91dfX1968W90NBQ3eTJxsaG7XTQRWBeBe1oamrSzbcqKip052V0nxe3YF7Vtvz8fF051dbWMs30wgsvhIWF8Xg8ttMBQPdibm4+bNiwYcOGMX89Mx8auXfv3pqaGt2bwXt5ebEdU4/QVf9oPXmysrJi+umJJ57o2v8CAMCwWFlZjRgxYsSIEURUW1vLTLZ27NjR1NSkOy/Dw8OD7ZgdDGuARETV1dUvvvhi67eNsLa2ZjsUdC85OTmbN2/+8MMP2Q4Chqq6uprprcuXL0+YMGHBggVsJ+pImFcREVVUVBgZGf3www9sB4HuS6vV5uTksJ0CDJidnd2YMWPGjBmTkJDw5ZdfdrGuwnnVAABdTdfbVkdXAQAA16GrAACA69BVAADAdegqAADgOnQVAABwHboKAAC4Dl0FAABch64CAACuQ1cBAADXoasAAIDr0FUAAMB16CoAAOA6dBUAAHAdugoAALgOXQUAAFzXrT8XeM2aNXv27OHz+a2/CRqNJiEhgdVc0I2sW7du+/btRkZGun+EPB5PrVbjHyHcr7fffvuvv/7Sfcnj8bRaraOj45EjR1jN1TG69bxq5syZnp6ePB6P/z9ENGDAALZzQTcyY8aMf/8jjI6OZjsXGJ6YmBgXFxfdPyQej8fj8fr06cN2ro7RrbvKx8enX79+ra+xtrZ+6qmn2EsE3Y6np+fAgQNbX2NlZTVnzhz2EoGhCgwMvK2ZXF1dZ82axV6ijtStu4r5q9bd3V33ZXBwcGRkJKuJoNuZMWOGm5ub7suAgADMq+DBzJo1y8XFhbms0WhCQ0NDQkLYDtUxuntX+fj4REREMJft7e3nzZvHdiLodjw8PHTlZGVlNXfuXLYTgaEKDAwMDQ1lLvfo0SMmJobtRB2mu3cVs2vl4eFBRBKJpHfv3mzHge5o1qxZzNRKIpFERUWxHQcM2Jw5c5ipVUhISFBQENtxOgy66tbUytLSEpsEwBZ3d/fo6GhLS0tMquA/kkgkYWFhdnZ2M2fOZDtLR7rLOevpTbU7ijJzmutrFS2dmKqzaTQalUolEonYDqJH3maWCo26j7XjM17BbGe5u13FWZeqS/g8XlZTPdtZOolWq1Equ/g/wts4iE2cxaZTevTsY+3Adpa7KJA1bitMT22olalVLRo123HuwoB+ofU0s1JpNf1tnWe7B7R/ZHtddbG69Kf81EH2Lk5iU1O+QA85ofPw+bxyubRGIf+zLG9X//FivhHbie5oSdI5L1NLJ2NTF2MzXvd9+V/XJ9eqS2TN12rLJ7v4jHHyYDvOHSXUV63PjB/l6GEnMrYUiLrza1I7HI/HK5U3V7bI4uoqfuo7qr0j7/R9P1yW91d5/l27DgyOXKP+MD326MBJbAdp2/MJZ0Kt7MOt7NkOAp1nT3FWpI3T425+bAdpw4Xq0l8L0570CGQ7SBeX3lR3prKonbpqe7+qUaU8VlGIouqSjPlGMR6Sz7O4+LYIf5Tm9DS3RlF1N1N79IyrqyiWNbMd5HZqrXZnUQaKqhMEmFv3sXbcUZx5pwPa7qrkhiqePmMBu1yNTS9Ul7Cdog2Xa8rsRMZspwAWGPF4Nxqr2U5xu5sN1Sqthu0U3YWj2ORSVemdbm27q4plUi9TC32mAjaZGQm9TC2rFXK2g9xOq9W6GJuynQJY4G5iUS6Tsp3idsXyZl8zK7ZTdBeuJmZ83h1nSW2fMSFVK2WcP9cF/otCWZOGOLdFnCttIMKUvjtSatT1xLkZTLNKKVPjN2En4RFlNdXd6Va8vgoAALgOXQUAAFyHrgIAAK5DVwEAANehqwAAgOvQVQAAwHXoKgAA4Dp0FQAAcB26CgAAuA5dBQAAXIeuAgAArkNXAQAA17HcVRUlhTHRkphoSVODoX5U+dlDe2OiJW/PefS/DNIFvg//1959xzdV7n8A/yZpkibde+9C0xY6aAuUvYeoWJlqEUQFRBS5yOWnUsTBuF4VBEXExZDLRlBQZIMKAm1pC4WW7tK9053R5PfHqaGW0gEp57T9vF/8cZqcPPmSnJxPnuc8Oadr2fL+8shw2Y5PV+urQRbfwbiL5yPDZQsnDdZvs4sjRkWGy66eO6HfZqFF2AO0ieWsEgrFsuAwWXCYQGBARCUFuZHhsl93b2O3qkev2esAnc3BzVMWHGbv0njd9Au/HI4Ml2UkJz5wgx16B7m5nTd7EXr1CZQFh5mYW7BdV49w7/azdNq49ctfZbuuB6f3+lneM1rY2K7YvFP35+VTv7JaDmuavQ7Q2SbPnj959nzdn1dOP+yG16F3kJvbebMXYdEH69mrpcdptv2k3bxemJPt7NmL1aIeXGfUr5+smj9+QE2lfP2h0zYOTlUV5a9MDCeiyS+8Mm3eYiL6POpff536ZV7Uuvramh2ffNBv6Kh+Q0cd2LpxdMTMwROe+NeUsUS05bfL/1k8NyMpkYh2bVy3a+O6r0/FSIyMMpJuHPx6U0ZyooBv0G/YyCkvvd6eL3pnjuw9/9PB3MxUsUTaq0/w5NnzPXz76O46fXBPYW6WwEBo5+T69EuLggYNJ6Kc9JT/e+4JidT4433H92/9LOb301Ij4/HTZ42bNot5YHlx0c4NazKSbpSXFDu5efYfPf7xyJcFAkGzp44MlxHRB9sOevj4E9HuLz4+9sM3A8dMZD75GbduHPvftykJcTXVld7+gX0HDhk/bZaBUFiUd0f3Ohibmj1MnT3HnbTbb0U+OfSxiPlRa4lo28fvnzr4P75A8N25eAMDg7zM9H8/85itk8unB04unBheWVH+r482nz96KP7S+W0Xrm95f/kfvx4ZN23WtPlvvDwmhGkwas4UD5n/B98f1Gg0h7/fHH3+VGFOlpd/0KjJ0weOmdh6Mc3ewY3vvHHlzPFnX19ubml98tDunNTb3n0CX16x2tLGPuqFKc2286/XvHPlzPGZi5aVFxWe+3n/sk+3yoLCSovyd3yyOjMpUV5eYmlrLwsKm7XkLYlR4xVQd3/+34u/HS0vKfT2Dxz62D/Gn1vfAjUaza7P1t248mdRfq6FtU2f/oOenrtILJHe+yIsjhhVWpC3eO3GsBHjiKiVelr5z3bOO89dzIv2kHvCI9u+ZPrcMRdOR4bLlvzni5Bhox9gT8i8L822q/u1c+rg7m0fv+cfOqj/yHHH920vzs9zcvOcvWxl777BTGu3E2L3bP6kICu9vr7O1sE5fPzjk2cvYO5q9vkaO/W5pvXvvHiLd/8rKLaffsYAfYP7E1FG0g0iSoq7SkR8Pj/1ejxzb+btm0TkHzrAQGjA7G2/WbPCwsbG2t6haSMjnpxu4+hMRH0HDH5q7kIDkTA/K+PDhbPiLp7vN3SUd9+g04f2vPvS9Nqa6taLuZ0Q+926d9NuJoQOH2tpYxd9/uSaRXPKiguI6Oiub79b9+6dtOTQ4WN7B/RLv3X946Xz4y6eJyIDkZiIFPW1G/5vUYNKZWPvVJiTvePT1XfSbjPNfr3m7StnjgsMDAaNezz/Tsb+LRv2bv6kQ69SaVH+e/OfuXLmNy//gAnTZ+Xfydy96aNj//v23jUfps6ew9zKRrd1Me87n8/XNDSkJcbptkZZcH8iMhCKiWjXpv/cuna1d2BI00YMRMKn5i5klkdNnj7iyelEtO2jVYe++VylVIydGlmUm/151JJTB3d3qDahSEhEV8+d/PH7zS5evRo06utX/tz12boWt3MDoZCIzh7Zd/rH3W69/aRGJtWV8vdeeibmwimxRDJq8gyVQnnh2KGPlsxjGj91cPexXd+WlxQOHDPRQCTa88XH7S/s6M5vftu3o7y0ZMjEJ1UK5Zkf965f/irfQHDvi9BU6/W08p/taXwCQx5+T9gnbLBfyAAicnDzeGruQgdXjwfbE967XbXSDlNSdlrSmSP7QoePsbS2zUq59emyVxR1tcyH68NXIm/Hxzh7y8LHPl6Yl7N/y4a9X37y9xP94/PVrH69BJXesop5h9JvMe9QNI/HGzT+8eSEGI1GU1MlL8jOtHF0trJ14PMFRFSUe2de1LoPvjvY7Pvg6IgZNo4uRBQwYOjUl18XCkXnjh5U1NWFDBvz4vL3X1+9YeLMOUW5d34/dqj1Yg58vYmIJkW+tGDlfz74/qCHr39dbfXpH/fW19Ye/u4LIpq7/L0FK/+z9L9fjp3yHBEd/GYTEfH5POZbZ7+ho+evXBf11S5rByciunHlIrPZJfz1h1AoWrV1z7x3VjNfUX/bt0Op6MBl4NMSE9QqlX9Y+OtrPps6/40Vm3c+v3QFE/NNPUydPYqJuYWhVJqTdltZX1dXU5OdkjR4/JNElBwXq9saZUEhRMQT8IlIKBJvPHz27U3/OEokFIqmvvw6n88nopERM0ZHzKirqTp/7BARvfr+JzMXLn3/u/0GQuH+rRs0mg5ctZbH4xNRYXbGe9/un/vv9yIXv0VECX/92eJ2zjx7aWHB2p0/rdyyy7WX7MT+nWXFBXYubqt3Hn5+6YoPvj8gMDBIuX4t9o+zTJ+biMZNm7Xog/UrNu/07hPYzqrqa2t/3rGViBau+ujF5e+v3vGjgUicdjMhNeFasxeh2QNbr6eV/2xP490n6OH3hIHhQ337DSAiRzfPqS+/7uju+WB7wnu3q9ba4fGIqKq87PUP1894Zemqb/caCIXV8oro86eI6MDWzzQazaDxT7y96fuX3/5wwcp1RPTr7m01VfJ7P1/N6tfXa6ufrJIFhereodvxMW69/fqEDVYrFZlJNxt3GYGhupUlUuPB459oT7O346KJyNOvcfjO0y+AiJKuRbfyEJVKmXTtChH17T+IueWD7w7+cClp2rzFSXFX62triWjgmEnMXWEjxzHjck3n3gwaP4nZhbl6+xBRZUUZEd24eomIPPz6GpuZE1G/ISN/uJS0/fcbIrFh+18lBxd3Ikq8eunzqCUHvt4oLysZNzWyd0C/Zqs9TJ09jW+/AVqtNj3pxs2Yv4goZPgYOxe329djiSg96QYR9e0/RLfykImTDaXSNtu8nRDXoFYbCIXuvf2IyNTc0t7ZraZSnpue0tHygoaMkhoZE5GHzJ+I6mqq1Gr1/VbuExbu4ObBLMdfvEBEIUNGCYUiIjK3tmG2k8Srl5SK+uyUJCIKHjKCWXn4E1PbWU/6zYS62moi6hM2iIhMLSy3nY//4VKSf1h46w9spZ4H+892V70DgjmyJ9Rpul212Y69q7udixuz2Xv5BTCfo/ra2luxV4ho4OjGkfCQYWP4AoFapboVe1n32HZ+vh6Yfo5Xufn4GUqlGbdu1NXUZCbfHDdtliw4lIhuX49R1NcTkU/w3XfI0t7h3sM8LWL2v/u3bNi/ZYPuxqK8nNYeUl6qaWggIt3IfpPWyolILJHoXlBTSytmobpSzvRXiEgsMWIWRGIx04NhDlYREfNRfGAu3j6Rb7x1YOvGv079SkSHv9vs4u0z5aXXQoeP0VedPY1PUOi1P85m3Epkxnj9Q8N9AkKunP1No9FkJt+0dnCysLHVrWxt79SeNivLy4hIrVIxB350CnOzXbx9OlSeoaTxHRQZSpgFTYOaDFr+0FnbO+qWq+TlRGT29/vO5AoRVVeWM99jiEjy99ZoZGraznpKiwqZTYsZHWq/VurR3dKh/2x35eot48ieUKfpdtVmO5ImuzhjMwsiqqupqamq0Gq1RGRmZcPcZWBgYGRqVlVeVlVR0eSJ2vX5emD62ZL4fL4sKCzu4vnzRw9otVqfoBBreydza9uU69ca1A1ExAxfMtr59hCR1MSUiEZFzBg46u6RbZFE0tpDjBo/t7XVlc3uMjO3JCJlfb1SUc/0h6rljZ80UwtL3XKLjIxNiKimuqqdlatVKmahruYfD5kwY/aQiU+l3Yi/nRBz4djhO6nJP363uVlWPUydPQ0zgpp263phTpazV2+JkVHvgOALxw5dOXNcpaj3DQprurJA0K5RBGbXLxQbLvt4S9PbnTy89V3+PzDjQgxTc8ui3Ds1VXe34Sq5nIjMLKx132Bq5I2d7Gp5xT2NtbwFGpmYEJGirk6tUnUorlqppyP/xe6PO3vCJiXdfZY226lrcgyMGd8zNjUzNjXn8XharVa3U1Wr1fXV1URkZnl3A2jn5+uB6a11Zhjwt307iUgWHEZEvv36J8fHZiQnWtk62Dq6tKcRHvGISKlsPAjk7RdIRBUlxX6hA/1CBxqIRPnZma1/xiRGRkyH9/pffzC3rF++KDJctvfLT3yCQiVGJlqt9sqZ35i7Lp08xtTZZofJ3cePGUJhvphkpyRFhsvmDA9k3s5/FCA11h1cVdTV3rh89zBSyo24nRvWpN6ICxw0bNqCJR9uP0RERTnZzVp4mDp7GncfP0OpNCU+Niv5JpNMsqAwIvpt/w/NvsO2gccjIpVCQURevn2JSKWol5iY+IUO9A0ZkJWSVFFWIjEy0lfZzbbzewUOGs5Mo2Iip7QwLznuKhEFDR4hEhsy3buEy38QkVar/eufM+Bb2QLdZX7MQsJfvxORSqlY9MSwyHBZ/KXfm70I7a9HX69Jt6GfPSGPx3xhZf58gD1hi9pspyA7k/mBnbysNO1GPDNOKJZIfUMGEtHl08eZ1aLPnVCplIZSqW+/sBafqFn9eqG3Hjozvao4L8fJ3cvU3JKIevUNvnTiKBENntCuMVkisrCxIaLzPx9U1NaOmfrsuGmRpw/vif39zIcLZ/XqG3Tm8L6aSvkb6zYyBxLuZ8pLr38eteSX3d/Ly0uKcu6k3IizsLabOPMFQ6n06Zde3fXZum/WrrgZe6WipDDhrz/4fP6MhUvbLCxs5DiPPdsykhJXvTTDPzT84omfiShi7kIjE7NmawYNGXHpxNHdn39cVlR448pFMyvrorw7zF1lRQW/7d1x+fSv4WMniSXSmzGXicgvdECzFh6mzp5GIBD06hN8/cqfROQTFMIMuBubmackxOomAbaHhY1daUHega83yYJCnn5x0dDHIn7/5ce1r80dMv6JovycuD/PufXyDR87SV9lN9vO711h/Iznz/20PzczLWruVFlg6OWzxxvU6n5DRzHfyodNitj12brf9u2okpeXFuQV5GQ1fWwrW6Cljf3YKc+dPLjry/eWDxo/Ke7ihYqSor79BweGD733RWh/PdCUXvaE5tY2RJR07eq+Lz/tO2DIg+0J79VmO8Zm5msWzh72+NPxf11QqZTm1rYhQ0cS0fQFb3yw4Oq5n/aXlxSamlleOnWM2dPee6jl3vqnLVjCoTnrTPwy06l7BzXOCfYJbJw14BPU3q+342c8LzU2Lcq9c3TXt1qNxsHNY+l/t/QK6Jd07erPO762d3Z7bfWG0OHjWm9k4JiJ//rvl7aOLn8e/zk1MT5g4JD/2/QdM7w+ceac+VFrTc2tLhw9mPDXHx4y/5Vbd3v7tz2NSigSv/X59iETnywtzD97ZJ/UyHTmwqVNf0yq8+yif/uFDlQp6hOjL42f/nz4uMeJSKVUEdGAUROeXxrF5wl+3b3t8Heb5aXF0xf8a/7K/9zbyAPX2QPpti7/0MYJAj6BoURkZmVt7+zazkYi5i4kopvRl/46+QsRzV4aNWbKs0R04sAPydeix0x5dtmnW/U19fbe7fzeFaRGxqu+3hsybHRuRurJg7tU9YqJz8x5bXXjMYbx058fOXk6j8e7+NvPPD7/hWWriKjh74kMrWyBRDT7zahpC97g8XinD+2pqigb9viU19dsbPFFaH890JRe9oQDRz/m7NVbpVL+tGOrvKzkwfaE92qzHSs7x5mvvnny4K6C7ExbR5c3P9kiFImJyNs/MGrLLg+Zf/zFC7//etjMwvrld1ZPfGZOe+rvaJH3w2MOmjWzPTspv75mhHXnHisDFq1Pjfs8aLiNqF1D3o/M9Cu/vuzex8Sgw4Mb0NVdLivg83mveXLrC9mB3NRbVeVjbds1cNelnfv5wDdrVrj2kq3ZcZitGtRazbrkmKODWu59dslZOts+fq+2uuXfwU18Zg7zc30A/SorLmzlV7cLV/330ZYDQCcO/JB6I77FuwIHDh084clHXlEn6pJZNefNd9kuAXocSxs7BBJwyripkeOmRrJdxSPSJbMKAAD0aMQTU0e0+0flrMC1FgEAgOuQVQAAwHXIKgAA4DpkFQAAcB2yCgAAuA5ZBQAAXIesAgAArkNWAQAA1yGrAACA61o+b4UhT2DY7uuAQVfkKNHb1Zj0yElizNfb2cyhKxHyBYZ8zu1zRHzsCR8dHvHc7nORkfv2q6wNDQvqazuzKmCTSqtJrarg2knWmSsHFiv0eX026CoKFbVWYkO2q2jOTizJrathu4qeolhZp2xo4RI5jJazytPIHN9uu7FiRV1/Szu2q2hBoJl1haqF69JCt6cl8jBqfuVS1nkYmxro77pl0LpSZX2Ihc397m05qzykJi4Sk7PFOZ1ZGLDmUF7abDdftqtowQtufr8UZCpbuvwgdGOXygukfIMAUyu2C2nOViQNMLM+XpjVjnXhYe3LSZnv0fd+97Z8rUXG+tQ4uVox3MpJIsDp2LuJMpVi953kFT79fUzM2a6lZVVq1ZyYkzOce7tKjNmuBTpdvabhUllBg1azwieM7Vru6+vMm+k1FeNsXbEn7CR5itodWbe2hYw1F4rut05rWUVE+3JTjuRnaLQaqaA7X6pVo9EoVUpD7g2X65GdWJIgLw2xsJ3lKvPm3mBLU/Wahs9S484U5/QztylS1LFdziOi1WoVCoWhYXfeCJvRaDUVKuVTjp6zXbnYy2/q54KMn/IzKpQKa7FEqWlgu5w2dKEdmp1YGltRNMzaaaFnX3OhuJU128gqItIQlSnry5Td+Yh3RkbG999///7777NdSCfiEblKTcTcm2rVivSaSrW2p4wH5uTkfPHFF2vXrmW7kEfHVCiyF0vZrqK9NEQVyvoSVT21sctkX3Jy8r59+6KiotgupG18Hs/TyKw9v51qu0vLJ7IWGVqLukBEP7AGA0NhcXlvY44Oi/VYnkambJfw6AiEpQZFZdgIOYtPZCkytOwKe8JagVhcKu9m2xJ+CwwAAFyHrAIAAK5DVgEAANchqwAAgOuQVQAAwHXIKgAA4DpkFQAAcB2yCgAAuA5ZBQAAXIesAgAArkNWAQAA1yGrAACA65BVAADAdcgqAADgOmQVAABwHbIKAAC4DlkFAABch6wCAACuQ1YBAADXIasAAIDrkFUAAMB1yCoAAOA6A7YL4AQTExM+n79p06bAwMDAwEAzMzO2K4Ieh8fjubq6sl0FdG2FhYUxMTHnzp1zcnJiuxY942m1WrZr4ISMjIwLFy7Ex8fHx8dbWFgE/s3NzY3t0qBHSEtLe/vtt/fu3ct2IdDFlJSUREdHR0dHx8TEKJXK0NDQkJCQ8ePHi8VitkvTJ/SrGnl4eHh4eDDLmZmZ8fHxcXFx27dvl8vlgU2wXSYAAJWVlTHhFBMTU11dHRoaGhoaOnv2bBcXF7ZL6yzoV7WhoqIi/m8JCQlNcwtDhaBH6FdB6+RyOZNP0dHRFRUVTD6FhIT0kLEfZFXHxDdhbm6OoULQF2QV3KuqqkqXTyUlJSEhISEhIaGhoZ6enmyX9qhhDLBjmo4EZmVlMaG1Y8eOiooKDBUCwMOrqalhBveio6Pz8vKYztNTTz3l7e3NdmlsQr9KP+RyedMuV0BAgC63zM3N2a4OugD0q3qy+vp6Jpyio6Ozs7OZfAoJCfHx8WG7NK5AVnWKprllZmamyy13d3e2SwOOQlb1NEqlksmnmJiY1NRUZnAvNDTU19eX7dK4CGOAnaLFocKdO3eWl5c3HSrk8XhsVwoAj05DQ4Ou/5SUlMTk07Jly/z9/dkujevQr3qkmg0V9u3bV5dbFhYWbFcHbEK/qrvSarW6fLpx44ZufkRAQADbpXUlyCo2JSQk6HLLxMQkKCgIQ4U9FrKqm4mNjWXG92JjY3Xje0FBQWzX1VUhq7giOzs7Li6OyS0MFfZAyKpuID4+XncKiaCgIN0UCbbr6g6QVVyEocIeCFnVRV2/fl2XT3369GG6UCEhIXw+zgyuT8iqLqDZUKEut3QnhYJuAFnVhSQmJuqm8PXu3Vt3CgkDA8xW6yzIqi4mOztbl1ulpaVNhwrxPa5LQ1ZxXFJSkq7/5OnpqRvf62aniOUsZFUXVllZ2XSo0N/fX5dblpaWbFcHHYOs4qDbt2/rThHr7Oys6z9JJBK2S+txkFXdx/Xr13W5ZWRkpMutHnjqsK4IWcURaWlpulPw2dvbM+EUGhpqZGTE+L4rNQAAFhFJREFUdmk9GrKqe7pz544ut4qLi5sOFQoEArargxYgq1iUkZGhyydra2vdT6BMTU3ZLg0aIau6v6qqqqZDhb6+vrrcsrKyYru6nm7ZsmUnT57k8/k83j8+jLGxsazW1f1lZWXpThFramqq6z/hBJ7chKzqcW7cuKHLLYlEosstLy8vtkvriZKSkpYuXVpYWKi7paGhwcfHZ8+ePazW1T3l5OToTiEhlUp18yPwpY37kFU9Wk5Oji63ioqKMFTIirfffvvEiRO6P0UiUVRU1MSJE1ktqvvIy8vTzS83MDDQnULCxsaG7dKgA5BV0AhDhWxJSkp68803CwoKmD979eq1e/dutovq2goLC3X9JyLS5ZOdnR3bpcEDQlZByzBU+CitWLHi+PHjRCQWi6OioiZMmMB2RV1PSUlJ9N/UarVufoSjoyPbpYEeIKugbU2HCgsLC3W5FRQUhKFCvdB1rfz9/bdv3852OV1GWVmZbv5eTU1N6N+cnZ3ZLg30DFkFHVNdXd10qFAmk2GoUC/eeeedc+fOvfvuu+PGjWO7Fk6Ty+W6/pNcLtf9PtfNzY3t0qATIavgoSQmJupyy9DQkJWhwuiKoj9K8irVyry6mkf2pHqnUCqKiopdunSHgMczF4p8jC1mOveSCPR5Zryqqird+Y1KSkp0x59wSsyeA1kFepObm6vLrYKCgqazCjvvnJ7bs26l1FTYiaVOEiMe4eIpLKtWq0pU9WeLcj7tO8Tb+KF+qFRTU6ObH5Gfn68b38MR054JWQWdoqamhgkt5qJcPj4+utyytrbW17N8k5mYU1c9wQ6DP5yzPfvWq54BfUz/MSz86aef/vTTT+fOnbvfo+rr63X9p+zsbN3vc3v37v1IqgbuQlbBo9B0qFAsFutyy9vb+4HbjK4oOpKfPskO11DmIoWmYW9OyudBw/l/d3aXL19+4cIFpVIZExPTdE2lUqk7P2xqaqru+JOvry9LtQMXIavgUWs6VJifn990VmErQ4WjRo2aPn36ggULdLdsSI0T8HhhFvjFDEftyE6a79En0My6oaHhlVdeuXbtmlar1Wg0sbGxDQ0Nuv5TUlKSLp/8/f3Zrho4ClkFbNINFTKjhfcbKnzyySfz8vLEYvGIESNWr17N3PjerSt9zSwdDY3ZKx9a80dpvo+xeYCSv3z58tu3b+uuryYUCrVare76uQEBAWxXCl0Asgo45H5DhXPnzq2trSUiHo/n5+e3fv16S0vL+dfOTrBzdTDElRo46nRxjrC8Mmb9V2lpaU1vFwgEly9fZq8u6JKQVcBRTYcKU1JSeLzGwx4ajcbV1XXlypVf8eTIKi47XZxzdu+BssMnmC8ZTd9BnEUeOgpZBV1ASEiIbk/HsLOzM1n28ozefZFVnHW6OEdSVeuaWZSQkJCcnFxVVVVfXy+Xy7VaLbIKOqqzfvUCoC8RERFMUGk0Gq1WKxQKLS0tBQIBMyoIXGZnZzc1dNjUqVOJKD8/Pz09PTk5+dq1azNnzsRFT6BDkFXAdaWlpRKJxNLSUiqVenl5MbOZZTLZ/Gtn2S4NOsDBwcHBwWHw4MFsFwJdErIKuO7ChQu//PKLTCbz9PRkuxYAYAeyCrqAxx57jO0SAIBNfLYLAAAAaAOyCgAAuA5ZBQAAXIesAgAArkNWAQAA1yGrAACA65BV0IOUFResX/7qi6OCXxrdT15WujhiVGS47Oq5Ew/T5htPj44Ml0WfP0FE534+EBkue/v5p/RXMgAQsgp6lv9t/CjmwmlzK5sJM+eIxKJefQJlwWEm5hb6at/cykYWHObu090uwnThl8OR4bKM5ES2C4GeC78Fhh6kqqKCiEY+Oe3xWS8T0aIP1uu3/aBBw4MGDddvm1xw5fSvbJcAPR2yCnqKF4YFqFRKItqz+ZM9mz/57PCZ9xdElhbkLV67MWzEuBMHdu345IN+Q0dNnvPKkW1f3oq9au/i+uxry3379WcefmL/zt9/PZKbkWpmad13wOCpLy82tbBs9hTnfj7wzZoVrr1ka3YcLsjOfHPGhGYrrN15xMXbR6PRHP5+c/T5U4U5WV7+QaMmTx84ZmLrxeekp/zfc09IpMbLPv1q54Y1EmPTtzd930o7CycNriwrfW31hgvHDqUlJmg02oGjxs9e9q5AICAilUq5d/MncRfPl+bnGZmZu3j1mvXGO47unkSkex36DR11YOvGYZOe/mn7FqbNqDlTJr/wyrR5i/X0hgB0AMYAoad4cs4CG0dnIuoTNuipuQulxiZN7xUKhURUlJO9acUbFja2UiPjjKTEze8uU6tURHTup/07Pl1dUpD32Mw5EiPjMz/u/XbdytafTmgolgWHMf+c3L2YGw2EIiLa9tGqQ998rlIqxk6NLMrN/jxqyamDu9toTSgiIoWibvN7/66trnL17t16O8z6O9evsXFw7jtgSG2V/MyRfcd++Ja597O3Xj++Z3tNZcXwJ6eYW9lcv/znqpdnlhcXEZGB0ICJxm/WrLCwsbFxcnpq7kLmUaMmT/fvN+BBX36Ah4J+FfQUEXMX3rp2tTgvp2//QZMiX2p2L4/PJ6KcjNRV3+z19g/Mn56xbObE8pLCO+m3PXz8025dlwWHDZsUMWzS0/5hg1a/+vy1P8+q1WoDg/t+gqxsHVZs3klEarX6/XnPENHIydMd3DzqaqrOHztERK++/4l7b7/Hnn3h9SeH79+6YVTEDN1V3u/FE/CJSNPQ4Bvcf37UWiJqox0ej4j8QwfMeXMlEVlY2fyy+/sLvxx6cva8W7FX4v48R0TvfrXH3tW9oaEh6oUp2SlJv+7Z9uxr/+bzBURUlHtnXtS6YY81ThL5adsWjUYzMmKGR7c7FAddBfpVAHfZOrl4+wcSkYObh0RqTERVFeVE9OLy91ds3jls0tNEZGFrz8RGbVVle9rc/9WG9FvXXXvJZi15m4huJ8Q1qNUGQqF7bz8iMjW3tHd2q6mU56antKe1sVOeYxba007AgKGNC+FDiaggO7Oupiru0gUi8vD1t3d1Z64oHzJsNBElXr2ke6BEajx4/BMdfPEAOhH6VQB3GUrvXmVYaCiuq63WNDQQ0cUTR/dtWV+Sn9vRBm9cvXjsh28MpUZvrNskEhsSUWV5GRGpVarIcFnTNQtzs128fdps0NrRkVloTzuSv8c5jc3MmYXa6iomfc0srHUPMTW3IqJqeYXuFkt7B+bIFgBHIKsA2lBamPflqmVarXb00zMHjJxQWpT/1QdvteeBleVlX6z8FxEtWLnW1tGFudHI1JSIhGLDZR9vabqyk4d3e9pkxuja2U5ddRWzUFvZ2AU0MjFnpoTU/H0XEVVXVhCRqdXdqSIIKuAajAECtCE/O1Or1RLRc68t9wsdqKxXMLdrNZpWHqXVaj9fubSqomL8jOdDh4/T3e7l25eIVIp6iYmJX+hA35ABWSlJFWUlEiOjVlq7V3vauXL2BFN57J9nicjJs5ehVBo0aBgRpSXGF+flMIfTrpz9jYiCBo2475PxeESkUig6VCGAHqFfBdAGR3cvPp+v0Wi2rn7bzsk15o+zTp69ctNTDny98YnnX77fo84fPXgz+hIRpd+68eHCWcyN/UeOGzdt1tDHIn7/5ce1r80dMv6JovycuD/PufXyDR87qUNVmVlZt9lOcnz0h69EOrl7nTmyj4hGR8wgIllQWMiw0TEXTq+aN3PAyAmJsZdz01OsbB0mPvPC/Z7LwsautCDvwNebxk19pmnuAjwy6FcBtMHSxm7KvNfdevnGX/y9MDd7ydpN0+YttrJ3/OvUrzX3n14hLytlFlISYpOuXWX+FdzJJqLZS6PGTHmWiE4c+CH5WvSYKc8u+3Qrj8fraGFttjNt/hKVUnnmyD6+QDB+xvO6eRmLPlw/8Zk5irq6Ewd+yM9KDxk2etU3e6VGxvd7ooi5C4noZvSl1MTrHS0SQC94zBABQJcz/9rZCXauDoYdGzrrIRZHjNL9zJmtGk4X57hLTZ5zaXvCCECbMAYIwAllxYV7vvj4fvcuXPXfR1sOALcgqwA4wdLGDoEEcD/IKoBu6LMfz7BdAoA+YW4FAABwHbIKAAC4DlkFAABch6wCAACuQ1YBAADXIasAAIDrkFUAAMB1yCoAAOA6ZBUAAHAdsgq6KlOhSMDDBsxdQj5PiDcI9ARbEnRVhnxBmbKe7SrgvkoV9dZiCdtVQDeBrIKuyt/UqlKtZLsKuC+1VutuZMp2FdBNIKugq5rp3OtU0R1lqxeSB7ZcLS+yF0s9pcgq0A9kFXRh3/Yb/V3WzWKMBHLMX2UFhYqaZb37sV0IdB+4LjB0baXK+o9TYjNrKv1MrWq68pCgVqtVKpVisZjtQh4cj8erVCnlKmWYpd1ir0C2y4FuBVkF3UGhojaztqpWrWK7kAdXWFi4Z8+exYsXs13Ig+PxeJZCsbuRqamBiO1aoLvBtRahO7ATS+3EUrareChplYofU7JH2jizXQgAF+F4FQAAcB2yCgAAuA5ZBQAAXIesAgAArkNWAQAA1yGrAACA65BVAADAdcgqAADgOmQVAABwHbIKAAC4DlkFAABch6wCAACuQ1YBAADXIasAAIDrkFUAAMB1yCoAAOA6ZBUAAHAdsgoAALgOWQUAAFyHrAIAAK5DVgEAANchqwAAgOuQVQCcwOPxLC0t2a4CgKOQVQCcoNVqy8rK2K4CgKOQVQAAwHXIKgAA4DpkFQAAcB2yCgAAuA5ZBQAAXIesAgAArkNWAQAA1yGrAACA65BVAADAdcgqAADgOmQVAABwHbIKAAC4DlkFAABch6wCAACuQ1YBAADXIasAAIDreFqtlu0aAHquF198MSYmRiAQEJFGo+HxeDweT6vVxsbGsl0aAIegXwXApkWLFtnb2zMRJRAI+Hw+j8fz8vJiuy4AbkFWAbApODhYJpM1vcXQ0HDmzJnsVQTARcgqAJbNmjXL2tpa96ezs/OUKVNYrQiAc5BVACwLCQnx8/NjlsVi8YwZM9iuCIBzkFUA7IuMjGS6Vu7u7hEREWyXA8A5yCoA9vXr18/f318ikUyfPp3tWgC4yIDtAgC6mGJFnY1YcrGs4FJpfr1GXa5S9DGxCja3uV5ZEicveeDluunjLRzN3UcM3pF9K6GyNMDUKtDMJl5e/MDL1ytLS5X1Q6wcQ8xtqhtUxgIh268cwIPD76sA2iuhsvTL9ITahob8+hrdjVotEY94elkm4vFI94l8yGUdQ77AwdDIUiRe4NHXTWqi95cF4BFAVgG0oVyl2JGVJBIIThRm1TSo2S7nwTkYSp9y8CKiCEdPtmsB6BhkFUBrqhtU86+dqVKp6zVdOKWashSJTQxEW4NH8dqxMgBHIKsA7ut08Z29OSmZtVVsF6J/Y21dJjt49jY2Z7sQgHZBVgG07NPUa+dL8+rU3aQ7dS97sXRJr+BgM+t2rAvAMsxZB2jBiaLsU8U53TioiKhAUfvR7Zgb8lK2CwFoG7IKoLnsuurvs26pNRq2C+l0pcr6IwXpbFcB0DaMAQL8Q2JV2drk6CJFHduFPDpuUpOvg0exXQVAa9CvAviHE4XZPSqoiKhcqdifm8p2FQCtQVYB/ENPCyoiqlQrJQKcwgY4DVkFcNfHKbExFUVsV9GaO4d+OT1qulbfx9K2Zd1Kr5Hrt00APUJWATTKqK28XlnGdhVtqErJMOntwePr+ZNbqVZ+np6g3zYB9AhZBdBIq9XWqlVsV9GGqtvpJl7undGyokFTqqzvjJYBHh4GqQEaaYnkamXntV9fUpb5w6GSv2I0SqW5v6z3ohcM7ayJqDIp9fJLb4ZtWZex40DJxWhjbw/nyeNcIiYSkbahIe27PUUXLqvkldaDQr1efKYqLcvxibGdUV6dRm0lMuyMlgEeHvpVAI2+yrjReY1XJqVefmGJori038crg/+zojavIGHVx8xhp+rMHCLK2n3E/ZmnRhzfZdU/KHnDN+raOiJK2bLzzoFjHrOmhO/caOTqFLtklVat7qR+VU5d9a2q8s5oGeDhIasAGuXW1bRjrQd0679fGnu69V31L6mzg0kvD79lr8ivJ1WnZxFR7Z1cvkjU+9U5FsF9hMZGliEB2oYGZVmFqrI6+8BR9+eedhg3XGRm6v5shKGdDREZe7l1UpHHCjI7qWWAh4SsAiAiUmu1/qaWndR4VWpmZXKac8QEvrDxgocia0siUpSUE1F1WpZ1eIjE0Y65S1FUQkRia8uSy7Faldp+zFBdOyJLcyN3FwOppJPqNDbA9RiBo3C8CoCIyIDHc5YYd1LjVbfTiShhxUfNbheaGDNJ5vT4mLsrp2VJXRwFhuLKWymG9ra6DCOiurwCk14enVQkc/L1zmsc4GEgqwAanSq600ktq+vqBBLDoI9WNLvd2NtNVVVTX1Bk7Hl3WK8mI5sJJEVxmaGt1d1GauvkibetB/brpCKJKLVG7mVk1nntAzwwZBVAI0OBoJNaFluYa5QqM7/eArGIuaX2Tp6hgy3fwKAiIYmITLzvTpeoTE5zfzaCiIjP0yjvzqHPP35Oq1abeHdWv4pHPMxZB87C8SqARkt6BXdSy1YD+xnaWad/t4dJqazdh6+++railDlYlWlgJNUN9NUVFKmra5jZExbBfSqTUvNPXii7duPOj7/mHTtFRMbenTIJkIhsDSWDLO07qXGAh4R+FUAjX2MLZ4lxTl213ls2kEoC17yVvPHbk0Oe4otE5gG+AR/+W2JnwxydMpV569asTssiImYM0GnS6JrMO4mrN2rVapuhA+xGDa7NKxRbd9YEkOFWTu5S005qHOAh4ZogAI1KlPWL488X99RxsKmO3vM8/NmuAqBl6FcBNLIWGdqIpa1nVfr2/fUFzU9uq6qu4fH5LU4l54uEsiXz9Fhk0R9XSv682uJdynK5yKLlmRG2IwZZD2hthNOAx3eXmuipRgD9Q78K4C4Nad9KvHStopjtQh61x+zc3vAOYrsKgPvC3AqAu/jEi3DwZLuKR81OLHnNK5DtKgBag6wC+AcvYzMHQyO2q3h0+DyerVgq4PHYLgSgNcgqgH+wEUlWysJ6zsEbd6nJGv9wtqsAaAOOVwG0oEKpmBd3tkKlYLuQzjXW1mVZr048EQaAvqBfBdACc5F4jquvpUjMdiGdyEZs+IYX5lNA14A56wAte8zeTalpuCYvvlRWwHYtesYn6mduu8Czj5CPb6vQNWAMEKA1WqIv0hNOF9+pUavZrkU/7MSSD3wHuhmZYjYFdCHIKoC27ctNTa4qT6wqK+vKZ7XoZWyu1mgWeQX0NbVqx+oAHIKsAuiAT1KuXa0oNBeKFQ0Npar6uoYGImI6KFqtlni8di7//bnj8Xidskyk1RJZiiRmBqISRR2fx/sscJijoRH6UtBFIasAOkZNVN+gMhYIL5TmlSjqR9g4WQrFR/MzytXKSfbu7Vz+KT9DrlY+Ye9u3jnLR/LTecQba+siERjk1FV33mUkAR4NZBUAAHAdZgEBAADXIasAAIDrkFUAAMB1yCoAAOA6ZBUAAHAdsgoAALju/wExHb1TPRV+EwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.constants import START, END\n",
    "from langchain_teddynote.graphs import visualize_graph\n",
    "\n",
    "# ê·¸ë˜í”„ ìƒì„±\n",
    "builder = StateGraph(ResearchGraphState)\n",
    "\n",
    "# ë…¸ë“œ ì •ì˜\n",
    "builder.add_node(\"create_analysts\", create_analysts)\n",
    "builder.add_node(\"human_feedback\", human_feedback)\n",
    "builder.add_node(\"conduct_interview\", interview_builder.compile())\n",
    "builder.add_node(\"write_report\", write_report)\n",
    "builder.add_node(\"write_introduction\", write_introduction)\n",
    "builder.add_node(\"write_conclusion\", write_conclusion)\n",
    "builder.add_node(\"finalize_report\", finalize_report)\n",
    "\n",
    "# ì—£ì§€ ì •ì˜\n",
    "builder.add_edge(START, \"create_analysts\")\n",
    "builder.add_edge(\"create_analysts\", \"human_feedback\")\n",
    "builder.add_conditional_edges(\n",
    "    \"human_feedback\", initiate_all_interviews, [\"create_analysts\", \"conduct_interview\"]\n",
    ")\n",
    "\n",
    "# ì¸í„°ë·° ê²°ê³¼ ë³´ê³ ì„œ ì‘ì„±\n",
    "builder.add_edge(\"conduct_interview\", \"write_report\")\n",
    "builder.add_edge(\"conduct_interview\", \"write_introduction\")\n",
    "builder.add_edge(\"conduct_interview\", \"write_conclusion\")\n",
    "\n",
    "# ë³´ê³ ì„œ ìµœì¢… ì •ë¦¬\n",
    "builder.add_edge(\n",
    "    [\"write_conclusion\", \"write_report\", \"write_introduction\"], \"finalize_report\"\n",
    ")\n",
    "builder.add_edge(\"finalize_report\", END)\n",
    "\n",
    "# ì»´íŒŒì¼\n",
    "memory = MemorySaver()\n",
    "graph = builder.compile(interrupt_before=[\"human_feedback\"], checkpointer=memory)\n",
    "\n",
    "visualize_graph(graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mcreate_analysts\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "affiliation='Tech Innovations Journal' name='Alice Turing' role='Data Science Researcher' description='Alice is focused on understanding the technological advancements and practical implementations in data science. Her primary concern is how new methodologies, like Modular RAG, provide scalable and efficient solutions compared to traditional methods such as Naive RAG. Alice is motivated by the potential of modular systems to improve adaptability in rapidly changing environments.'\n",
      "affiliation='Production Efficiency Solutions Inc.' name='Mohammed Sinclair' role='Operations Analyst' description='Mohammed examines the benefits of Modular RAG over Naive RAG at the production level, primarily looking at operational efficiency and cost-effectiveness. His focus is on how modularity can lead to reduced downtime and better resource allocation in production environments. Mohammed is driven by finding solutions that reduce operational costs while maintaining high production standards.'\n",
      "affiliation='Cognitive Computing Lab' name='Rita Nakamura' role='Artificial Intelligence Specialist' description=\"Rita's focus is on the cognitive and learning aspects of AI systems, such as Modular RAG. She is concerned with how modular frameworks can enhance the learning capabilities and flexibility of AI, compared to more rigid traditional systems like Naive RAG. Rita is motivated by creating AI systems that are not only efficient but also capable of evolving with new data.\"\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36m__interrupt__\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# ì…ë ¥ ë°ì´í„° ì„¤ì •\n",
    "max_analysts = 3\n",
    "topic = \"Explain how Modular RAG differs from traditional Naive RAG and the benefits of using it at the production level.\"\n",
    "\n",
    "# config ì„¤ì •\n",
    "config = RunnableConfig(\n",
    "    recursion_limit=30,\n",
    "    configurable={\"thread_id\": random_uuid()},\n",
    ")\n",
    "\n",
    "# ì…ë ¥ ë°ì´í„° ì„¤ì •\n",
    "inputs = {\"topic\": topic, \"max_analysts\": max_analysts}\n",
    "\n",
    "# ê·¸ë˜í”„ ì‹¤í–‰: ì²« ë²ˆì§¸ ì¤‘ë‹¨ ì§€ì ê¹Œì§€\n",
    "invoke_graph(graph, inputs, config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'configurable': {'thread_id': '56d6c5d3-51f2-4c1b-b859-3eefcb4d2eb3',\n",
       "  'checkpoint_ns': '',\n",
       "  'checkpoint_id': '1f083d3b-644b-6600-8002-8769af338823'}}"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ìƒˆë¡œìš´ ë¶„ì„ê°€ ì¶”ê°€\n",
    "graph.update_state(\n",
    "    config,\n",
    "    {\"human_analyst_feedback\": \"Add Prof. Jeffrey Hinton as a head of AI analyst\"},\n",
    "    as_node=\"human_feedback\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mcreate_analysts\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "affiliation='Vector Institute' name='Prof. Jeffrey Hinton' role='Head of AI Analyst' description='As the head of AI analysts, Prof. Hinton brings a pioneering perspective on deep learning and neural networks. He is particularly interested in how Modular RAG can improve efficiency and scalability over Naive RAG, and what implications this has for advancing AI in production environments.'\n",
      "affiliation='OpenAI' name='Dr. Lisa Torres' role='Scalability Specialist' description='Dr. Torres focuses on the scalability benefits of Modular RAG versus Naive RAG. Her analysis centers around how Modular RAG can handle larger datasets more efficiently and its adaptability to various production-level scenarios.'\n",
      "affiliation='DeepMind' name='Dr. Amir Kahn' role='Performance Analyst' description=\"Dr. Kahn's expertise lies in evaluating the performance metrics of AI models. He compares Modular and Naive RAG in terms of speed and accuracy, providing an in-depth look at how these improvements can minimize latency and enhance reliability in real-world applications.\"\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36m__interrupt__\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# ê·¸ë˜í”„ ì‹¤í–‰\n",
    "invoke_graph(graph, None, config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'configurable': {'thread_id': '56d6c5d3-51f2-4c1b-b859-3eefcb4d2eb3',\n",
       "  'checkpoint_ns': '',\n",
       "  'checkpoint_id': '1f083d3b-7e43-6c88-8004-c644448fdcc5'}}"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ê·¸ë˜í”„ ì¬ê°œ\n",
    "graph.update_state(config, {\"human_analyst_feedback\": None}, as_node=\"human_feedback\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mask_question\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello, I'm John Harper, a technology writer exploring cutting-edge advancements in AI scalability. Dr. Torres, could you please explain how Modular RAG differs from traditional Naive RAG, specifically in terms of scalability and handling large datasets? How does modularity enhance its performance in production environments?\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mask_question\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello, Dr. Kahn, my name is Alex Ramirez, and I'm a technology journalist interested in exploring advancements in AI performance metrics. I'm particularly intrigued by the differences between Modular RAG and Naive RAG. Could you provide an explanation of how these two approaches differ specifically, and what benefits modular RAG offers when used in production environments?\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mask_question\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello, my name is Dr. Emily Carter and Iâ€™m thrilled to have the opportunity to explore the nuances of Modular RAG and its advantages over traditional Naive RAG with you, Prof. Hinton. As an aspiring AI researcher, Iâ€™m especially intrigued by the practical implications of these advancements in real-world applications. To start, could you explain how Modular RAG differentiates from traditional Naive RAG in its structure and functionality?\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36msearch_web\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "<Document href=\"https://rabiloo.com/blog/the-3-types-of-rag-models-naive-rag-modular-rag-and-advanced-rag\"/>\n",
      "Building AI systems requires choosing the right tools for the job, and Retrieval-Augmented Generation (RAG) offers various models, each designed to serve different needs. Essentially, a RAG model retrieves information from external databases, knowledge bases, or the web and then uses that data to generate relevant, meaningful responses. Naive RAG begins by indexing the data source for quick retrieval of relevant information. Modular RAG builds on the basic principles of Naive RAG by breaking down the retrieval and generation processes into separate, specialized modules. Unlike Naive RAG or Modular RAG, this stage is more precise and contextually aware, ensuring that the system retrieves only the most relevant and contextually appropriate data. The generation module in Advanced RAG then processes the refined data using sophisticated models like T5 or GPT-3.\n",
      "</Document>\n",
      "\n",
      "---\n",
      "\n",
      "<Document href=\"https://medium.com/@drjulija/what-are-naive-rag-advanced-rag-modular-rag-paradigms-edff410c202e\"/>\n",
      "I share the differences between Naive RAG, Advanced RAG and Modular RAG frameworks. Retrieval-Augmented Generation (RAG) framework is designed to help mitigate these challenges. RAG enhances LLMsâ€™ knowledge base with additional, domain-specific data. RAG systems can be split into 3 categories: See the comparison between all three paradigms of RAG â€” Naive RAG, Advanced RAG and Modular RAG below.\n",
      "</Document>\n",
      "\n",
      "---\n",
      "\n",
      "<Document href=\"https://www.digitalocean.com/community/conceptual-articles/rag-ai-agents-agentic-rag-comparative-analysis\"/>\n",
      "Jan 14, 2025Â·The Modular RAG architecture transcends the Naive and Advanced RAG models, offering improved adaptability and versatility. It uses multiple\n",
      "</Document>\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36msearch_arxiv\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "<Document source=\"http://arxiv.org/abs/2508.05650v1\" date=\"2025-07-26\" authors=\"Jiaxuan Liang, Shide Zhou, Kailong Wang\"/>\n",
      "<Title>\n",
      "OmniBench-RAG: A Multi-Domain Evaluation Platform for Retrieval-Augmented Generation Tools\n",
      "</Title>\n",
      "\n",
      "<Summary>\n",
      "While Retrieval Augmented Generation (RAG) is now widely adopted to enhance\n",
      "LLMs, evaluating its true performance benefits in a reproducible and\n",
      "interpretable way remains a major hurdle. Existing methods often fall short:\n",
      "they lack domain coverage, employ coarse metrics that miss sub document\n",
      "precision, and fail to capture computational trade offs. Most critically, they\n",
      "provide no standardized framework for comparing RAG effectiveness across\n",
      "different models and domains.\n",
      "  We introduce OmniBench RAG, a novel automated platform for multi domain\n",
      "evaluation of RAG systems. The platform quantifies performance gains across\n",
      "accuracy and efficiency dimensions, spanning nine knowledge fields including\n",
      "culture, geography, and health. We introduce two standardized metrics:\n",
      "Improvements (accuracy gains) and Transformation (efficiency differences\n",
      "between pre RAG and post RAG models), enabling reproducible comparisons across\n",
      "models and tasks. The platform features dynamic test generation, modular\n",
      "evaluation pipelines, and automated knowledge base construction. Our evaluation\n",
      "reveals striking variability in RAG effectiveness, from significant gains in\n",
      "culture to declines in mathematics, highlighting the critical importance of\n",
      "systematic, domain aware assessment. A demonstration video is available at:\n",
      "https://www.youtube.com/watch?v=BZx83QFcTCI. Code and datasets:\n",
      "https://github.com/Garnett-Liang/Omnibench-RAG.\n",
      "</Summary>\n",
      "\n",
      "<Content>\n",
      "OmniBench-RAG: A Multi-Domain Evaluation\n",
      "Platform for Retrieval-Augmented Generation Tools\n",
      "Jiaxuan Liang*, Shide Zhou*, and Kailong Wangâ€ \n",
      "Huazhong University of Science and Technology\n",
      "{liangjx, shidez, wangkl}@hust.edu.cn\n",
      "Abstractâ€”While Retrieval Augmented Generation (RAG) is\n",
      "now widely adopted to enhance LLMs, evaluating its true\n",
      "performance benefits in a reproducible and interpretable way\n",
      "remains a major hurdle. Existing methods often fall short: they\n",
      "lack domain coverage, employ coarse metrics that miss sub\n",
      "document precision, and fail to capture computational trade\n",
      "offs. Most critically, they provide no standardized framework\n",
      "for comparing RAG effectiveness across different models and\n",
      "domains.\n",
      "We introduce OmniBench RAG, a novel automated platform\n",
      "for multi domain evaluation of RAG systems. The platform\n",
      "quantifies performance gains across accuracy and efficiency\n",
      "dimensions, spanning nine knowledge fields including culture,\n",
      "geography, and health. We introduce two standardized metrics:\n",
      "Improvements (accuracy gains) and Transformation (efficiency\n",
      "differences between pre RAG and post RAG models), enabling\n",
      "reproducible comparisons across models and tasks. The platform\n",
      "features dynamic test generation, modular evaluation pipelines,\n",
      "and automated knowledge base construction. Our evaluation\n",
      "reveals striking variability in RAG effectiveness, from significant\n",
      "gains in culture to declines in mathematics, highlighting the\n",
      "critical importance of systematic, domain aware assessment. A\n",
      "demonstration video is available at: https://www.youtube.com/\n",
      "watch?v=BZx83QFcTCI. Code and datasets: https://github.com/\n",
      "Garnett-Liang/Omnibench-RAG.\n",
      "I. INTRODUCTION\n",
      "Retrieval-Augmented Generation (RAG) is a key technique\n",
      "for enhancing Large Language Models (LLMs) [1], [2]. By\n",
      "grounding model responses in external, verifiable knowledge,\n",
      "RAG promises to mitigate hallucinations [3], improve factual\n",
      "accuracy [4], and provide up-to-date information [5]. However,\n",
      "the true effectiveness of RAG is far from uniform. Recent\n",
      "studies [6], [7] reveal a significant disparity: while RAG\n",
      "can boost the accuracy of smaller models like Llama-3.2-3B-\n",
      "Instruct by as much as 38.12%, its impact on state-of-the-art\n",
      "models such as GPT-4o, which excel with extended context\n",
      "windows, is often less pronounced. This variability, which\n",
      "depends not only on the modelâ€™s scale but also heavily on\n",
      "the knowledge domain, underscores a critical challenge: the\n",
      "lack of a systematic platform to quantify the value of RAG\n",
      "across these diverse contexts.\n",
      "Current RAG evaluation approaches suffer from fundamen-\n",
      "tal limitations that impede reproducible and comprehensive as-\n",
      "sessment: First, they lack automated multi-domain evalua-\n",
      "tion capabilities and rely on non-deterministic components.\n",
      "*Jiaxuan Liang and Shide Zhou are co-first authors.\n",
      "â€ Kailong Wang is the corresponding author.\n",
      "Existing benchmarks typically require manual configuration\n",
      "for each knowledge domain and fail to provide unified assess-\n",
      "ment across diverse fields like finance, healthcare, or culture,\n",
      "making cross-domain performance analysis labor-intensive and\n",
      "inconsistent. Moreover, key metrics in leading frameworks\n",
      "(e.g., LLM-based scoring in Ragas [8]) inadvertently introduce\n",
      "randomness due to reliance on large language models in the\n",
      "evaluation loop, undermining result reproducibility. Second,\n",
      "they employ static datasets and coarse-grained metrics.\n",
      "Most frameworks rely on fixed benchmarks and document-\n",
      "level retrieval metrics (e.g., MRR@k[9]), missing the critical\n",
      "sub-document precision needed to assess whether models\n",
      "extract specific facts accurately. They also lack the ability to\n",
      "dynamically generate test cases that probe complex reasoning\n",
      "patterns. Third, they fail to capture the computational\n",
      "trade-offs inherent in RAG systems. Without automated\n",
      "profiling of resource utilization and efficiency metrics, practi-\n",
      "tioners cannot make informed decisions about the cost-benefit\n",
      "trade-offs of deploying RAG in production environments.\n",
      "Our \n",
      "</Content>\n",
      "</Document>\n",
      "\n",
      "---\n",
      "\n",
      "<Document source=\"http://arxiv.org/abs/2406.00944v3\" date=\"2025-02-28\" authors=\"Shicheng Xu, Liang Pang, Huawei Shen, Xueqi Cheng\"/>\n",
      "<Title>\n",
      "A Theory for Token-Level Harmonization in Retrieval-Augmented Generation\n",
      "</Title>\n",
      "\n",
      "<Summary>\n",
      "Retrieval-augmented generation (RAG) utilizes retrieved texts to enhance\n",
      "large language models (LLMs). Studies show that while RAG provides valuable\n",
      "external information (benefit), it may also mislead LLMs (detriment) with noisy\n",
      "or incorrect retrieved texts. Although many existing methods attempt to\n",
      "preserve benefit and avoid detriment, they lack a theoretical explanation for\n",
      "RAG. The benefit and detriment in the next token prediction of RAG remain a\n",
      "black box that cannot be quantified or compared in an explainable manner, so\n",
      "existing methods are data-driven, need additional utility evaluators or\n",
      "post-hoc. This paper takes the first step towards providing a theory to explain\n",
      "and trade off the benefit and detriment in RAG. First, we model RAG as the\n",
      "fusion between distribution of LLMs knowledge and distribution of retrieved\n",
      "texts. Then, we formalize the trade-off between the value of external knowledge\n",
      "(benefit) and its potential risk of misleading LLMs (detriment) in next token\n",
      "prediction of RAG by distribution difference in this fusion. Finally, we prove\n",
      "that the actual effect of RAG on the token, which is the comparison between\n",
      "benefit and detriment, can be predicted without any training or accessing the\n",
      "utility of retrieval. Based on our theory, we propose a practical novel method,\n",
      "Tok-RAG, which achieves collaborative generation between the pure LLM and RAG\n",
      "at token level to preserve benefit and avoid detriment. Experiments in\n",
      "real-world tasks using LLMs such as OPT, LLaMA-2, and Mistral show the\n",
      "effectiveness of our method and support our theoretical findings.\n",
      "</Summary>\n",
      "\n",
      "<Content>\n",
      "Published as a conference paper at ICLR 2025\n",
      "A THEORY FOR TOKEN-LEVEL HARMONIZATION IN\n",
      "RETRIEVAL-AUGMENTED GENERATION\n",
      "Shicheng Xu1,2, Liang Pang1âˆ—, Huawei Shen1, Xueqi Cheng1\n",
      "1CAS Key Laboratory of AI Safety, Institute of Computing Technology, Chinese Academy of Sciences\n",
      "2University of Chinese Academy of Sciences\n",
      "{xushicheng21s,pangliang,shenhuawei,cxq}@ict.ac.cn\n",
      "ABSTRACT\n",
      "Retrieval-augmented generation (RAG) utilizes retrieved texts to enhance large\n",
      "language models (LLMs). Studies show that while RAG provides valuable external\n",
      "information (benefit), it may also mislead LLMs (detriment) with noisy or incorrect\n",
      "retrieved texts. Although many existing methods attempt to preserve benefit and\n",
      "avoid detriment, they lack a theoretical explanation for RAG. The benefit and\n",
      "detriment in the next token prediction of RAG remain a â€™black boxâ€™ that cannot\n",
      "be quantified or compared in an explainable manner, so existing methods are data-\n",
      "driven, need additional utility evaluators or post-hoc. This paper takes the first step\n",
      "towards providing a theory to explain and trade off the benefit and detriment in\n",
      "RAG. First, we model RAG as the fusion between distribution of LLMâ€™s knowledge\n",
      "and distribution of retrieved texts. Then, we formalize the trade-off between the\n",
      "value of external knowledge (benefit) and its potential risk of misleading LLMs\n",
      "(detriment) in next token prediction of RAG by distribution difference in this\n",
      "fusion. Finally, we prove that the actual effect of RAG on the token, which is the\n",
      "comparison between benefit and detriment, can be predicted without any training or\n",
      "accessing the utility of retrieval. Based on our theory, we propose a practical novel\n",
      "method, Tok-RAG, which achieves collaborative generation between the pure\n",
      "LLM and RAG at token level to preserve benefit and avoid detriment. Experiments\n",
      "in real-world tasks using LLMs such as OPT, LLaMA-2, and Mistral show the\n",
      "effectiveness of our method and support our theoretical findings. Code is available1.\n",
      "1\n",
      "INTRODUCTION\n",
      "Retrieval-augmented generation (RAG) has shown promising performance in enhancing Large\n",
      "Language Models (LLMs) by integrating retrieved texts (Xu et al., 2023; Shi et al., 2023; Asai et al.,\n",
      "2023; Ram et al., 2023). Studies indicate that while RAG provides LLMs with valuable additional\n",
      "knowledge (benefit), it also poses a risk of misleading them (detriment) due to noisy or incorrect\n",
      "retrieved texts (Ram et al., 2023; Xu et al., 2024b;a; Jin et al., 2024; Xie et al., 2023). Existing\n",
      "methods attempt to preserve benefit and avoid detriment by adding utility evaluators for retrieval,\n",
      "prompt engineering, or fine-tuning LLMs (Asai et al., 2023; Ding et al., 2024; Xu et al., 2024b; Yoran\n",
      "et al., 2024; Ren et al., 2023; Mallen et al., 2022; Jiang et al., 2023). However, existing methods are\n",
      "data-driven, need evaluator for utility of retrieved texts or post-hoc. A theory-based method, focusing\n",
      "on core principles of RAG is urgently needed, which is crucial for reliable improvements without\n",
      "relying on additional training or utility evaluators and improving our understanding for RAG.\n",
      "This paper takes the first step in providing a theoretical framework to explain and trade off the benefit\n",
      "and detriment at token level in RAG and proposes a novel method to preserve benefit and avoid\n",
      "detriment based on our theoretical findings. Specifically, this paper pioneers in modeling next token\n",
      "prediction in RAG as the fusion between the distribution of LLMâ€™s knowledge and the distribution\n",
      "of retrieved texts as shown in Figure 1. Our theoretical derivation based on this formalizes the core\n",
      "of this fusion as the subtraction between two terms measured by the distribution difference: one is\n",
      "distribution completion and the other is distribution contradiction. Further analysis indicates that\n",
      "âˆ—Corresponding Author\n",
      "1https://github.com/xsc1234/Tok-RAG\n",
      "1\n",
      "arXiv:2406.00944v3  [cs.CL]  28 Feb 2025\n",
      "Published as a conference paper at ICLR 2025\n",
      "Query\n",
      "Wole\n",
      "Query\n",
      "Ernst\n",
      "Soyinka\n",
      "â€¦\n",
      "LLMâ€™s \n",
      "Distribution\n",
      "Retr\n",
      "</Content>\n",
      "</Document>\n",
      "\n",
      "---\n",
      "\n",
      "<Document source=\"http://arxiv.org/abs/2507.18515v1\" date=\"2025-07-24\" authors=\"Zezhou Yang, Ting Peng, Cuiyun Gao, Chaozheng Wang, Hailiang Huang, Yuetang Deng\"/>\n",
      "<Title>\n",
      "A Deep Dive into Retrieval-Augmented Generation for Code Completion: Experience on WeChat\n",
      "</Title>\n",
      "\n",
      "<Summary>\n",
      "Code completion, a crucial task in software engineering that enhances\n",
      "developer productivity, has seen substantial improvements with the rapid\n",
      "advancement of large language models (LLMs). In recent years,\n",
      "retrieval-augmented generation (RAG) has emerged as a promising method to\n",
      "enhance the code completion capabilities of LLMs, which leverages relevant\n",
      "context from codebases without requiring model retraining. While existing\n",
      "studies have demonstrated the effectiveness of RAG on public repositories and\n",
      "benchmarks, the potential distribution shift between open-source and\n",
      "closed-source codebases presents unique challenges that remain unexplored. To\n",
      "mitigate the gap, we conduct an empirical study to investigate the performance\n",
      "of widely-used RAG methods for code completion in the industrial-scale codebase\n",
      "of WeChat, one of the largest proprietary software systems. Specifically, we\n",
      "extensively explore two main types of RAG methods, namely identifier-based RAG\n",
      "and similarity-based RAG, across 26 open-source LLMs ranging from 0.5B to 671B\n",
      "parameters. For a more comprehensive analysis, we employ different retrieval\n",
      "techniques for similarity-based RAG, including lexical and semantic retrieval.\n",
      "Based on 1,669 internal repositories, we achieve several key findings: (1) both\n",
      "RAG methods demonstrate effectiveness in closed-source repositories, with\n",
      "similarity-based RAG showing superior performance, (2) the effectiveness of\n",
      "similarity-based RAG improves with more advanced retrieval techniques, where\n",
      "BM25 (lexical retrieval) and GTE-Qwen (semantic retrieval) achieve superior\n",
      "performance, and (3) the combination of lexical and semantic retrieval\n",
      "techniques yields optimal results, demonstrating complementary strengths.\n",
      "Furthermore, we conduct a developer survey to validate the practical utility of\n",
      "RAG methods in real-world development environments.\n",
      "</Summary>\n",
      "\n",
      "<Content>\n",
      "A Deep Dive into Retrieval-Augmented Generation\n",
      "for Code Completion: Experience on WeChat\n",
      "Zezhou Yang\n",
      "Tencent\n",
      "Guangzhou, China\n",
      "zezhouyang@tencent.com\n",
      "Ting Peng\n",
      "Tencent\n",
      "Guangzhou, China\n",
      "sakurapeng@tencent.com\n",
      "Cuiyun Gaoâˆ—\n",
      "The Chinese University of Hong Kong\n",
      "Hong Kong, China\n",
      "cuiyungao@outlook.com\n",
      "Chaozheng Wang\n",
      "The Chinese University of Hong Kong\n",
      "Hong Kong, China\n",
      "adf111178@gmail.com\n",
      "Hailiang Huang\n",
      "Tencent\n",
      "Guangzhou, China\n",
      "eraserhuang@tencent.com\n",
      "Yuetang Deng\n",
      "Tencent\n",
      "Guangzhou, China\n",
      "yuetangdeng@tencent.com\n",
      "Abstractâ€”Code completion, a crucial task in software engi-\n",
      "neering that enhances developer productivity, has seen substantial\n",
      "improvements with the rapid advancement of large language\n",
      "models (LLMs). In recent years, retrieval-augmented generation\n",
      "(RAG) has emerged as a promising method to enhance the code\n",
      "completion capabilities of LLMs, which leverages relevant con-\n",
      "text from codebases without requiring model retraining. While\n",
      "existing studies have demonstrated the effectiveness of RAG on\n",
      "public repositories and benchmarks, the potential distribution\n",
      "shift between open-source and closed-source codebases presents\n",
      "unique challenges that remain unexplored. To mitigate the gap,\n",
      "we conduct an empirical study to investigate the performance of\n",
      "widely-used RAG methods for code completion in the industrial-\n",
      "scale codebase of WeChat, one of the largest proprietary software\n",
      "systems. Specifically, we extensively explore two main types\n",
      "of RAG methods, namely identifier-based RAG and similarity-\n",
      "based RAG, across 26 open-source LLMs ranging from 0.5B\n",
      "to 671B parameters. For a more comprehensive analysis, we\n",
      "employ different retrieval techniques for similarity-based RAG,\n",
      "including lexical and semantic retrieval. Based on 1,669 internal\n",
      "repositories, we achieve several key findings: (1) both RAG\n",
      "methods demonstrate effectiveness in closed-source repositories,\n",
      "with similarity-based RAG showing superior performance, (2)\n",
      "the effectiveness of similarity-based RAG improves with more\n",
      "advanced retrieval techniques, where BM25 (lexical retrieval)\n",
      "and GTE-Qwen (semantic retrieval) achieve superior perfor-\n",
      "mance, and (3) the combination of lexical and semantic retrieval\n",
      "techniques yields optimal results, demonstrating complemen-\n",
      "tary strengths. Furthermore, we conduct a developer survey\n",
      "to validate the practical utility of RAG methods in real-world\n",
      "development environments.\n",
      "Index Termsâ€”large language model, retrieval-augmented gen-\n",
      "eration, code completion\n",
      "I. INTRODUCTION\n",
      "Code completion, which automatically predicts and sug-\n",
      "gests code fragments based on the surrounding programming\n",
      "context, has evolved from simple token-level suggestions to\n",
      "âˆ—Cuiyun Gao is the corresponding author.\n",
      "This research is supported by National Key R&D Program of China\n",
      "(No. 2022YFB3103900), National Natural Science Foundation of China\n",
      "under project (No. 62472126), Natural Science Foundation of Guangdong\n",
      "Province (Project No. 2023A1515011959), Shenzhen-Hong Kong Jointly\n",
      "Funded Project (Category A, No. SGDX20230116091246007), and Shenzhen\n",
      "Basic Research (General Project No. JCYJ20220531095214031).\n",
      "generating entire code blocks [1], [2]. Studies have demon-\n",
      "strated that code completion tools substantially enhance de-\n",
      "veloper productivity in real-world software development [3],\n",
      "[4]. Notably, 87% of professional developers report significant\n",
      "improvements in their coding efficiency when utilizing code\n",
      "completion tools in industrial settings [5]. Recent advances\n",
      "in large language models (LLMs) have further transformed\n",
      "various software engineering tasks [6]â€“[10], demonstrating\n",
      "unprecedented capabilities in code understanding and gen-\n",
      "eration. These models have achieved particularly impressive\n",
      "performance in code completion tasks [11]â€“[13].\n",
      "To enhance LLMsâ€™ performance on domain-specific tasks,\n",
      "researchers have explored Retrieval-Augmented Generation\n",
      "(RAG), which augments model inference by retrieving and in-\n",
      "corporating relevant context from the target codebase without\n",
      "requiring param\n",
      "</Content>\n",
      "</Document>\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36msearch_web\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "<Document href=\"https://rabiloo.com/blog/the-3-types-of-rag-models-naive-rag-modular-rag-and-advanced-rag\"/>\n",
      "Building AI systems requires choosing the right tools for the job, and Retrieval-Augmented Generation (RAG) offers various models, each designed to serve different needs. Essentially, a RAG model retrieves information from external databases, knowledge bases, or the web and then uses that data to generate relevant, meaningful responses. Naive RAG begins by indexing the data source for quick retrieval of relevant information. Modular RAG builds on the basic principles of Naive RAG by breaking down the retrieval and generation processes into separate, specialized modules. Unlike Naive RAG or Modular RAG, this stage is more precise and contextually aware, ensuring that the system retrieves only the most relevant and contextually appropriate data. The generation module in Advanced RAG then processes the refined data using sophisticated models like T5 or GPT-3.\n",
      "</Document>\n",
      "\n",
      "---\n",
      "\n",
      "<Document href=\"https://www.superteams.ai/blog/how-to-implement-naive-rag-advanced-rag-and-modular-rag\"/>\n",
      "Naive RAG is a paradigm that combines information retrieval with natural language generation to produce responses to queries or prompts. In Naive RAG, retrieval is typically performed using retrieval models that rank the indexed data based on its relevance to the input query. These models generate text based on the input query and the retrieved context, aiming to produce coherent and contextually relevant responses. Advanced RAG models may fine-tune embeddings to capture task-specific semantics or domain knowledge, thereby improving the quality of retrieved information and generated responses. Dynamic embedding techniques enable RAG models to adaptively adjust embeddings during inference based on the context of the query or retrieved information.\n",
      "</Document>\n",
      "\n",
      "---\n",
      "\n",
      "<Document href=\"https://medium.com/@drjulija/what-are-naive-rag-advanced-rag-modular-rag-paradigms-edff410c202e\"/>\n",
      "I share the differences between Naive RAG, Advanced RAG and Modular RAG frameworks. Retrieval-Augmented Generation (RAG) framework is designed to help mitigate these challenges. RAG enhances LLMsâ€™ knowledge base with additional, domain-specific data. RAG systems can be split into 3 categories: See the comparison between all three paradigms of RAG â€” Naive RAG, Advanced RAG and Modular RAG below.\n",
      "</Document>\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36msearch_arxiv\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "<Document source=\"http://arxiv.org/abs/2505.13006v1\" date=\"2025-05-19\" authors=\"Yuyang Li, Philip J. M. Kerbusch, Raimon H. R. Pruim, Tobias KÃ¤fer\"/>\n",
      "<Title>\n",
      "Evaluating the Performance of RAG Methods for Conversational AI in the Airport Domain\n",
      "</Title>\n",
      "\n",
      "<Summary>\n",
      "Airports from the top 20 in terms of annual passengers are highly dynamic\n",
      "environments with thousands of flights daily, and they aim to increase the\n",
      "degree of automation. To contribute to this, we implemented a Conversational AI\n",
      "system that enables staff in an airport to communicate with flight information\n",
      "systems. This system not only answers standard airport queries but also\n",
      "resolves airport terminology, jargon, abbreviations, and dynamic questions\n",
      "involving reasoning. In this paper, we built three different\n",
      "Retrieval-Augmented Generation (RAG) methods, including traditional RAG, SQL\n",
      "RAG, and Knowledge Graph-based RAG (Graph RAG). Experiments showed that\n",
      "traditional RAG achieved 84.84% accuracy using BM25 + GPT-4 but occasionally\n",
      "produced hallucinations, which is risky to airport safety. In contrast, SQL RAG\n",
      "and Graph RAG achieved 80.85% and 91.49% accuracy respectively, with\n",
      "significantly fewer hallucinations. Moreover, Graph RAG was especially\n",
      "effective for questions that involved reasoning. Based on our observations, we\n",
      "thus recommend SQL RAG and Graph RAG are better for airport environments, due\n",
      "to fewer hallucinations and the ability to handle dynamic questions.\n",
      "</Summary>\n",
      "\n",
      "<Content>\n",
      "arXiv:2505.13006v1  [cs.CL]  19 May 2025\n",
      "Evaluating the Performance of RAG Methods for Conversational AI in the\n",
      "Airport Domain\n",
      "Yuyang Li1, Philip J.M. Kerbusch2, Raimon H.R. Pruim2, Tobias KÃ¤fer1\n",
      "1Karlsruhe Institute of Technology, 2Royal Schiphol Group\n",
      "2Royal Schiphol Group, 1Karlsruhe Institute of Technology\n",
      "yuyang.li@kit.edu,\n",
      "tobias.kaefer@kit.edu\n",
      "Abstract\n",
      "Airports from the top 20 in terms of annual\n",
      "passengers are highly dynamic environments\n",
      "with thousands of flights daily, and they aim\n",
      "to increase the degree of automation. To con-\n",
      "tribute to this, we implemented a Conversa-\n",
      "tional AI system that enables staff in an air-\n",
      "port to communicate with flight information\n",
      "systems. This system not only answers stan-\n",
      "dard airport queries but also resolves airport ter-\n",
      "minology, jargon, abbreviations, and dynamic\n",
      "questions involving reasoning. In this paper, we\n",
      "built three different Retrieval-Augmented Gen-\n",
      "eration (RAG) methods, including traditional\n",
      "RAG, SQL RAG, and Knowledge Graph-based\n",
      "RAG (Graph RAG). Experiments showed that\n",
      "traditional RAG achieved 84.84% accuracy\n",
      "using BM25 + GPT-4 but occasionally pro-\n",
      "duced hallucinations, which is risky to airport\n",
      "safety. In contrast, SQL RAG and Graph RAG\n",
      "achieved 80.85% and 91.49% accuracy respec-\n",
      "tively, with significantly fewer hallucinations.\n",
      "Moreover, Graph RAG was especially effective\n",
      "for questions that involved reasoning. Based\n",
      "on our observations, we thus recommend SQL\n",
      "RAG and Graph RAG are better for airport en-\n",
      "vironments, due to fewer hallucinations and the\n",
      "ability to handle dynamic questions.\n",
      "1\n",
      "Introduction\n",
      "Amsterdam Airport Schiphol, one of the top 20\n",
      "airports in the world, ranked by annual passenger\n",
      "numbers, handles thousands of flights each day.\n",
      "These airports rely on staff like gate planners and\n",
      "apron controllers to access and update data across\n",
      "systems. For these employees, traditional database\n",
      "queries can be complex and time-consuming for\n",
      "some employees who are not query experts when\n",
      "they need flight information. A conversational AI\n",
      "system with a natural language query (NLQ) inter-\n",
      "face allows all employees to interact with systems\n",
      "naturally, asking questions like, â€œWhich fights are\n",
      "at ramp D07?â€ and receiving instant answers. This\n",
      "improves productivity, and streamlines workflows,\n",
      "especially in high-pressure areas like at the gate,\n",
      "where less educated workers require access to up-\n",
      "to-date information. By replacing strict query for-\n",
      "mats with intuitive, real-time responses, conversa-\n",
      "tional AI enhances decision-making and efficiency,\n",
      "making it a suitable solution for dynamic environ-\n",
      "ments such as airports.\n",
      "Building such a system is challenging because\n",
      "flight data is stored by experts in tables using avi-\n",
      "ation abbreviations. We need our system to un-\n",
      "derstand these datasets to answer questions from\n",
      "the airport domain. Additionally, ensuring avia-\n",
      "tion safety is a major concern; the system must\n",
      "be safe and enable employees to perform accurate\n",
      "operations. We address those challenges using two\n",
      "research questions.\n",
      "The first question is how to handle flight data so\n",
      "that our system can answer different questions. We\n",
      "divided the questions into three types:\n",
      "â€¢ Straightforward questions: Questions that\n",
      "can be directly answered from the flight data.\n",
      "â€¢ Questions involving specialized airport jar-\n",
      "gon, abbreviations, and incomplete queries:\n",
      "Operators often use shorthand or omit con-\n",
      "text. Flight â€œKL0123â€ might be referred to as\n",
      "â€œ0123â€ or â€œ123,â€ while gate â€œC05â€ might be\n",
      "shortened to â€œC5.â€ Abbreviations like â€œKLMâ€\n",
      "for â€œKLM Royal Dutch Airlinesâ€ or â€œDeltaâ€\n",
      "for â€œDelta Air Linesâ€ are also common. Op-\n",
      "erators frequently ask short, incomplete ques-\n",
      "tions, e. g., â€œWhich flights are at D04?â€ or\n",
      "â€œWhat is the gate for that Delta airline?â€ With-\n",
      "out resolving missing details such, these ques-\n",
      "tions cannot be answered.\n",
      "â€¢ Dynamic questions: Questions that involve\n",
      "additional calculations and reasoning, espe-\n",
      "cially related to time.\n",
      "Examples include\n",
      "â€œWhat is the connecting flightâ€™s onramp time\n",
      "for D\n",
      "</Content>\n",
      "</Document>\n",
      "\n",
      "---\n",
      "\n",
      "<Document source=\"http://arxiv.org/abs/2503.02922v1\" date=\"2025-03-04\" authors=\"Joyce Cahoon, Prerna Singh, Nick Litombe, Jonathan Larson, Ha Trinh, Yiwen Zhu, Andreas Mueller, Fotis Psallidas, Carlo Curino\"/>\n",
      "<Title>\n",
      "Optimizing open-domain question answering with graph-based retrieval augmented generation\n",
      "</Title>\n",
      "\n",
      "<Summary>\n",
      "In this work, we benchmark various graph-based retrieval-augmented generation\n",
      "(RAG) systems across a broad spectrum of query types, including OLTP-style\n",
      "(fact-based) and OLAP-style (thematic) queries, to address the complex demands\n",
      "of open-domain question answering (QA). Traditional RAG methods often fall\n",
      "short in handling nuanced, multi-document synthesis tasks. By structuring\n",
      "knowledge as graphs, we can facilitate the retrieval of context that captures\n",
      "greater semantic depth and enhances language model operations. We explore\n",
      "graph-based RAG methodologies and introduce TREX, a novel, cost-effective\n",
      "alternative that combines graph-based and vector-based retrieval techniques.\n",
      "Our benchmarking across four diverse datasets highlights the strengths of\n",
      "different RAG methodologies, demonstrates TREX's ability to handle multiple\n",
      "open-domain QA types, and reveals the limitations of current evaluation\n",
      "methods.\n",
      "  In a real-world technical support case study, we demonstrate how TREX\n",
      "solutions can surpass conventional vector-based RAG in efficiently synthesizing\n",
      "data from heterogeneous sources. Our findings underscore the potential of\n",
      "augmenting large language models with advanced retrieval and orchestration\n",
      "capabilities, advancing scalable, graph-based AI solutions.\n",
      "</Summary>\n",
      "\n",
      "<Content>\n",
      "Optimizing open-domain question answering with graph-based\n",
      "retrieval augmented generation\n",
      "Joyce Cahoon\n",
      "jcahoon@microsoft.com\n",
      "Microsoft\n",
      "Redmond, WA, USA\n",
      "Prerna Singh\n",
      "prernasingh@microsoft.com\n",
      "Microsoft\n",
      "Redmond, WA, USA\n",
      "Nick Litombe\n",
      "nicklitombe@microsoft.com\n",
      "Microsoft\n",
      "Austin, TX, USA\n",
      "Jonathan Larson\n",
      "jolarso@microsoft.com\n",
      "Microsoft\n",
      "Redmond, WA, USA\n",
      "Ha Trinh\n",
      "trinhha@microsoft.com\n",
      "Microsoft\n",
      "Redmond, WA, USA\n",
      "Yiwen Zhu\n",
      "yiwzh@microsoft.com\n",
      "Microsoft\n",
      "Mountain View, CA, USA\n",
      "Andreas Mueller\n",
      "amueller@microsoft.com\n",
      "Microsoft\n",
      "Mountain View, CA, USA\n",
      "Fotis Psallidas\n",
      "fotis.psallidas@microsoft.com\n",
      "Microsoft\n",
      "New York, NY, USA\n",
      "Carlo Curino\n",
      "carlo.curino@microsoft.com\n",
      "Microsoft\n",
      "Redmond, WA, USA\n",
      "ABSTRACT\n",
      "In this work, we benchmark various graph-based retrieval-augmented\n",
      "generation (RAG) systems across a broad spectrum of query types,\n",
      "including OLTP-style (fact-based) and OLAP-style (thematic) queries,\n",
      "to address the complex demands of open-domain question answer-\n",
      "ing (QA). Traditional RAG methods often fall short in handling nu-\n",
      "anced, multi-document synthesis tasks. By structuring knowledge\n",
      "as graphs, we can facilitate the retrieval of context that captures\n",
      "greater semantic depth and enhances language model operations.\n",
      "We explore graph-based RAG methodologies and introduce TREX,\n",
      "a novel, cost-effective alternative that combines graph-based and\n",
      "vector-based retrieval techniques. Our benchmarking across four di-\n",
      "verse datasets highlights the strengths of different RAG methodolo-\n",
      "gies, demonstrates TREXâ€™s ability to handle multiple open-domain\n",
      "QA types, and reveals the limitations of current evaluation methods.\n",
      "In a real-world technical support case study, we demonstrate\n",
      "how TREX solutions can surpass conventional vector-based RAG in\n",
      "efficiently synthesizing data from heterogeneous sources. Our find-\n",
      "ings underscore the potential of augmenting large language models\n",
      "with advanced retrieval and orchestration capabilities, advancing\n",
      "scalable, graph-based AI solutions.\n",
      "1\n",
      "INTRODUCTION\n",
      "Traditionally, knowledge workersâ€”such as executives, managers,\n",
      "and analystsâ€”relied on data warehousing and operational databases\n",
      "to make faster, more informed decisions [4]. Today, the range of\n",
      "decision support tools has expanded to include foundational mod-\n",
      "els, with AI software spending projected to reach $297.9 billion by\n",
      "2027 [15]. Although the modern knowledge worker operates in a\n",
      "vastly different environment, one not limited to carefully curated\n",
      "data warehouses with complex multi-dimensional models for on-\n",
      "line analytical processing (OLAP) or highly structured operational\n",
      "databases supporting online transaction processing (OLTP) [4], the\n",
      "core challenges and types of questions that these decision support\n",
      "systems address remain highly relevant.\n",
      "The need for decision support tools capable of processing vast\n",
      "amounts of unstructured data at scale is particularly evident in\n",
      "open-domain question answering (QA). This field, which spans Nat-\n",
      "ural Language Processing (NLP), Information Extraction (IE), and\n",
      "Information Retrieval (IR), focuses on answering questions without\n",
      "relying on predefined context [50]. Large language models (LLMs)\n",
      "such as GPT [42], Claude [2], and Llama [41] have emerged as\n",
      "powerful tools for this purpose, generating human-like responses\n",
      "to complex queries while processing extensive text inputs. In vari-\n",
      "ous enterprise applications that leverage LLMs, user queries can\n",
      "often be classified as either OLTP or OLAP. OLTP-style queries are\n",
      "simple, fact-based questions that can be answered through direct\n",
      "key-value lookups, retrieval from single text snippets, or by locally\n",
      "traversing multiple related pieces of text, while OLAP-style queries\n",
      "are open-ended, thematic, and require aggregating, synthesizing\n",
      "and abstracting information across multiple documents [47]. Just\n",
      "as operational databases are optimized for OLTP tasks and data\n",
      "warehouses for OLAP workloads, specialized LLM applications are\n",
      "now emerging to address these distinct query types, with tailored\n",
      "approach\n",
      "</Content>\n",
      "</Document>\n",
      "\n",
      "---\n",
      "\n",
      "<Document source=\"http://arxiv.org/abs/2412.05838v1\" date=\"2024-12-08\" authors=\"Aniruddha Salve, Saba Attar, Mahesh Deshmukh, Sayali Shivpuje, Arnab Mitra Utsab\"/>\n",
      "<Title>\n",
      "A Collaborative Multi-Agent Approach to Retrieval-Augmented Generation Across Diverse Data\n",
      "</Title>\n",
      "\n",
      "<Summary>\n",
      "Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by\n",
      "incorporating external, domain-specific data into the generative process. While\n",
      "LLMs are highly capable, they often rely on static, pre-trained datasets,\n",
      "limiting their ability to integrate dynamic or private data. Traditional RAG\n",
      "systems typically use a single-agent architecture to handle query generation,\n",
      "data retrieval, and response synthesis. However, this approach becomes\n",
      "inefficient when dealing with diverse data sources, such as relational\n",
      "databases, document stores, and graph databases, often leading to performance\n",
      "bottlenecks and reduced accuracy. This paper proposes a multi-agent RAG system\n",
      "to address these limitations. Specialized agents, each optimized for a specific\n",
      "data source, handle query generation for relational, NoSQL, and document-based\n",
      "systems. These agents collaborate within a modular framework, with query\n",
      "execution delegated to an environment designed for compatibility across various\n",
      "database types. This distributed approach enhances query efficiency, reduces\n",
      "token overhead, and improves response accuracy by ensuring that each agent\n",
      "focuses on its specialized task. The proposed system is scalable and adaptable,\n",
      "making it ideal for generative AI workflows that require integration with\n",
      "diverse, dynamic, or private data sources. By leveraging specialized agents and\n",
      "a modular execution environment, the system provides an efficient and robust\n",
      "solution for handling complex, heterogeneous data environments in generative AI\n",
      "applications.\n",
      "</Summary>\n",
      "\n",
      "<Content>\n",
      "A COLLABORATIVE MULTI-AGENT APPROACH TO\n",
      "RETRIEVAL-AUGMENTED GENERATION ACROSS DIVERSE DATA\n",
      "SOURCES\n",
      "Aniruddha Salve\n",
      "iASYS Technology Solutions Pvt. Ltd.\n",
      "Pune, Maharashtra, India\n",
      "aniruddha.salve@iasys.co.in\n",
      "Mahesh Deshmukh\n",
      "iASYS Technology Solutions Pvt. Ltd.\n",
      "Pune, Maharashtra, India\n",
      "mahesh.deshmukh@iasys.co.in\n",
      "Saba Attar\n",
      "SVPMâ€™s College of Engineering\n",
      "Baramati, Pune, Maharashtra, India\n",
      "sabaattar1702@gmail.com\n",
      "Sayali Shivpuje\n",
      "SVPMâ€™s College of Engineering\n",
      "Baramati, Pune, Maharashtra, India\n",
      "shivpujesayali.2243@gmail.com\n",
      "Arnab Mitra Utsab\n",
      "School of Data and Sciences\n",
      "Brac University\n",
      "Dhaka, Bangladesh\n",
      "arnab.mitra.utsab@g.bracu.ac.bd\n",
      "ABSTRACT\n",
      "Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by incorporating\n",
      "external, domain-specific data into the generative process. While LLMs are highly capable, they\n",
      "often rely on static, pre-trained datasets, limiting their ability to integrate dynamic or private data.\n",
      "Traditional RAG systems typically use a single-agent architecture to handle query generation, data\n",
      "retrieval, and response synthesis. However, this approach becomes inefficient when dealing with\n",
      "diverse data sources, such as relational databases, document stores, and graph databases, often leading\n",
      "to performance bottlenecks and reduced accuracy.\n",
      "This paper proposes a Multi-Agent RAG system to address these limitations. Specialized agents, each\n",
      "optimized for a specific data source, handle query generation for relational, NoSQL, and document-\n",
      "based systems. These agents collaborate within a modular framework, with query execution delegated\n",
      "to an environment designed for compatibility across various database types. This distributed approach\n",
      "enhances query efficiency, reduces token overhead, and improves response accuracy by ensuring that\n",
      "each agent focuses on its specialized task.\n",
      "The proposed system is scalable and adaptable, making it ideal for generative AI workflows that\n",
      "require integration with diverse, dynamic, or private data sources. By leveraging specialized agents\n",
      "and a modular execution environment, the system provides an efficient and robust solution for\n",
      "handling complex, heterogeneous data environments in generative AI applications.\n",
      "Keywords Multi-Agent RAG Systems Â· Retrieval-Augmented Generation Â· Large Language Models Â· Database\n",
      "Integration Â· Generative AI\n",
      "arXiv:2412.05838v1  [cs.AI]  8 Dec 2024\n",
      "1\n",
      "Introduction\n",
      "Large Language Models (LLMs) have significantly advanced natural language processing by enabling sophisticated\n",
      "query interpretation and text generation. [1, 8] Despite their capabilities, LLMs are limited by their reliance on static\n",
      "pre-trained datasets, which restricts their ability to incorporate dynamic, domain-specific, or private data into their\n",
      "responses. Retrieval-Augmented Generation (RAG) systems address this challenge by integrating external data retrieval\n",
      "with generative processes, providing more context-aware and accurate outputs.\n",
      "Traditional RAG systems typically employ single-agent architectures where a single system is responsible for query\n",
      "generation, data retrieval, and response synthesis. While effective for basic use cases, these monolithic designs often\n",
      "face limitations when dealing with diverse data sources, such as relational databases, document stores, and graph-based\n",
      "data [19]. These systems also require elaborate prompts containing schemas, examples, and user queries, leading to\n",
      "inefficiencies in token usage, increased processing latency, and potential inaccuracies in query handling.\n",
      "To overcome these challenges, this paper proposes a Multi-Agent RAG system [22, 23]. Unlike traditional approaches,\n",
      "this system delegates the task of query generation to specialized agents, each tailored to a specific type of database.\n",
      "These agents generate optimized, database-specific queries without directly executing or retrieving data. Queries are\n",
      "executed in a separate execution environment, ensuring compatibility with diverse data storage systems. The retrieved\n",
      "context is then com\n",
      "</Content>\n",
      "</Document>\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36msearch_arxiv\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "<Document source=\"http://arxiv.org/abs/2407.21059v1\" date=\"2024-07-26\" authors=\"Yunfan Gao, Yun Xiong, Meng Wang, Haofen Wang\"/>\n",
      "<Title>\n",
      "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks\n",
      "</Title>\n",
      "\n",
      "<Summary>\n",
      "Retrieval-augmented Generation (RAG) has markedly enhanced the capabilities\n",
      "of Large Language Models (LLMs) in tackling knowledge-intensive tasks. The\n",
      "increasing demands of application scenarios have driven the evolution of RAG,\n",
      "leading to the integration of advanced retrievers, LLMs and other complementary\n",
      "technologies, which in turn has amplified the intricacy of RAG systems.\n",
      "However, the rapid advancements are outpacing the foundational RAG paradigm,\n",
      "with many methods struggling to be unified under the process of\n",
      "\"retrieve-then-generate\". In this context, this paper examines the limitations\n",
      "of the existing RAG paradigm and introduces the modular RAG framework. By\n",
      "decomposing complex RAG systems into independent modules and specialized\n",
      "operators, it facilitates a highly reconfigurable framework. Modular RAG\n",
      "transcends the traditional linear architecture, embracing a more advanced\n",
      "design that integrates routing, scheduling, and fusion mechanisms. Drawing on\n",
      "extensive research, this paper further identifies prevalent RAG\n",
      "patterns-linear, conditional, branching, and looping-and offers a comprehensive\n",
      "analysis of their respective implementation nuances. Modular RAG presents\n",
      "innovative opportunities for the conceptualization and deployment of RAG\n",
      "systems. Finally, the paper explores the potential emergence of new operators\n",
      "and paradigms, establishing a solid theoretical foundation and a practical\n",
      "roadmap for the continued evolution and practical deployment of RAG\n",
      "technologies.\n",
      "</Summary>\n",
      "\n",
      "<Content>\n",
      "1\n",
      "Modular RAG: Transforming RAG Systems into\n",
      "LEGO-like Reconfigurable Frameworks\n",
      "Yunfan Gao, Yun Xiong, Meng Wang, Haofen Wang\n",
      "Abstractâ€”Retrieval-augmented\n",
      "Generation\n",
      "(RAG)\n",
      "has\n",
      "markedly enhanced the capabilities of Large Language Models\n",
      "(LLMs) in tackling knowledge-intensive tasks. The increasing\n",
      "demands of application scenarios have driven the evolution\n",
      "of RAG, leading to the integration of advanced retrievers,\n",
      "LLMs and other complementary technologies, which in turn\n",
      "has amplified the intricacy of RAG systems. However, the rapid\n",
      "advancements are outpacing the foundational RAG paradigm,\n",
      "with many methods struggling to be unified under the process\n",
      "of â€œretrieve-then-generateâ€. In this context, this paper examines\n",
      "the limitations of the existing RAG paradigm and introduces\n",
      "the modular RAG framework. By decomposing complex RAG\n",
      "systems into independent modules and specialized operators, it\n",
      "facilitates a highly reconfigurable framework. Modular RAG\n",
      "transcends the traditional linear architecture, embracing a\n",
      "more advanced design that integrates routing, scheduling, and\n",
      "fusion mechanisms. Drawing on extensive research, this paper\n",
      "further identifies prevalent RAG patternsâ€”linear, conditional,\n",
      "branching, and loopingâ€”and offers a comprehensive analysis\n",
      "of their respective implementation nuances. Modular RAG\n",
      "presents\n",
      "innovative\n",
      "opportunities\n",
      "for\n",
      "the\n",
      "conceptualization\n",
      "and deployment of RAG systems. Finally, the paper explores\n",
      "the potential emergence of new operators and paradigms,\n",
      "establishing a solid theoretical foundation and a practical\n",
      "roadmap for the continued evolution and practical deployment\n",
      "of RAG technologies.\n",
      "Index Termsâ€”Retrieval-augmented generation, large language\n",
      "model, modular system, information retrieval\n",
      "I. INTRODUCTION\n",
      "L\n",
      "ARGE Language Models (LLMs) have demonstrated\n",
      "remarkable capabilities, yet they still face numerous\n",
      "challenges, such as hallucination and the lag in information up-\n",
      "dates [1]. Retrieval-augmented Generation (RAG), by access-\n",
      "ing external knowledge bases, provides LLMs with important\n",
      "contextual information, significantly enhancing their perfor-\n",
      "mance on knowledge-intensive tasks [2]. Currently, RAG, as\n",
      "an enhancement method, has been widely applied in various\n",
      "practical application scenarios, including knowledge question\n",
      "answering, recommendation systems, customer service, and\n",
      "personal assistants. [3]â€“[6]\n",
      "During the nascent stages of RAG , its core framework is\n",
      "constituted by indexing, retrieval, and generation, a paradigm\n",
      "referred to as Naive RAG [7]. However, as the complexity\n",
      "of tasks and the demands of applications have escalated, the\n",
      "Yunfan Gao is with Shanghai Research Institute for Intelligent Autonomous\n",
      "Systems, Tongji University, Shanghai, 201210, China.\n",
      "Yun Xiong is with Shanghai Key Laboratory of Data Science, School of\n",
      "Computer Science, Fudan University, Shanghai, 200438, China.\n",
      "Meng Wang and Haofen Wang are with College of Design and Innovation,\n",
      "Tongji University, Shanghai, 20092, China. (Corresponding author: Haofen\n",
      "Wang. E-mail: carter.whfcarter@gmail.com)\n",
      "limitations of Naive RAG have become increasingly apparent.\n",
      "As depicted in Figure 1, it predominantly hinges on the\n",
      "straightforward similarity of chunks, result in poor perfor-\n",
      "mance when confronted with complex queries and chunks with\n",
      "substantial variability. The primary challenges of Naive RAG\n",
      "include: 1) Shallow Understanding of Queries. The semantic\n",
      "similarity between a query and document chunk is not always\n",
      "highly consistent. Relying solely on similarity calculations\n",
      "for retrieval lacks an in-depth exploration of the relationship\n",
      "between the query and the document [8]. 2) Retrieval Re-\n",
      "dundancy and Noise. Feeding all retrieved chunks directly\n",
      "into LLMs is not always beneficial. Research indicates that\n",
      "an excess of redundant and noisy information may interfere\n",
      "with the LLMâ€™s identification of key information, thereby\n",
      "increasing the risk of generating erroneous and hallucinated\n",
      "responses. [9]\n",
      "To overcome the aforementioned limitations, \n",
      "</Content>\n",
      "</Document>\n",
      "\n",
      "---\n",
      "\n",
      "<Document source=\"http://arxiv.org/abs/2505.13006v1\" date=\"2025-05-19\" authors=\"Yuyang Li, Philip J. M. Kerbusch, Raimon H. R. Pruim, Tobias KÃ¤fer\"/>\n",
      "<Title>\n",
      "Evaluating the Performance of RAG Methods for Conversational AI in the Airport Domain\n",
      "</Title>\n",
      "\n",
      "<Summary>\n",
      "Airports from the top 20 in terms of annual passengers are highly dynamic\n",
      "environments with thousands of flights daily, and they aim to increase the\n",
      "degree of automation. To contribute to this, we implemented a Conversational AI\n",
      "system that enables staff in an airport to communicate with flight information\n",
      "systems. This system not only answers standard airport queries but also\n",
      "resolves airport terminology, jargon, abbreviations, and dynamic questions\n",
      "involving reasoning. In this paper, we built three different\n",
      "Retrieval-Augmented Generation (RAG) methods, including traditional RAG, SQL\n",
      "RAG, and Knowledge Graph-based RAG (Graph RAG). Experiments showed that\n",
      "traditional RAG achieved 84.84% accuracy using BM25 + GPT-4 but occasionally\n",
      "produced hallucinations, which is risky to airport safety. In contrast, SQL RAG\n",
      "and Graph RAG achieved 80.85% and 91.49% accuracy respectively, with\n",
      "significantly fewer hallucinations. Moreover, Graph RAG was especially\n",
      "effective for questions that involved reasoning. Based on our observations, we\n",
      "thus recommend SQL RAG and Graph RAG are better for airport environments, due\n",
      "to fewer hallucinations and the ability to handle dynamic questions.\n",
      "</Summary>\n",
      "\n",
      "<Content>\n",
      "arXiv:2505.13006v1  [cs.CL]  19 May 2025\n",
      "Evaluating the Performance of RAG Methods for Conversational AI in the\n",
      "Airport Domain\n",
      "Yuyang Li1, Philip J.M. Kerbusch2, Raimon H.R. Pruim2, Tobias KÃ¤fer1\n",
      "1Karlsruhe Institute of Technology, 2Royal Schiphol Group\n",
      "2Royal Schiphol Group, 1Karlsruhe Institute of Technology\n",
      "yuyang.li@kit.edu,\n",
      "tobias.kaefer@kit.edu\n",
      "Abstract\n",
      "Airports from the top 20 in terms of annual\n",
      "passengers are highly dynamic environments\n",
      "with thousands of flights daily, and they aim\n",
      "to increase the degree of automation. To con-\n",
      "tribute to this, we implemented a Conversa-\n",
      "tional AI system that enables staff in an air-\n",
      "port to communicate with flight information\n",
      "systems. This system not only answers stan-\n",
      "dard airport queries but also resolves airport ter-\n",
      "minology, jargon, abbreviations, and dynamic\n",
      "questions involving reasoning. In this paper, we\n",
      "built three different Retrieval-Augmented Gen-\n",
      "eration (RAG) methods, including traditional\n",
      "RAG, SQL RAG, and Knowledge Graph-based\n",
      "RAG (Graph RAG). Experiments showed that\n",
      "traditional RAG achieved 84.84% accuracy\n",
      "using BM25 + GPT-4 but occasionally pro-\n",
      "duced hallucinations, which is risky to airport\n",
      "safety. In contrast, SQL RAG and Graph RAG\n",
      "achieved 80.85% and 91.49% accuracy respec-\n",
      "tively, with significantly fewer hallucinations.\n",
      "Moreover, Graph RAG was especially effective\n",
      "for questions that involved reasoning. Based\n",
      "on our observations, we thus recommend SQL\n",
      "RAG and Graph RAG are better for airport en-\n",
      "vironments, due to fewer hallucinations and the\n",
      "ability to handle dynamic questions.\n",
      "1\n",
      "Introduction\n",
      "Amsterdam Airport Schiphol, one of the top 20\n",
      "airports in the world, ranked by annual passenger\n",
      "numbers, handles thousands of flights each day.\n",
      "These airports rely on staff like gate planners and\n",
      "apron controllers to access and update data across\n",
      "systems. For these employees, traditional database\n",
      "queries can be complex and time-consuming for\n",
      "some employees who are not query experts when\n",
      "they need flight information. A conversational AI\n",
      "system with a natural language query (NLQ) inter-\n",
      "face allows all employees to interact with systems\n",
      "naturally, asking questions like, â€œWhich fights are\n",
      "at ramp D07?â€ and receiving instant answers. This\n",
      "improves productivity, and streamlines workflows,\n",
      "especially in high-pressure areas like at the gate,\n",
      "where less educated workers require access to up-\n",
      "to-date information. By replacing strict query for-\n",
      "mats with intuitive, real-time responses, conversa-\n",
      "tional AI enhances decision-making and efficiency,\n",
      "making it a suitable solution for dynamic environ-\n",
      "ments such as airports.\n",
      "Building such a system is challenging because\n",
      "flight data is stored by experts in tables using avi-\n",
      "ation abbreviations. We need our system to un-\n",
      "derstand these datasets to answer questions from\n",
      "the airport domain. Additionally, ensuring avia-\n",
      "tion safety is a major concern; the system must\n",
      "be safe and enable employees to perform accurate\n",
      "operations. We address those challenges using two\n",
      "research questions.\n",
      "The first question is how to handle flight data so\n",
      "that our system can answer different questions. We\n",
      "divided the questions into three types:\n",
      "â€¢ Straightforward questions: Questions that\n",
      "can be directly answered from the flight data.\n",
      "â€¢ Questions involving specialized airport jar-\n",
      "gon, abbreviations, and incomplete queries:\n",
      "Operators often use shorthand or omit con-\n",
      "text. Flight â€œKL0123â€ might be referred to as\n",
      "â€œ0123â€ or â€œ123,â€ while gate â€œC05â€ might be\n",
      "shortened to â€œC5.â€ Abbreviations like â€œKLMâ€\n",
      "for â€œKLM Royal Dutch Airlinesâ€ or â€œDeltaâ€\n",
      "for â€œDelta Air Linesâ€ are also common. Op-\n",
      "erators frequently ask short, incomplete ques-\n",
      "tions, e. g., â€œWhich flights are at D04?â€ or\n",
      "â€œWhat is the gate for that Delta airline?â€ With-\n",
      "out resolving missing details such, these ques-\n",
      "tions cannot be answered.\n",
      "â€¢ Dynamic questions: Questions that involve\n",
      "additional calculations and reasoning, espe-\n",
      "cially related to time.\n",
      "Examples include\n",
      "â€œWhat is the connecting flightâ€™s onramp time\n",
      "for D\n",
      "</Content>\n",
      "</Document>\n",
      "\n",
      "---\n",
      "\n",
      "<Document source=\"http://arxiv.org/abs/2410.14167v1\" date=\"2024-10-18\" authors=\"Jiajing Chen, Runyuan Bao, Hongye Zheng, Zhen Qi, Jianjun Wei, Jiacheng Hu\"/>\n",
      "<Title>\n",
      "Optimizing Retrieval-Augmented Generation with Elasticsearch for Enhanced Question-Answering Systems\n",
      "</Title>\n",
      "\n",
      "<Summary>\n",
      "This study aims to improve the accuracy and quality of large-scale language\n",
      "models (LLMs) in answering questions by integrating Elasticsearch into the\n",
      "Retrieval Augmented Generation (RAG) framework. The experiment uses the\n",
      "Stanford Question Answering Dataset (SQuAD) version 2.0 as the test dataset and\n",
      "compares the performance of different retrieval methods, including traditional\n",
      "methods based on keyword matching or semantic similarity calculation, BM25-RAG\n",
      "and TF-IDF- RAG, and the newly proposed ES-RAG scheme. The results show that\n",
      "ES-RAG not only has obvious advantages in retrieval efficiency but also\n",
      "performs well in key indicators such as accuracy, which is 0.51 percentage\n",
      "points higher than TF-IDF-RAG. In addition, Elasticsearch's powerful search\n",
      "capabilities and rich configuration options enable the entire\n",
      "question-answering system to better handle complex queries and provide more\n",
      "flexible and efficient responses based on the diverse needs of users. Future\n",
      "research directions can further explore how to optimize the interaction\n",
      "mechanism between Elasticsearch and LLM, such as introducing higher-level\n",
      "semantic understanding and context-awareness capabilities, to achieve a more\n",
      "intelligent and humanized question-answering experience.\n",
      "</Summary>\n",
      "\n",
      "<Content>\n",
      "Optimizing Retrieval-Augmented Generation with\n",
      "Elasticsearch for Enhanced Question-Answering\n",
      "Systems\n",
      "Jiajing Chen\n",
      "New York University\n",
      "New York, USA\n",
      "Zhen Qi\n",
      "Northeastern University\n",
      "Boston, USA\n",
      "Runyuan Bao\n",
      "Johns Hopkins University\n",
      "Baltimore, USA\n",
      "Jianjun Wei\n",
      "Washington University in St. Louis\n",
      "St Louis, USA\n",
      "The Chinese University of Hong\n",
      "Kong\n",
      "Hong Kong, China\n",
      "Abstractâ€”This study aims to improve the accuracy and\n",
      "quality\n",
      "of\n",
      "large-scale\n",
      "language\n",
      "models\n",
      "(LLMs)\n",
      "in\n",
      "answering questions by integrating Elasticsearch into the\n",
      "Retrieval Augmented Generation (RAG) framework. The\n",
      "experiment uses the Stanford Question Answering Dataset\n",
      "(SQuAD) version 2.0 as the test dataset and compares the\n",
      "performance of different retrieval methods, including\n",
      "traditional\n",
      "methods\n",
      "based\n",
      "on\n",
      "keyword\n",
      "matching\n",
      "or\n",
      "semantic similarity calculation, BM25-RAG and TF-IDF-\n",
      "RAG, and the newly proposed ES-RAG scheme. The\n",
      "results show that ES-RAG not only has obvious advantages\n",
      "in retrieval efficiency but also performs well in key\n",
      "indicators such as accuracy, which is 0.51 percentage\n",
      "points\n",
      "higher\n",
      "than\n",
      "TF-IDF-RAG.\n",
      "In\n",
      "addition,\n",
      "Elasticsearch's\n",
      "powerful\n",
      "search\n",
      "capabilities\n",
      "and\n",
      "rich\n",
      "configuration options enable the entire question-answering\n",
      "system to better handle complex queries and provide more\n",
      "flexible and efficient responses based on the diverse needs\n",
      "of users. Future research directions can further explore\n",
      "how to optimize the interaction mechanism between\n",
      "Elasticsearch and LLM, such as introducing higher-level\n",
      "semantic\n",
      "understanding\n",
      "and\n",
      "context-awareness\n",
      "capabilities, to achieve a more intelligent and humanized\n",
      "question-answering experience.\n",
      "Keywords-Retrieval-enhanced generation, Elasticsearch,\n",
      "Large language models, Information retrieval\n",
      "I.\n",
      "INTRODUCTION\n",
      "In today's era of information explosion, efficiently extracting\n",
      "valuable information from massive data has become a crucial\n",
      "issue. With the continuous advancement of natural language\n",
      "processing technology, Large Language Models (LLMs) have\n",
      "shown unprecedented application potential in many fields with\n",
      "their powerful text understanding and generation capabilities\n",
      "[1]. However, although LLMs are able to answer questions or\n",
      "complete tasks based on their internally learned knowledge\n",
      "base, they often have limitations in mastering the latest or\n",
      "specific domain knowledge [2]. To solve this problem,\n",
      "researchers\n",
      "proposed\n",
      "a\n",
      "method\n",
      "that\n",
      "combines\n",
      "retrieval-\n",
      "augmented\n",
      "generation\n",
      "(RAG),\n",
      "which\n",
      "combines\n",
      "external\n",
      "document retrieval with LLM so that the model can take\n",
      "advantage of more information when generating responses. An\n",
      "extensive and up-to-date information source significantly\n",
      "improves the quality and accuracy of answers [3]. The core\n",
      "idea of RAG is that when faced with a query, the retrieval\n",
      "system is first used to find the most relevant documents from a\n",
      "large-scale document collection, and then these documents are\n",
      "provided to LLM as context to help it better understand the\n",
      "background of the problem. And generate more accurate\n",
      "answers accordingly [4].\n",
      "The traditional RAG framework usually uses a method based\n",
      "on keyword matching or semantic similarity calculation to\n",
      "implement the document retrieval process. Although this\n",
      "method has achieved certain success, it has difficulty in\n",
      "processing complex queries, improving the recall rate, and\n",
      "ensuring the diversity of retrieval results. There are still certain\n",
      "limitations. To this end, this article proposes an innovative\n",
      "improvement planâ€”that is, using Elasticsearch (ES) as the core\n",
      "search engine in RAG. Elasticsearch is an open-source\n",
      "distributed search and analysis engine. It not only supports full-\n",
      "text search but also provides rich aggregation functions to\n",
      "quickly and accurately index and retrieve unstructured data.\n",
      "Compared with traditional methods, using Elasticsearch can\n",
      "define query conditions more flexibly, support multiple types\n",
      "of field types such as text, numbers, etc., and have powerful\n",
      "sorting\n",
      "capabilities\n",
      "and\n",
      "efficient\n",
      "performance,\n",
      "which\n",
      "is\n",
      "important for improving the overall\n",
      "</Content>\n",
      "</Document>\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36msearch_web\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "<Document href=\"https://rabiloo.com/blog/the-3-types-of-rag-models-naive-rag-modular-rag-and-advanced-rag\"/>\n",
      "Building AI systems requires choosing the right tools for the job, and Retrieval-Augmented Generation (RAG) offers various models, each designed to serve different needs. Essentially, a RAG model retrieves information from external databases, knowledge bases, or the web and then uses that data to generate relevant, meaningful responses. Naive RAG begins by indexing the data source for quick retrieval of relevant information. Modular RAG builds on the basic principles of Naive RAG by breaking down the retrieval and generation processes into separate, specialized modules. Unlike Naive RAG or Modular RAG, this stage is more precise and contextually aware, ensuring that the system retrieves only the most relevant and contextually appropriate data. The generation module in Advanced RAG then processes the refined data using sophisticated models like T5 or GPT-3.\n",
      "</Document>\n",
      "\n",
      "---\n",
      "\n",
      "<Document href=\"https://adasci.org/how-does-modular-rag-improve-upon-naive-rag/\"/>\n",
      "Naive RAG, the initial implementation of Retrieval-Augmented Generation, operates on a straightforward principle: retrieve relevant documents from an external knowledge base and use these documents to inform the generative process. The retrieval process in Naive RAG is relatively static and lacks flexibility, often leading to inefficiencies and suboptimal integration with the generative model. By adopting a modular architecture, this approach addresses the limitations of Naive RAG, offering enhanced flexibility, scalability, and efficiency. Unlike Naive RAG, which operates as a monolithic entity, Modular RAG breaks down the retrieval and generation processes into distinct, interchangeable modules. * *Seamless Integration*: Generative models in Modular RAG are designed to seamlessly integrate with various retrieval modules, enhancing the coherence and relevance of generated responses.\n",
      "</Document>\n",
      "\n",
      "---\n",
      "\n",
      "<Document href=\"https://medium.com/@drjulija/what-are-naive-rag-advanced-rag-modular-rag-paradigms-edff410c202e\"/>\n",
      "I share the differences between Naive RAG, Advanced RAG and Modular RAG frameworks. Retrieval-Augmented Generation (RAG) framework is designed to help mitigate these challenges. RAG enhances LLMsâ€™ knowledge base with additional, domain-specific data. RAG systems can be split into 3 categories: See the comparison between all three paradigms of RAG â€” Naive RAG, Advanced RAG and Modular RAG below.\n",
      "</Document>\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36manswer_question\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: expert\n",
      "\n",
      "Certainly, let's delve into the distinctions between Modular RAG and Naive RAG, particularly focusing on their applications and benefits in production environments.\n",
      "\n",
      "Naive RAG combines information retrieval with natural language generation to answer queries or prompts. It typically uses retrieval models that rank indexed data based on relevance to the input query. This approach generates responses by combining the input query with the retrieved context, aiming for coherence and relevance [3].\n",
      "\n",
      "In contrast, Modular RAG builds upon the basic principles of Naive RAG. It decomposes the retrieval and generation processes into separate, specialized modules. This separation allows each module to be optimized independently, offering greater control over each component of the RAG process. This modular structure enhances the precision and contextual awareness of the system, ensuring that the retrieved data is not only relevant but also contextually appropriate. The specialized design of each module allows for integration with sophisticated models, such as T5 or GPT-3, in the generation phase [4][5].\n",
      "\n",
      "**Benefits of Modular RAG in Production:**\n",
      "\n",
      "1. **Improved Precision and Contextual Relevance:**\n",
      "   - Modular RAG's separate modules allow for more precise retrieval, ensuring that only the most relevant and contextually suitable data is drawn for generation. This reduces the noise and potential inaccuracies that can arise from a single, monolithic approach [4].\n",
      "\n",
      "2. **Scalability and Flexibility:**\n",
      "   - The modular nature means that components can be scaled or modified independently without affecting the entire system. This flexibility is particularly beneficial in dynamic production environments where specific tasks may require different handling or specialized models [4][5].\n",
      "\n",
      "3. **Enhanced Efficiency:**\n",
      "   - By optimizing each module independently, Modular RAG can achieve efficiencies that aren't possible with Naive RAG. This results in reduced latency and improved response times, which are critical in real-world applications where speed and reliability are paramount [4].\n",
      "\n",
      "In summary, while Naive RAG offers a simpler, integrated approach to combining retrieval and generation, Modular RAG provides a more refined, flexible framework that improves accuracy, efficiency, and contextual relevanceâ€”key factors that enhance performance in production settings.\n",
      "\n",
      "Sources:\n",
      "[3] https://www.superteams.ai/blog/how-to-implement-naive-rag-advanced-rag-and-modular-rag\n",
      "[4] https://rabiloo.com/blog/the-3-types-of-rag-models-naive-rag-modular-rag-and-advanced-rag\n",
      "[5] https://medium.com/@drjulija/what-are-naive-rag-advanced-rag-modular-rag-paradigms-edff410c202e\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36manswer_question\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: expert\n",
      "\n",
      "Certainly, Prof. Hinton. Modular RAG differentiates from the traditional Naive RAG mainly in its architecture and approach to the retrieval and generation processes.\n",
      "\n",
      "1. **Architecture**: Naive RAG operates on a straightforward \"retrieve-then-generate\" model, where the retrieval process is relatively static and lacks flexibility. It functions as a single, monolithic entity [5]. In contrast, Modular RAG decomposes the retrieval and generation into independent modules and specialized operators, promoting a highly reconfigurable and scalable framework [1][4]. This modular approach allows for seamless integration of various retrieval techniques with generative models, enhancing the coherence and relevance of generated responses [5].\n",
      "\n",
      "2. **Flexibility and Scalability**: The modular nature of Modular RAG allows for interchangeable modules that can be independently updated or replaced, enabling easier adaptation to specific task requirements and technological advancements. This is in contrast with Naive RAG which is less adaptable to evolving needs [5].\n",
      "\n",
      "3. **Efficiency**: Modular RAG introduces advanced design elements like routing, scheduling, and fusion mechanisms. These elements optimize the flow of information and improve system efficiency, especially when handling complex queries, compared to the linear architecture of Naive RAG [1].\n",
      "\n",
      "The benefits of using Modular RAG in production environments include:\n",
      "\n",
      "- **Increased Efficiency**: By supporting routing, scheduling, and fusion mechanisms, Modular RAG can handle complex and diverse queries efficiently, thus improving the performance on knowledge-intensive tasks [1].\n",
      "\n",
      "- **Scalability**: The ability to reconfigure modules allows enterprises to scale the system up or down based on their computational and task-specific demands without overhauling the entire system [1].\n",
      "\n",
      "- **Enhanced Integration**: Modular RAG's design facilitates better integration with different retrieval mechanisms and LLMs, leading to more accurate and contextually aware responses [4][5].\n",
      "\n",
      "In sum, the Modular RAG framework offers a comprehensive roadmap for advancing RAG systems in a flexible, efficient, and scalable manner, making it well-suited for deployment in dynamic production environments.\n",
      "\n",
      "**Sources:**\n",
      "\n",
      "[1] http://arxiv.org/abs/2407.21059v1\n",
      "[4] https://rabiloo.com/blog/the-3-types-of-rag-models-naive-rag-modular-rag-and-advanced-rag\n",
      "[5] https://adasci.org/how-does-modular-rag-improve-upon-naive-rag/\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mask_question\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "That's a comprehensive overview! Iâ€™m curious, could you provide a specific example or case where implementing Modular RAG in a production environment significantly reduced latency compared to Naive RAG? How was the real-world reliability enhanced in this case?\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mask_question\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Thank you for the detailed explanation, Prof. Hinton. Itâ€™s fascinating how Modular RAGâ€™s flexibility and efficiency potentially transform production applications. Could you provide a specific example or a case study of a production environment where Modular RAG has notably improved performance over Naive RAG?\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36manswer_question\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: expert\n",
      "\n",
      "Certainly, Dr. Torres. Modular RAG (Retrieval-Augmented Generation) diverges from the traditional Naive RAG approach by breaking down the retrieval and generation processes into separate, specialized modules. This design promotes scalability and efficiency, especially when dealing with large datasets.\n",
      "\n",
      "**Differences between Modular RAG and Naive RAG:**\n",
      "\n",
      "1. **Architecture**: \n",
      "   - **Naive RAG** operates as a single monolithic system where retrieval and generation processes are intertwined, potentially leading to inefficiencies when scaling [2].\n",
      "   - **Modular RAG**, on the other hand, divides these tasks into dedicated modules. This separation allows each module to be optimized individually, improving system performance and accuracy [3].\n",
      "\n",
      "2. **Scalability**:\n",
      "   - The modular approach enhances scalability because it can efficiently manage larger datasets by leveraging specialized units that focus on specific tasks. This is particularly beneficial in environments requiring the integration of diverse, extensive, or dynamic data sources [3].\n",
      "   \n",
      "3. **Adaptability**:\n",
      "   - Modular RAG is more adaptable to production-level scenarios due to its flexibility in module management. It can easily incorporate or update modules without overhauling the entire system, unlike Naive RAG, which might require significant restructuring when upgrading components or adding new functionalities [4].\n",
      "\n",
      "**Benefits of Using Modular RAG at the Production Level:**\n",
      "\n",
      "- **Enhanced Query Efficiency**: By delegating specific queries to modules optimized for different data sources (relational, NoSQL, etc.), Modular RAG reduces token overhead and processing latency, thereby speeding up response times and improving accuracy [3].\n",
      "- **Robustness and Flexibility**: Its architecture allows for easy integration of new data sources or adjustment to changing data types, making it highly suitable for dynamic production environments such as those involving real-time data updates [3].\n",
      "- **Improved Contextual Awareness**: Modular RAG can achieve better semantic depth by refining the retrieval process to ensure only the most pertinent and contextually appropriate data is processed [2].\n",
      "\n",
      "In summary, by structuring retrieval and generation into distinct, specialized tasks, Modular RAG offers significant scalability benefits, making it well-suited for complex, large-scale production environments. This modularity not only enhances performance but also facilitates adaptability to ever-changing organizational data needs [3].\n",
      "\n",
      "**Sources**:\n",
      "- [2] https://rabiloo.com/blog/the-3-types-of-rag-models-naive-rag-modular-rag-and-advanced-rag\n",
      "- [3] http://arxiv.org/abs/2412.05838v1\n",
      "- [4] https://www.digitalocean.com/community/conceptual-articles/rag-ai-agents-agentic-rag-comparative-analysis\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36msearch_arxiv\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "<Document source=\"http://arxiv.org/abs/2408.05933v1\" date=\"2024-08-12\" authors=\"Fei Liu, Zejun Kang, Xing Han\"/>\n",
      "<Title>\n",
      "Optimizing RAG Techniques for Automotive Industry PDF Chatbots: A Case Study with Locally Deployed Ollama Models\n",
      "</Title>\n",
      "\n",
      "<Summary>\n",
      "With the growing demand for offline PDF chatbots in automotive industrial\n",
      "production environments, optimizing the deployment of large language models\n",
      "(LLMs) in local, low-performance settings has become increasingly important.\n",
      "This study focuses on enhancing Retrieval-Augmented Generation (RAG) techniques\n",
      "for processing complex automotive industry documents using locally deployed\n",
      "Ollama models. Based on the Langchain framework, we propose a multi-dimensional\n",
      "optimization approach for Ollama's local RAG implementation. Our method\n",
      "addresses key challenges in automotive document processing, including\n",
      "multi-column layouts and technical specifications. We introduce improvements in\n",
      "PDF processing, retrieval mechanisms, and context compression, tailored to the\n",
      "unique characteristics of automotive industry documents. Additionally, we\n",
      "design custom classes supporting embedding pipelines and an agent supporting\n",
      "self-RAG based on LangGraph best practices. To evaluate our approach, we\n",
      "constructed a proprietary dataset comprising typical automotive industry\n",
      "documents, including technical reports and corporate regulations. We compared\n",
      "our optimized RAG model and self-RAG agent against a naive RAG baseline across\n",
      "three datasets: our automotive industry dataset, QReCC, and CoQA. Results\n",
      "demonstrate significant improvements in context precision, context recall,\n",
      "answer relevancy, and faithfulness, with particularly notable performance on\n",
      "the automotive industry dataset. Our optimization scheme provides an effective\n",
      "solution for deploying local RAG systems in the automotive sector, addressing\n",
      "the specific needs of PDF chatbots in industrial production environments. This\n",
      "research has important implications for advancing information processing and\n",
      "intelligent production in the automotive industry.\n",
      "</Summary>\n",
      "\n",
      "<Content>\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Optimizing RAG Techniques for Automotive Industry PDF Chatbots: A Case Study with \n",
      "Locally Deployed Ollama Models \n",
      "Optimizing RAG Techniques Based on Locally Deployed Ollama Models \n",
      "A Case Study with Locally Deployed Ollama Models \n",
      "Fei Liu * \n",
      "China Automotive Technology & Research Center, liufei@catarc.ac.cn \n",
      "Zejun Kang \n",
      "China Automotive Technology & Research Center, kangzejun@catarc.ac.cn \n",
      "Xing Han \n",
      "China Automotive Technology & Research Center, hanxing@catarc.ac.cn \n",
      "With the growing demand for offline PDF chatbots in automotive industrial production environments, optimizing the deployment \n",
      "of large language models (LLMs) in local, low-performance settings has become increasingly important. This study focuses on \n",
      "enhancing Retrieval-Augmented Generation (RAG) techniques for processing complex automotive industry documents using \n",
      "locally deployed Ollama models. \n",
      "Based on the Langchain framework, we propose a multi-dimensional optimization approach for Ollama's local RAG \n",
      "implementation. Our method addresses key challenges in automotive document processing, including multi-column layouts and \n",
      "technical specifications. We introduce improvements in PDF processing, retrieval mechanisms, and context compression, tailored \n",
      "to the unique characteristics of automotive industry documents. Additionally, we design custom classes supporting embedding \n",
      "pipelines and an agent supporting self-RAG based on LangGraph best practices. \n",
      "To evaluate our approach, we constructed a proprietary dataset comprising typical automotive industry documents, including \n",
      "technical reports and corporate regulations. We compared our optimized RAG model and self-RAG agent against a naive RAG \n",
      "baseline across three datasets: our automotive industry dataset, QReCC, and CoQA. Results demonstrate significant improvements \n",
      "in context precision, context recall, answer relevancy, and faithfulness, with particularly notable performance on the automotive \n",
      "industry dataset. \n",
      "Our optimization scheme provides an effective solution for deploying local RAG systems in the automotive sector, addressing the \n",
      "specific needs of PDF chatbots in industrial production environments. This research has important implications for advancing \n",
      "information processing and intelligent production in the automotive industry. \n",
      " \n",
      "* Place the footnote text for the author (if applicable) here.  \n",
      "CCS CONCEPTS â€¢ Computing methodologies â€¢ Artificial intelligence â€¢ Natural language processing â€¢ Natural language \n",
      "generation \n",
      " \n",
      "Additional Keywords and Phrases: Automotive Industry, Langchain, self-rag, PDF Processing, RAG, Ollama \n",
      "1 INTRODUCTION \n",
      "1.1 Research Background \n",
      "The automotive industry is undergoing a significant digital transformation, with an increasing reliance on complex \n",
      "technical documentation for various processes [1]. This shift encompasses design, manufacturing, and quality \n",
      "control, all of which now heavily depend on efficient information management systems [2]. The growing volume of \n",
      "technical documents, often in PDF format, has created a pressing need for advanced information retrieval and \n",
      "question-answering capabilities in industrial settings [3]. \n",
      "Large Language Models (LLMs) have emerged as powerful tools in natural language processing, demonstrating \n",
      "remarkable abilities in tasks such as document understanding and question answering [4]. These models have \n",
      "shown potential in handling the complex, domain-specific language often found in automotive documentation. \n",
      "However, the application of LLMs in industrial environments presents unique challenges, particularly in terms of \n",
      "computational resources and data privacy [5]. \n",
      "Among the various techniques developed to enhance LLM performance, Retrieval-Augmented Generation (RAG) \n",
      "has gained significant attention [6]. RAG combines the generative capabilities of LLMs with external knowledge \n",
      "retrieval, allowing for more accurate and contextually relevant responses. This approach, initially proposed by \n",
      "Lew\n",
      "</Content>\n",
      "</Document>\n",
      "\n",
      "---\n",
      "\n",
      "<Document source=\"http://arxiv.org/abs/2507.18515v1\" date=\"2025-07-24\" authors=\"Zezhou Yang, Ting Peng, Cuiyun Gao, Chaozheng Wang, Hailiang Huang, Yuetang Deng\"/>\n",
      "<Title>\n",
      "A Deep Dive into Retrieval-Augmented Generation for Code Completion: Experience on WeChat\n",
      "</Title>\n",
      "\n",
      "<Summary>\n",
      "Code completion, a crucial task in software engineering that enhances\n",
      "developer productivity, has seen substantial improvements with the rapid\n",
      "advancement of large language models (LLMs). In recent years,\n",
      "retrieval-augmented generation (RAG) has emerged as a promising method to\n",
      "enhance the code completion capabilities of LLMs, which leverages relevant\n",
      "context from codebases without requiring model retraining. While existing\n",
      "studies have demonstrated the effectiveness of RAG on public repositories and\n",
      "benchmarks, the potential distribution shift between open-source and\n",
      "closed-source codebases presents unique challenges that remain unexplored. To\n",
      "mitigate the gap, we conduct an empirical study to investigate the performance\n",
      "of widely-used RAG methods for code completion in the industrial-scale codebase\n",
      "of WeChat, one of the largest proprietary software systems. Specifically, we\n",
      "extensively explore two main types of RAG methods, namely identifier-based RAG\n",
      "and similarity-based RAG, across 26 open-source LLMs ranging from 0.5B to 671B\n",
      "parameters. For a more comprehensive analysis, we employ different retrieval\n",
      "techniques for similarity-based RAG, including lexical and semantic retrieval.\n",
      "Based on 1,669 internal repositories, we achieve several key findings: (1) both\n",
      "RAG methods demonstrate effectiveness in closed-source repositories, with\n",
      "similarity-based RAG showing superior performance, (2) the effectiveness of\n",
      "similarity-based RAG improves with more advanced retrieval techniques, where\n",
      "BM25 (lexical retrieval) and GTE-Qwen (semantic retrieval) achieve superior\n",
      "performance, and (3) the combination of lexical and semantic retrieval\n",
      "techniques yields optimal results, demonstrating complementary strengths.\n",
      "Furthermore, we conduct a developer survey to validate the practical utility of\n",
      "RAG methods in real-world development environments.\n",
      "</Summary>\n",
      "\n",
      "<Content>\n",
      "A Deep Dive into Retrieval-Augmented Generation\n",
      "for Code Completion: Experience on WeChat\n",
      "Zezhou Yang\n",
      "Tencent\n",
      "Guangzhou, China\n",
      "zezhouyang@tencent.com\n",
      "Ting Peng\n",
      "Tencent\n",
      "Guangzhou, China\n",
      "sakurapeng@tencent.com\n",
      "Cuiyun Gaoâˆ—\n",
      "The Chinese University of Hong Kong\n",
      "Hong Kong, China\n",
      "cuiyungao@outlook.com\n",
      "Chaozheng Wang\n",
      "The Chinese University of Hong Kong\n",
      "Hong Kong, China\n",
      "adf111178@gmail.com\n",
      "Hailiang Huang\n",
      "Tencent\n",
      "Guangzhou, China\n",
      "eraserhuang@tencent.com\n",
      "Yuetang Deng\n",
      "Tencent\n",
      "Guangzhou, China\n",
      "yuetangdeng@tencent.com\n",
      "Abstractâ€”Code completion, a crucial task in software engi-\n",
      "neering that enhances developer productivity, has seen substantial\n",
      "improvements with the rapid advancement of large language\n",
      "models (LLMs). In recent years, retrieval-augmented generation\n",
      "(RAG) has emerged as a promising method to enhance the code\n",
      "completion capabilities of LLMs, which leverages relevant con-\n",
      "text from codebases without requiring model retraining. While\n",
      "existing studies have demonstrated the effectiveness of RAG on\n",
      "public repositories and benchmarks, the potential distribution\n",
      "shift between open-source and closed-source codebases presents\n",
      "unique challenges that remain unexplored. To mitigate the gap,\n",
      "we conduct an empirical study to investigate the performance of\n",
      "widely-used RAG methods for code completion in the industrial-\n",
      "scale codebase of WeChat, one of the largest proprietary software\n",
      "systems. Specifically, we extensively explore two main types\n",
      "of RAG methods, namely identifier-based RAG and similarity-\n",
      "based RAG, across 26 open-source LLMs ranging from 0.5B\n",
      "to 671B parameters. For a more comprehensive analysis, we\n",
      "employ different retrieval techniques for similarity-based RAG,\n",
      "including lexical and semantic retrieval. Based on 1,669 internal\n",
      "repositories, we achieve several key findings: (1) both RAG\n",
      "methods demonstrate effectiveness in closed-source repositories,\n",
      "with similarity-based RAG showing superior performance, (2)\n",
      "the effectiveness of similarity-based RAG improves with more\n",
      "advanced retrieval techniques, where BM25 (lexical retrieval)\n",
      "and GTE-Qwen (semantic retrieval) achieve superior perfor-\n",
      "mance, and (3) the combination of lexical and semantic retrieval\n",
      "techniques yields optimal results, demonstrating complemen-\n",
      "tary strengths. Furthermore, we conduct a developer survey\n",
      "to validate the practical utility of RAG methods in real-world\n",
      "development environments.\n",
      "Index Termsâ€”large language model, retrieval-augmented gen-\n",
      "eration, code completion\n",
      "I. INTRODUCTION\n",
      "Code completion, which automatically predicts and sug-\n",
      "gests code fragments based on the surrounding programming\n",
      "context, has evolved from simple token-level suggestions to\n",
      "âˆ—Cuiyun Gao is the corresponding author.\n",
      "This research is supported by National Key R&D Program of China\n",
      "(No. 2022YFB3103900), National Natural Science Foundation of China\n",
      "under project (No. 62472126), Natural Science Foundation of Guangdong\n",
      "Province (Project No. 2023A1515011959), Shenzhen-Hong Kong Jointly\n",
      "Funded Project (Category A, No. SGDX20230116091246007), and Shenzhen\n",
      "Basic Research (General Project No. JCYJ20220531095214031).\n",
      "generating entire code blocks [1], [2]. Studies have demon-\n",
      "strated that code completion tools substantially enhance de-\n",
      "veloper productivity in real-world software development [3],\n",
      "[4]. Notably, 87% of professional developers report significant\n",
      "improvements in their coding efficiency when utilizing code\n",
      "completion tools in industrial settings [5]. Recent advances\n",
      "in large language models (LLMs) have further transformed\n",
      "various software engineering tasks [6]â€“[10], demonstrating\n",
      "unprecedented capabilities in code understanding and gen-\n",
      "eration. These models have achieved particularly impressive\n",
      "performance in code completion tasks [11]â€“[13].\n",
      "To enhance LLMsâ€™ performance on domain-specific tasks,\n",
      "researchers have explored Retrieval-Augmented Generation\n",
      "(RAG), which augments model inference by retrieving and in-\n",
      "corporating relevant context from the target codebase without\n",
      "requiring param\n",
      "</Content>\n",
      "</Document>\n",
      "\n",
      "---\n",
      "\n",
      "<Document source=\"http://arxiv.org/abs/2506.03401v1\" date=\"2025-06-03\" authors=\"Xiwei Xu, Hans Weytjens, Dawen Zhang, Qinghua Lu, Ingo Weber, Liming Zhu\"/>\n",
      "<Title>\n",
      "RAGOps: Operating and Managing Retrieval-Augmented Generation Pipelines\n",
      "</Title>\n",
      "\n",
      "<Summary>\n",
      "Recent studies show that 60% of LLM-based compound systems in enterprise\n",
      "environments leverage some form of retrieval-augmented generation (RAG), which\n",
      "enhances the relevance and accuracy of LLM (or other genAI) outputs by\n",
      "retrieving relevant information from external data sources. LLMOps involves the\n",
      "practices and techniques for managing the lifecycle and operations of LLM\n",
      "compound systems in production environments. It supports enhancing LLM systems\n",
      "through continuous operations and feedback evaluation. RAGOps extends LLMOps by\n",
      "incorporating a strong focus on data management to address the continuous\n",
      "changes in external data sources. This necessitates automated methods for\n",
      "evaluating and testing data operations, enhancing retrieval relevance and\n",
      "generation quality. In this paper, we (1) characterize the generic architecture\n",
      "of RAG applications based on the 4+1 model view for describing software\n",
      "architectures, (2) outline the lifecycle of RAG systems, which integrates the\n",
      "management lifecycles of both the LLM and the data, (3) define the key design\n",
      "considerations of RAGOps across different stages of the RAG lifecycle and\n",
      "quality trade-off analyses, (4) highlight the overarching research challenges\n",
      "around RAGOps, and (5) present two use cases of RAG applications and the\n",
      "corresponding RAGOps considerations.\n",
      "</Summary>\n",
      "\n",
      "<Content>\n",
      "arXiv:2506.03401v1  [cs.SE]  3 Jun 2025\n",
      "RAGOps: Operating and Managing Retrieval-Augmented Generation Pipelines\n",
      "Xiwei Xua,c, Hans Weytjensb, Dawen Zhanga, Qinghua Lua,c, Ingo Weberb,d, Liming Zhua,c\n",
      "aCSIROâ€™s Data61, Australia\n",
      "bTechnical University of Munich, School of CIT, Germany\n",
      "cUniversity of New South Wales, School of Computer Science and Engineering, Australia\n",
      "dFraunhofer Society, Munich, Germany\n",
      "Abstract\n",
      "Recent studies show that 60% of LLM-based compound systems in enterprise environments leverage some form\n",
      "of retrieval-augmented generation (RAG), which enhances the relevance and accuracy of LLM (or other genAI) out-\n",
      "puts by retrieving relevant information from external data sources. LLMOps involves the practices and techniques for\n",
      "managing the lifecycle and operations of LLM compound systems in production environments. It supports enhancing\n",
      "LLM systems through continuous operations and feedback evaluation. RAGOps extends LLMOps by incorporating\n",
      "a strong focus on data management to address the continuous changes in external data sources. This necessitates\n",
      "automated methods for evaluating and testing data operations, enhancing retrieval relevance and generation quality.\n",
      "In this paper, we (1) characterize the generic architecture of RAG applications based on the 4+1 model view for de-\n",
      "scribing software architectures, (2) outline the lifecycle of RAG systems, which integrates the management lifecycles\n",
      "of both the LLM and the data, (3) define the key design considerations of RAGOps across different stages of the RAG\n",
      "lifecycle and quality trade-off analyses, (4) highlight the overarching research challenges around RAGOps, and (5)\n",
      "present two use cases of RAG applications and the corresponding RAGOps considerations.\n",
      "Keywords: LLM, LLMOps, RAG, RAGOps\n",
      "1. Introduction\n",
      "Large Language Models (LLMs) can be instructed through prompting to perform a wide range of tasks, such as\n",
      "programming and translation. A notable trend in their application is the integration of LLMs into compound software\n",
      "systems, which consist of multiple components beyond the core language model [1]. Compound LLM systems can\n",
      "perform dynamic behaviors, whereas LLMs by themselves are inherently constrained by their reliance on static train-\n",
      "ing on datasets from some point in time, resulting in fixed parametric knowledge and limited grounding in specific\n",
      "contexts in which the systems are used, such as a given organization. In enterprise settings, 60% of the LLM com-\n",
      "pound systems incorporate some form of retrieval-augmented generation [2] (RAG), which improves the relevance,\n",
      "accuracy, and dynamism of LLM outputs by retrieving information from external data. RAG offers a solution to\n",
      "common challenges faced by LLMs, such as hallucinations, outdated data, and the difficulty of removing parametric\n",
      "knowledge, and open up the possibility to access proprietary, internal data from organizations using existing LLMs.\n",
      "By integrating real-time information retrieval, RAG enables continuous updates, possibly incorporating very recent\n",
      "information. RAG systems are compound systems that consist of multiple components blending the LLMâ€™s parametric\n",
      "knowledge with external data retrieval, including, but not limited to retrieval sources, retriever, and generator [3].\n",
      "LLMOps refers to the practices and techniques used to manage the lifecycle and operation of LLM and LLM com-\n",
      "pound systems in production environments. The current state of LLMOps includes a variety of automated tools123\n",
      "designed to observe, monitor, optimize, and scale LLM applications. The functionality of these tools includes but\n",
      "is not limited to, model versioning, performance monitoring, continuous retraining, inference optimization, and in-\n",
      "frastructure scaling to accommodate fluctuating demand. Current LLMOps tools predominantly focus on model and\n",
      "1â€œManaged MLflow,â€ Databricks, accessed 5 April 2025, https://www.databricks.com/product/managed-mlflow\n",
      "2â€œIntelligent Observability,â€ New Relic, accessed 5 April 2025, http\n",
      "</Content>\n",
      "</Document>\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36msearch_arxiv\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "<Document source=\"http://arxiv.org/abs/2408.05933v1\" date=\"2024-08-12\" authors=\"Fei Liu, Zejun Kang, Xing Han\"/>\n",
      "<Title>\n",
      "Optimizing RAG Techniques for Automotive Industry PDF Chatbots: A Case Study with Locally Deployed Ollama Models\n",
      "</Title>\n",
      "\n",
      "<Summary>\n",
      "With the growing demand for offline PDF chatbots in automotive industrial\n",
      "production environments, optimizing the deployment of large language models\n",
      "(LLMs) in local, low-performance settings has become increasingly important.\n",
      "This study focuses on enhancing Retrieval-Augmented Generation (RAG) techniques\n",
      "for processing complex automotive industry documents using locally deployed\n",
      "Ollama models. Based on the Langchain framework, we propose a multi-dimensional\n",
      "optimization approach for Ollama's local RAG implementation. Our method\n",
      "addresses key challenges in automotive document processing, including\n",
      "multi-column layouts and technical specifications. We introduce improvements in\n",
      "PDF processing, retrieval mechanisms, and context compression, tailored to the\n",
      "unique characteristics of automotive industry documents. Additionally, we\n",
      "design custom classes supporting embedding pipelines and an agent supporting\n",
      "self-RAG based on LangGraph best practices. To evaluate our approach, we\n",
      "constructed a proprietary dataset comprising typical automotive industry\n",
      "documents, including technical reports and corporate regulations. We compared\n",
      "our optimized RAG model and self-RAG agent against a naive RAG baseline across\n",
      "three datasets: our automotive industry dataset, QReCC, and CoQA. Results\n",
      "demonstrate significant improvements in context precision, context recall,\n",
      "answer relevancy, and faithfulness, with particularly notable performance on\n",
      "the automotive industry dataset. Our optimization scheme provides an effective\n",
      "solution for deploying local RAG systems in the automotive sector, addressing\n",
      "the specific needs of PDF chatbots in industrial production environments. This\n",
      "research has important implications for advancing information processing and\n",
      "intelligent production in the automotive industry.\n",
      "</Summary>\n",
      "\n",
      "<Content>\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Optimizing RAG Techniques for Automotive Industry PDF Chatbots: A Case Study with \n",
      "Locally Deployed Ollama Models \n",
      "Optimizing RAG Techniques Based on Locally Deployed Ollama Models \n",
      "A Case Study with Locally Deployed Ollama Models \n",
      "Fei Liu * \n",
      "China Automotive Technology & Research Center, liufei@catarc.ac.cn \n",
      "Zejun Kang \n",
      "China Automotive Technology & Research Center, kangzejun@catarc.ac.cn \n",
      "Xing Han \n",
      "China Automotive Technology & Research Center, hanxing@catarc.ac.cn \n",
      "With the growing demand for offline PDF chatbots in automotive industrial production environments, optimizing the deployment \n",
      "of large language models (LLMs) in local, low-performance settings has become increasingly important. This study focuses on \n",
      "enhancing Retrieval-Augmented Generation (RAG) techniques for processing complex automotive industry documents using \n",
      "locally deployed Ollama models. \n",
      "Based on the Langchain framework, we propose a multi-dimensional optimization approach for Ollama's local RAG \n",
      "implementation. Our method addresses key challenges in automotive document processing, including multi-column layouts and \n",
      "technical specifications. We introduce improvements in PDF processing, retrieval mechanisms, and context compression, tailored \n",
      "to the unique characteristics of automotive industry documents. Additionally, we design custom classes supporting embedding \n",
      "pipelines and an agent supporting self-RAG based on LangGraph best practices. \n",
      "To evaluate our approach, we constructed a proprietary dataset comprising typical automotive industry documents, including \n",
      "technical reports and corporate regulations. We compared our optimized RAG model and self-RAG agent against a naive RAG \n",
      "baseline across three datasets: our automotive industry dataset, QReCC, and CoQA. Results demonstrate significant improvements \n",
      "in context precision, context recall, answer relevancy, and faithfulness, with particularly notable performance on the automotive \n",
      "industry dataset. \n",
      "Our optimization scheme provides an effective solution for deploying local RAG systems in the automotive sector, addressing the \n",
      "specific needs of PDF chatbots in industrial production environments. This research has important implications for advancing \n",
      "information processing and intelligent production in the automotive industry. \n",
      " \n",
      "* Place the footnote text for the author (if applicable) here.  \n",
      "CCS CONCEPTS â€¢ Computing methodologies â€¢ Artificial intelligence â€¢ Natural language processing â€¢ Natural language \n",
      "generation \n",
      " \n",
      "Additional Keywords and Phrases: Automotive Industry, Langchain, self-rag, PDF Processing, RAG, Ollama \n",
      "1 INTRODUCTION \n",
      "1.1 Research Background \n",
      "The automotive industry is undergoing a significant digital transformation, with an increasing reliance on complex \n",
      "technical documentation for various processes [1]. This shift encompasses design, manufacturing, and quality \n",
      "control, all of which now heavily depend on efficient information management systems [2]. The growing volume of \n",
      "technical documents, often in PDF format, has created a pressing need for advanced information retrieval and \n",
      "question-answering capabilities in industrial settings [3]. \n",
      "Large Language Models (LLMs) have emerged as powerful tools in natural language processing, demonstrating \n",
      "remarkable abilities in tasks such as document understanding and question answering [4]. These models have \n",
      "shown potential in handling the complex, domain-specific language often found in automotive documentation. \n",
      "However, the application of LLMs in industrial environments presents unique challenges, particularly in terms of \n",
      "computational resources and data privacy [5]. \n",
      "Among the various techniques developed to enhance LLM performance, Retrieval-Augmented Generation (RAG) \n",
      "has gained significant attention [6]. RAG combines the generative capabilities of LLMs with external knowledge \n",
      "retrieval, allowing for more accurate and contextually relevant responses. This approach, initially proposed by \n",
      "Lew\n",
      "</Content>\n",
      "</Document>\n",
      "\n",
      "---\n",
      "\n",
      "<Document source=\"http://arxiv.org/abs/2507.18515v1\" date=\"2025-07-24\" authors=\"Zezhou Yang, Ting Peng, Cuiyun Gao, Chaozheng Wang, Hailiang Huang, Yuetang Deng\"/>\n",
      "<Title>\n",
      "A Deep Dive into Retrieval-Augmented Generation for Code Completion: Experience on WeChat\n",
      "</Title>\n",
      "\n",
      "<Summary>\n",
      "Code completion, a crucial task in software engineering that enhances\n",
      "developer productivity, has seen substantial improvements with the rapid\n",
      "advancement of large language models (LLMs). In recent years,\n",
      "retrieval-augmented generation (RAG) has emerged as a promising method to\n",
      "enhance the code completion capabilities of LLMs, which leverages relevant\n",
      "context from codebases without requiring model retraining. While existing\n",
      "studies have demonstrated the effectiveness of RAG on public repositories and\n",
      "benchmarks, the potential distribution shift between open-source and\n",
      "closed-source codebases presents unique challenges that remain unexplored. To\n",
      "mitigate the gap, we conduct an empirical study to investigate the performance\n",
      "of widely-used RAG methods for code completion in the industrial-scale codebase\n",
      "of WeChat, one of the largest proprietary software systems. Specifically, we\n",
      "extensively explore two main types of RAG methods, namely identifier-based RAG\n",
      "and similarity-based RAG, across 26 open-source LLMs ranging from 0.5B to 671B\n",
      "parameters. For a more comprehensive analysis, we employ different retrieval\n",
      "techniques for similarity-based RAG, including lexical and semantic retrieval.\n",
      "Based on 1,669 internal repositories, we achieve several key findings: (1) both\n",
      "RAG methods demonstrate effectiveness in closed-source repositories, with\n",
      "similarity-based RAG showing superior performance, (2) the effectiveness of\n",
      "similarity-based RAG improves with more advanced retrieval techniques, where\n",
      "BM25 (lexical retrieval) and GTE-Qwen (semantic retrieval) achieve superior\n",
      "performance, and (3) the combination of lexical and semantic retrieval\n",
      "techniques yields optimal results, demonstrating complementary strengths.\n",
      "Furthermore, we conduct a developer survey to validate the practical utility of\n",
      "RAG methods in real-world development environments.\n",
      "</Summary>\n",
      "\n",
      "<Content>\n",
      "A Deep Dive into Retrieval-Augmented Generation\n",
      "for Code Completion: Experience on WeChat\n",
      "Zezhou Yang\n",
      "Tencent\n",
      "Guangzhou, China\n",
      "zezhouyang@tencent.com\n",
      "Ting Peng\n",
      "Tencent\n",
      "Guangzhou, China\n",
      "sakurapeng@tencent.com\n",
      "Cuiyun Gaoâˆ—\n",
      "The Chinese University of Hong Kong\n",
      "Hong Kong, China\n",
      "cuiyungao@outlook.com\n",
      "Chaozheng Wang\n",
      "The Chinese University of Hong Kong\n",
      "Hong Kong, China\n",
      "adf111178@gmail.com\n",
      "Hailiang Huang\n",
      "Tencent\n",
      "Guangzhou, China\n",
      "eraserhuang@tencent.com\n",
      "Yuetang Deng\n",
      "Tencent\n",
      "Guangzhou, China\n",
      "yuetangdeng@tencent.com\n",
      "Abstractâ€”Code completion, a crucial task in software engi-\n",
      "neering that enhances developer productivity, has seen substantial\n",
      "improvements with the rapid advancement of large language\n",
      "models (LLMs). In recent years, retrieval-augmented generation\n",
      "(RAG) has emerged as a promising method to enhance the code\n",
      "completion capabilities of LLMs, which leverages relevant con-\n",
      "text from codebases without requiring model retraining. While\n",
      "existing studies have demonstrated the effectiveness of RAG on\n",
      "public repositories and benchmarks, the potential distribution\n",
      "shift between open-source and closed-source codebases presents\n",
      "unique challenges that remain unexplored. To mitigate the gap,\n",
      "we conduct an empirical study to investigate the performance of\n",
      "widely-used RAG methods for code completion in the industrial-\n",
      "scale codebase of WeChat, one of the largest proprietary software\n",
      "systems. Specifically, we extensively explore two main types\n",
      "of RAG methods, namely identifier-based RAG and similarity-\n",
      "based RAG, across 26 open-source LLMs ranging from 0.5B\n",
      "to 671B parameters. For a more comprehensive analysis, we\n",
      "employ different retrieval techniques for similarity-based RAG,\n",
      "including lexical and semantic retrieval. Based on 1,669 internal\n",
      "repositories, we achieve several key findings: (1) both RAG\n",
      "methods demonstrate effectiveness in closed-source repositories,\n",
      "with similarity-based RAG showing superior performance, (2)\n",
      "the effectiveness of similarity-based RAG improves with more\n",
      "advanced retrieval techniques, where BM25 (lexical retrieval)\n",
      "and GTE-Qwen (semantic retrieval) achieve superior perfor-\n",
      "mance, and (3) the combination of lexical and semantic retrieval\n",
      "techniques yields optimal results, demonstrating complemen-\n",
      "tary strengths. Furthermore, we conduct a developer survey\n",
      "to validate the practical utility of RAG methods in real-world\n",
      "development environments.\n",
      "Index Termsâ€”large language model, retrieval-augmented gen-\n",
      "eration, code completion\n",
      "I. INTRODUCTION\n",
      "Code completion, which automatically predicts and sug-\n",
      "gests code fragments based on the surrounding programming\n",
      "context, has evolved from simple token-level suggestions to\n",
      "âˆ—Cuiyun Gao is the corresponding author.\n",
      "This research is supported by National Key R&D Program of China\n",
      "(No. 2022YFB3103900), National Natural Science Foundation of China\n",
      "under project (No. 62472126), Natural Science Foundation of Guangdong\n",
      "Province (Project No. 2023A1515011959), Shenzhen-Hong Kong Jointly\n",
      "Funded Project (Category A, No. SGDX20230116091246007), and Shenzhen\n",
      "Basic Research (General Project No. JCYJ20220531095214031).\n",
      "generating entire code blocks [1], [2]. Studies have demon-\n",
      "strated that code completion tools substantially enhance de-\n",
      "veloper productivity in real-world software development [3],\n",
      "[4]. Notably, 87% of professional developers report significant\n",
      "improvements in their coding efficiency when utilizing code\n",
      "completion tools in industrial settings [5]. Recent advances\n",
      "in large language models (LLMs) have further transformed\n",
      "various software engineering tasks [6]â€“[10], demonstrating\n",
      "unprecedented capabilities in code understanding and gen-\n",
      "eration. These models have achieved particularly impressive\n",
      "performance in code completion tasks [11]â€“[13].\n",
      "To enhance LLMsâ€™ performance on domain-specific tasks,\n",
      "researchers have explored Retrieval-Augmented Generation\n",
      "(RAG), which augments model inference by retrieving and in-\n",
      "corporating relevant context from the target codebase without\n",
      "requiring param\n",
      "</Content>\n",
      "</Document>\n",
      "\n",
      "---\n",
      "\n",
      "<Document source=\"http://arxiv.org/abs/2506.03401v1\" date=\"2025-06-03\" authors=\"Xiwei Xu, Hans Weytjens, Dawen Zhang, Qinghua Lu, Ingo Weber, Liming Zhu\"/>\n",
      "<Title>\n",
      "RAGOps: Operating and Managing Retrieval-Augmented Generation Pipelines\n",
      "</Title>\n",
      "\n",
      "<Summary>\n",
      "Recent studies show that 60% of LLM-based compound systems in enterprise\n",
      "environments leverage some form of retrieval-augmented generation (RAG), which\n",
      "enhances the relevance and accuracy of LLM (or other genAI) outputs by\n",
      "retrieving relevant information from external data sources. LLMOps involves the\n",
      "practices and techniques for managing the lifecycle and operations of LLM\n",
      "compound systems in production environments. It supports enhancing LLM systems\n",
      "through continuous operations and feedback evaluation. RAGOps extends LLMOps by\n",
      "incorporating a strong focus on data management to address the continuous\n",
      "changes in external data sources. This necessitates automated methods for\n",
      "evaluating and testing data operations, enhancing retrieval relevance and\n",
      "generation quality. In this paper, we (1) characterize the generic architecture\n",
      "of RAG applications based on the 4+1 model view for describing software\n",
      "architectures, (2) outline the lifecycle of RAG systems, which integrates the\n",
      "management lifecycles of both the LLM and the data, (3) define the key design\n",
      "considerations of RAGOps across different stages of the RAG lifecycle and\n",
      "quality trade-off analyses, (4) highlight the overarching research challenges\n",
      "around RAGOps, and (5) present two use cases of RAG applications and the\n",
      "corresponding RAGOps considerations.\n",
      "</Summary>\n",
      "\n",
      "<Content>\n",
      "arXiv:2506.03401v1  [cs.SE]  3 Jun 2025\n",
      "RAGOps: Operating and Managing Retrieval-Augmented Generation Pipelines\n",
      "Xiwei Xua,c, Hans Weytjensb, Dawen Zhanga, Qinghua Lua,c, Ingo Weberb,d, Liming Zhua,c\n",
      "aCSIROâ€™s Data61, Australia\n",
      "bTechnical University of Munich, School of CIT, Germany\n",
      "cUniversity of New South Wales, School of Computer Science and Engineering, Australia\n",
      "dFraunhofer Society, Munich, Germany\n",
      "Abstract\n",
      "Recent studies show that 60% of LLM-based compound systems in enterprise environments leverage some form\n",
      "of retrieval-augmented generation (RAG), which enhances the relevance and accuracy of LLM (or other genAI) out-\n",
      "puts by retrieving relevant information from external data sources. LLMOps involves the practices and techniques for\n",
      "managing the lifecycle and operations of LLM compound systems in production environments. It supports enhancing\n",
      "LLM systems through continuous operations and feedback evaluation. RAGOps extends LLMOps by incorporating\n",
      "a strong focus on data management to address the continuous changes in external data sources. This necessitates\n",
      "automated methods for evaluating and testing data operations, enhancing retrieval relevance and generation quality.\n",
      "In this paper, we (1) characterize the generic architecture of RAG applications based on the 4+1 model view for de-\n",
      "scribing software architectures, (2) outline the lifecycle of RAG systems, which integrates the management lifecycles\n",
      "of both the LLM and the data, (3) define the key design considerations of RAGOps across different stages of the RAG\n",
      "lifecycle and quality trade-off analyses, (4) highlight the overarching research challenges around RAGOps, and (5)\n",
      "present two use cases of RAG applications and the corresponding RAGOps considerations.\n",
      "Keywords: LLM, LLMOps, RAG, RAGOps\n",
      "1. Introduction\n",
      "Large Language Models (LLMs) can be instructed through prompting to perform a wide range of tasks, such as\n",
      "programming and translation. A notable trend in their application is the integration of LLMs into compound software\n",
      "systems, which consist of multiple components beyond the core language model [1]. Compound LLM systems can\n",
      "perform dynamic behaviors, whereas LLMs by themselves are inherently constrained by their reliance on static train-\n",
      "ing on datasets from some point in time, resulting in fixed parametric knowledge and limited grounding in specific\n",
      "contexts in which the systems are used, such as a given organization. In enterprise settings, 60% of the LLM com-\n",
      "pound systems incorporate some form of retrieval-augmented generation [2] (RAG), which improves the relevance,\n",
      "accuracy, and dynamism of LLM outputs by retrieving information from external data. RAG offers a solution to\n",
      "common challenges faced by LLMs, such as hallucinations, outdated data, and the difficulty of removing parametric\n",
      "knowledge, and open up the possibility to access proprietary, internal data from organizations using existing LLMs.\n",
      "By integrating real-time information retrieval, RAG enables continuous updates, possibly incorporating very recent\n",
      "information. RAG systems are compound systems that consist of multiple components blending the LLMâ€™s parametric\n",
      "knowledge with external data retrieval, including, but not limited to retrieval sources, retriever, and generator [3].\n",
      "LLMOps refers to the practices and techniques used to manage the lifecycle and operation of LLM and LLM com-\n",
      "pound systems in production environments. The current state of LLMOps includes a variety of automated tools123\n",
      "designed to observe, monitor, optimize, and scale LLM applications. The functionality of these tools includes but\n",
      "is not limited to, model versioning, performance monitoring, continuous retraining, inference optimization, and in-\n",
      "frastructure scaling to accommodate fluctuating demand. Current LLMOps tools predominantly focus on model and\n",
      "1â€œManaged MLflow,â€ Databricks, accessed 5 April 2025, https://www.databricks.com/product/managed-mlflow\n",
      "2â€œIntelligent Observability,â€ New Relic, accessed 5 April 2025, http\n",
      "</Content>\n",
      "</Document>\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mask_question\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Fascinating insights, Dr. Torres. Could you share a specific example or case study where Modular RAG has been successfully implemented, highlighting its scalability and adaptability benefits in a production scenario?\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36msearch_web\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "<Document href=\"https://adasci.org/how-does-modular-rag-improve-upon-naive-rag/\"/>\n",
      "Naive RAG, the initial implementation of Retrieval-Augmented Generation, operates on a straightforward principle: retrieve relevant documents from an external knowledge base and use these documents to inform the generative process. The retrieval process in Naive RAG is relatively static and lacks flexibility, often leading to inefficiencies and suboptimal integration with the generative model. By adopting a modular architecture, this approach addresses the limitations of Naive RAG, offering enhanced flexibility, scalability, and efficiency. Unlike Naive RAG, which operates as a monolithic entity, Modular RAG breaks down the retrieval and generation processes into distinct, interchangeable modules. * *Seamless Integration*: Generative models in Modular RAG are designed to seamlessly integrate with various retrieval modules, enhancing the coherence and relevance of generated responses.\n",
      "</Document>\n",
      "\n",
      "---\n",
      "\n",
      "<Document href=\"https://medium.com/@drjulija/what-are-naive-rag-advanced-rag-modular-rag-paradigms-edff410c202e\"/>\n",
      "I share the differences between Naive RAG, Advanced RAG and Modular RAG frameworks. Retrieval-Augmented Generation (RAG) framework is designed to help mitigate these challenges. RAG enhances LLMsâ€™ knowledge base with additional, domain-specific data. RAG systems can be split into 3 categories: See the comparison between all three paradigms of RAG â€” Naive RAG, Advanced RAG and Modular RAG below.\n",
      "</Document>\n",
      "\n",
      "---\n",
      "\n",
      "<Document href=\"https://ai.plainenglish.io/naive-rag-advanced-rag-modular-rag-b18b8669193e\"/>\n",
      "Generate response with LLMllm = ChatOpenAI(model=\"gpt-4o\")query = \"What is LangChain?\"docs = retriever.get_relevant_documents(query)context = \"\\n\\n\".join([doc.page_content for doc in docs])response = llm.invoke(f\"Context: {context}\\nQuestion: {query}\")print(response) from import from import from import from import Generate Response with Optimized Inputllm = ChatOpenAI(model=\"gpt-4o\")prompt = PromptTemplate.from_template(\"Answer based on context:\\n{context}\\nQuestion: {query}\")response = llm.invoke(prompt.format(context=context, query=\"What is LangChain?\"))print(response) from import from import from import from import from import from import from langchain.vectorstores import Qdrant  # Vector store for embeddingsfrom langchain.embeddings import OpenAIEmbeddings  # Embeddings modelfrom langchain.text_splitter import RecursiveCharacterTextSplitter  # Splits documents into chunksfrom langchain.chains import RetrievalQA  # Retrieval-Augmented Generation (RAG)from langchain.prompts import ChatPromptTemplate  # Prompt templatesfrom langchain.chat_models import ChatOpenAI  # GPT-based LLM from import\n",
      "</Document>\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36msearch_web\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "<Document href=\"https://adasci.org/how-does-modular-rag-improve-upon-naive-rag/\"/>\n",
      "Naive RAG, the initial implementation of Retrieval-Augmented Generation, operates on a straightforward principle: retrieve relevant documents from an external knowledge base and use these documents to inform the generative process. The retrieval process in Naive RAG is relatively static and lacks flexibility, often leading to inefficiencies and suboptimal integration with the generative model. By adopting a modular architecture, this approach addresses the limitations of Naive RAG, offering enhanced flexibility, scalability, and efficiency. Unlike Naive RAG, which operates as a monolithic entity, Modular RAG breaks down the retrieval and generation processes into distinct, interchangeable modules. * *Seamless Integration*: Generative models in Modular RAG are designed to seamlessly integrate with various retrieval modules, enhancing the coherence and relevance of generated responses.\n",
      "</Document>\n",
      "\n",
      "---\n",
      "\n",
      "<Document href=\"https://zilliz.com/blog/advancing-llms-native-advanced-modular-rag-approaches\"/>\n",
      "Jan 8, 2025Â·This post explores the key components of RAG, its evolution, technical implementation, evaluation methods, and potential for real-world\n",
      "</Document>\n",
      "\n",
      "---\n",
      "\n",
      "<Document href=\"https://kth.diva-portal.org/smash/get/diva2:1971472/FULLTEXT01.pdf\"/>\n",
      "65 xiv | List of Tables List of acronyms and abbreviations | xv List of acronyms and abbreviations Advanced RAG Advanced Retrieval-Augmented Generation AI Artificial Intelligence AQ Ambiguous Questions BERT Bidirectional Encoder Representations from Trans-formers CoRAG Chain-of-Retrieval Augmented Generation CoT Chain-of-Thought GPT Generative Pretrained Transformer Graph RAG Graph Retrieval-Augmented Generation HyDE Hypothetical Document Embeddings IRCoT Interleaving Retrieval with Chain-of-Thought LLM Large Language Model LoRA Low-Rank Adaptation LSTM Long Short-Term Memory MHQA Multi-hop Question Answering ML Machine Learning MMR Maximal Marginal Relevance Modular RAG Modular Retrieval-Augmented Generation MRR Mean Reciprocal Rank Naive RAG Naive Retrieval-Augmented Generation NER Named Entity Extraction NLG Natural Language Generation NLP Natural Language Processing NLU Natural Language Understanding xvi | List of acronyms and abbreviations PEFT Efficient-Fine-Tuning QA Question Answering QFS Query-Focused Summarization RAG Retrieval-Augmented Generation RAPTOR Recursive Abstractive Processing for Tree-Organized Retrieval Recursive RAG Recursive Retrieval-Augmented Generation RNN Recurrent Neural Network ToC Tree of Clarifications Introduction | 1 Chapter 1 Introduction This chapter introduces the research area of this study and the problem domain.\n",
      "</Document>\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36msearch_arxiv\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "<Document source=\"http://arxiv.org/abs/2508.05650v1\" date=\"2025-07-26\" authors=\"Jiaxuan Liang, Shide Zhou, Kailong Wang\"/>\n",
      "<Title>\n",
      "OmniBench-RAG: A Multi-Domain Evaluation Platform for Retrieval-Augmented Generation Tools\n",
      "</Title>\n",
      "\n",
      "<Summary>\n",
      "While Retrieval Augmented Generation (RAG) is now widely adopted to enhance\n",
      "LLMs, evaluating its true performance benefits in a reproducible and\n",
      "interpretable way remains a major hurdle. Existing methods often fall short:\n",
      "they lack domain coverage, employ coarse metrics that miss sub document\n",
      "precision, and fail to capture computational trade offs. Most critically, they\n",
      "provide no standardized framework for comparing RAG effectiveness across\n",
      "different models and domains.\n",
      "  We introduce OmniBench RAG, a novel automated platform for multi domain\n",
      "evaluation of RAG systems. The platform quantifies performance gains across\n",
      "accuracy and efficiency dimensions, spanning nine knowledge fields including\n",
      "culture, geography, and health. We introduce two standardized metrics:\n",
      "Improvements (accuracy gains) and Transformation (efficiency differences\n",
      "between pre RAG and post RAG models), enabling reproducible comparisons across\n",
      "models and tasks. The platform features dynamic test generation, modular\n",
      "evaluation pipelines, and automated knowledge base construction. Our evaluation\n",
      "reveals striking variability in RAG effectiveness, from significant gains in\n",
      "culture to declines in mathematics, highlighting the critical importance of\n",
      "systematic, domain aware assessment. A demonstration video is available at:\n",
      "https://www.youtube.com/watch?v=BZx83QFcTCI. Code and datasets:\n",
      "https://github.com/Garnett-Liang/Omnibench-RAG.\n",
      "</Summary>\n",
      "\n",
      "<Content>\n",
      "OmniBench-RAG: A Multi-Domain Evaluation\n",
      "Platform for Retrieval-Augmented Generation Tools\n",
      "Jiaxuan Liang*, Shide Zhou*, and Kailong Wangâ€ \n",
      "Huazhong University of Science and Technology\n",
      "{liangjx, shidez, wangkl}@hust.edu.cn\n",
      "Abstractâ€”While Retrieval Augmented Generation (RAG) is\n",
      "now widely adopted to enhance LLMs, evaluating its true\n",
      "performance benefits in a reproducible and interpretable way\n",
      "remains a major hurdle. Existing methods often fall short: they\n",
      "lack domain coverage, employ coarse metrics that miss sub\n",
      "document precision, and fail to capture computational trade\n",
      "offs. Most critically, they provide no standardized framework\n",
      "for comparing RAG effectiveness across different models and\n",
      "domains.\n",
      "We introduce OmniBench RAG, a novel automated platform\n",
      "for multi domain evaluation of RAG systems. The platform\n",
      "quantifies performance gains across accuracy and efficiency\n",
      "dimensions, spanning nine knowledge fields including culture,\n",
      "geography, and health. We introduce two standardized metrics:\n",
      "Improvements (accuracy gains) and Transformation (efficiency\n",
      "differences between pre RAG and post RAG models), enabling\n",
      "reproducible comparisons across models and tasks. The platform\n",
      "features dynamic test generation, modular evaluation pipelines,\n",
      "and automated knowledge base construction. Our evaluation\n",
      "reveals striking variability in RAG effectiveness, from significant\n",
      "gains in culture to declines in mathematics, highlighting the\n",
      "critical importance of systematic, domain aware assessment. A\n",
      "demonstration video is available at: https://www.youtube.com/\n",
      "watch?v=BZx83QFcTCI. Code and datasets: https://github.com/\n",
      "Garnett-Liang/Omnibench-RAG.\n",
      "I. INTRODUCTION\n",
      "Retrieval-Augmented Generation (RAG) is a key technique\n",
      "for enhancing Large Language Models (LLMs) [1], [2]. By\n",
      "grounding model responses in external, verifiable knowledge,\n",
      "RAG promises to mitigate hallucinations [3], improve factual\n",
      "accuracy [4], and provide up-to-date information [5]. However,\n",
      "the true effectiveness of RAG is far from uniform. Recent\n",
      "studies [6], [7] reveal a significant disparity: while RAG\n",
      "can boost the accuracy of smaller models like Llama-3.2-3B-\n",
      "Instruct by as much as 38.12%, its impact on state-of-the-art\n",
      "models such as GPT-4o, which excel with extended context\n",
      "windows, is often less pronounced. This variability, which\n",
      "depends not only on the modelâ€™s scale but also heavily on\n",
      "the knowledge domain, underscores a critical challenge: the\n",
      "lack of a systematic platform to quantify the value of RAG\n",
      "across these diverse contexts.\n",
      "Current RAG evaluation approaches suffer from fundamen-\n",
      "tal limitations that impede reproducible and comprehensive as-\n",
      "sessment: First, they lack automated multi-domain evalua-\n",
      "tion capabilities and rely on non-deterministic components.\n",
      "*Jiaxuan Liang and Shide Zhou are co-first authors.\n",
      "â€ Kailong Wang is the corresponding author.\n",
      "Existing benchmarks typically require manual configuration\n",
      "for each knowledge domain and fail to provide unified assess-\n",
      "ment across diverse fields like finance, healthcare, or culture,\n",
      "making cross-domain performance analysis labor-intensive and\n",
      "inconsistent. Moreover, key metrics in leading frameworks\n",
      "(e.g., LLM-based scoring in Ragas [8]) inadvertently introduce\n",
      "randomness due to reliance on large language models in the\n",
      "evaluation loop, undermining result reproducibility. Second,\n",
      "they employ static datasets and coarse-grained metrics.\n",
      "Most frameworks rely on fixed benchmarks and document-\n",
      "level retrieval metrics (e.g., MRR@k[9]), missing the critical\n",
      "sub-document precision needed to assess whether models\n",
      "extract specific facts accurately. They also lack the ability to\n",
      "dynamically generate test cases that probe complex reasoning\n",
      "patterns. Third, they fail to capture the computational\n",
      "trade-offs inherent in RAG systems. Without automated\n",
      "profiling of resource utilization and efficiency metrics, practi-\n",
      "tioners cannot make informed decisions about the cost-benefit\n",
      "trade-offs of deploying RAG in production environments.\n",
      "Our \n",
      "</Content>\n",
      "</Document>\n",
      "\n",
      "---\n",
      "\n",
      "<Document source=\"http://arxiv.org/abs/2504.07803v1\" date=\"2025-04-10\" authors=\"Mattia Rengo, Senad Beadini, Domenico Alfano, Roberto Abbruzzese\"/>\n",
      "<Title>\n",
      "A System for Comprehensive Assessment of RAG Frameworks\n",
      "</Title>\n",
      "\n",
      "<Summary>\n",
      "Retrieval Augmented Generation (RAG) has emerged as a standard paradigm for\n",
      "enhancing the factual accuracy and contextual relevance of Large Language\n",
      "Models (LLMs) by integrating retrieval mechanisms. However, existing evaluation\n",
      "frameworks fail to provide a holistic black-box approach to assessing RAG\n",
      "systems, especially in real-world deployment scenarios. To address this gap, we\n",
      "introduce SCARF (System for Comprehensive Assessment of RAG Frameworks), a\n",
      "modular and flexible evaluation framework designed to benchmark deployed RAG\n",
      "applications systematically. SCARF provides an end-to-end, black-box evaluation\n",
      "methodology, enabling a limited-effort comparison across diverse RAG\n",
      "frameworks. Our framework supports multiple deployment configurations and\n",
      "facilitates automated testing across vector databases and LLM serving\n",
      "strategies, producing a detailed performance report. Moreover, SCARF integrates\n",
      "practical considerations such as response coherence, providing a scalable and\n",
      "adaptable solution for researchers and industry professionals evaluating RAG\n",
      "applications. Using the REST APIs interface, we demonstrate how SCARF can be\n",
      "applied to real-world scenarios, showcasing its flexibility in assessing\n",
      "different RAG frameworks and configurations. SCARF is available at GitHub\n",
      "repository.\n",
      "</Summary>\n",
      "\n",
      "<Content>\n",
      "arXiv:2504.07803v1  [cs.CL]  10 Apr 2025\n",
      "A System for Comprehensive Assessment of RAG\n",
      "Frameworks\n",
      "Mattia Rengo , Senad Beadini , Domenico Alfano\n",
      ", Roberto Abbruzzese\n",
      "R&D Department, Eustema S.p.A.\n",
      "Napoli, Italy\n",
      "{m.rengo, s.beadini, d.alfano, r.abbruzzese}@eustema.it\n",
      "Code: https://github.com/Eustema-S-p-A/SCARF\n",
      "Abstractâ€”Retrieval\n",
      "Augmented\n",
      "Generation\n",
      "(RAG)\n",
      "has\n",
      "emerged as a standard paradigm for enhancing the factual accu-\n",
      "racy and contextual relevance of Large Language Models (LLMs)\n",
      "by integrating retrieval mechanisms. However, existing evaluation\n",
      "frameworks fail to provide a holistic black-box approach to\n",
      "assessing RAG systems, especially in real-world deployment\n",
      "scenarios. To address this gap, we introduce SCARF (System for\n",
      "Comprehensive Assessment of RAG Frameworks), a modular and\n",
      "flexible evaluation framework designed to benchmark deployed\n",
      "RAG applications systematically. SCARF provides an end-to-\n",
      "end, black-box evaluation methodology, enabling a limited-effort\n",
      "comparison across diverse RAG frameworks. Our framework\n",
      "supports multiple deployment configurations and facilitates au-\n",
      "tomated testing across vector databases and LLM serving strate-\n",
      "gies, producing a detailed performance report. Moreover, SCARF\n",
      "integrates practical considerations such as response coherence,\n",
      "providing a scalable and adaptable solution for researchers\n",
      "and industry professionals evaluating RAG applications. Using\n",
      "the REST APIs interface, we demonstrate how SCARF can\n",
      "be applied to real-world scenarios, showcasing its flexibility in\n",
      "assessing different RAG frameworks and configurations. SCARF\n",
      "is available at GitHub repository.\n",
      "Index Termsâ€”Retrieval-Augmented Generation, RAG Evalu-\n",
      "ation, LLM\n",
      "I. INTRODUCTION\n",
      "Retrieval-Augmented Generation (RAG) represents a re-\n",
      "markable\n",
      "advancement\n",
      "in\n",
      "Natural\n",
      "Language\n",
      "Processing\n",
      "(NLP), significantly enhancing the performance of generative\n",
      "Language Models (LMs). By combining the capabilities of\n",
      "LMs with external knowledge bases, RAG allows responses\n",
      "that are not only more accurate but also contextually relevant.\n",
      "Merging information retrieval with language generation, RAG\n",
      "systems address a major limitation of standalone LMs: their\n",
      "tendency to generate responses that, while coherent, may\n",
      "lack factual accuracy or grounding. Since traditional Large\n",
      "Language Models (LLMs) rely solely on pre-trained data, they\n",
      "may generate factually incorrect information and unreliable\n",
      "outputs. RAG systems address this limitation by retrieving\n",
      "relevant information in real-time, making them well-suited for\n",
      "tasks that require up-to-date, accurate, and context-aware re-\n",
      "sponses, such as answering questions, generating content, and\n",
      "supporting various real-world applications. RAG approaches\n",
      "[1] have evolved rapidly, resulting in a wide variety of system\n",
      "variants [2]â€“[5] and benchmarking frameworks [6]. Many\n",
      "existing evaluation methods [7], [8] focus on assessing specific\n",
      "components, such as the relevance of retrieved documents or\n",
      "the quality of generated responses. However, these methods\n",
      "often lack a comprehensive perspective, failing to provide\n",
      "a holistic, end-to-end evaluation that considers not only the\n",
      "interplay between the retrieval and generation components\n",
      "but also the flexibility to experiment with and optimize the\n",
      "underlying technical tools used in these processes. Another\n",
      "critical yet often overlooked aspect is the variability introduced\n",
      "by different deployment frameworks used for LLMs. Tools\n",
      "such as vLLM [9], OpenLLM [10], and Ollama [11] im-\n",
      "plement diverse optimization strategies, including techniques\n",
      "like quantization, batching, and caching. These strategies can\n",
      "have a significant impact on key performance metrics, such as\n",
      "latency, system efficiency, and overall responsiveness. These\n",
      "variations are particularly important in real-world scenarios,\n",
      "where deployment choices can directly affect the user experi-\n",
      "ence and system scalability. Despite their importance, existing\n",
      "evaluation methods rarely integrate the ability to manage and\n",
      "measur\n",
      "</Content>\n",
      "</Document>\n",
      "\n",
      "---\n",
      "\n",
      "<Document source=\"http://arxiv.org/abs/2502.11228v2\" date=\"2025-05-22\" authors=\"Mohammad Reza Rezaei, Adji Bousso Dieng\"/>\n",
      "<Title>\n",
      "Vendi-RAG: Adaptively Trading-Off Diversity And Quality Significantly Improves Retrieval Augmented Generation With LLMs\n",
      "</Title>\n",
      "\n",
      "<Summary>\n",
      "Retrieval-augmented generation (RAG) enhances large language models (LLMs)\n",
      "for domain-specific question-answering (QA) tasks by leveraging external\n",
      "knowledge sources. However, traditional RAG systems primarily focus on\n",
      "relevance-based retrieval and often struggle with redundancy, especially when\n",
      "reasoning requires connecting information from multiple sources. This paper\n",
      "introduces Vendi-RAG, a framework based on an iterative process that jointly\n",
      "optimizes retrieval diversity and answer quality. This joint optimization leads\n",
      "to significantly higher accuracy for multi-hop QA tasks. Vendi-RAG leverages\n",
      "the Vendi Score (VS), a flexible similarity-based diversity metric, to promote\n",
      "semantic diversity in document retrieval. It then uses an LLM judge that\n",
      "evaluates candidate answers, generated after a reasoning step, and outputs a\n",
      "score that the retriever uses to balance relevance and diversity among the\n",
      "retrieved documents during each iteration. Experiments on three challenging\n",
      "datasets -- HotpotQA, MuSiQue, and 2WikiMultiHopQA -- demonstrate Vendi-RAG's\n",
      "effectiveness in multi-hop reasoning tasks. The framework achieves significant\n",
      "accuracy improvements over traditional single-step and multi-step RAG\n",
      "approaches, with accuracy increases reaching up to +4.2% on HotpotQA, +4.1% on\n",
      "2WikiMultiHopQA, and +1.3% on MuSiQue compared to Adaptive-RAG, the current\n",
      "best baseline. The benefits of Vendi-RAG are even more pronounced as the number\n",
      "of retrieved documents increases. Finally, we evaluated Vendi-RAG across\n",
      "different LLM backbones, including GPT-3.5, GPT-4, and GPT-4o-mini, and\n",
      "observed consistent improvements, demonstrating that the framework's advantages\n",
      "are model-agnostic.\n",
      "</Summary>\n",
      "\n",
      "<Content>\n",
      "Vendi-RAG: Adaptively Trading-Off Diversity And\n",
      "Quality Significantly Improves Retrieval\n",
      "Augmented Generation With LLMs\n",
      "Mohammad R. Rezaei1, 3 and Adji Bousso Dieng2, 3\n",
      "1Institute of Biomedical Engineering, University of Toronto\n",
      "2Department of Computer Science, Princeton University\n",
      "3Vertaix\n",
      "May 26, 2025\n",
      "Abstract\n",
      "Retrieval-augmented generation (RAG) enhances large language models (LLMs)\n",
      "for domain-specific question-answering (QA) tasks by leveraging external\n",
      "knowledge sources. However, traditional RAG systems primarily focus on\n",
      "relevance-based retrieval and often struggle with redundancy, especially when\n",
      "reasoning requires connecting information from multiple sources. This paper\n",
      "introduces Vendi-RAG, a framework based on an iterative process that jointly\n",
      "optimizes retrieval diversity and answer quality. This joint optimization leads\n",
      "to significantly higher accuracy for multi-hop QA tasks. Vendi-RAG leverages\n",
      "the Vendi Score (VS), a flexible similarity-based diversity metric, to promote\n",
      "semantic diversity in document retrieval. It then uses an LLM judge that eval-\n",
      "uates candidate answers, generated after a reasoning step, and outputs a\n",
      "score that the retriever uses to balance relevance and diversity among the\n",
      "retrieved documents during each iteration. Experiments on three challenging\n",
      "datasetsâ€”HotpotQA, MuSiQue, and 2WikiMultiHopQAâ€”demonstrate Vendi-\n",
      "RAGâ€™s effectiveness in multi-hop reasoning tasks. The framework achieves sig-\n",
      "nificant accuracy improvements over traditional single-step or multi-step RAG\n",
      "approaches, with accuracy increases reaching +4.2% on HotpotQA, +4.1% on\n",
      "2WikiMultiHopQA, and +1.3% on MuSiQue compared to Adaptive-RAG, the\n",
      "current best baseline. The benefits of Vendi-RAG are even more pronounced as\n",
      "the number of retrieved documents increases. Finally, we evaluated Vendi-RAG\n",
      "across different LLM backbones, including GPT-3.5, GPT-4, and GPT-4o-mini,\n",
      "and observed consistent improvements, demonstrating that the frameworkâ€™s\n",
      "advantages are model-agnostic.\n",
      "Keywords: RAG, LLMs, Question Answering, NLP, Diversity, Vendi Scoring\n",
      "1\n",
      "Introduction\n",
      "Retrieval-augmented generation (RAG) has emerged as a transformative framework\n",
      "for enhancing the performance of large language models (LLMs) in domain-specific\n",
      "tasks such as question-answering (QA). By retrieving relevant information from\n",
      "external sources beyond the training set, RAG enables LLMs to answer specialized\n",
      "1\n",
      "arXiv:2502.11228v2  [cs.CL]  22 May 2025\n",
      "Retrieval\n",
      "Query\n",
      "Answer\n",
      "LLM Judge \n",
      "Quality Check\n",
      "Rewritten  \n",
      "Query\n",
      "Final \n",
      "Answer\n",
      "Vendi Score\n",
      "Vendi Score\n",
      "Diversity  \n",
      "weight s\n",
      "Decision Block \n",
      "s > threshold Thr \n",
      "Iteration < N\n",
      "â€¦\n",
      "Reasoning\n",
      "Figure 1: The process begins with an initial retrieval step, where a diverse set of\n",
      "documents is retrieved using the Vendi Score, ensuring broad semantic coverage.\n",
      "Next, leveraging a reasoning step to construct a coherent path to the final answer,\n",
      "the LLM generates an answer, which then undergoes quality assessment by an LLM\n",
      "judge. Based on the answer quality, the retriever is adjusted to balance diversity and\n",
      "relevance: high-quality answers limit the emphasis on diversity, while low-quality\n",
      "answers prompt the retriever to prioritize diversity more heavily. This adjustment\n",
      "is controlled by an adaptive parameter, s, which is updated over iterations. The\n",
      "process continues until the answer quality reaches an optimal threshold, denoted\n",
      "by Thr. Finally, the highest-quality responses and documents are selected, ensuring\n",
      "both diversity and accuracy.\n",
      "queries more effectively Achiam et al. (2023); Team et al. (2023); Jiang et al. (2024).\n",
      "This approach has been particularly successful in single-hop QA, where a question\n",
      "can be answered using information from a single document Raiaan et al. (2024);\n",
      "Kwiatkowski et al. (2019). For instance, answering a question such as \"Who wrote\n",
      "the novel Frankenstein?\" only requires retrieving relevant information from a single\n",
      "document containing this fact.\n",
      "However, multi-hop QA introduces significantly greater complexit\n",
      "</Content>\n",
      "</Document>\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36msearch_web\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "<Document href=\"https://adasci.org/how-does-modular-rag-improve-upon-naive-rag/\"/>\n",
      "Naive RAG, the initial implementation of Retrieval-Augmented Generation, operates on a straightforward principle: retrieve relevant documents from an external knowledge base and use these documents to inform the generative process. The retrieval process in Naive RAG is relatively static and lacks flexibility, often leading to inefficiencies and suboptimal integration with the generative model. By adopting a modular architecture, this approach addresses the limitations of Naive RAG, offering enhanced flexibility, scalability, and efficiency. Unlike Naive RAG, which operates as a monolithic entity, Modular RAG breaks down the retrieval and generation processes into distinct, interchangeable modules. * *Seamless Integration*: Generative models in Modular RAG are designed to seamlessly integrate with various retrieval modules, enhancing the coherence and relevance of generated responses.\n",
      "</Document>\n",
      "\n",
      "---\n",
      "\n",
      "<Document href=\"https://rabiloo.com/blog/the-3-types-of-rag-models-naive-rag-modular-rag-and-advanced-rag\"/>\n",
      "Building AI systems requires choosing the right tools for the job, and Retrieval-Augmented Generation (RAG) offers various models, each designed to serve different needs. Essentially, a RAG model retrieves information from external databases, knowledge bases, or the web and then uses that data to generate relevant, meaningful responses. Naive RAG begins by indexing the data source for quick retrieval of relevant information. Modular RAG builds on the basic principles of Naive RAG by breaking down the retrieval and generation processes into separate, specialized modules. Unlike Naive RAG or Modular RAG, this stage is more precise and contextually aware, ensuring that the system retrieves only the most relevant and contextually appropriate data. The generation module in Advanced RAG then processes the refined data using sophisticated models like T5 or GPT-3.\n",
      "</Document>\n",
      "\n",
      "---\n",
      "\n",
      "<Document href=\"https://medium.com/@drjulija/what-are-naive-rag-advanced-rag-modular-rag-paradigms-edff410c202e\"/>\n",
      "I share the differences between Naive RAG, Advanced RAG and Modular RAG frameworks. Retrieval-Augmented Generation (RAG) framework is designed to help mitigate these challenges. RAG enhances LLMsâ€™ knowledge base with additional, domain-specific data. RAG systems can be split into 3 categories: See the comparison between all three paradigms of RAG â€” Naive RAG, Advanced RAG and Modular RAG below.\n",
      "</Document>\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36manswer_question\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: expert\n",
      "\n",
      "One compelling example of how Modular RAG can enhance performance in a production environment is illustrated in the automotive industry. Here, the demand for efficient document processing in local, low-performance settings, such as offline PDF chatbots, underscores the need for a more optimized approach [1].\n",
      "\n",
      "In this context, Modular RAG was utilized to process complex automotive industry documents, which often required handling multi-column layouts and technical specifications. The modular architecture allowed for tailored retrieval mechanisms and context compression specifically designed to cater to the unique characteristics of automotive documents [1].\n",
      "\n",
      "The study showed significant improvements over a Naive RAG baseline in metrics such as context precision, context recall, answer relevancy, and faithfulness, particularly on specific automotive datasets. The Modular RAG framework's ability to optimize PDF processing and retrieval specifically for the automotive industry's needs resulted in a more efficient and reliable system that could support the industry's rigorous information processing demands [1].\n",
      "\n",
      "This example highlights how Modular RAG can be strategically implemented to optimize performance for document-intensive tasks in a production setting, overcoming the inefficiencies typically associated with a Naive RAG approach.\n",
      "\n",
      "**Source:**\n",
      "\n",
      "[1] http://arxiv.org/abs/2408.05933v1\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36msave_interview\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "\u001b[1;32minterview\u001b[0m:\n",
      "Human: So you said you were writing an article on Explain how Modular RAG differs from traditional Naive RAG and the benefits of using it at the production level.?\n",
      "AI: Hello, my name is Dr. Emily Carter and Iâ€™m thrilled to have the opportunity to explore the nuances of Modular RAG and its advantages over traditional Naive RAG with you, Prof. Hinton. As an aspiring AI researcher, Iâ€™m especially intrigued by the practical implications of these advancements in real-world applications. To start, could you explain how Modular RAG differentiates from traditional Naive RAG in its structure and functionality?\n",
      "AI: Certainly, Prof. Hinton. Modular RAG differentiates from the traditional Naive RAG mainly in its architecture and approach to the retrieval and generation processes.\n",
      "\n",
      "1. **Architecture**: Naive RAG operates on a straightforward \"retrieve-then-generate\" model, where the retrieval process is relatively static and lacks flexibility. It functions as a single, monolithic entity [5]. In contrast, Modular RAG decomposes the retrieval and generation into independent modules and specialized operators, promoting a highly reconfigurable and scalable framework [1][4]. This modular approach allows for seamless integration of various retrieval techniques with generative models, enhancing the coherence and relevance of generated responses [5].\n",
      "\n",
      "2. **Flexibility and Scalability**: The modular nature of Modular RAG allows for interchangeable modules that can be independently updated or replaced, enabling easier adaptation to specific task requirements and technological advancements. This is in contrast with Naive RAG which is less adaptable to evolving needs [5].\n",
      "\n",
      "3. **Efficiency**: Modular RAG introduces advanced design elements like routing, scheduling, and fusion mechanisms. These elements optimize the flow of information and improve system efficiency, especially when handling complex queries, compared to the linear architecture of Naive RAG [1].\n",
      "\n",
      "The benefits of using Modular RAG in production environments include:\n",
      "\n",
      "- **Increased Efficiency**: By supporting routing, scheduling, and fusion mechanisms, Modular RAG can handle complex and diverse queries efficiently, thus improving the performance on knowledge-intensive tasks [1].\n",
      "\n",
      "- **Scalability**: The ability to reconfigure modules allows enterprises to scale the system up or down based on their computational and task-specific demands without overhauling the entire system [1].\n",
      "\n",
      "- **Enhanced Integration**: Modular RAG's design facilitates better integration with different retrieval mechanisms and LLMs, leading to more accurate and contextually aware responses [4][5].\n",
      "\n",
      "In sum, the Modular RAG framework offers a comprehensive roadmap for advancing RAG systems in a flexible, efficient, and scalable manner, making it well-suited for deployment in dynamic production environments.\n",
      "\n",
      "**Sources:**\n",
      "\n",
      "[1] http://arxiv.org/abs/2407.21059v1\n",
      "[4] https://rabiloo.com/blog/the-3-types-of-rag-models-naive-rag-modular-rag-and-advanced-rag\n",
      "[5] https://adasci.org/how-does-modular-rag-improve-upon-naive-rag/\n",
      "AI: Thank you for the detailed explanation, Prof. Hinton. Itâ€™s fascinating how Modular RAGâ€™s flexibility and efficiency potentially transform production applications. Could you provide a specific example or a case study of a production environment where Modular RAG has notably improved performance over Naive RAG?\n",
      "AI: One compelling example of how Modular RAG can enhance performance in a production environment is illustrated in the automotive industry. Here, the demand for efficient document processing in local, low-performance settings, such as offline PDF chatbots, underscores the need for a more optimized approach [1].\n",
      "\n",
      "In this context, Modular RAG was utilized to process complex automotive industry documents, which often required handling multi-column layouts and technical specifications. The modular architecture allowed for tailored retrieval mechanisms and context compression specifically designed to cater to the unique characteristics of automotive documents [1].\n",
      "\n",
      "The study showed significant improvements over a Naive RAG baseline in metrics such as context precision, context recall, answer relevancy, and faithfulness, particularly on specific automotive datasets. The Modular RAG framework's ability to optimize PDF processing and retrieval specifically for the automotive industry's needs resulted in a more efficient and reliable system that could support the industry's rigorous information processing demands [1].\n",
      "\n",
      "This example highlights how Modular RAG can be strategically implemented to optimize performance for document-intensive tasks in a production setting, overcoming the inefficiencies typically associated with a Naive RAG approach.\n",
      "\n",
      "**Source:**\n",
      "\n",
      "[1] http://arxiv.org/abs/2408.05933v1\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36manswer_question\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: expert\n",
      "\n",
      "Yes, certainly! One compelling case study showcasing the successful implementation of Modular RAG is found in the development of a Conversational AI system tailored for airport environments. Airports are highly dynamic with thousands of daily flights, requiring systems that can handle complex, domain-specific data efficiently.\n",
      "\n",
      "**Case Study: Conversational AI in Airports**\n",
      "\n",
      "- **Implementation**: In this specific scenario, a Modular RAG system was implemented to enable airport staff to communicate with flight information systems seamlessly. The system was designed to interpret and respond to diverse queries involving airport jargon, abbreviations, and dynamic questions that necessitated reasoning based on real-time data [1].\n",
      "\n",
      "- **Scalability and Adaptability**: The Modular RAG's architecture allowed the system to manage extensive datasets efficiently, handling queries from multiple data sources like relational databases and knowledge graphs. This modularity ensured that each part of the system could be optimized for accuracy and speed, significantly reducing the incidence of errors and inefficiencies prevalent in Naive RAG implementations [1].\n",
      "\n",
      "- **Benefits Observed**: \n",
      "  - **Reduced Errors**: Modular RAG achieved higher accuracy in providing answers with fewer hallucinations compared to traditional methods, increasing safety and reliability in critical scenarios requiring precise information [1].\n",
      "  - **Enhanced Efficiency**: By breaking down the retrieval and generation components into distinct modules, the system improved query handling speed and accuracy, crucial for real-time, high-pressure environments like airports [1].\n",
      "  - **Real-Time Adaptability**: The flexibility inherent in a modular setup allowed for dynamic updating and integrating of new datasets without disrupting overall system functionality, a critical requirement given the ever-evolving nature of flight data [1].\n",
      "\n",
      "This example clearly demonstrates Modular RAG's ability to scale and adapt within a complex production environment, ensuring both efficiency and reliability in handling large and varied datasets.\n",
      "\n",
      "**Sources**:\n",
      "- [1] http://arxiv.org/abs/2505.13006v1\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36msave_interview\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "\u001b[1;32minterview\u001b[0m:\n",
      "Human: So you said you were writing an article on Explain how Modular RAG differs from traditional Naive RAG and the benefits of using it at the production level.?\n",
      "AI: Hello, I'm John Harper, a technology writer exploring cutting-edge advancements in AI scalability. Dr. Torres, could you please explain how Modular RAG differs from traditional Naive RAG, specifically in terms of scalability and handling large datasets? How does modularity enhance its performance in production environments?\n",
      "AI: Certainly, Dr. Torres. Modular RAG (Retrieval-Augmented Generation) diverges from the traditional Naive RAG approach by breaking down the retrieval and generation processes into separate, specialized modules. This design promotes scalability and efficiency, especially when dealing with large datasets.\n",
      "\n",
      "**Differences between Modular RAG and Naive RAG:**\n",
      "\n",
      "1. **Architecture**: \n",
      "   - **Naive RAG** operates as a single monolithic system where retrieval and generation processes are intertwined, potentially leading to inefficiencies when scaling [2].\n",
      "   - **Modular RAG**, on the other hand, divides these tasks into dedicated modules. This separation allows each module to be optimized individually, improving system performance and accuracy [3].\n",
      "\n",
      "2. **Scalability**:\n",
      "   - The modular approach enhances scalability because it can efficiently manage larger datasets by leveraging specialized units that focus on specific tasks. This is particularly beneficial in environments requiring the integration of diverse, extensive, or dynamic data sources [3].\n",
      "   \n",
      "3. **Adaptability**:\n",
      "   - Modular RAG is more adaptable to production-level scenarios due to its flexibility in module management. It can easily incorporate or update modules without overhauling the entire system, unlike Naive RAG, which might require significant restructuring when upgrading components or adding new functionalities [4].\n",
      "\n",
      "**Benefits of Using Modular RAG at the Production Level:**\n",
      "\n",
      "- **Enhanced Query Efficiency**: By delegating specific queries to modules optimized for different data sources (relational, NoSQL, etc.), Modular RAG reduces token overhead and processing latency, thereby speeding up response times and improving accuracy [3].\n",
      "- **Robustness and Flexibility**: Its architecture allows for easy integration of new data sources or adjustment to changing data types, making it highly suitable for dynamic production environments such as those involving real-time data updates [3].\n",
      "- **Improved Contextual Awareness**: Modular RAG can achieve better semantic depth by refining the retrieval process to ensure only the most pertinent and contextually appropriate data is processed [2].\n",
      "\n",
      "In summary, by structuring retrieval and generation into distinct, specialized tasks, Modular RAG offers significant scalability benefits, making it well-suited for complex, large-scale production environments. This modularity not only enhances performance but also facilitates adaptability to ever-changing organizational data needs [3].\n",
      "\n",
      "**Sources**:\n",
      "- [2] https://rabiloo.com/blog/the-3-types-of-rag-models-naive-rag-modular-rag-and-advanced-rag\n",
      "- [3] http://arxiv.org/abs/2412.05838v1\n",
      "- [4] https://www.digitalocean.com/community/conceptual-articles/rag-ai-agents-agentic-rag-comparative-analysis\n",
      "AI: Fascinating insights, Dr. Torres. Could you share a specific example or case study where Modular RAG has been successfully implemented, highlighting its scalability and adaptability benefits in a production scenario?\n",
      "AI: Yes, certainly! One compelling case study showcasing the successful implementation of Modular RAG is found in the development of a Conversational AI system tailored for airport environments. Airports are highly dynamic with thousands of daily flights, requiring systems that can handle complex, domain-specific data efficiently.\n",
      "\n",
      "**Case Study: Conversational AI in Airports**\n",
      "\n",
      "- **Implementation**: In this specific scenario, a Modular RAG system was implemented to enable airport staff to communicate with flight information systems seamlessly. The system was designed to interpret and respond to diverse queries involving airport jargon, abbreviations, and dynamic questions that necessitated reasoning based on real-time data [1].\n",
      "\n",
      "- **Scalability and Adaptability**: The Modular RAG's architecture allowed the system to manage extensive datasets efficiently, handling queries from multiple data sources like relational databases and knowledge graphs. This modularity ensured that each part of the system could be optimized for accuracy and speed, significantly reducing the incidence of errors and inefficiencies prevalent in Naive RAG implementations [1].\n",
      "\n",
      "- **Benefits Observed**: \n",
      "  - **Reduced Errors**: Modular RAG achieved higher accuracy in providing answers with fewer hallucinations compared to traditional methods, increasing safety and reliability in critical scenarios requiring precise information [1].\n",
      "  - **Enhanced Efficiency**: By breaking down the retrieval and generation components into distinct modules, the system improved query handling speed and accuracy, crucial for real-time, high-pressure environments like airports [1].\n",
      "  - **Real-Time Adaptability**: The flexibility inherent in a modular setup allowed for dynamic updating and integrating of new datasets without disrupting overall system functionality, a critical requirement given the ever-evolving nature of flight data [1].\n",
      "\n",
      "This example clearly demonstrates Modular RAG's ability to scale and adapt within a complex production environment, ensuring both efficiency and reliability in handling large and varied datasets.\n",
      "\n",
      "**Sources**:\n",
      "- [1] http://arxiv.org/abs/2505.13006v1\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36manswer_question\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: expert\n",
      "\n",
      "In a specific implementation of Retrieval-Augmented Generation (RAG) within the automotive industry, Modular RAG demonstrated significant improvements in real-world reliability and latency reduction compared to Naive RAG. This was particularly evident in processing complex industry documents, such as those with multi-column layouts and detailed technical specifications.\n",
      "\n",
      "**Case Study: Automotive Industry Document Processing**\n",
      "\n",
      "1. **Improved PDF Processing and Contextual Relevance:**\n",
      "   - The study highlighted that Modular RAG's approach allowed for targeted enhancements in PDF processing, retrieval mechanisms, and context compression tailored to automotive documents. This specificity was not feasible with Naive RAG, which lacks the distinct, interchangeable modules for specialized tasks [7].\n",
      "\n",
      "2. **Enhanced Efficiency with Modular Architecture:**\n",
      "   - By deploying a modular architecture, the system was able to break down the retrieval and generation processes. Each module was optimized independently. For example, improvements were made in retrieval accuracy using advanced indexing techniques and sophisticated linguistic models in the generation phase. This modular optimization led to reduced latency and improved overall system efficiency [7].\n",
      "\n",
      "3. **Real-World Reliability:**\n",
      "   - The separation of retrieval and generation into dedicated modules enhanced the reliability of responses, as each module could be updated and tuned without disrupting the overall system. This was particularly beneficial in ensuring the responses were both relevant and precise in real-time, a key requirement for industrial applications like PDF chatbots in production environments where efficiency and accuracy are crucial [7].\n",
      "\n",
      "By adopting a modular structure, the automotive case study achieved a remarkable boost in both operational speed and reliability, illustrating the practical benefits of Modular RAG over the more simplified Naive RAG framework.\n",
      "\n",
      "[7] http://arxiv.org/abs/2408.05933v1\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36msave_interview\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "\u001b[1;32minterview\u001b[0m:\n",
      "Human: So you said you were writing an article on Explain how Modular RAG differs from traditional Naive RAG and the benefits of using it at the production level.?\n",
      "AI: Hello, Dr. Kahn, my name is Alex Ramirez, and I'm a technology journalist interested in exploring advancements in AI performance metrics. I'm particularly intrigued by the differences between Modular RAG and Naive RAG. Could you provide an explanation of how these two approaches differ specifically, and what benefits modular RAG offers when used in production environments?\n",
      "AI: Certainly, let's delve into the distinctions between Modular RAG and Naive RAG, particularly focusing on their applications and benefits in production environments.\n",
      "\n",
      "Naive RAG combines information retrieval with natural language generation to answer queries or prompts. It typically uses retrieval models that rank indexed data based on relevance to the input query. This approach generates responses by combining the input query with the retrieved context, aiming for coherence and relevance [3].\n",
      "\n",
      "In contrast, Modular RAG builds upon the basic principles of Naive RAG. It decomposes the retrieval and generation processes into separate, specialized modules. This separation allows each module to be optimized independently, offering greater control over each component of the RAG process. This modular structure enhances the precision and contextual awareness of the system, ensuring that the retrieved data is not only relevant but also contextually appropriate. The specialized design of each module allows for integration with sophisticated models, such as T5 or GPT-3, in the generation phase [4][5].\n",
      "\n",
      "**Benefits of Modular RAG in Production:**\n",
      "\n",
      "1. **Improved Precision and Contextual Relevance:**\n",
      "   - Modular RAG's separate modules allow for more precise retrieval, ensuring that only the most relevant and contextually suitable data is drawn for generation. This reduces the noise and potential inaccuracies that can arise from a single, monolithic approach [4].\n",
      "\n",
      "2. **Scalability and Flexibility:**\n",
      "   - The modular nature means that components can be scaled or modified independently without affecting the entire system. This flexibility is particularly beneficial in dynamic production environments where specific tasks may require different handling or specialized models [4][5].\n",
      "\n",
      "3. **Enhanced Efficiency:**\n",
      "   - By optimizing each module independently, Modular RAG can achieve efficiencies that aren't possible with Naive RAG. This results in reduced latency and improved response times, which are critical in real-world applications where speed and reliability are paramount [4].\n",
      "\n",
      "In summary, while Naive RAG offers a simpler, integrated approach to combining retrieval and generation, Modular RAG provides a more refined, flexible framework that improves accuracy, efficiency, and contextual relevanceâ€”key factors that enhance performance in production settings.\n",
      "\n",
      "Sources:\n",
      "[3] https://www.superteams.ai/blog/how-to-implement-naive-rag-advanced-rag-and-modular-rag\n",
      "[4] https://rabiloo.com/blog/the-3-types-of-rag-models-naive-rag-modular-rag-and-advanced-rag\n",
      "[5] https://medium.com/@drjulija/what-are-naive-rag-advanced-rag-modular-rag-paradigms-edff410c202e\n",
      "AI: That's a comprehensive overview! Iâ€™m curious, could you provide a specific example or case where implementing Modular RAG in a production environment significantly reduced latency compared to Naive RAG? How was the real-world reliability enhanced in this case?\n",
      "AI: In a specific implementation of Retrieval-Augmented Generation (RAG) within the automotive industry, Modular RAG demonstrated significant improvements in real-world reliability and latency reduction compared to Naive RAG. This was particularly evident in processing complex industry documents, such as those with multi-column layouts and detailed technical specifications.\n",
      "\n",
      "**Case Study: Automotive Industry Document Processing**\n",
      "\n",
      "1. **Improved PDF Processing and Contextual Relevance:**\n",
      "   - The study highlighted that Modular RAG's approach allowed for targeted enhancements in PDF processing, retrieval mechanisms, and context compression tailored to automotive documents. This specificity was not feasible with Naive RAG, which lacks the distinct, interchangeable modules for specialized tasks [7].\n",
      "\n",
      "2. **Enhanced Efficiency with Modular Architecture:**\n",
      "   - By deploying a modular architecture, the system was able to break down the retrieval and generation processes. Each module was optimized independently. For example, improvements were made in retrieval accuracy using advanced indexing techniques and sophisticated linguistic models in the generation phase. This modular optimization led to reduced latency and improved overall system efficiency [7].\n",
      "\n",
      "3. **Real-World Reliability:**\n",
      "   - The separation of retrieval and generation into dedicated modules enhanced the reliability of responses, as each module could be updated and tuned without disrupting the overall system. This was particularly beneficial in ensuring the responses were both relevant and precise in real-time, a key requirement for industrial applications like PDF chatbots in production environments where efficiency and accuracy are crucial [7].\n",
      "\n",
      "By adopting a modular structure, the automotive case study achieved a remarkable boost in both operational speed and reliability, illustrating the practical benefits of Modular RAG over the more simplified Naive RAG framework.\n",
      "\n",
      "[7] http://arxiv.org/abs/2408.05933v1\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mwrite_section\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "## Advancing AI: The Promise and Potential of Modular Retrieval-Augmented Generation\n",
      "\n",
      "### Summary\n",
      "\n",
      "Retrieval-augmented Generation (RAG) has become a transformative approach in the realm of artificial intelligence, particularly for enhancing the capabilities of Large Language Models (LLMs) in knowledge-intensive tasks. Prof. Hinton, a leading authority on neural networks and deep learning, is particularly focused on the potential improvements that Modular RAG can bring over traditional Naive RAG models. The most compelling development in this area is the conceptualization of RAG systems akin to LEGO-like frameworks through modular decomposition, which ensures high reconfigurability and adaptability in various production environments [1].\n",
      "\n",
      "The source documents contain insights on Modular RAGâ€™s potential to redefine the efficiency and scalability of AI systems through a non-linear architecture that incorporates advanced design elements such as routing, scheduling, and fusion mechanisms [1][2]. These modular structures allow for the integration of advanced retrieval methods and LLMs, enhancing the systemâ€™s ability to manage complex queries with greater precision and efficacy. This transformative evolution of RAG is evident in various domains, ranging from conversational AIs in airports to PDF chatbots in the automotive industry [3][4].\n",
      "\n",
      "Surprisingly, recent studies highlight that while traditional RAG systems experience limitations due to redundant and noise-laden retrievals, Modular RAG offers a pathway to mitigate such challenges by embracing a more segmented and precise retrieval methodology. The Modularity enhances the flexibility, scalability, and efficiency of these systems, aligning them more closely with the dynamism of real-world applications [5][6].\n",
      "\n",
      "Sources referenced:\n",
      "1. Yunfan Gao, Yun Xiong, Meng Wang, Haofen Wang. \"Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks.\" (2024).\n",
      "2. Yuyang Li et al. \"Evaluating the Performance of RAG Methods for Conversational AI in the Airport Domain.\" (2025).\n",
      "3. Fei Liu et al. \"Optimizing RAG Techniques for Automotive Industry PDF Chatbots: A Case Study with Locally Deployed Ollama Models.\" (2024).\n",
      "4. Jiajing Chen et al. \"Optimizing Retrieval-Augmented Generation with Elasticsearch for Enhanced Question-Answering Systems.\" (2024).\n",
      "5. Zezhou Yang et al. \"A Deep Dive into Retrieval-Augmented Generation for Code Completion: Experience on WeChat.\" (2025).\n",
      "6. Xiwei Xu et al. \"RAGOps: Operating and Managing Retrieval-Augmented Generation Pipelines.\" (2025).\n",
      "\n",
      "### Comprehensive Analysis\n",
      "\n",
      "The advancement of Retrieval-Augmented Generation (RAG) through a modular approach represents a significant leap forward in AI technology, especially in terms of efficiency and scalability in production environments. This section delves into the key insights derived from the selected documents, focusing on the unique attributes of Modular RAG and its implications for AI development.\n",
      "\n",
      "#### Modular RAG vs. Naive RAG\n",
      "\n",
      "- **Decomposition into Modules**: Modular RAG deconstructs complex systems into independent modules, each with specialized operators that allow flexible and reconfigurable use. This approach helps in transcending the linear \"retrieve-then-generate\" architecture, leading to more nuanced and dynamic interpolations [1].\n",
      "- **Scalability and Flexibility**: Unlike Naive RAG that predominantly operates as a monolithic entity, the modular framework inherently supports scalability by enabling seamless integration and replacement of individual modules. This characteristic is vital for rapidly evolving application scenarios where adaptability is crucial [2][3].\n",
      "- **Enhanced Functionality and Precision**: By incorporating routing, scheduling, and fusion mechanisms, Modular RAG achieves better operational precision, handling complex queries effectively. In the context of conversational AI systems at airports, for instance, this modularity significantly reduces hallucinations and improves the system's performance on dynamic questions [4].\n",
      "\n",
      "#### Implementation in Diverse Sectors\n",
      "\n",
      "- **Conversational AI in Dynamic Environments**: In highly dynamic settings like airports, Modular RAG outperforms traditional models by effectively handling jargon, abbreviations, and dynamic queries. Knowledge Graph-based RAG, a variant of Modular RAG, has shown superior accuracy in these applications, providing a safer and more reliable AI component in critical operations [2].\n",
      "- **Automotive Industry Adoption**: Automating PDF-based chatbots within the automotive sector exemplifies another application area. By optimizing RAG techniques through modular frameworks, systems can better handle automotive-specific documentation complexities, like multi-column layouts and technical jargon, thus elevating the precision and relevance of responses [3].\n",
      "- **Code Completion in Software Engineering**: The adoption of RAG for code completion, especially in industrial-scale systems like WeChat, highlights the advantages of combining lexical and semantic retrieval techniques within a modular framework. These techniques enable the system to effectively navigate closed-source codebases, thus enhancing developer productivity and accuracy in software development environments [5].\n",
      "\n",
      "#### Operational and Management Insights\n",
      "\n",
      "- **RAGOps**: Operational frameworks like RAGOps extend these benefits by focusing on data management and operational life cycle automation, addressing the evolving requirements in enterprise environments. This approach underscores the importance of integrating real-time retrieval with LLM systems, ensuring that the AI components remain updated and relevant [6].\n",
      "- **Enterprise Applications**: By leveraging modular RAG structures, enterprises can significantly enhance the performance of compound AI systems. This enhancement is crucial for applications demanding high adaptability and precision, ensuring that AI solutions remain robust and efficient despite the constant influx of new data and evolving application needs [6].\n",
      "\n",
      "#### Future Prospects and Challenges\n",
      "\n",
      "- **Research Directions**: Future research could explore the integration of higher-level semantic understanding within Modular RAG systems to further refine their capability in handling multi-faceted queries and dynamic scenarios [4].\n",
      "- **Technical Challenges**: A prominent challenge in deploying Modular RAG involves ensuring seamless interoperability between diverse modules and maintaining the quality of data retrieval processes. Addressing these challenges is essential to fully leverage the potential of RAG in advancing AI technology [5].\n",
      "\n",
      "In summary, Modular RAG offers transformative solutions for enhancing AI systems, making them more adaptable, efficient, and precise. The integration of modular architectures into RAG systems aligns well with real-world demands, offering a sustainable pathway for the evolution of AI in commercial and industrial applications.\n",
      "\n",
      "### Sources\n",
      "[1] http://arxiv.org/abs/2407.21059v1  \n",
      "[2] http://arxiv.org/abs/2505.13006v1  \n",
      "[3] http://arxiv.org/abs/2408.05933v1  \n",
      "[4] http://arxiv.org/abs/2410.14167v1  \n",
      "[5] http://arxiv.org/abs/2507.18515v1  \n",
      "[6] http://arxiv.org/abs/2506.03401v1  \n",
      "[7] https://rabiloo.com/blog/the-3-types-of-rag-models-naive-rag-modular-rag-and-advanced-rag  \n",
      "[8] https://adasci.org/how-does-modular-rag-improve-upon-naive-rag/\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mconduct_interview\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "## Advancing AI: The Promise and Potential of Modular Retrieval-Augmented Generation\n",
      "\n",
      "### Summary\n",
      "\n",
      "Retrieval-augmented Generation (RAG) has become a transformative approach in the realm of artificial intelligence, particularly for enhancing the capabilities of Large Language Models (LLMs) in knowledge-intensive tasks. Prof. Hinton, a leading authority on neural networks and deep learning, is particularly focused on the potential improvements that Modular RAG can bring over traditional Naive RAG models. The most compelling development in this area is the conceptualization of RAG systems akin to LEGO-like frameworks through modular decomposition, which ensures high reconfigurability and adaptability in various production environments [1].\n",
      "\n",
      "The source documents contain insights on Modular RAGâ€™s potential to redefine the efficiency and scalability of AI systems through a non-linear architecture that incorporates advanced design elements such as routing, scheduling, and fusion mechanisms [1][2]. These modular structures allow for the integration of advanced retrieval methods and LLMs, enhancing the systemâ€™s ability to manage complex queries with greater precision and efficacy. This transformative evolution of RAG is evident in various domains, ranging from conversational AIs in airports to PDF chatbots in the automotive industry [3][4].\n",
      "\n",
      "Surprisingly, recent studies highlight that while traditional RAG systems experience limitations due to redundant and noise-laden retrievals, Modular RAG offers a pathway to mitigate such challenges by embracing a more segmented and precise retrieval methodology. The Modularity enhances the flexibility, scalability, and efficiency of these systems, aligning them more closely with the dynamism of real-world applications [5][6].\n",
      "\n",
      "Sources referenced:\n",
      "1. Yunfan Gao, Yun Xiong, Meng Wang, Haofen Wang. \"Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks.\" (2024).\n",
      "2. Yuyang Li et al. \"Evaluating the Performance of RAG Methods for Conversational AI in the Airport Domain.\" (2025).\n",
      "3. Fei Liu et al. \"Optimizing RAG Techniques for Automotive Industry PDF Chatbots: A Case Study with Locally Deployed Ollama Models.\" (2024).\n",
      "4. Jiajing Chen et al. \"Optimizing Retrieval-Augmented Generation with Elasticsearch for Enhanced Question-Answering Systems.\" (2024).\n",
      "5. Zezhou Yang et al. \"A Deep Dive into Retrieval-Augmented Generation for Code Completion: Experience on WeChat.\" (2025).\n",
      "6. Xiwei Xu et al. \"RAGOps: Operating and Managing Retrieval-Augmented Generation Pipelines.\" (2025).\n",
      "\n",
      "### Comprehensive Analysis\n",
      "\n",
      "The advancement of Retrieval-Augmented Generation (RAG) through a modular approach represents a significant leap forward in AI technology, especially in terms of efficiency and scalability in production environments. This section delves into the key insights derived from the selected documents, focusing on the unique attributes of Modular RAG and its implications for AI development.\n",
      "\n",
      "#### Modular RAG vs. Naive RAG\n",
      "\n",
      "- **Decomposition into Modules**: Modular RAG deconstructs complex systems into independent modules, each with specialized operators that allow flexible and reconfigurable use. This approach helps in transcending the linear \"retrieve-then-generate\" architecture, leading to more nuanced and dynamic interpolations [1].\n",
      "- **Scalability and Flexibility**: Unlike Naive RAG that predominantly operates as a monolithic entity, the modular framework inherently supports scalability by enabling seamless integration and replacement of individual modules. This characteristic is vital for rapidly evolving application scenarios where adaptability is crucial [2][3].\n",
      "- **Enhanced Functionality and Precision**: By incorporating routing, scheduling, and fusion mechanisms, Modular RAG achieves better operational precision, handling complex queries effectively. In the context of conversational AI systems at airports, for instance, this modularity significantly reduces hallucinations and improves the system's performance on dynamic questions [4].\n",
      "\n",
      "#### Implementation in Diverse Sectors\n",
      "\n",
      "- **Conversational AI in Dynamic Environments**: In highly dynamic settings like airports, Modular RAG outperforms traditional models by effectively handling jargon, abbreviations, and dynamic queries. Knowledge Graph-based RAG, a variant of Modular RAG, has shown superior accuracy in these applications, providing a safer and more reliable AI component in critical operations [2].\n",
      "- **Automotive Industry Adoption**: Automating PDF-based chatbots within the automotive sector exemplifies another application area. By optimizing RAG techniques through modular frameworks, systems can better handle automotive-specific documentation complexities, like multi-column layouts and technical jargon, thus elevating the precision and relevance of responses [3].\n",
      "- **Code Completion in Software Engineering**: The adoption of RAG for code completion, especially in industrial-scale systems like WeChat, highlights the advantages of combining lexical and semantic retrieval techniques within a modular framework. These techniques enable the system to effectively navigate closed-source codebases, thus enhancing developer productivity and accuracy in software development environments [5].\n",
      "\n",
      "#### Operational and Management Insights\n",
      "\n",
      "- **RAGOps**: Operational frameworks like RAGOps extend these benefits by focusing on data management and operational life cycle automation, addressing the evolving requirements in enterprise environments. This approach underscores the importance of integrating real-time retrieval with LLM systems, ensuring that the AI components remain updated and relevant [6].\n",
      "- **Enterprise Applications**: By leveraging modular RAG structures, enterprises can significantly enhance the performance of compound AI systems. This enhancement is crucial for applications demanding high adaptability and precision, ensuring that AI solutions remain robust and efficient despite the constant influx of new data and evolving application needs [6].\n",
      "\n",
      "#### Future Prospects and Challenges\n",
      "\n",
      "- **Research Directions**: Future research could explore the integration of higher-level semantic understanding within Modular RAG systems to further refine their capability in handling multi-faceted queries and dynamic scenarios [4].\n",
      "- **Technical Challenges**: A prominent challenge in deploying Modular RAG involves ensuring seamless interoperability between diverse modules and maintaining the quality of data retrieval processes. Addressing these challenges is essential to fully leverage the potential of RAG in advancing AI technology [5].\n",
      "\n",
      "In summary, Modular RAG offers transformative solutions for enhancing AI systems, making them more adaptable, efficient, and precise. The integration of modular architectures into RAG systems aligns well with real-world demands, offering a sustainable pathway for the evolution of AI in commercial and industrial applications.\n",
      "\n",
      "### Sources\n",
      "[1] http://arxiv.org/abs/2407.21059v1  \n",
      "[2] http://arxiv.org/abs/2505.13006v1  \n",
      "[3] http://arxiv.org/abs/2408.05933v1  \n",
      "[4] http://arxiv.org/abs/2410.14167v1  \n",
      "[5] http://arxiv.org/abs/2507.18515v1  \n",
      "[6] http://arxiv.org/abs/2506.03401v1  \n",
      "[7] https://rabiloo.com/blog/the-3-types-of-rag-models-naive-rag-modular-rag-and-advanced-rag  \n",
      "[8] https://adasci.org/how-does-modular-rag-improve-upon-naive-rag/\n",
      "==================================================\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-Soffyuxth7vsUnobnzk0MGeR on tokens per min (TPM): Limit 30000, Used 29334, Requested 11245. Please try again in 21.158s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[231], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# ê·¸ë˜í”„ ì‹¤í–‰\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43minvoke_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_teddynote/messages.py:409\u001b[0m, in \u001b[0;36minvoke_graph\u001b[0;34m(graph, inputs, config, node_names, callback)\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m namespace[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(namespace) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroot graph\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;66;03m# subgraphs=True ë¥¼ í†µí•´ ì„œë¸Œê·¸ë˜í”„ì˜ ì¶œë ¥ë„ í¬í•¨\u001b[39;00m\n\u001b[0;32m--> 409\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m namespace, chunk \u001b[38;5;129;01min\u001b[39;00m graph\u001b[38;5;241m.\u001b[39mstream(\n\u001b[1;32m    410\u001b[0m     inputs, config, stream_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupdates\u001b[39m\u001b[38;5;124m\"\u001b[39m, subgraphs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    411\u001b[0m ):\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m node_name, node_chunk \u001b[38;5;129;01min\u001b[39;00m chunk\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    413\u001b[0m         \u001b[38;5;66;03m# node_namesê°€ ë¹„ì–´ìˆì§€ ì•Šì€ ê²½ìš°ì—ë§Œ í•„í„°ë§\u001b[39;00m\n\u001b[1;32m    414\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(node_names) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m node_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m node_names:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/main.py:2647\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[0m\n\u001b[1;32m   2645\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mmatch_cached_writes():\n\u001b[1;32m   2646\u001b[0m     loop\u001b[38;5;241m.\u001b[39moutput_writes(task\u001b[38;5;241m.\u001b[39mid, task\u001b[38;5;241m.\u001b[39mwrites, cached\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m-> 2647\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mtick(\n\u001b[1;32m   2648\u001b[0m     [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mvalues() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t\u001b[38;5;241m.\u001b[39mwrites],\n\u001b[1;32m   2649\u001b[0m     timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[1;32m   2650\u001b[0m     get_waiter\u001b[38;5;241m=\u001b[39mget_waiter,\n\u001b[1;32m   2651\u001b[0m     schedule_task\u001b[38;5;241m=\u001b[39mloop\u001b[38;5;241m.\u001b[39maccept_push,\n\u001b[1;32m   2652\u001b[0m ):\n\u001b[1;32m   2653\u001b[0m     \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[1;32m   2654\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m _output(\n\u001b[1;32m   2655\u001b[0m         stream_mode, print_mode, subgraphs, stream\u001b[38;5;241m.\u001b[39mget, queue\u001b[38;5;241m.\u001b[39mEmpty\n\u001b[1;32m   2656\u001b[0m     )\n\u001b[1;32m   2657\u001b[0m loop\u001b[38;5;241m.\u001b[39mafter_tick()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/_runner.py:253\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;66;03m# panic on failure or timeout\u001b[39;00m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 253\u001b[0m     \u001b[43m_panic_or_proceed\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfutures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdone\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfutures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpanic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tb \u001b[38;5;241m:=\u001b[39m exc\u001b[38;5;241m.\u001b[39m__traceback__:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/_runner.py:511\u001b[0m, in \u001b[0;36m_panic_or_proceed\u001b[0;34m(futs, timeout_exc_cls, panic)\u001b[0m\n\u001b[1;32m    509\u001b[0m                 interrupts\u001b[38;5;241m.\u001b[39mappend(exc)\n\u001b[1;32m    510\u001b[0m             \u001b[38;5;28;01melif\u001b[39;00m fut \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m SKIP_RERAISE_SET:\n\u001b[0;32m--> 511\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m    512\u001b[0m \u001b[38;5;66;03m# raise combined interrupts\u001b[39;00m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m interrupts:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/_executor.py:81\u001b[0m, in \u001b[0;36mBackgroundExecutor.done\u001b[0;34m(self, task)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Remove the task from the tasks dict when it's done.\"\"\"\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 81\u001b[0m     \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GraphBubbleUp:\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;66;03m# This exception is an interruption signal, not an error\u001b[39;00m\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;66;03m# so we don't want to re-raise it on exit\u001b[39;00m\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mpop(task)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/concurrent/futures/_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/concurrent/futures/_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/_retry.py:42\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[1;32m     40\u001b[0m     task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     44\u001b[0m     ns: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/_internal/_runnable.py:657\u001b[0m, in \u001b[0;36mRunnableSeq.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    655\u001b[0m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[1;32m    656\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[0;32m--> 657\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    659\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/main.py:3026\u001b[0m, in \u001b[0;36mPregel.invoke\u001b[0;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[0m\n\u001b[1;32m   3023\u001b[0m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m|\u001b[39m Any] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   3024\u001b[0m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m-> 3026\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream(\n\u001b[1;32m   3027\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   3028\u001b[0m     config,\n\u001b[1;32m   3029\u001b[0m     context\u001b[38;5;241m=\u001b[39mcontext,\n\u001b[1;32m   3030\u001b[0m     stream_mode\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupdates\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   3031\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3032\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m stream_mode,\n\u001b[1;32m   3033\u001b[0m     print_mode\u001b[38;5;241m=\u001b[39mprint_mode,\n\u001b[1;32m   3034\u001b[0m     output_keys\u001b[38;5;241m=\u001b[39moutput_keys,\n\u001b[1;32m   3035\u001b[0m     interrupt_before\u001b[38;5;241m=\u001b[39minterrupt_before,\n\u001b[1;32m   3036\u001b[0m     interrupt_after\u001b[38;5;241m=\u001b[39minterrupt_after,\n\u001b[1;32m   3037\u001b[0m     durability\u001b[38;5;241m=\u001b[39mdurability,\n\u001b[1;32m   3038\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3039\u001b[0m ):\n\u001b[1;32m   3040\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   3041\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(chunk) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/main.py:2647\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[0m\n\u001b[1;32m   2645\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mmatch_cached_writes():\n\u001b[1;32m   2646\u001b[0m     loop\u001b[38;5;241m.\u001b[39moutput_writes(task\u001b[38;5;241m.\u001b[39mid, task\u001b[38;5;241m.\u001b[39mwrites, cached\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m-> 2647\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mtick(\n\u001b[1;32m   2648\u001b[0m     [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mvalues() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t\u001b[38;5;241m.\u001b[39mwrites],\n\u001b[1;32m   2649\u001b[0m     timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[1;32m   2650\u001b[0m     get_waiter\u001b[38;5;241m=\u001b[39mget_waiter,\n\u001b[1;32m   2651\u001b[0m     schedule_task\u001b[38;5;241m=\u001b[39mloop\u001b[38;5;241m.\u001b[39maccept_push,\n\u001b[1;32m   2652\u001b[0m ):\n\u001b[1;32m   2653\u001b[0m     \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[1;32m   2654\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m _output(\n\u001b[1;32m   2655\u001b[0m         stream_mode, print_mode, subgraphs, stream\u001b[38;5;241m.\u001b[39mget, queue\u001b[38;5;241m.\u001b[39mEmpty\n\u001b[1;32m   2656\u001b[0m     )\n\u001b[1;32m   2657\u001b[0m loop\u001b[38;5;241m.\u001b[39mafter_tick()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/_runner.py:162\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[0m\n\u001b[1;32m    160\u001b[0m t \u001b[38;5;241m=\u001b[39m tasks[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 162\u001b[0m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m                \u001b[49m\u001b[43m_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m                \u001b[49m\u001b[43mweakref\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m                \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m                \u001b[49m\u001b[43mfutures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweakref\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m                \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschedule_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m                \u001b[49m\u001b[43msubmit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/_retry.py:42\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[1;32m     40\u001b[0m     task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     44\u001b[0m     ns: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/_internal/_runnable.py:657\u001b[0m, in \u001b[0;36mRunnableSeq.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    655\u001b[0m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[1;32m    656\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[0;32m--> 657\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    659\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/_internal/_runnable.py:401\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    399\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(ret)\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 401\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[1;32m    403\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "Cell \u001b[0;32mIn[218], line 238\u001b[0m, in \u001b[0;36mwrite_section\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;66;03m# ì„¹ì…˜ ì‘ì„±ì„ ìœ„í•œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì •ì˜\u001b[39;00m\n\u001b[1;32m    237\u001b[0m system_message \u001b[38;5;241m=\u001b[39m section_writer_instructions\u001b[38;5;241m.\u001b[39mformat(focus\u001b[38;5;241m=\u001b[39manalyst\u001b[38;5;241m.\u001b[39mdescription)\n\u001b[0;32m--> 238\u001b[0m section \u001b[38;5;241m=\u001b[39m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mSystemMessage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msystem_message\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mHumanMessage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mUse this source to write your section: \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mcontext\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;66;03m# ìƒíƒœì— ì„¹ì…˜ ì¶”ê°€\u001b[39;00m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msections\u001b[39m\u001b[38;5;124m\"\u001b[39m: [section\u001b[38;5;241m.\u001b[39mcontent]}\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:393\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    389\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    390\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    391\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    392\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatGeneration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m--> 393\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    403\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:1019\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m   1011\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m   1012\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1016\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m   1017\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m   1018\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m-> 1019\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:837\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    834\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[1;32m    835\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    836\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 837\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    838\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    840\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    843\u001b[0m         )\n\u001b[1;32m    844\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    845\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:1085\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1083\u001b[0m     result \u001b[38;5;241m=\u001b[39m generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[1;32m   1084\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1085\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1086\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m   1087\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1088\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1089\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_openai/chat_models/base.py:1183\u001b[0m, in \u001b[0;36mBaseChatOpenAI._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1181\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m raw_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(raw_response, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp_response\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1182\u001b[0m         e\u001b[38;5;241m.\u001b[39mresponse \u001b[38;5;241m=\u001b[39m raw_response\u001b[38;5;241m.\u001b[39mhttp_response  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m-> 1183\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m   1184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1185\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minclude_response_headers\n\u001b[1;32m   1186\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m raw_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(raw_response, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1188\u001b[0m ):\n\u001b[1;32m   1189\u001b[0m     generation_info \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(raw_response\u001b[38;5;241m.\u001b[39mheaders)}\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_openai/chat_models/base.py:1178\u001b[0m, in \u001b[0;36mBaseChatOpenAI._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1171\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _construct_lc_result_from_responses_api(\n\u001b[1;32m   1172\u001b[0m             response,\n\u001b[1;32m   1173\u001b[0m             schema\u001b[38;5;241m=\u001b[39moriginal_schema_obj,\n\u001b[1;32m   1174\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mgeneration_info,\n\u001b[1;32m   1175\u001b[0m             output_version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_version,\n\u001b[1;32m   1176\u001b[0m         )\n\u001b[1;32m   1177\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1178\u001b[0m         raw_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwith_raw_response\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1179\u001b[0m         response \u001b[38;5;241m=\u001b[39m raw_response\u001b[38;5;241m.\u001b[39mparse()\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_legacy_response.py:364\u001b[0m, in \u001b[0;36mto_raw_response_wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    360\u001b[0m extra_headers[RAW_RESPONSE_HEADER] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    362\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextra_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m extra_headers\n\u001b[0;32m--> 364\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cast(LegacyAPIResponse[R], \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_utils/_utils.py:287\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 287\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py:1147\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   1101\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m   1102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m   1103\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m   1145\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m   1146\u001b[0m     validate_response_format(response_format)\n\u001b[0;32m-> 1147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1150\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m   1151\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1152\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1153\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maudio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1154\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1155\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1156\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1157\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1158\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1159\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_completion_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1160\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1162\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodalities\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1163\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1165\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1166\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1167\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprompt_cache_key\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1168\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreasoning_effort\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1169\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1170\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msafety_identifier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1171\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1172\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1173\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1174\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1175\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1176\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1177\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1178\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1179\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1181\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mverbosity\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1184\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweb_search_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1185\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1186\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[1;32m   1187\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[1;32m   1188\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1189\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1190\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m   1192\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1196\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_base_client.py:1259\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1245\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1246\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1247\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1254\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1255\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1256\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1257\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1258\u001b[0m     )\n\u001b[0;32m-> 1259\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_base_client.py:1047\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1044\u001b[0m             err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1046\u001b[0m         log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1047\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1051\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould not resolve response (should never happen)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-Soffyuxth7vsUnobnzk0MGeR on tokens per min (TPM): Limit 30000, Used 29334, Requested 11245. Please try again in 21.158s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}"
     ]
    }
   ],
   "source": [
    "# ê·¸ë˜í”„ ì‹¤í–‰\n",
    "invoke_graph(graph, None, config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "# ê·¸ë˜í”„ì˜ ìµœì¢… ìƒíƒœ ê°€ì ¸ì˜¤ê¸°\n",
    "final_state = graph.get_state(config)\n",
    "\n",
    "# ìµœì¢… ë³´ê³ ì„œ ê°€ì ¸ì˜¤ê¸°\n",
    "report = final_state.values.get(\"final_report\")\n",
    "\n",
    "# ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ìµœì¢… ë³´ê³ ì„œ ì¶œë ¥\n",
    "display(Markdown(report))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMs2OSEsEhI+eP+P38efuYH",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
