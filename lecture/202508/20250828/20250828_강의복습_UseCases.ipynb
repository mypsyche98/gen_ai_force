{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아래 링크를 복사하여 웹 브라우저에 붙여넣으세요.\n",
      "https://accounts.google.com/o/oauth2/auth?client_id=35726703810-4v13dfqmilhgv6shlc3cv9i3ktuh73j1.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&access_type=offline&response_type=code\n",
      "Authentication successful.\n",
      "mykey => set HUGGINGFACEHUB_API_TOKEN is done\n",
      "mykey => set OPENAI_API_KEY is done\n",
      "mykey => set LANGCHAIN_API_KEY is done\n",
      "mykey => set LANGCHAIN_HUB_API_KEY is done\n",
      "mykey => set GOOGLE_API_KEY is done\n",
      "mykey => set UPSTAGE_API_KEY is done\n",
      "mykey => set COHERE_API_KEY is done\n",
      "mykey => set JINA_API_KEY is done\n",
      "mykey => set ANTHROPIC_API_KEY is done\n",
      "mykey => set DEEPL_API_KEY is done\n",
      "mykey => set TAVILY_API_KEY is done\n",
      "mykey => set TOGETHER_API_KEY is done\n"
     ]
    }
   ],
   "source": [
    "import mykeys\n",
    "\n",
    "mykeys.setOsEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "project_name = 'CH17_LangGraph_SUB03'\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = project_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith 추적을 시작합니다.\n",
      "[프로젝트명]\n",
      "CH17_LangGraph_SUB03\n"
     ]
    }
   ],
   "source": [
    "from langchain_teddynote import logging\n",
    "\n",
    "# 프로젝트 이름을 입력합니다.\n",
    "logging.langsmith(project_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote import logging\n",
    "\n",
    "# set_enable=False 로 지정하면 추적을 하지 않습니다.\n",
    "logging.langsmith(project_name, set_enable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CH17 LangGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "03. Use Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "01. 에이전트 대화 시뮬레이션 (고객 응대 시나리오)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph.message import add_messages  \n",
    "from typing import Annotated  \n",
    "from typing_extensions import TypedDict  \n",
    "\n",
    "\n",
    "# State 정의  \n",
    "class State(TypedDict):  \n",
    "    messages: Annotated[list, add_messages]  # 사용자 - 상담사 간의 대화 메시지  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List  \n",
    "from langchain_teddynote.models import LLMs, get_model_name  \n",
    "from langchain_openai import ChatOpenAI  \n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder  \n",
    "from langchain_core.messages import HumanMessage, AIMessage, BaseMessage  \n",
    "from langchain_core.output_parsers import StrOutputParser  \n",
    "\n",
    "# 모델 이름 설정  \n",
    "MODEL_NAME = get_model_name(LLMs.GPT4_1)  \n",
    "\n",
    "\n",
    "def call_chatbot(messages: List[BaseMessage]) -> dict:  \n",
    "    # LangChain ChatOpenAI 모델을 Agent 로 변경할 수 있습니다.  \n",
    "    prompt = ChatPromptTemplate.from_messages(  \n",
    "        [  \n",
    "            (  \n",
    "                \"system\",  \n",
    "                \"You are a customer support agent for an airline. Answer in Korean.\",  \n",
    "            ),  \n",
    "            MessagesPlaceholder(variable_name=\"messages\"),  \n",
    "        ]  \n",
    "    )  \n",
    "    model = ChatOpenAI(model=MODEL_NAME, temperature=0.6)  \n",
    "    chain = prompt | model | StrOutputParser()  \n",
    "    return chain.invoke({\"messages\": messages})  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'안녕하세요! 무엇을 도와드릴까요?'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_chatbot([(\"user\", \"안녕하세요?\")])  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder  \n",
    "from langchain_openai import ChatOpenAI  \n",
    "\n",
    "\n",
    "def create_scenario(name: str, instructions: str):  \n",
    "    # 시스템 프롬프트를 정의: 필요에 따라 변경  \n",
    "    system_prompt_template = \"\"\"You are a customer of an airline company. \\  \n",
    "You are interacting with a user who is a customer support person. \\  \n",
    "\n",
    "Your name is {name}.  \n",
    "\n",
    "# Instructions:  \n",
    "{instructions}  \n",
    "\n",
    "[IMPORTANT]  \n",
    "- When you are finished with the conversation, respond with a single word 'FINISHED'  \n",
    "- You must speak in Korean.\"\"\"  \n",
    "\n",
    "    # 대화 메시지와 시스템 프롬프트를 결합하여 채팅 프롬프트 템플릿을 생성합니다.  \n",
    "    prompt = ChatPromptTemplate.from_messages(  \n",
    "        [  \n",
    "            (\"system\", system_prompt_template),  \n",
    "            MessagesPlaceholder(variable_name=\"messages\"),  \n",
    "        ]  \n",
    "    )  \n",
    "\n",
    "    # 특정 사용자 이름과 지시사항을 사용하여 프롬프트를 부분적으로 채웁니다.  \n",
    "    prompt = prompt.partial(name=name, instructions=instructions)  \n",
    "    return prompt  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m System Message \u001b[0m================================\n",
      "\n",
      "You are a customer of an airline company. \\  \n",
      "You are interacting with a user who is a customer support person. \\  \n",
      "\n",
      "Your name is \u001b[33;1m\u001b[1;3m{name}\u001b[0m.  \n",
      "\n",
      "# Instructions:  \n",
      "\u001b[33;1m\u001b[1;3m{instructions}\u001b[0m  \n",
      "\n",
      "[IMPORTANT]  \n",
      "- When you are finished with the conversation, respond with a single word 'FINISHED'  \n",
      "- You must speak in Korean.\n",
      "\n",
      "=============================\u001b[1m Messages Placeholder \u001b[0m=============================\n",
      "\n",
      "\u001b[33;1m\u001b[1;3m{messages}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# 사용자 지시사항을 정의합니다.  \n",
    "instructions = \"\"\"You are tyring to get a refund for the trip you took to Jeju Island. \\  \n",
    "You want them to give you ALL the money back. This trip happened last year.\"\"\"  \n",
    "\n",
    "# 사용자 이름을 정의합니다.  \n",
    "name = \"Charlie\"  \n",
    "\n",
    "create_scenario(name, instructions).pretty_print()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI 챗봇 모델을 초기화합니다.  \n",
    "model = ChatOpenAI(model=MODEL_NAME, temperature=0.6)  \n",
    "\n",
    "# 시뮬레이션된 사용자 대화를 생성합니다.  \n",
    "simulated_user = create_scenario(name, instructions) | model | StrOutputParser()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'안녕하세요. 저는 작년에 제주도에 다녀온 여행에 대한 환불을 받고 싶어서 연락드렸습니다. 전액 환불을 원합니다. 어떻게 해야 하나요?'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage  \n",
    "\n",
    "# 시뮬레이션된 사용자에게 메시지를 전달  \n",
    "messages = [HumanMessage(content=\"안녕하세요? 어떻게 도와 드릴까요?\")]  \n",
    "simulated_user.invoke({\"messages\": messages})  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage  \n",
    "\n",
    "\n",
    "# 상담사 역할  \n",
    "def ai_assistant_node(messages):  \n",
    "    # 상담사 응답 호출  \n",
    "    ai_response = call_chatbot(messages)  \n",
    "\n",
    "    # AI 상담사의 응답을 반환  \n",
    "    return {\"messages\": [(\"assistant\", ai_response)]}  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [('assistant',\n",
       "   '환불을 원하시는 경우, 아래 절차를 따라주시면 됩니다.\\n\\n1. **항공권 예매 내역 확인**: 예매하신 항공권의 예약번호(PNR)와 승객 성함을 준비해 주세요.\\n2. **구매 경로 확인**: 공식 홈페이지, 모바일 앱, 여행사 등 어떤 경로로 예매하셨는지 확인해 주세요.\\n3. **환불 신청**:  \\n   - 공식 홈페이지 또는 모바일 앱: [내 예약] 메뉴에서 환불 신청이 가능합니다.\\n   - 여행사 예매: 예매하신 여행사로 문의해 주세요.\\n   - 고객센터 전화: 저희 고객센터(☎️)로 연락 주시면 안내해 드립니다.\\n\\n**유의사항**:  \\n- 항공권 종류(환불 가능/불가)에 따라 환불 규정 및 수수료가 다를 수 있습니다.  \\n- 일부 할인 항공권은 환불이 불가능할 수 있습니다.  \\n- 결제하신 수단으로 환불 처리되며, 영업일 기준 5~7일 정도 소요될 수 있습니다.\\n\\n예약번호와 성함을 알려주시면, 보다 자세히 안내해 드릴 수 있습니다. 추가로 궁금하신 점 있으시면 언제든 문의해 주세요!')]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_assistant_node(  \n",
    "    [  \n",
    "        (\"user\", \"안녕하세요?\"),  \n",
    "        (\"assistant\", \"안녕하세요! 어떻게 도와드릴까요?\"),  \n",
    "        (\"user\", \"환불 어떻게 하나요?\"),  \n",
    "    ]  \n",
    ")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _swap_roles(messages):  \n",
    "    # 메시지의 역할을 교환: 시뮬레이션 사용자 단계에서 메시지 타입을 AI -> Human, Human -> AI 로 교환합니다.  \n",
    "    new_messages = []  \n",
    "    for m in messages:  \n",
    "        if isinstance(m, AIMessage):  \n",
    "            # AIMessage 인 경우, HumanMessage 로 변환합니다.  \n",
    "            new_messages.append(HumanMessage(content=m.content))  \n",
    "        else:  \n",
    "            # HumanMessage 인 경우, AIMessage 로 변환합니다.  \n",
    "            new_messages.append(AIMessage(content=m.content))  \n",
    "    return new_messages  \n",
    "\n",
    "\n",
    "# 상담사 역할(AI Assistant) 노드 정의  \n",
    "def ai_assistant_node(state: State):  \n",
    "    # 상담사 응답 호출  \n",
    "    ai_response = call_chatbot(state[\"messages\"])  \n",
    "\n",
    "    # AI 상담사의 응답을 반환  \n",
    "    return {\"messages\": [(\"assistant\", ai_response)]}  \n",
    "\n",
    "\n",
    "# 시뮬레이션된 사용자(Simulated User) 노드 정의  \n",
    "def simulated_user_node(state: State):  \n",
    "    # 메시지 타입을 교환: AI -> Human, Human -> AI  \n",
    "    new_messages = _swap_roles(state[\"messages\"])  \n",
    "\n",
    "    # 시뮬레이션된 사용자를 호출  \n",
    "    response = simulated_user.invoke({\"messages\": new_messages})  \n",
    "    return {\"messages\": [(\"user\", response)]}  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_continue(state: State):  \n",
    "    # 메시지 리스트의 길이가 6보다 크면 'end'를 반환합니다.  \n",
    "    if len(state[\"messages\"]) > 6:  \n",
    "        return \"end\"  \n",
    "    # 마지막 메시지의 내용이 'FINISHED'라면 'end'를 반환합니다.  \n",
    "    elif state[\"messages\"][-1].content == \"FINISHED\":  \n",
    "        return \"end\"  \n",
    "    # 위의 조건에 해당하지 않으면 'continue'를 반환합니다.  \n",
    "    else:  \n",
    "        return \"continue\"  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph  \n",
    "\n",
    "# StateGraph 인스턴스 생성  \n",
    "graph_builder = StateGraph(State)  \n",
    "\n",
    "# 노드 정의  \n",
    "graph_builder.add_node(\"simulated_user\", simulated_user_node)  \n",
    "graph_builder.add_node(\"ai_assistant\", ai_assistant_node)  \n",
    "\n",
    "# 엣지 정의 (챗봇 -> 시뮬레이션된 사용자)  \n",
    "graph_builder.add_edge(\"ai_assistant\", \"simulated_user\")  \n",
    "\n",
    "# 조건부 엣지 정의  \n",
    "graph_builder.add_conditional_edges(  \n",
    "    \"simulated_user\",  \n",
    "    should_continue,  \n",
    "    {  \n",
    "        \"end\": END,  # 종료 조건이 충족되면 시뮬레이션을 중단  \n",
    "        \"continue\": \"ai_assistant\",  # 종료 조건이 충족되지 않으면 상담사 역할 노드로 메시지를 전달  \n",
    "    },  \n",
    ")  \n",
    "\n",
    "# 시작점 설정  \n",
    "graph_builder.set_entry_point(\"ai_assistant\")  \n",
    "\n",
    "# 그래프 컴파일  \n",
    "simulation = graph_builder.compile()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_teddynote.graphs import visualize_graph  \n",
    "\n",
    "visualize_graph(simulation)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mai_assistant\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "안녕하세요, 고객님. 불편을 드려 정말 죄송합니다. 어떤 문제로 인해 화가 나셨는지 말씀해 주시면, 최대한 빠르고 성심껏 도와드리겠습니다. 자세한 상황을 알려주시면 감사하겠습니다.\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36msimulated_user\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "지난해 제주도 여행을 다녀왔는데, 정말 최악이었어요. 비행기가 계속 지연되고, 서비스도 너무 별로였고, 수하물도 늦게 나와서 일정이 다 망가졌어요. 이런 경우 전액 환불 받을 수 있죠?\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mai_assistant\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "고객님, 우선 지난해 여행에서 많은 불편을 겪으신 점 진심으로 사과드립니다. 말씀해주신 비행기 지연, 서비스 문제, 수하물 지연 등으로 인해 여행 일정에 큰 차질이 생기셨던 점 충분히 이해하고 있습니다.\n",
      "\n",
      "다만, 전액 환불 가능 여부는 항공권 종류(환불 가능/불가), 지연 시간, 당시의 정확한 상황(기상, 항공사 귀책 등)에 따라 달라질 수 있습니다. 일반적으로 항공편 지연이나 수하물 지연만으로 전액 환불이 바로 진행되진 않지만, 보상 또는 일부 환불, 마일리지, 바우처 등으로 지원이 가능한 경우도 있습니다.\n",
      "\n",
      "정확한 안내를 위해 예약하셨던 항공권의 예약번호나 성함, 당시 상황(지연 시간, 수하물 수령 시간 등)을 알려주시면 확인 후 최대한 도움드릴 수 있도록 하겠습니다. 추가 정보를 남겨주시면 바로 확인해드리겠습니다.\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36msimulated_user\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "저는 Charlie이고, 예약번호는 JEJU1234입니다.  \n",
      "그날 비행기가 4시간 넘게 지연됐고, 수하물도 2시간이나 기다렸어요.  \n",
      "여행 첫날 일정이 다 망가졌고, 여행 내내 기분이 안 좋았습니다.  \n",
      "솔직히 이런 경험 하고 돈을 한 푼도 쓰고 싶지 않아요.  \n",
      "전액 환불 꼭 해주세요.  \n",
      "다른 보상이나 바우처 말고, 그냥 돈으로 다 돌려주세요.  \n",
      "가능하죠?\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mai_assistant\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Charlie님, 다시 한 번 큰 불편을 드린 점 진심으로 사과드립니다.\n",
      "\n",
      "남겨주신 예약번호(JEJU1234)와 상세한 상황을 확인하였습니다. 말씀하신 대로 4시간 이상의 항공편 지연과 2시간의 수하물 지연은 매우 심각한 불편을 초래한 점 충분히 공감합니다.\n",
      "\n",
      "다만, 항공사 규정과 항공운송 약관상 전액 환불은 일반적으로 항공편이 취소되거나, 항공사의 중대한 귀책 사유로 탑승이 불가능한 경우에 한해 제공됩니다. 항공편이 운항되었고, 고객님께서 탑승하신 경우에는 전액 환불이 어렵다는 점 양해 부탁드립니다.\n",
      "\n",
      "하지만, 4시간 이상의 지연과 수하물 지연에 대해서는 규정에 따라 일정 금액의 보상 또는 배상(현금, 마일리지, 바우처 등) 절차가 가능하며, 고객님의 불편을 최대한 반영해 드릴 수 있도록 내부적으로도 추가 검토를 요청하겠습니다.\n",
      "\n",
      "전액 환불을 원하시는 고객님의 심정을 충분히 이해하며, 혹시라도 추가적으로 원하시는 사항이나 문의가 있으시면 언제든 말씀해 주세요. 최선을 다해 도와드리겠습니다.\n",
      "\n",
      "보다 자세한 보상 안내와 진행 절차를 위하여 고객님의 연락처(이메일 또는 전화번호)를 남겨주시면 담당자가 직접 연락드려 확인 후 안내드리겠습니다. 다시 한 번 불편을 드려 죄송합니다.\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36msimulated_user\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "전액 환불이 어렵다는 말씀이신가요?  \n",
      "그럼 제가 여행 내내 망친 기분과 손해는 어떻게 보상받죠?  \n",
      "저는 돈으로만 받고 싶어요. 바우처나 마일리지는 필요 없습니다.  \n",
      "솔직히 이런 일이 있었으면 항공사가 책임지고 전액 환불해줘야 한다고 생각합니다.  \n",
      "다른 방법 말고, 전액 환불 해주세요.  \n",
      "다시 한 번 확인해 주세요.  \n",
      "정말 실망스럽네요."
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableConfig  \n",
    "from langchain_teddynote.messages import stream_graph, random_uuid  \n",
    "\n",
    "\n",
    "# config 설정(재귀 최대 횟수, thread_id)  \n",
    "config = RunnableConfig(recursion_limit=10, configurable={\"thread_id\": random_uuid()})  \n",
    "\n",
    "# 입력 메시지 설정  \n",
    "inputs = {  \n",
    "    \"messages\": [HumanMessage(content=\"안녕하세요? 저 지금 좀 화가 많이 났습니다^^\")]  \n",
    "}  \n",
    "\n",
    "# 그래프 스트리밍  \n",
    "stream_graph(simulation, inputs, config, node_names=[\"simulated_user\", \"ai_assistant\"])  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "02. 사용자 요구사항 기반 메타 프롬프트 생성 에이전트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel\n",
    "from langchain_teddynote.models import get_model_name, LLMs\n",
    "\n",
    "# 사용자 요구사항 수집을 위한 시스템 메시지 템플릿\n",
    "template = \"\"\"Your job is to get information from a user about what type of prompt template they want to create.\n",
    "\n",
    "You should get the following information from them:\n",
    "\n",
    "- What the objective of the prompt is\n",
    "- What variables will be passed into the prompt template\n",
    "- Any constraints for what the output should NOT do\n",
    "- Any requirements that the output MUST adhere to\n",
    "\n",
    "If you are not able to discern this info, ask them to clarify! Do not attempt to wildly guess.\n",
    "\n",
    "After you are able to discern all the information, call the relevant tool.\n",
    "\n",
    "[IMPORTANT] Your conversation should be in Korean. Your generated prompt should be in English.\"\"\"\n",
    "\n",
    "\n",
    "# 사용자 메시지 목록을 받아 시스템 메시지와 결합하여 반환\n",
    "def get_messages_info(messages):\n",
    "    # 사용자 요구사항 수집을 위한 시스템 메시지와 기존 메시지 결합\n",
    "    return [SystemMessage(content=template)] + messages\n",
    "\n",
    "\n",
    "# LLM에 대한 프롬프트 지침을 정의하는 데이터 모델\n",
    "class PromptInstructions(BaseModel):\n",
    "    \"\"\"Instructions on how to prompt the LLM.\"\"\"\n",
    "\n",
    "    # 프롬프트의 목표\n",
    "    objective: str\n",
    "    # 프롬프트 템플릿에 전달될 변수 목록\n",
    "    variables: List[str]\n",
    "    # 출력에서 피해야 할 제약 조건 목록\n",
    "    constraints: List[str]\n",
    "    # 출력이 반드시 따라야 할 요구 사항 목록\n",
    "    requirements: List[str]\n",
    "\n",
    "\n",
    "# 최신 LLM 모델 이름 가져오기\n",
    "MODEL_NAME = get_model_name(LLMs.GPT4_1)\n",
    "# LLM 초기화\n",
    "llm = ChatOpenAI(temperature=0, model=MODEL_NAME)\n",
    "# PromptInstructions 구조체를 바인딩\n",
    "llm_with_tool = llm.bind_tools([PromptInstructions])\n",
    "\n",
    "\n",
    "# 상태 정보를 기반으로 메시지 체인을 생성하고 LLM 호출\n",
    "def info_chain(state):\n",
    "    # 상태에서 메시지 정보를 가져와 시스템 메시지와 결합\n",
    "    messages = get_messages_info(state[\"messages\"])\n",
    "    # LLM을 호출하여 응답 생성\n",
    "    response = llm_with_tool.invoke(messages)\n",
    "    # 생성된 응답을 메시지 목록으로 반환\n",
    "    return {\"messages\": [response]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage, ToolMessage, SystemMessage\n",
    "\n",
    "# 프롬프트를 생성하는 메타 프롬프트 정의\n",
    "META_PROMPT = \"\"\"Given a task description or existing prompt, produce a detailed system prompt to guide a language model in completing the task effectively.\n",
    "\n",
    "# Guidelines\n",
    "\n",
    "- Understand the Task: Grasp the main objective, goals, requirements, constraints, and expected output.\n",
    "- Minimal Changes: If an existing prompt is provided, improve it only if it's simple. For complex prompts, enhance clarity and add missing elements without altering the original structure.\n",
    "- Reasoning Before Conclusions**: Encourage reasoning steps before any conclusions are reached. ATTENTION! If the user provides examples where the reasoning happens afterward, REVERSE the order! NEVER START EXAMPLES WITH CONCLUSIONS!\n",
    "    - Reasoning Order: Call out reasoning portions of the prompt and conclusion parts (specific fields by name). For each, determine the ORDER in which this is done, and whether it needs to be reversed.\n",
    "    - Conclusion, classifications, or results should ALWAYS appear last.\n",
    "- Examples: Include high-quality examples if helpful, using placeholders [in brackets] for complex elements.\n",
    "   - What kinds of examples may need to be included, how many, and whether they are complex enough to benefit from placeholders.\n",
    "- Clarity and Conciseness: Use clear, specific language. Avoid unnecessary instructions or bland statements.\n",
    "- Formatting: Use markdown features for readability. DO NOT USE ``` CODE BLOCKS UNLESS SPECIFICALLY REQUESTED.\n",
    "- Preserve User Content: If the input task or prompt includes extensive guidelines or examples, preserve them entirely, or as closely as possible. If they are vague, consider breaking down into sub-steps. Keep any details, guidelines, examples, variables, or placeholders provided by the user.\n",
    "- Constants: DO include constants in the prompt, as they are not susceptible to prompt injection. Such as guides, rubrics, and examples.\n",
    "- Output Format: Explicitly the most appropriate output format, in detail. This should include length and syntax (e.g. short sentence, paragraph, JSON, etc.)\n",
    "    - For tasks outputting well-defined or structured data (classification, JSON, etc.) bias toward outputting a JSON.\n",
    "    - JSON should never be wrapped in code blocks (```) unless explicitly requested.\n",
    "\n",
    "The final prompt you output should adhere to the following structure below. Do not include any additional commentary, only output the completed system prompt. SPECIFICALLY, do not include any additional messages at the start or end of the prompt. (e.g. no \"---\")\n",
    "\n",
    "[Concise instruction describing the task - this should be the first line in the prompt, no section header]\n",
    "\n",
    "[Additional details as needed.]\n",
    "\n",
    "[Optional sections with headings or bullet points for detailed steps.]\n",
    "\n",
    "# Steps [optional]\n",
    "\n",
    "[optional: a detailed breakdown of the steps necessary to accomplish the task]\n",
    "\n",
    "# Output Format\n",
    "\n",
    "[Specifically call out how the output should be formatted, be it response length, structure e.g. JSON, markdown, etc]\n",
    "\n",
    "[User given variables should be wrapped in {{brackets}}]\n",
    "\n",
    "<Question>\n",
    "{{question}}\n",
    "</Question>\n",
    "\n",
    "<Answer>\n",
    "{{answer}}\n",
    "</Answer>\n",
    "\n",
    "# Examples [optional]\n",
    "\n",
    "[Optional: 1-3 well-defined examples with placeholders if necessary. Clearly mark where examples start and end, and what the input and output are. User placeholders as necessary.]\n",
    "[If the examples are shorter than what a realistic example is expected to be, make a reference with () explaining how real examples should be longer / shorter / different. AND USE PLACEHOLDERS! ]\n",
    "\n",
    "# Notes [optional]\n",
    "\n",
    "[optional: edge cases, details, and an area to call or repeat out specific important considerations]\n",
    "\n",
    "# Based on the following requirements, write a good prompt template:\n",
    "\n",
    "{reqs}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# 프롬프트 생성을 위한 메시지 가져오기 함수\n",
    "# 도구 호출 이후의 메시지만 가져옴\n",
    "def get_prompt_messages(messages: list):\n",
    "    # 도구 호출 정보를 저장할 변수 초기화\n",
    "    tool_call = None\n",
    "    # 도구 호출 이후의 메시지를 저장할 리스트 초기화\n",
    "    other_msgs = []\n",
    "    # 메시지 목록을 순회하며 도구 호출 및 기타 메시지 처리\n",
    "    for m in messages:\n",
    "        # AI 메시지 중 도구 호출이 있는 경우 도구 호출 정보 저장\n",
    "        if isinstance(m, AIMessage) and m.tool_calls:\n",
    "            tool_call = m.tool_calls[0][\"args\"]\n",
    "        # ToolMessage는 건너뜀\n",
    "        elif isinstance(m, ToolMessage):\n",
    "            continue\n",
    "        # 도구 호출 이후의 메시지를 리스트에 추가\n",
    "        elif tool_call is not None:\n",
    "            other_msgs.append(m)\n",
    "    # 시스템 메시지와 도구 호출 이후의 메시지를 결합하여 반환\n",
    "    return [SystemMessage(content=META_PROMPT.format(reqs=tool_call))] + other_msgs\n",
    "\n",
    "\n",
    "# 프롬프트 생성 체인 함수 정의\n",
    "def prompt_gen_chain(state):\n",
    "    # 상태에서 프롬프트 메시지를 가져옴\n",
    "    messages = get_prompt_messages(state[\"messages\"])\n",
    "    # LLM을 호출하여 응답 생성\n",
    "    response = llm.invoke(messages)\n",
    "    # 생성된 응답을 메시지 목록으로 반환\n",
    "    return {\"messages\": [response]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END\n",
    "\n",
    "\n",
    "# 상태 결정 함수 정의\n",
    "# 상태에서 메시지 목록을 가져옴\n",
    "def get_state(state):\n",
    "    messages = state[\"messages\"]\n",
    "    # 마지막 메시지가 AIMessage이고 도구 호출이 있는 경우\n",
    "    if isinstance(messages[-1], AIMessage) and messages[-1].tool_calls:\n",
    "        # 도구 메시지를 추가해야 하는 상태 반환\n",
    "        return \"add_tool_message\"\n",
    "    # 마지막 메시지가 HumanMessage가 아닌 경우\n",
    "    elif not isinstance(messages[-1], HumanMessage):\n",
    "        # 대화 종료 상태 반환\n",
    "        return END\n",
    "    # 기본적으로 정보 수집 상태 반환\n",
    "    return \"info\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_core.messages import ToolMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "\n",
    "# State 정의\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "\n",
    "# 메모리에 대화 기록을 저장하기 위한 MemorySaver 초기화\n",
    "memory = MemorySaver()\n",
    "\n",
    "# 상태 그래프 초기화\n",
    "workflow = StateGraph(State)\n",
    "\n",
    "# 노드 추가\n",
    "workflow.add_node(\"info\", info_chain)\n",
    "workflow.add_node(\"prompt\", prompt_gen_chain)\n",
    "\n",
    "\n",
    "# 도구 메시지 추가 상태 노드 정의\n",
    "@workflow.add_node\n",
    "def add_tool_message(state: State):\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            ToolMessage(\n",
    "                content=\"Prompt generated!\",\n",
    "                tool_call_id=state[\"messages\"][-1].tool_calls[0][\n",
    "                    \"id\"\n",
    "                ],  # 상태에서 도구 호출 ID를 가져와 메시지에 추가\n",
    "            )\n",
    "        ]\n",
    "    }\n",
    "\n",
    "\n",
    "# 조건부 상태 전환 정의\n",
    "workflow.add_conditional_edges(\"info\", get_state, [\"add_tool_message\", \"info\", END])\n",
    "\n",
    "# 엣지 정의\n",
    "workflow.add_edge(\"add_tool_message\", \"prompt\")\n",
    "workflow.add_edge(\"prompt\", END)\n",
    "workflow.add_edge(START, \"info\")\n",
    "\n",
    "# 그래프 컴파일\n",
    "graph = workflow.compile(checkpointer=memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_teddynote.graphs import visualize_graph\n",
    "\n",
    "visualize_graph(graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[사용자] \n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36minfo\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "안녕하세요! 어떤 종류의 프롬프트 템플릿을 만들고 싶으신가요?\n",
      "\n",
      "아래 정보를 알려주시면 맞춤형 프롬프트를 만들어드릴 수 있습니다:\n",
      "1. 프롬프트의 목적(목표)은 무엇인가요?\n",
      "2. 프롬프트에 어떤 변수(입력값)가 들어가야 하나요?\n",
      "3. 출력 결과에서 반드시 피해야 할(하지 말아야 할) 점이 있나요?\n",
      "4. 출력 결과가 반드시 지켜야 할 요구사항이 있나요?\n",
      "\n",
      "자세히 설명해주시면 더 좋은 결과를 드릴 수 있습니다!\n",
      "[사용자] 광고용으로 사용할 아주 귀엽고 사랑스러운 고양이 털은 복슬복슬하고 길면안돼 살은 적당이 있어야돼\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36minfo\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "좋은 정보 감사합니다! 몇 가지 추가로 확인하고 싶습니다.\n",
      "\n",
      "1. 프롬프트의 목적: 광고용으로 사용할 귀엽고 사랑스러운 고양이 묘사(설명)를 생성하는 것이 맞나요? 아니면 이미지 생성, 슬로건, 문구 등 다른 목적이 있으신가요?\n",
      "2. 변수: 프롬프트에 포함되어야 할 변수(예: 고양이의 색상, 나이, 성격 등)가 있다면 알려주세요.\n",
      "3. 출력에서 반드시 피해야 할 점: \"털이 복슬복슬하고 길면 안 된다\"는 점을 명확히 이해했습니다. 혹시 추가로 피해야 할 점이 있을까요?\n",
      "4. 출력에서 반드시 지켜야 할 요구사항: \"살이 적당히 있어야 한다\" 외에 추가로 꼭 들어가야 할 요소가 있을까요? 예를 들어, 특정 분위기, 감정, 단어 사용 등.\n",
      "\n",
      "조금만 더 구체적으로 알려주시면 최적의 프롬프트를 만들어드릴 수 있습니다!\n",
      "[사용자] 삼색냥이로 해줘\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36minfo\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "감사합니다! 정리해서 확인드릴게요.\n",
      "\n",
      "1. 목적: 광고용으로 사용할 아주 귀엽고 사랑스러운 고양이(삼색냥이)를 묘사하는 프롬프트\n",
      "2. 변수: 삼색 고양이(삼색냥이)\n",
      "3. 피해야 할 점(제약): 털이 복슬복슬하거나 길면 안 됨\n",
      "4. 반드시 지켜야 할 요구사항: 살이 적당히 있어야 함\n",
      "\n",
      "혹시 추가로 포함하고 싶은 변수(예: 고양이의 나이, 성격, 배경 등)나, 반드시 들어가야 할 문구, 분위기, 감정 등이 있을까요?  \n",
      "없으시다면 바로 프롬프트를 생성해드리겠습니다!\n",
      "[사용자] \n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36minfo\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mprompt\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Write a persuasive and heartwarming description of a very cute and lovely calico cat for advertising purposes.\n",
      "\n",
      "- Focus on making the calico cat sound appealing, adorable, and lovable.\n",
      "- Do not describe the cat’s fur as fluffy or long.\n",
      "- Clearly mention that the cat has a moderate amount of body fat, using positive and endearing language.\n",
      "- Use vivid, engaging details to help potential adopters or buyers imagine the cat’s personality and charm.\n",
      "\n",
      "# Steps\n",
      "\n",
      "1. Consider the calico cat’s appearance, personality, and unique features.\n",
      "2. Emphasize the cat’s cuteness and lovability.\n",
      "3. Describe the body type as having a moderate amount of body fat in a positive way.\n",
      "4. Avoid any mention of fluffy or long fur.\n",
      "5. Write the description in a warm, inviting tone suitable for advertising.\n",
      "\n",
      "# Output Format\n",
      "\n",
      "Provide a single, well-written paragraph (3-5 sentences) suitable for use in an advertisement.\n",
      "\n",
      "<Question>\n",
      "Describe a very cute and lovely calico cat for advertising purposes.\n",
      "</Question>\n",
      "\n",
      "<Answer>\n",
      "[Your description here]\n",
      "</Answer>\n",
      "\n",
      "# Examples\n",
      "\n",
      "Example 1:\n",
      "<Question>\n",
      "Describe a very cute and lovely calico cat for advertising purposes.\n",
      "</Question>\n",
      "<Answer>\n",
      "Meet Patches, a delightful calico cat whose vibrant coat and gentle eyes will instantly steal your heart. With her soft, colorful fur and a charmingly rounded figure, she’s the perfect cuddle companion. Patches loves to lounge in sunny spots and greet you with a sweet purr, making every day brighter. Her moderate, healthy body shape adds to her irresistible appeal—she’s just waiting for a loving home to call her own!\n",
      "</Answer>\n",
      "\n",
      "Example 2:\n",
      "<Question>\n",
      "Describe a very cute and lovely calico cat for advertising purposes.\n",
      "</Question>\n",
      "<Answer>\n",
      "This adorable calico cat is the picture of affection, with a cheerful personality and a beautifully patterned coat. She has a moderate amount of body fat, giving her a huggable, endearing look that’s impossible to resist. Always ready for playtime or a cozy nap by your side, she’s sure to bring warmth and joy to any home.\n",
      "</Answer>\n",
      "\n",
      "# Notes\n",
      "\n",
      "- Do not use the words “fluffy” or “long” to describe the fur.\n",
      "- The description should be positive and suitable for an advertisement.\n",
      "- The cat’s moderate body fat should be presented as a lovable trait.\n",
      "[사용자] \n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36minfo\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "혹시 추가로 포함하고 싶은 변수(예: 고양이의 나이, 성격, 배경 등)나, 반드시 들어가야 할 문구, 분위기, 감정 등이 있으신가요?  \n",
      "아니면 지금까지 주신 정보(삼색냥이, 털이 복슬복슬하거나 길면 안 됨, 살이 적당히 있어야 함)만으로 충분하다면, 바로 사용하실 수 있는 영어 프롬프트를 위에 안내해드렸습니다.\n",
      "\n",
      "더 추가하거나 수정할 내용이 있으시면 말씀해 주세요!  \n",
      "필요하신 경우, 프롬프트를 다시 생성해드릴 수도 있습니다.\n",
      "[사용자] 없어\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36minfo\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "알겠습니다! 추가 정보 없이, 지금까지 주신 조건만 반영해서 프롬프트를 사용하시면 됩니다.\n",
      "\n",
      "아래 영어 프롬프트를 광고용 삼색냥이 묘사에 활용하실 수 있습니다:\n",
      "\n",
      "Write a persuasive and heartwarming description of a very cute and lovely calico cat for advertising purposes.\n",
      "\n",
      "- Focus on making the calico cat sound appealing, adorable, and lovable.\n",
      "- Do not describe the cat’s fur as fluffy or long.\n",
      "- Clearly mention that the cat has a moderate amount of body fat, using positive and endearing language.\n",
      "- Use vivid, engaging details to help potential adopters or buyers imagine the cat’s personality and charm.\n",
      "\n",
      "필요하실 때 언제든 다시 요청해 주세요!\n",
      "[사용자] q\n",
      "AI: 다음에 또 만나요!\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "from langchain_teddynote.messages import stream_graph\n",
    "\n",
    "# 구성 설정 초기화 (고유한 thread_id 생성)\n",
    "config = {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\n",
    "\n",
    "# 무한 루프 시작\n",
    "while True:\n",
    "    try:\n",
    "        # 사용자 입력 받기\n",
    "        user = input(\"User (q/Q to quit): \")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # 사용자 입력 출력\n",
    "    print(f\"\\n[사용자] {user}\")\n",
    "\n",
    "    # 'q' 또는 'Q' 입력 시 루프 종료\n",
    "    if user in {\"q\", \"Q\"}:\n",
    "        print(\"AI: 다음에 또 만나요!\")\n",
    "        break\n",
    "\n",
    "    # 출력 변수 초기화\n",
    "    output = None\n",
    "\n",
    "    stream_graph(\n",
    "        graph,\n",
    "        {\"messages\": [HumanMessage(content=user)]},\n",
    "        config=config,\n",
    "        node_names=[\"info\", \"prompt\"],\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "03. CRAG(Corrective RAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rag.pdf import PDFRetrievalChain\n",
    "\n",
    "# PDF 문서를 로드\n",
    "pdf = PDFRetrievalChain([\"data/SPRI_AI_Brief_2023년12월호_F.pdf\"]).create_chain()\n",
    "\n",
    "# retriever 와 chain을 생성\n",
    "pdf_retriever = pdf.retriever\n",
    "pdf_chain = pdf.chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_teddynote.models import get_model_name, LLMs\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# 모델 이름 가져오기\n",
    "MODEL_NAME = get_model_name(LLMs.GPT4_1)\n",
    "\n",
    "\n",
    "# 검색된 문서의 관련성 여부를 이진 점수로 평가하는 데이터 모델\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"A binary score to determine the relevance of the retrieved document.\"\"\"\n",
    "\n",
    "    # 문서가 질문과 관련이 있는지 여부를 'yes' 또는 'no'로 나타내는 필드\n",
    "    binary_score: str = Field(\n",
    "        description=\"Documents are relevant to the question, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "\n",
    "# LLM 초기화\n",
    "llm = ChatOpenAI(model=MODEL_NAME, temperature=0)\n",
    "\n",
    "# GradeDocuments 데이터 모델을 사용하여 구조화된 출력을 생성하는 LLM\n",
    "structured_llm_grader = llm.with_structured_output(GradeDocuments)\n",
    "\n",
    "# 시스템 프롬프트 정의\n",
    "system = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "    If the document contains keyword(s) or semantic meaning related to the question, grade it as relevant. \\n\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\"\n",
    "\n",
    "# 채팅 프롬프트 템플릿 생성\n",
    "grade_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Retrieval 평가기 초기화\n",
    "retrieval_grader = grade_prompt | structured_llm_grader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary_score='yes'\n"
     ]
    }
   ],
   "source": [
    "# 질문 정의\n",
    "question = \"삼성전자가 개발한 생성AI 에 대해 설명하세요.\"\n",
    "\n",
    "# 문서 검색\n",
    "docs = pdf_retriever.invoke(question)\n",
    "\n",
    "# 검색된 문서 중 1번 index 문서의 페이지 내용을 추출\n",
    "doc_txt = docs[1].page_content\n",
    "\n",
    "# 검색된 문서와 질문을 사용하여 관련성 평가를 실행하고 결과 출력\n",
    "print(retrieval_grader.invoke({\"question\": question, \"document\": doc_txt}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "삼성전자가 개발한 생성AI는 ‘삼성 가우스(Samsung Gauss)’로, 언어 모델, 코드 모델, 이미지 모델의 3개 모델로 구성되어 있다. 삼성 가우스는 온디바이스(On-device)에서 작동 가능하며, 외부로 사용자 정보가 유출될 위험이 없는 것이 특징이다. 언어 모델은 메일 작성, 문서 요약, 번역 업무를 지원하고, 코드 모델은 AI 코딩 어시스턴트 ‘코드아이(code.i)’로 사내 소프트웨어 개발에 최적화되어 있다. 이미지 모델은 창의적 이미지 생성, 이미지 변환, 저해상도 이미지의 고해상도 전환을 지원한다. 삼성전자는 삼성 가우스를 다양한 제품에 단계적으로 탑재할 계획이다.\n",
      "\n",
      "**Source**\n",
      "- data/SPRI_AI_Brief_2023년12월호_F.pdf (p.13)\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "# LangChain Hub에서 RAG 프롬프트를 가져와 사용\n",
    "prompt = hub.pull(\"teddynote/rag-prompt\")\n",
    "\n",
    "# LLM 초기화\n",
    "llm = ChatOpenAI(model_name=MODEL_NAME, temperature=0)\n",
    "\n",
    "\n",
    "# 문서 포맷팅\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(\n",
    "        [\n",
    "            f'<document><content>{doc.page_content}</content><source>{doc.metadata[\"source\"]}</source><page>{doc.metadata[\"page\"]+1}</page></document>'\n",
    "            for doc in docs\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "# 체인 생성\n",
    "rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "\n",
    "# 체인 실행 및 결과 출력\n",
    "generation = rag_chain.invoke({\"context\": format_docs(docs), \"question\": question})\n",
    "print(generation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# LLM 설정\n",
    "llm = ChatOpenAI(model=MODEL_NAME, temperature=0)\n",
    "\n",
    "# Query Rewrite 시스템 프롬프트\n",
    "system = \"\"\"You a question re-writer that converts an input question to a better version that is optimized \n",
    "for web search. Look at the input and try to reason about the underlying semantic intent / meaning.\"\"\"\n",
    "\n",
    "# 프롬프트 정의\n",
    "re_write_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Here is the initial question: \\n\\n {question} \\n Formulate an improved question.\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Question Re-writer 체인 초기화\n",
    "question_rewriter = re_write_prompt | llm | StrOutputParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[원본 질문]: \"삼성전자가 개발한 생성AI 에 대해 설명하세요.\"\n",
      "[쿼리 재작성]: 삼성전자가 개발한 생성 AI의 주요 특징과 활용 사례는 무엇인가요?\n"
     ]
    }
   ],
   "source": [
    "# 실행 및 결과 확인\n",
    "print(f'[원본 질문]: \"{question}\"')\n",
    "print(\"[쿼리 재작성]:\", question_rewriter.invoke({\"question\": question}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 웹 검색 도구 초기화\n",
    "from langchain_teddynote.tools.tavily import TavilySearch\n",
    "\n",
    "# 최대 검색 결과를 3으로 설정\n",
    "web_search_tool = TavilySearch(max_results=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'url': 'https://v.daum.net/v/20241121100020817', 'title': \"삼성전자, 자체 개발한 생성형 AI '삼성 가우스2' 공개 - Daum\", 'content': '### 삼성전자, 자체 개발한 생성형 AI \\'삼성 가우스2\\' 공개 또한 \"삼성전자 고유의 생성형 AI 모델인 \\'삼성 가우스2\\'를 공개하고, 향상된 성능과 효율, 그리고 활용 방안에 대해 설명할 예정\"이라며 \"이를 통해 업무 생산성 향상과 단계적 제품 적용으로 더욱 편리하고 차별화된 고객 경험을 제공할 것\"이라고 말했다. 삼성전자는 키노트 발표에서 지난해 처음 공개한 생성형 AI 모델 \\'삼성 가우스1(Samsung Gauss1)\\'의 후속 모델인 \\'삼성 가우스2(Samsung Gauss2)\\'를 공개했다. 삼성 가우스 포탈(Samsung Gauss Portal)은 \\'삼성 가우스\\'의 대화형 AI 서비스로 ▲문서 요약 ▲번역 ▲메일 작성 등 DX부문 직원들의 다양한 사무 업무를 빠르고 효율적으로 처리할 수 있도록 지원하고 있다. 삼성전자는 향후 \\'삼성 가우스2\\'를 통해 \\'코드아이(code.i)\\'서비스의 지속적인 성능 개선, \\'삼성 가우스 포탈(Samsung Gauss Portal)\\'의 자연어 질의응답 성능 향상, 표와 차트의 이해, 이미지 생성 등 멀티모달 기능을 지원해 사내 생산성 향상에 기여할 계획이다.', 'score': 0.83549726, 'raw_content': '# [지디넷코리아](https://v.daum.net/channel/294/home)\\n\\n## 검색\\n\\n검색\\n\\n\\n\\n## 속보\\n\\n### 삼성전자, 자체 개발한 생성형 AI \\'삼성 가우스2\\' 공개\\n\\n장경윤 기자 2024. 11. 21. 10:00\\n\\n**음성재생 설정**\\n\\n[닫기](#none)\\n\\n**번역*beta*** Translated by [kaka i](https://kakao.ai/)\\n\\n[닫기](#none)\\n\\n**번역중** Now in translation\\n\\n**글자크기 설정** 파란원을 좌우로 움직이시면 글자크기가 변경 됩니다.\\n\\n가\\n\\n\\n\\n\\n\\n\\n\\n가\\n\\n이 글자크기로 변경됩니다.\\n\\n(예시) 가장 빠른 뉴스가 있고 다양한 정보, 쌍방향 소통이 숨쉬는 다음뉴스를 만나보세요. 다음뉴스는 국내외 주요이슈와 실시간 속보, 문화생활 및 다양한 분야의 뉴스를 입체적으로 전달하고 있습니다.\\n\\n[닫기](#none)\\n\\n**21일 \\'삼성 개발자 콘퍼런스 코리아\\'서 최신 기술과 비전 소개**\\n\\n(지디넷코리아=장경윤 기자)삼성전자는\\xa0\\'삼성 개발자 콘퍼런스 코리아 2024(Samsung Developer Conference Korea 2024, SDC24 Korea)\\'를 온라인으로 개최했다고 21일 밝혔다.\\n\\n삼성전자는 소프트웨어 개발자들과 소통하고 협력하기 위해 2014년부터 매년 개발자 행사를 개최해 왔으며, 올해로 11주년을 맞이했다.\\n\\n이날 콘퍼런스에서는 ▲생성형 AI ▲소프트웨어(S/W) 플랫폼 ▲IoT ▲헬스케어 ▲통신 ▲데이터 등 제품에 탑재된 다양한 소프트웨어의 연구 분야는 물론 오픈소스 개발 문화에 이르기까지 다양한 세션이 마련됐다.\\n\\n삼성전자 DX부문 최고기술책임자(CTO) 겸 삼성리서치장인 전경훈 사장은 환영사를 통해 \"삼성전자는 인공지능과 데이터 분석과 같은 최신 소프트웨어 기술 확보에 집중하여 새로운 라이프스타일을 창조하고 고객의 삶을 개선하는데 주력하고 있다\"고 강조했다.\\n\\n또한 \"삼성전자 고유의 생성형 AI 모델인 \\'삼성 가우스2\\'를 공개하고, 향상된 성능과 효율, 그리고 활용 방안에 대해 설명할 예정\"이라며 \"이를 통해 업무 생산성 향상과 단계적 제품 적용으로 더욱 편리하고 차별화된 고객 경험을 제공할 것\"이라고 말했다.\\n\\n**■ 자체 개발 생성형 AI 모델 2세대 \\'삼성 가우스2\\' 공개**\\n\\n삼성전자는 키노트 발표에서 지난해 처음 공개한 생성형 AI 모델 \\'삼성 가우스1(Samsung Gauss1)\\'의 후속 모델인 \\'삼성 가우스2(Samsung Gauss2)\\'를 공개했다.\\n\\n\\'삼성 가우스2\\'는 언어·코드·이미지 통합 멀티모달(Multimodal) 모델로 기존 \\'삼성 가우스1\\'과 달리 여러가지 데이터 유형을 동시 처리할 수 있는 2세대 버전이다. 서비스 용도에 따라 ▲콤팩트(Compact) ▲밸런스드(Balanced) ▲슈프림(Supreme) 세 가지 모델로 구성되어 있다.\\n\\n멀티모달은 텍스트, 이미지, 오디오, 비디오 등 다양한 유형의 데이터(모달리티)를 함께 고려하여 서로의 관계성을 학습 및 처리하는 인공지능이다. 인간이 오감을 모두 활용해 정보를 이해하고 처리하는 방식과 유사하다.\\n\\n\\'콤팩트\\'는 제한된 컴퓨팅 환경에서도 효율적으로 작동하도록 설계된 소형 모델이다. 온디바이스(On-Device) 환경에서 기기의 특성을 최대한 활용해 기기에 최적화된 성능을 제공한다.\\n\\n\\'밸런스드\\'는 클라우드(Cloud) 기반으로 성능, 속도 면에서 안정성과 효율성의 균형을 맞춘 모델이다. 다양한 작업에서도 균형 잡힌 성능을 제공한다.\\n\\n마지막으로 \\'슈프림\\'은 최고 성능을 목표로 하는 고성능 모델이다. \\'밸런스드\\' 모델을 기반으로 \\'전문가 혼합(MoE, Mixture of Experts)\\' 기술을 이용해 학습과 추론 과정에서 계산량을 대폭 줄여 성능과 효율성 모두를 높일 수 있었다.\\n\\n전문가 혼합 기술은 특정 작업 처리 시 가장 알맞은 일부 전문가 모델들만 선택, 활성화해 처리하는 방식을 뜻한다. 필요한 연산 자원을 절약해 효율적으로 성능을 높일 수 있다.\\n\\n또한 \\'삼성 가우스2\\'는 모델에 따라 9~14개국의 언어와 다양한 프로그래밍 언어를 지원한다. 삼성전자는 자체적으로 거대언어모델(LLM) 학습 안정화 기법을 개발해 적용하고, 자체 \\'토크나이저(Tokenizer)\\'를 설계해 지원하는 언어에 대한 효율성을 극대화했다.\\n\\n토크나이저는\\xa0글자, 이미지, 음성 등의 입력을 생성형 모델이 처리 할 수 있는 토큰으로 변환 및 분할하는 알고리즘으로, 데이터를 벡터 형태의 숫자 형식인 토큰으로 표현하는 방법이다.\\n\\n\\'밸런스드\\'와 \\'슈프림\\' 모델은 현재 공개되어 있는 대표적인 오픈소스 생성형 AI 모델들 대비 영어, 한국어 등 다양한 언어 답변 생성, 코딩 등 주요 지표에서 동등 이상의 성능을 제공한다. 시간당 처리 속도는 1.5~3배 이상으로 그만큼 AI의 답변 생성이 빨라 사용자의 대기시간이 줄어들고 효율적인 일처리가 가능하다.\\n\\n**■ 사내 맞춤형 개발 \\'삼성 가우스\\',****다양한 업무·제품 개발에 활용**\\n\\n자체 생성형 AI 모델의 장점은 필요한 목적과 응용분야에 맞춰 최고의 성능을 내도록 맞춤형 개발에 용이하다는 것이다. 실제로 \\'삼성 가우스\\'는 이런 맞춤형 개발의 장점을 살려 직원들의 다양한 업무에서 활용되고 있다.\\n\\n\\'코드아이(code.i)\\'는 사내 S/W 개발자를 지원하는 \\'삼성 가우스\\' 모델의 코딩 어시스턴트 서비스이다. 최근에는 \\'삼성 가우스2\\' 모델로 업그레이드되어 DX부문의 사업부 및 일부 해외 연구소에서 활용되고 있다.\\n\\n\\'코드아이\\'는 작년 12월 서비스 시작 시점 대비 현재 월별 사용량은 약 4배 이상 증가했고, 삼성전자 DX부문 전체 S/W 개발자의 약 60%가 사용하고 있다.\\n\\n삼성 가우스 포탈(Samsung Gauss Portal)은 \\'삼성 가우스\\'의 대화형 AI 서비스로 ▲문서 요약 ▲번역 ▲메일 작성 등 DX부문 직원들의 다양한 사무 업무를 빠르고 효율적으로 처리할 수 있도록 지원하고 있다. 지난 4월에는 해외 법인으로 서비스가 확대되었다.\\n\\n또 삼성전자는 올해 8월부터 \\'삼성 가우스\\'를 콜센터에 적용해 상담 내용을 자동 분류하고 요약하는 등 상담원의 업무를 보조하고 있다.\\n\\n삼성전자는 향후 \\'삼성 가우스2\\'를 통해 \\'코드아이(code.i)\\'서비스의 지속적인 성능 개선, \\'삼성 가우스 포탈(Samsung Gauss Portal)\\'의 자연어 질의응답 성능 향상, 표와 차트의 이해, 이미지 생성 등 멀티모달 기능을 지원해 사내 생산성 향상에 기여할 계획이다.\\n\\n또한 삼성전자는 \\'모두를 위한 AI(AI for All)\\' 라는 AI 비전 아래, 사용자들이 더욱 편리하고 즐거운 일상을 누릴 수 있도록 전 제품 군에 AI 서비스를 제공할 예정이다. 향후 지식 그래프(Knowledge Graph) 기술과도 결합해 한층 강화된 개인화 서비스를 선보일 것으로 기대된다.\\n\\n**■ S/W 플랫폼, IoT, 헬스케어, 통신, 데이터 등 다양한 주제로 발표 진행**\\n\\n\\'삼성 가우스2\\' 발표에 이어, \\'삼성의 플랫폼에 대한 고객경험\\'을 주제로 ▲스마트싱스(SmartThings) 플랫폼 고객 경험 ▲소프트웨어 플랫폼 고객경험 향상에 대한 키노트가 이어졌다.\\n\\n이후 ▲헬스케어의 미래와 삼성 헬스 에코시스템 전략 ▲생성형 AI를 통한 스마트싱스(SmartThings) 고객 VOC 경험 개선기 ▲code.i: 삼성의 AI 코딩 어시스턴트를 이해하다 ▲당신의 일상에 활력을 더하는 TV 기반 라이프스타일 콘텐츠 허브 소개 ▲AI 비전(Vision) 기술과 데이터 활용을 통한 삼성 가전 AI 솔루션 등 총 29개의 다양한 기술 세션이 이어졌다.\\n\\n장경윤 기자(jkyoon@zdnet.co.kr)\\n\\nCopyright © 지디넷코리아. 무단전재 및 재배포 금지.\\n\\n**지디넷코리아에서 직접 확인하세요.** 해당 언론사로 이동합니다.\\n\\n* [삼성전자, 美 AI 반도체 스타트업 \\'엔파브리카\\'에 투자](https://zdnet.co.kr/view/?no=20241120103015&re=D_20241121083205)\\n* [삼성전자 \\'글로벌 골즈 앱\\' 앤썸 어워드 수상...사회적 영향력 인정](https://zdnet.co.kr/view/?no=20241120091602&re=D_20241121083205)\\n* [삼성전자, 갤럭시워치6 등 기존 모델 \\'원 UI 6 워치\\' 업데이트 실시](https://zdnet.co.kr/view/?no=20241120085039&re=D_20241121083205)\\n* [삼성전자, \\'XR 스마트안경\\' 내년 3분기 출시 전망...애플과 맞대결](https://zdnet.co.kr/view/?no=20241119120941&re=D_20241121083205)\\n* [내년 정부 R&D예산 35.3조원 \\'역대 최대\\'…AI, 106% 증액](https://zdnet.co.kr/view/?no=20250822102653)\\n* [퀄컴칩 비싸다?...삼성 옥죄는 모바일 AP 비용 부담의 진짜 이유](https://zdnet.co.kr/view/?no=20250821104639)\\n* [[유미\\'s 픽] \\'모바일 강자\\' 유라클, AI에 승부수...빠른 시장 안착에 기대감 \\'업\\'](https://zdnet.co.kr/view/?no=20250820182031)\\n* [[데이터 주권] AI 경쟁력, 국가 협력 통한 데이터 개방이 핵심](https://zdnet.co.kr/view/?no=20250821093019)\\n* [4인 가족에 유모차·캠핑장비까지…패밀리카 끝판왕이라는 \\'이 車\\'](https://zdnet.co.kr/view/?no=20250821102605)\\n* [\"전기료 인상 불가피\" 대통령 발언에 산업계 근심](https://zdnet.co.kr/view/?no=20250821173845)'}, {'url': 'https://www.esgeconomy.com/news/articleView.html?idxno=5090', 'title': \"삼성전자, 자체 개발한 생성형 AI '삼성 가우스' 첫 공개 - ESG경제\", 'content': \"### 삼성전자, 자체 개발한 생성형 AI '삼성 가우스' 첫 공개 삼성전자가 자체 개발한 생성형 AI '삼성 가우스'. [ESG경제=김도산 기자] 삼성전자가 자체 개발한 생성형 인공지능(AI) 모델 '삼성 가우스(Samsung Gauss)‘를 처음 공개했다. 삼성전자는 삼성 가우스를 활용해 임직원의 업무 생산성을 높이고, 생성형 AI 모델을 단계적으로 제품에 탑재해 새로운 사용자 경험을 제공한다는 계획이다. 삼성전자는 8일 삼성전자 서울R&D캠퍼스에서 '삼성 AI 포럼 2023' 2일차 행사를 열고 삼성리서치에서 개발한 삼성 가우스를 처음으로 공개했다고 밝혔다. 삼성전자는 생성형 AI 모델들을 다양한 제품에 단계적으로 탑재할 계획인데, 업계에서는 이르면 내년 1월 공개할 갤럭시 S24에 생성형 AI 기능이 탑재될 것으로 보고 있다. 한편 삼성리서치에서 주관한 이날 ’'삼성 AI 포럼 2023' 포럼에는 삼성전자의 AI 연구 인력과 학계·업계 AI 전문가 150여명이 참석해 생성형 AI 기술의 발전 방향을 논의하고 최신 기술 동향을 공유했다.\", 'score': 0.8342047, 'raw_content': '## 상단영역\\n\\n# [ESG경제](https://www.esgeconomy.com)\\n\\n![ESG경제](https://cdn.esgeconomy.com/image/logo/toplogo_20201116093648.png)\\n\\n## 본문영역\\n\\n![ESG경제](https://cdn.esgeconomy.com/image/logo/printlogo_20201106110340.png)\\n\\n#### SNS 기사보내기\\n\\n#### SNS 기사보내기\\n\\n### 현재위치\\n\\n### 삼성전자, 자체 개발한 생성형 AI \\'삼성 가우스\\' 첫 공개\\n\\n#### SNS 기사보내기\\n\\n#### SNS 기사보내기\\n\\n#### \\'삼성 AI 포럼\\' 2일차…온디바이스 기반 AI 기술도 소개 업무생산성 향상…내년 1월 갤럭시 S24에 탑재 가능성\\n\\n![삼성전자가 자체 개발한 생성형 AI \\'삼성 가우스\\'. 사진=삼성전자 제공](https://cdn.esgeconomy.com/news/photo/202311/5090_6771_5138.jpg)\\n\\n[ESG경제=김도산 기자] 삼성전자가 자체 개발한 생성형 인공지능(AI) 모델 \\'삼성 가우스(Samsung Gauss)‘를 처음 공개했다.\\n\\n삼성전자는 삼성 가우스를 활용해 임직원의 업무 생산성을 높이고, 생성형 AI 모델을 단계적으로 제품에 탑재해 새로운 사용자 경험을 제공한다는 계획이다.\\n\\n삼성전자는 8일 삼성전자 서울R&D캠퍼스에서 \\'삼성 AI 포럼 2023\\' 2일차 행사를 열고 삼성리서치에서 개발한 삼성 가우스를 처음으로 공개했다고 밝혔다.\\n\\n삼성 가우스는 머신 러닝과 AI 기술의 근본이 되는 정규분포 이론을 정립한 독일의 천재 수학자이자 ’수학의 왕‘으로 불리는 칼 프리드리히 가우스의 이름을 땄다. 삼성 가우스는 머신 러닝 기술을 기반으로 ▲언어 모델 ▲코드 모델 ▲이미지 모델 등 3가지 모델로 구성됐다.\\n\\n언어 모델은 클라우드와 온디바이스(On-device)를 위한 다양한 모델로 이뤄졌다. 메일 작성과 문서 요약, 번역 등의 업무를 더 쉽고 빠르게 처리할 수 있게 해주며 기기를 더욱 스마트하게 제어해 소비자 경험을 향상시킬 수 있다.\\n\\n코드 모델을 기반으로 개발된 AI 코딩 어시스턴트 \\'코드아이\\'(code.i)는 사내 소프트웨어 개발에 최적화돼 개발자들이 쉽고 빠르게 코딩할 수 있도록 도와준다. 대화형 인터페이스를 통해 코드 설명이나 테스트 케이스 생성 등의 서비스도 제공한다.\\n\\n이미지 모델은 사진이나 그림 등 창의적인 이미지를 손쉽게 만들고 기존 이미지를 원하는 대로 바꿀 수 있도록 도와준다. 저해상도 이미지의 고해상도 전환도 쉽게 가능해진다.\\n\\n삼성전자는 현재 사내에서 삼성 가우스의 베타 테스트를 진행 중이며, 연내에 임직원이 사용할 수 있도록 오픈할 예정이다.\\n\\n삼성전자는 챗GPT 등 외부 생성형 AI 사용 확대로 보안 우려가 커지자. 그동안 번역과 문서 요약 등 임직원의 수요를 고려해 자체 생성형 AI를 개발해 왔다.\\n\\n삼성전자는 생성형 AI 모델들을 다양한 제품에 단계적으로 탑재할 계획인데, 업계에서는 이르면 내년 1월 공개할 갤럭시 S24에 생성형 AI 기능이 탑재될 것으로 보고 있다.\\n\\n![‘삼성 AI 포럼 2023’을 알리는 포스터. 사진=삼성전자 제공](https://cdn.esgeconomy.com/news/photo/202311/5090_6772_5210.jpg)\\n\\n한편 삼성리서치에서 주관한 이날 ’\\'삼성 AI 포럼 2023\\' 포럼에는 삼성전자의 AI 연구 인력과 학계·업계 AI 전문가 150여명이 참석해 생성형 AI 기술의 발전 방향을 논의하고 최신 기술 동향을 공유했다. 생성형 AI 기술이 발전하면서 인간의 삶이 어떻게 변화할지에 대한 심도 잇는 논의도 이뤄졌다.\\n\\n김대현 삼성리서치 글로벌AI센터 부센터장(부사장)은 \"삼성전자는 AI 기술 개발뿐만 아니라 AI 활용 시 보안과 안전성을 확보하기 위해 \\'AI 윤리 원칙\\'을 정해 이를 준수하기 위해 노력하고 있다\"며 \"생성형 AI에 대한 지속적 연구를 통해 소비자의 경험 가치를 높여 나갈 계획\"이라고 말했다.\\n\\n이날 포럼에서는 또 \\'생각의 사슬\\' 논문의 저자이기도 한 오픈AI의 제이슨 웨이 연구원이 \\'대규모 언어 모델 르네상스의 새로운 패러다임\\'을 주제로 대규모 언어 모델로 인한 패러다임 변화와 AI 분야에 미치는 영향에 대해 강연했다.\\n\\n#### 관련기사\\n\\n#### 키워드\\n\\n#### SNS 기사보내기\\n\\n#### **기사 댓글** 0\\n\\n##### 비회원 로그인\\n\\n##### 댓글쓰기\\n\\n## 하단영역\\n\\n![ESG경제](https://cdn.esgeconomy.com/image/logo/downlogo_20201106110340.png)\\n\\n### 하단메뉴\\n\\n### 매체정보\\n\\n![ND소프트](https://cdn.esgeconomy.com/image/ndsoft.svg)\\n\\n## 전체메뉴'}, {'url': 'http://www.seouleconews.com/news/articleView.html?idxno=80148', 'title': '삼성전자 \"AI로 새로운 시대 이끌 것…생성형 AI 기능 발굴\"', 'content': '[서울이코노미뉴스 이보라 기자]\\xa0김대현 삼성리서치 글로벌 AI센터장은 6일 \"인공지능(AI)이 가져올 미래상은 삼성전자의 중요한 화두\"라며 \"일상에서 누구나 쉽게 접근, 활용할 수 있는 AI로 새로운 시대를 이끌 것\"이라고 말했다. 삼성전자는 \\'모두를 위한 AI\\'(AI for All)라는 비전을 가지고 TV, 가전, 스마트폰 등 다양한 제품과 서비스에 AI를 접목하고 있다. 그는 \"삼성전자는 삼성 제품과 서비스를 더욱 유용하게 만들 수 있는 생성형 AI 기능을 발굴하고 최적화하고 있다\"며 \"데이터를 단순 가공·분석하는 것에서 나아가\\xa0사용자 요구에 따라 독창적 결과를 만드는 생성형 AI로 새로운 경험을 제공할 것\"이라고 설명했다. 김 센터장은 또 \\'개인화 경험과 사용성을 강화하는 운영기술\\'에 대해 \"AI 경험을 효율적으로 구현하기 위해 하이브리드 AI를 적용하고 있다\"고 했다. 김 센터장은 \"삼성전자는 AI 개발에 있어 개인정보 보호와 보안을 최우선으로 하고 있다\"며 \"모바일, TV에 이어 향후 가전제품에도 (녹스 매트릭스를) 확대 적용할 계획\"이라고 밝혔다.', 'score': 0.81770587, 'raw_content': '삼성전자 \"AI로 새로운 시대 이끌 것…생성형 AI 기능 발굴\"\\n\\n\\n\\n\\n\\n\\n\\n\\n삼성전자 \"AI로 새로운 시대 이끌 것…생성형 AI 기능 발굴\"\\n\\n\\n\\n\\n* 이보라 기자\\n* 승인 2024.11.06 14:14\\n* 댓글\\n  0\\n\\n* 트위터\\n* 페이스북\\n* 카카오스토리\\n\\n\\n\\n\\n---\\n\\n\\n\\n김대현 삼성리서치 글로벌 AI센터장 뉴스룸 기고문\\n\\n[서울이코노미뉴스 이보라 기자]\\xa0김대현 삼성리서치 글로벌 AI센터장은 6일 \"인공지능(AI)이 가져올 미래상은 삼성전자의 중요한 화두\"라며 \"일상에서 누구나 쉽게 접근, 활용할 수 있는 AI로 새로운 시대를 이끌 것\"이라고 말했다.\\n\\n김 센터장은 6일 삼성전자 뉴스룸 기고문을 통해 \"AI는 인터넷 혁명, 모바일 혁명에 이어 우리 시대의 패러다임을 바꿀 핵심기술\"이라며 이같이 밝혔다.\\n\\n삼성전자는 \\'모두를 위한 AI\\'(AI for All)라는 비전을 가지고 TV, 가전, 스마트폰 등 다양한 제품과 서비스에 AI를 접목하고 있다.\\n\\n김 센터장은 일상속 맞춤형 AI를 위한 기술혁신 방안으로 생성형 AI와 운영기술, 보안기술 등을 제시했다.\\n\\n그는 \"삼성전자는 삼성 제품과 서비스를 더욱 유용하게 만들 수 있는 생성형 AI 기능을 발굴하고 최적화하고 있다\"며 \"데이터를 단순 가공·분석하는 것에서 나아가\\xa0사용자 요구에 따라 독창적 결과를 만드는 생성형 AI로 새로운 경험을 제공할 것\"이라고 설명했다.\\n\\n특히 개인화 AI의 주요기술 중 하나인 지식그래프 기술을 개발하고, 생성형 AI와 유기적으로 연결해 사용자 맞춤형 서비스를 지원한다는 계획이다.\\n\\n앞서 삼성전자는 삼성리서치 주도로 지난 7월 세계 최고 수준의 지식그래프 원천기술을 보유하고 있는 영국 스타트업인 \\'옥스퍼드 시멘틱 테크놀로지스\\'(OST)를 인수한 바 있다.\\n\\n지식그래프는 관련있는 정보를 서로 연결된 그래프 형태로 표현해 주는 기술이다. 삼성전자는 온디바이스 AI와 결합해 차별화된 개인화 AI 경험을 제공한다는 전략이다.\\n\\n김 센터장은 또 \\'개인화 경험과 사용성을 강화하는 운영기술\\'에 대해 \"AI 경험을 효율적으로 구현하기 위해 하이브리드 AI를 적용하고 있다\"고 했다.\\n\\n하이브리드 AI는 온디바이스 AI와 클라우드 AI를 함께 사용해 속도와 안전성을 균형적으로 제공하는 기술이다.\\n\\n보안설루션 \\'녹스 매트릭스\\'를 개발하는 등 AI 서비스를 위한 보안도 더욱 강화하는 중이다.\\xa0 김 센터장은 \"삼성전자는 AI 개발에 있어 개인정보 보호와 보안을 최우선으로 하고 있다\"며 \"모바일, TV에 이어 향후 가전제품에도 (녹스 매트릭스를) 확대 적용할 계획\"이라고 밝혔다.\\n\\n삼성전자는 지난 9월 출범한 국가인공지능위원회에 참가해 정부와 함께 국가 AI 경쟁력 강화에도 나서고 있다.\\n\\n저작권자 © 서울이코노미뉴스 무단전재 및 재배포 금지\\n\\n[**이보라 기자**](mailto:seouleconews@naver.com)\\n[다른기사 보기](/news/articleList.html?sc_area=I&sc_word=newsfc12)\\n\\n\\n\\n---\\n\\n\\n\\n\\n\\n\\n**댓글**\\n0 \\n\\n\\n\\n\\n\\n[최신순](/news/articleView.html?idxno=80148&replyAll=&reply_sc_order_by=I#reply)\\n [추천순](/news/articleView.html?idxno=80148&replyAll=&reply_sc_order_by=C#reply)\\n\\n\\n\\n---\\n\\n이 기사를 공유합니다\\n\\n댓글삭제\\n\\n삭제한 댓글은 다시 복구할 수 없습니다.  \\n그래도 삭제하시겠습니까?\\n\\n기사공유하기\\n\\n프린트\\n\\n메일보내기'}]\n"
     ]
    }
   ],
   "source": [
    "# 웹 검색 도구 실행\n",
    "results = web_search_tool.invoke({\"query\": question})\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, List\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "\n",
    "# 상태 정의\n",
    "class GraphState(TypedDict):\n",
    "    question: Annotated[str, \"The question to answer\"]\n",
    "    generation: Annotated[str, \"The generation from the LLM\"]\n",
    "    web_search: Annotated[str, \"Whether to add search\"]\n",
    "    documents: Annotated[List[str], \"The documents retrieved\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "\n",
    "# 문서 검색 노드\n",
    "def retrieve(state: GraphState):\n",
    "    print(\"\\n==== RETRIEVE ====\\n\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # 문서 검색 수행\n",
    "    documents = pdf_retriever.invoke(question)\n",
    "    return {\"documents\": documents}\n",
    "\n",
    "\n",
    "# 답변 생성 노드\n",
    "def generate(state: GraphState):\n",
    "    print(\"\\n==== GENERATE ====\\n\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # RAG를 사용한 답변 생성\n",
    "    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
    "    return {\"generation\": generation}\n",
    "\n",
    "\n",
    "# 문서 평가 노드\n",
    "def grade_documents(state: GraphState):\n",
    "    print(\"\\n==== [CHECK DOCUMENT RELEVANCE TO QUESTION] ====\\n\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # 필터링된 문서\n",
    "    filtered_docs = []\n",
    "    relevant_doc_count = 0\n",
    "\n",
    "    for d in documents:\n",
    "        # Question-Document 의 관련성 평가\n",
    "        score = retrieval_grader.invoke(\n",
    "            {\"question\": question, \"document\": d.page_content}\n",
    "        )\n",
    "        grade = score.binary_score\n",
    "\n",
    "        if grade == \"yes\":\n",
    "            print(\"==== [GRADE: DOCUMENT RELEVANT] ====\")\n",
    "            # 관련 있는 문서를 filtered_docs 에 추가\n",
    "            filtered_docs.append(d)\n",
    "            relevant_doc_count += 1\n",
    "        else:\n",
    "            print(\"==== [GRADE: DOCUMENT NOT RELEVANT] ====\")\n",
    "            continue\n",
    "\n",
    "    # 관련 문서가 없으면 웹 검색 수행\n",
    "    web_search = \"Yes\" if relevant_doc_count == 0 else \"No\"\n",
    "    return {\"documents\": filtered_docs, \"web_search\": web_search}\n",
    "\n",
    "\n",
    "# 쿼리 재작성 노드\n",
    "def query_rewrite(state: GraphState):\n",
    "    print(\"\\n==== [REWRITE QUERY] ====\\n\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # 질문 재작성\n",
    "    better_question = question_rewriter.invoke({\"question\": question})\n",
    "    return {\"question\": better_question}\n",
    "\n",
    "\n",
    "# 웹 검색 노드\n",
    "def web_search(state: GraphState):\n",
    "    print(\"\\n==== [WEB SEARCH] ====\\n\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # 웹 검색 수행\n",
    "    docs = web_search_tool.invoke({\"query\": question})\n",
    "    # 검색 결과를 문서 형식으로 변환\n",
    "    web_results = \"\\n\".join([d[\"content\"] for d in docs])\n",
    "    web_results = Document(page_content=web_results)\n",
    "    documents.append(web_results)\n",
    "\n",
    "    return {\"documents\": documents}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decide_to_generate(state: GraphState):\n",
    "    # 평가된 문서를 기반으로 다음 단계 결정\n",
    "    print(\"==== [ASSESS GRADED DOCUMENTS] ====\")\n",
    "    # 웹 검색 필요 여부\n",
    "    web_search = state[\"web_search\"]\n",
    "\n",
    "    if web_search == \"Yes\":\n",
    "        # 웹 검색으로 정보 보강이 필요한 경우\n",
    "        print(\n",
    "            \"==== [DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, QUERY REWRITE] ====\"\n",
    "        )\n",
    "        # 쿼리 재작성 노드로 라우팅\n",
    "        return \"query_rewrite\"\n",
    "    else:\n",
    "        # 관련 문서가 존재하므로 답변 생성 단계(generate) 로 진행\n",
    "        print(\"==== [DECISION: GENERATE] ====\")\n",
    "        return \"generate\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph, START\n",
    "\n",
    "# 그래프 상태 초기화\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# 노드 정의\n",
    "workflow.add_node(\"retrieve\", retrieve)\n",
    "workflow.add_node(\"grade_documents\", grade_documents)\n",
    "workflow.add_node(\"generate\", generate)\n",
    "workflow.add_node(\"query_rewrite\", query_rewrite)\n",
    "workflow.add_node(\"web_search_node\", web_search)\n",
    "\n",
    "# 엣지 연결\n",
    "workflow.add_edge(START, \"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "\n",
    "# 문서 평가 노드에서 조건부 엣지 추가\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"query_rewrite\": \"query_rewrite\",\n",
    "        \"generate\": \"generate\",\n",
    "    },\n",
    ")\n",
    "\n",
    "# 엣지 연결\n",
    "workflow.add_edge(\"query_rewrite\", \"web_search_node\")\n",
    "workflow.add_edge(\"web_search_node\", \"generate\")\n",
    "workflow.add_edge(\"generate\", END)\n",
    "\n",
    "# 그래프 컴파일\n",
    "app = workflow.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_teddynote.graphs import visualize_graph\n",
    "\n",
    "visualize_graph(app)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== RETRIEVE ====\n",
      "\n",
      "\n",
      "==== [CHECK DOCUMENT RELEVANCE TO QUESTION] ====\n",
      "\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mgrade_documents\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "{\"binary_score\":\"yes\"}==== [GRADE: DOCUMENT RELEVANT] ====\n",
      "{\"binary_score\":\"yes\"}==== [GRADE: DOCUMENT RELEVANT] ====\n",
      "{\"binary_score==== [GRADE: DOCUMENT RELEVANT] ====\n",
      "\":\"yes\"}{\"binary_score\":\"yes\"}==== [GRADE: DOCUMENT RELEVANT] ====\n",
      "{\"binary_score\":\"no\"}==== [GRADE: DOCUMENT NOT RELEVANT] ====\n",
      "{\"binary_score\":\"no\"}==== [GRADE: DOCUMENT NOT RELEVANT] ====\n",
      "{\"binary_score\":\"yes\"}==== [GRADE: DOCUMENT RELEVANT] ====\n",
      "{\"binary_score\":\"yes\"}==== [GRADE: DOCUMENT RELEVANT] ====\n",
      "{\"binary_score\":\"no\"}==== [GRADE: DOCUMENT NOT RELEVANT] ====\n",
      "{\"binary_score\":\"no\"}==== [GRADE: DOCUMENT NOT RELEVANT] ====\n",
      "==== [ASSESS GRADED DOCUMENTS] ====\n",
      "==== [DECISION: GENERATE] ====\n",
      "\n",
      "==== GENERATE ====\n",
      "\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mgenerate\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "삼성전자가 개발한 생성형 AI의 이름은 ‘삼성 가우스’이다.\n",
      "\n",
      "**Source**\n",
      "- data/SPRI_AI_Brief_2023년12월호_F.pdf (p.12)"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_teddynote.messages import stream_graph, invoke_graph, random_uuid\n",
    "\n",
    "# config 설정(재귀 최대 횟수, thread_id)\n",
    "config = RunnableConfig(recursion_limit=20, configurable={\"thread_id\": random_uuid()})\n",
    "\n",
    "# 질문 입력\n",
    "inputs = {\n",
    "    \"question\": \"삼성전자가 개발한 생성형 AI 의 이름은?\",\n",
    "}\n",
    "\n",
    "# 스트리밍 형식으로 그래프 실행\n",
    "stream_graph(\n",
    "    app,\n",
    "    inputs,\n",
    "    config,\n",
    "    [\"retrieve\", \"grade_documents\", \"query_rewrite\", \"web_search_node\", \"generate\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config 설정(재귀 최대 횟수, thread_id)\n",
    "config = RunnableConfig(recursion_limit=20, configurable={\"thread_id\": random_uuid()})\n",
    "\n",
    "# 질문 입력\n",
    "inputs = {\n",
    "    \"question\": \"2024년 노벨문학상 수상자의 이름은?\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== RETRIEVE ====\n",
      "\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mretrieve\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "page_content='CES 2024\n",
      "이번 전시에는 500곳 이상의 한국기업 참가 예정\n",
      "기간 장소 홈페이지\n",
      "2024.1.9~12 미국, 라스베가스 https://www.ces.tech/\n",
      "- 머신러닝 및 응용에 관한 국제 컨퍼런스(AIMLA 2024)는\n",
      "인공지능 및 머신러닝의 이론, 방법론 및 실용적 접근에 관한\n",
      "지식과 최신 연구 결과 공유\n",
      "- 이론 및 실무 측면에서 인공지능, 기계학습의 주요 분야를\n",
      "논의하고, 학계, 산업계의 연구자와 실무자들에게 해당 분\n",
      "AIMLA 2024\n",
      "야의 최첨단 개발 소식 공유\n",
      "기간 장소 홈페이지' metadata={'source': 'data/SPRI_AI_Brief_2023년12월호_F.pdf', 'file_path': 'data/SPRI_AI_Brief_2023년12월호_F.pdf', 'page': 21, 'total_pages': 23, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13462', 'Producer': 'Hancom PDF 1.3.0.542', 'CreationDate': \"D:20231208132838+09'00'\", 'ModDate': \"D:20231208132838+09'00'\", 'PDFVersion': '1.4'}\n",
      "page_content='n 참가국들은 튜링상을 수상한 AI 학자인 요슈아 벤지오 교수가 주도하는 ‘과학의 현황(State of\n",
      "the Science)’ 보고서 작성에도 합의했으며, 보고서를 통해 첨단 AI의 위험과 가능성에 관한\n",
      "기존 연구를 과학적으로 평가하고 향후 AI 안전 연구를 위한 우선순위를 제시할 계획\n",
      "n 한국은 영국 정부와 6개월 뒤에 온라인으로 AI 미니 정상회의를 공동 개최하기로 합의했으며,\n",
      "프랑스 정부와는 1년 후 대면 정상회의를 개최할 예정' metadata={'source': 'data/SPRI_AI_Brief_2023년12월호_F.pdf', 'file_path': 'data/SPRI_AI_Brief_2023년12월호_F.pdf', 'page': 5, 'total_pages': 23, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13462', 'Producer': 'Hancom PDF 1.3.0.542', 'CreationDate': \"D:20231208132838+09'00'\", 'ModDate': \"D:20231208132838+09'00'\", 'PDFVersion': '1.4'}\n",
      "page_content='£예술가들의 AI 저작권 침해 소송, 저작권 미등록과 증거불충분으로 기각\n",
      "n 미국 캘리포니아 북부지방법원의 윌리엄 오릭(William Orrick) 판사는 2023년 10월 30일 미드저니\n",
      "(Midjourney), 스태빌리티AI(Stability AI), 디비언트아트(DeviantArt)에 제기된 저작권 침해 소송을 기각\n",
      "∙ 2023년 1월 예술가 사라 앤더슨(Sarah Anderson), 캘리 맥커넌(Kelly McKernan), 칼라' metadata={'source': 'data/SPRI_AI_Brief_2023년12월호_F.pdf', 'file_path': 'data/SPRI_AI_Brief_2023년12월호_F.pdf', 'page': 6, 'total_pages': 23, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13462', 'Producer': 'Hancom PDF 1.3.0.542', 'CreationDate': \"D:20231208132838+09'00'\", 'ModDate': \"D:20231208132838+09'00'\", 'PDFVersion': '1.4'}\n",
      "page_content='£AI 기술 중 머신러닝, 텐서플로우, 딥러닝의 임금 프리미엄이 높게 평가\n",
      "n 옥스퍼드 인터넷 연구소(Oxford Internet Institute)가 2023년 10월 24일 962개 기술과 2만 5천\n",
      "명을 대상으로 한 연구에서 AI를 포함한 주요 기술의 경제적 가치를 분석한 결과를 발표\n",
      "∙ 연구에 따르면 한 기술의 경제적 가치는 근로자의 여타 역량과 얼마나 잘 결합하는지를 보여주는\n",
      "‘상보성(complementarity)’에 따라 결정됨' metadata={'source': 'data/SPRI_AI_Brief_2023년12월호_F.pdf', 'file_path': 'data/SPRI_AI_Brief_2023년12월호_F.pdf', 'page': 20, 'total_pages': 23, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13462', 'Producer': 'Hancom PDF 1.3.0.542', 'CreationDate': \"D:20231208132838+09'00'\", 'ModDate': \"D:20231208132838+09'00'\", 'PDFVersion': '1.4'}\n",
      "page_content='오르티즈(Karla Ortiz)는 이미지 생성 AI 서비스를 개발한 3개 기업을 상대로 저작권 침해 소송을 제기\n",
      "∙ 예술가들은 3개 기업이 AI 모델을 학습시키기 위해 원작자 동의 없이 작품을 학습 데이터셋에\n",
      "포함하여 저작권을 침해했다고 주장했으며, 법원은 지난 4월 피소 기업들이 제출한 기각 신청을\n",
      "수용해 소송을 기각\n",
      "n 오릭 판사는 판결문에서 소송을 기각한 핵심 이유로 예술가들의 저작권 미등록을 제시\n",
      "∙ 판결문은 소송을 제기한 캘리 맥커넌과 칼라 오르티즈가 미국 저작권청에 예술 작품에 대한 저작권을' metadata={'source': 'data/SPRI_AI_Brief_2023년12월호_F.pdf', 'file_path': 'data/SPRI_AI_Brief_2023년12월호_F.pdf', 'page': 6, 'total_pages': 23, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13462', 'Producer': 'Hancom PDF 1.3.0.542', 'CreationDate': \"D:20231208132838+09'00'\", 'ModDate': \"D:20231208132838+09'00'\", 'PDFVersion': '1.4'}\n",
      "page_content='▹ 구글 딥마인드, 범용 AI 모델의 기능과 동작에 대한 분류 체계 발표······························16\n",
      "▹ 갈릴레오의 LLM 환각 지수 평가에서 GPT-4가 가장 우수 ···········································17\n",
      "4. 인력/교육\n",
      "▹ 영국 옥스퍼드 인터넷 연구소, AI 기술자의 임금이 평균 21% 높아·······························18\n",
      "Ⅱ\n",
      ". 주요 행사' metadata={'source': 'data/SPRI_AI_Brief_2023년12월호_F.pdf', 'file_path': 'data/SPRI_AI_Brief_2023년12월호_F.pdf', 'page': 1, 'total_pages': 23, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13462', 'Producer': 'Hancom PDF 1.3.0.542', 'CreationDate': \"D:20231208132838+09'00'\", 'ModDate': \"D:20231208132838+09'00'\", 'PDFVersion': '1.4'}\n",
      "page_content='소송은 인정되어 계속 진행됨\n",
      "☞ 출처: Venturebeat, Midjourney, Stability AI and DeviantArt win a victory in copyright case by artists- but the\n",
      "fight continues, 2023.10.30.\n",
      "4' metadata={'source': 'data/SPRI_AI_Brief_2023년12월호_F.pdf', 'file_path': 'data/SPRI_AI_Brief_2023년12월호_F.pdf', 'page': 6, 'total_pages': 23, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13462', 'Producer': 'Hancom PDF 1.3.0.542', 'CreationDate': \"D:20231208132838+09'00'\", 'ModDate': \"D:20231208132838+09'00'\", 'PDFVersion': '1.4'}\n",
      "page_content='달 안에 기금 지원을 위한 제안 요청을 받을 계획\n",
      "n 프런티어 모델 포럼은 출범 이후 업계 전반에 걸쳐 AI 레드팀 구성에 관한 모범사례 공유를 추진하는\n",
      "한편, 첨단 AI 모델의 취약점이나 잠재적으로 위험한 기능 및 위험 완화 관련 정보를 공유할 수\n",
      "있는 공개 절차도 개발 중\n",
      "☞ 출처: Google, Anthropic, Google, Microsoft and OpenAI announce Executive Director of the Frontier Model' metadata={'source': 'data/SPRI_AI_Brief_2023년12월호_F.pdf', 'file_path': 'data/SPRI_AI_Brief_2023년12월호_F.pdf', 'page': 9, 'total_pages': 23, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13462', 'Producer': 'Hancom PDF 1.3.0.542', 'CreationDate': \"D:20231208132838+09'00'\", 'ModDate': \"D:20231208132838+09'00'\", 'PDFVersion': '1.4'}\n",
      "page_content='12\n",
      "2023년 월호' metadata={'source': 'data/SPRI_AI_Brief_2023년12월호_F.pdf', 'file_path': 'data/SPRI_AI_Brief_2023년12월호_F.pdf', 'page': 0, 'total_pages': 23, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13462', 'Producer': 'Hancom PDF 1.3.0.542', 'CreationDate': \"D:20231208132838+09'00'\", 'ModDate': \"D:20231208132838+09'00'\", 'PDFVersion': '1.4'}\n",
      "page_content='1,000만 달러 이상을 기부\n",
      "∙ 또한 신기술의 거버넌스와 안전 분야에서 전문성을 갖춘 브루킹스 연구소 출신의 크리스 메서롤(Chris\n",
      "Meserole)을 포럼의 상무이사로 임명\n",
      "n 최근 AI 기술이 급속히 발전하면서 AI 안전에 관한 연구가 부족한 시점에, 포럼은 이러한 격차를 해소\n",
      "하기 위해 AI 안전 기금을 조성\n",
      "∙ 참여사들은 지난 7월 백악관 주재의 AI 안전 서약에서 외부자의 AI 시스템 취약점 발견과 신고를\n",
      "촉진하기로 약속했으며, 약속을 이행하기 위해 기금을 활용해 외부 연구집단의 AI 시스템 평가에\n",
      "자금을 지원할 계획' metadata={'source': 'data/SPRI_AI_Brief_2023년12월호_F.pdf', 'file_path': 'data/SPRI_AI_Brief_2023년12월호_F.pdf', 'page': 9, 'total_pages': 23, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13462', 'Producer': 'Hancom PDF 1.3.0.542', 'CreationDate': \"D:20231208132838+09'00'\", 'ModDate': \"D:20231208132838+09'00'\", 'PDFVersion': '1.4'}\n",
      "==================================================\n",
      "\n",
      "==== [CHECK DOCUMENT RELEVANCE TO QUESTION] ====\n",
      "\n",
      "==== [GRADE: DOCUMENT NOT RELEVANT] ====\n",
      "==== [GRADE: DOCUMENT NOT RELEVANT] ====\n",
      "==== [GRADE: DOCUMENT NOT RELEVANT] ====\n",
      "==== [GRADE: DOCUMENT NOT RELEVANT] ====\n",
      "==== [GRADE: DOCUMENT NOT RELEVANT] ====\n",
      "==== [GRADE: DOCUMENT NOT RELEVANT] ====\n",
      "==== [GRADE: DOCUMENT NOT RELEVANT] ====\n",
      "==== [GRADE: DOCUMENT NOT RELEVANT] ====\n",
      "==== [GRADE: DOCUMENT NOT RELEVANT] ====\n",
      "==== [GRADE: DOCUMENT NOT RELEVANT] ====\n",
      "==== [ASSESS GRADED DOCUMENTS] ====\n",
      "==== [DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, QUERY REWRITE] ====\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mgrade_documents\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "\u001b[1;32mweb_search\u001b[0m:\n",
      "Yes\n",
      "==================================================\n",
      "\n",
      "==== [REWRITE QUERY] ====\n",
      "\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mquery_rewrite\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "\u001b[1;32mquestion\u001b[0m:\n",
      "2024년 노벨문학상 수상자는 누구인가요?\n",
      "==================================================\n",
      "\n",
      "==== [WEB SEARCH] ====\n",
      "\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mweb_search_node\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "page_content='김명일 기자 한국 소설가 한강(54)이 2024년 노벨문학상 수상자로 선정됐다. 한국인이 노벨상을 수상한 것은 지난 2000년 평화상을 탄 고(故) 김대중 전 대통령에 이어 두번째다. 이날 문학상에 이어 오는 11일 평화상, 14일 경제학상 수상자가 발표될 예정이다. 노벨상 시상식은 알프레드 노벨의 기일인 12월 10일 스웨덴 스톡홀름(생리의학·물리·화학·문학·경제상)과 노르웨이 오슬로(평화상)에서 열린다. 한편 한강은 지난 2016년 한국인 최초로 세계 3대 문학상 중 하나인 ‘맨부커상’도 수상한 바 있다. 김명일 기자 조선NS 김명일 기자입니다. 많은 제보 부탁드립니다. 제보 메일 mi737@chosun.com “외국인이 뼈다귀 해장국으로 성공해 수십 년 동안 장사 중인 노포(老鋪)가 한국에 있다면 어떨 것 같습니까?” 전 세계 최고 식당이 모... “외국인이 뼈다귀 해장국으로 성공해 수십 년 동안 장사 중인 노포(老鋪)가 한국에 있다면 어떨 것 같습니까?” 전 세계 최고 식당이 모... 대한민국 국민으로서 최초의 노벨상을 수상하게 되었음을 진심으로 축하합니다.\n",
      "2024년 노벨 문학상에 한국 소설가 한강 / YTN\n",
      "\n",
      " YTN\n",
      "26 likes\n",
      "1728 views\n",
      "10 Oct 2024\n",
      "한국 소설가 한강 씨가 2024년 노벨 문학상 수상자로 뽑혔습니다.\n",
      "\n",
      "스웨덴 노벨위원회는 한강 씨의 작품에 대해 \"역사적 트라우마를 직시하고 인간 삶의 연약함을 드러내는 강렬한 시적 산문\"이라며 선정 이유를 밝혔습니다.\n",
      "\n",
      "한국 작가가 노벨 문학상을 받는 것은 이번이 처음입니다.\n",
      "\n",
      "또, 한국인의 노벨상 수상은 지난 2000년 평화상을 탄 고 김대중 전 대통령에 이어 두 번째입니다.\n",
      "\n",
      "수상자에게는 상금 천100만 크로나, 우리 돈으로 13억 4천만 원과 메달, 증서가 수여됩니다.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "YTN 홍주예 (hongkiza@ytn.co.kr)\n",
      "\n",
      "※ '당신의 제보가 뉴스가 됩니다'\n",
      "[카카오톡] YTN 검색해 채널 추가\n",
      "[전화] 02-398-8585\n",
      "[메일] social@ytn.co.kr\n",
      "\n",
      "▶ 기사 원문 : https://www.ytn.co.kr/_ln/0104_202410102023354769\n",
      "▶ 제보 하기 : https://mj.ytn.co.kr/mj/mj_write.php\n",
      "\n",
      "▣ YTN 유튜브 채널 구독 : http://goo.gl/Ytb5SZ\n",
      "\n",
      "ⓒ YTN 무단 전재, 재배포금지 및 AI 데이터 활용 금지\n",
      "5 comments\n",
      "# 2024년 노벨문학상 수상 작가, 한강 2024년 노벨문학상 수상자로 한강 작가가 선정되었습니다. 한강 작가의 노벨문학상 수상은 한국인 최초이자 아시아 여성 최초입니다. 한강 작가의 수상을 진심으로 축하드립니다. ✔ 기사 보기:  한강 **카테고리** 문학과지성 시인선,시 **출간일** 2013년 11월 15일 **사양** · 165쪽 한강 **카테고리** 소설,소설 선집 **출간일** 2018년 11월 9일 **사양** 변형판 130x195 · 325쪽 한강 지음 **카테고리** 장편소설 **사양** 신국판 152x225mm · 330쪽 한강 지음 **카테고리** 장편소설 **사양** · 390쪽 한강 **카테고리** 소설,소설 선집 **사양** 변형판 130x195 · 308쪽 내 여자의 열매 한강 **카테고리** 소설,소설 선집 **사양** 변형판 130x195 · 406쪽 ### 한강 소설가 1993년 『문학과사회』 겨울호에 시 「서울의 겨울」 외 네 편을 발표하고 이듬해 『서울신문』 신춘문예에 단편소설 「붉은 닻」이 당선되어 작품 활동을 시작했다. 소설집 『여수의 사랑』 『내 여자의 열매』 … 자세히 보기'\n",
      "==================================================\n",
      "\n",
      "==== GENERATE ====\n",
      "\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mgenerate\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "\u001b[1;32mgeneration\u001b[0m:\n",
      "2024년 노벨문학상 수상자는 한국 소설가 한강입니다.\n",
      "\n",
      "**Source**\n",
      "- https://www.ytn.co.kr/_ln/0104_202410102023354769\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# 그래프 실행\n",
    "invoke_graph(app, inputs, config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== RETRIEVE ====\n",
      "\n",
      "\n",
      "==== [CHECK DOCUMENT RELEVANCE TO QUESTION] ====\n",
      "\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mgrade_documents\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "{\"binary_score\":\"no\"}==== [GRADE: DOCUMENT NOT RELEVANT] ====\n",
      "{\"binary_score\":\"no\"}==== [GRADE: DOCUMENT NOT RELEVANT] ====\n",
      "{\"binary_score\":\"no\"}==== [GRADE: DOCUMENT NOT RELEVANT] ====\n",
      "{\"binary_score\":\"no\"}==== [GRADE: DOCUMENT NOT RELEVANT] ====\n",
      "{\"binary_score\":\"no\"}==== [GRADE: DOCUMENT NOT RELEVANT] ====\n",
      "{\"binary_score\":\"no\"}==== [GRADE: DOCUMENT NOT RELEVANT] ====\n",
      "{\"binary_score\":\"no\"}==== [GRADE: DOCUMENT NOT RELEVANT] ====\n",
      "{\"binary_score\":\"no\"}==== [GRADE: DOCUMENT NOT RELEVANT] ====\n",
      "{\"binary_score\":\"no==== [GRADE: DOCUMENT NOT RELEVANT] ====\n",
      "\"}{\"binary_score\":\"no\"}==== [GRADE: DOCUMENT NOT RELEVANT] ====\n",
      "==== [ASSESS GRADED DOCUMENTS] ====\n",
      "==== [DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, QUERY REWRITE] ====\n",
      "\n",
      "==== [REWRITE QUERY] ====\n",
      "\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mquery_rewrite\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "2024년 노벨문학상 수상자는 누구인가요?\n",
      "==== [WEB SEARCH] ====\n",
      "\n",
      "\n",
      "==== GENERATE ====\n",
      "\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mgenerate\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "2024년 노벨문학상 수상자는 한국 소설가 한강입니다.\n",
      "\n",
      "**Source**\n",
      "- https://www.ytn.co.kr/_ln/0104_202410102023354769"
     ]
    }
   ],
   "source": [
    "# 그래프 실행\n",
    "stream_graph(\n",
    "    app,\n",
    "    inputs,\n",
    "    config,\n",
    "    [\"retrieve\", \"grade_documents\", \"query_rewrite\", \"generate\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "04. Self-RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rag.pdf import PDFRetrievalChain\n",
    "\n",
    "# PDF 문서를 로드합니다.\n",
    "pdf = PDFRetrievalChain([\"data/SPRI_AI_Brief_2023년12월호_F.pdf\"]).create_chain()\n",
    "\n",
    "# retriever와 chain을 생성합니다.\n",
    "pdf_retriever = pdf.retriever\n",
    "pdf_chain = pdf.chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_teddynote.models import get_model_name, LLMs\n",
    "\n",
    "# 최신모델 이름 설정\n",
    "MODEL_NAME = get_model_name(LLMs.GPT4_1)\n",
    "\n",
    "\n",
    "# 데이터 모델 정의: 검색된 문서의 관련성을 이진 점수로 평가하기 위한 데이터 모델\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"A binary score to determine the relevance of the retrieved documents.\"\"\"\n",
    "\n",
    "    # 문서가 질문에 관련이 있는지 여부를 'yes' 또는 'no'로 나타내는 필드\n",
    "    binary_score: str = Field(\n",
    "        description=\"Documents are relevant to the question, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "\n",
    "# LLM 초기화\n",
    "llm = ChatOpenAI(model=MODEL_NAME, temperature=0)\n",
    "\n",
    "# GradeDocuments 데이터 모델을 사용하여 LLM의 구조화된 출력 생성\n",
    "structured_llm_grader = llm.with_structured_output(GradeDocuments)\n",
    "\n",
    "# 시스템 프롬프트 정의: 검색된 문서가 사용자 질문에 관련이 있는지 평가하는 시스템 역할 정의\n",
    "system = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "    It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
    "    If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\"\n",
    "\n",
    "# 채팅 프롬프트 템플릿 생성\n",
    "grade_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 검색 평가기 생성\n",
    "retrieval_grader = grade_prompt | structured_llm_grader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary_score='yes'\n"
     ]
    }
   ],
   "source": [
    "# 질문 정의\n",
    "question = \"삼성전자가 개발한 생성형 AI 의 이름은?\"\n",
    "\n",
    "# 문서 검색\n",
    "docs = pdf_retriever.invoke(question)\n",
    "\n",
    "# 검색된 문서 중 두 번째 문서의 페이지 콘텐츠 추출\n",
    "doc_txt = docs[1].page_content\n",
    "\n",
    "# 검색 평가기 호출 및 결과 출력\n",
    "print(retrieval_grader.invoke({\"question\": question, \"document\": doc_txt}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "삼성전자가 개발한 생성형 AI의 이름은 ‘삼성 가우스’(Samsung Gauss)이다.\n",
      "\n",
      "**Source**\n",
      "- data/SPRI_AI_Brief_2023년12월호_F.pdf (p.13)\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# LangChain Hub에서 프롬프트 가져오기\n",
    "prompt = hub.pull(\"teddynote/rag-prompt\")\n",
    "\n",
    "# 기본 LLM 초기화, 모델 이름과 온도 설정\n",
    "llm = ChatOpenAI(model_name=MODEL_NAME, temperature=0)\n",
    "\n",
    "\n",
    "# 문서 포맷팅 함수\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(\n",
    "        [\n",
    "            f'<document><content>{doc.page_content}</content><source>{doc.metadata[\"source\"]}</source><page>{doc.metadata[\"page\"]+1}</page></document>'\n",
    "            for doc in docs\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "# RAG 체인 생성\n",
    "rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# 체인 실행\n",
    "generation = rag_chain.invoke({\"context\": format_docs(docs), \"question\": question})\n",
    "print(generation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "# 데이터 모델 정의: 생성된 답변이 사실에 기반하고 있는지 여부를 이진 점수로 평가하기 위한 데이터 모델\n",
    "class Groundednesss(BaseModel):\n",
    "    \"\"\"A binary score indicating whether the generated answer is grounded in the facts.\"\"\"\n",
    "\n",
    "    # 답변이 사실에 기반하고 있는지 여부를 'yes' 또는 'no'로 나타내는 필드\n",
    "    binary_score: str = Field(\n",
    "        description=\"Answer is grounded in the facts, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "\n",
    "# LLM 초기화\n",
    "llm = ChatOpenAI(model=MODEL_NAME, temperature=0)\n",
    "\n",
    "# 구조화된 출력과 함께 LLM 설정\n",
    "structured_llm_grader = llm.with_structured_output(Groundednesss)\n",
    "\n",
    "# 시스템 프롬프트 정의\n",
    "system = \"\"\"You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts. \\n \n",
    "Give a binary score 'yes' or 'no'. 'Yes' means that the answer is grounded in / supported by the set of facts.\"\"\"\n",
    "\n",
    "# 채팅 프롬프트 템플릿 생성\n",
    "groundedness_checking_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Set of facts: \\n\\n {documents} \\n\\n LLM generation: {generation}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 답변의 할루시네이션 평가기 생성\n",
    "groundedness_grader = groundedness_checking_prompt | structured_llm_grader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Groundednesss(binary_score='yes')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 할루시네이션 평가기 호출(yes: 사실에 기반하고 있음, no: 사실에 기반하고 있지 않음)\n",
    "groundedness_grader.invoke({\"documents\": format_docs(docs), \"generation\": generation})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "class GradeAnswer(BaseModel):\n",
    "    \"\"\"A binary score indicating whether the question is addressed.\"\"\"\n",
    "\n",
    "    # 답변의 관련성 평가: 'yes' 또는 'no'로 표기(yes: 관련성 있음, no: 관련성 없음)\n",
    "    binary_score: str = Field(\n",
    "        description=\"Answer addresses the question, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model=MODEL_NAME, temperature=0)\n",
    "\n",
    "# llm 에 GradeAnswer 바인딩\n",
    "structured_llm_grader = llm.with_structured_output(GradeAnswer)\n",
    "\n",
    "# 시스템 프롬프트 정의\n",
    "system = \"\"\"You are a grader assessing whether an answer addresses / resolves a question \\n \n",
    "     Give a binary score 'yes' or 'no'. Yes' means that the answer resolves the question.\"\"\"\n",
    "\n",
    "# 프롬프트 생성\n",
    "answer_grader_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"User question: \\n\\n {question} \\n\\n LLM generation: {generation}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 답변 평가기 생성\n",
    "answer_grader = answer_grader_prompt | structured_llm_grader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model=MODEL_NAME, temperature=0)\n",
    "\n",
    "# 시스템 프롬프트 정의\n",
    "# 입력 질문을 벡터스토어 검색에 최적화된 형태로 변환하는 시스템 역할 정의\n",
    "system = \"\"\"You a question re-writer that converts an input question to a better version that is optimized \\n \n",
    "     for vectorstore retrieval. Look at the input and try to reason about the underlying semantic intent / meaning.\"\"\"\n",
    "\n",
    "# 시스템 메시지와 초기 질문을 포함한 프롬프트 템플릿 생성\n",
    "re_write_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Here is the initial question: \\n\\n {question} \\n Formulate an improved question.\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 질문 재작성기 생성\n",
    "question_rewriter = re_write_prompt | llm | StrOutputParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'삼성전자가 개발한 생성형 인공지능(AI) 모델의 공식 명칭은 무엇인가?'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 질문 재작성기 호출\n",
    "question_rewriter.invoke({\"question\": question})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from typing_extensions import TypedDict, Annotated\n",
    "\n",
    "\n",
    "# 그래프의 상태를 나타내는 클래스 정의\n",
    "class GraphState(TypedDict):\n",
    "    # 질문을 나타내는 문자열\n",
    "    question: Annotated[str, \"Question\"]\n",
    "    # LLM에 의해 생성된 응답을 나타내는 문자열\n",
    "    generation: Annotated[str, \"LLM Generation\"]\n",
    "    # 문서의 목록을 나타내는 문자열 리스트\n",
    "    documents: Annotated[List[str], \"Retrieved Documents\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문서 검색\n",
    "def retrieve(state):\n",
    "    print(\"==== [RETRIEVE] ====\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # 검색 수행\n",
    "    documents = pdf_retriever.invoke(question)\n",
    "    return {\"documents\": documents}\n",
    "\n",
    "\n",
    "# 답변 생성\n",
    "def generate(state):\n",
    "    print(\"==== [GENERATE] ====\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # RAG 생성\n",
    "    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
    "    return {\"generation\": generation}\n",
    "\n",
    "\n",
    "# 검색된 문서의 관련성 평가\n",
    "def grade_documents(state):\n",
    "    print(\"==== [GRADE DOCUMENTS] ====\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # 각 문서 점수 평가\n",
    "    filtered_docs = []\n",
    "    for d in documents:\n",
    "        score = retrieval_grader.invoke(\n",
    "            {\"question\": question, \"document\": d.page_content}\n",
    "        )\n",
    "        grade = score.binary_score\n",
    "        if grade == \"yes\":\n",
    "            print(\"==== GRADE: DOCUMENT RELEVANT ====\")\n",
    "            filtered_docs.append(d)\n",
    "        else:\n",
    "            print(\"==== GRADE: DOCUMENT NOT RELEVANT ====\")\n",
    "            continue\n",
    "    return {\"documents\": filtered_docs}\n",
    "\n",
    "\n",
    "# 질문 변환\n",
    "def transform_query(state):\n",
    "    print(\"==== [TRANSFORM QUERY] ====\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # 질문 재작성\n",
    "    better_question = question_rewriter.invoke({\"question\": question})\n",
    "    return {\"question\": better_question}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 답변 생성 여부 결정\n",
    "def decide_to_generate(state):\n",
    "    print(\"==== [ASSESS GRADED DOCUMENTS] ====\")\n",
    "    state[\"question\"]\n",
    "    filtered_documents = state[\"documents\"]\n",
    "\n",
    "    if not filtered_documents:\n",
    "        # 모든 문서가 관련성이 없는 경우\n",
    "        # 새로운 쿼리 생성\n",
    "        print(\n",
    "            \"==== [DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY] ====\"\n",
    "        )\n",
    "        return \"transform_query\"\n",
    "    else:\n",
    "        # 관련 문서가 있는 경우 답변 생성\n",
    "        print(\"==== [DECISION: GENERATE] ====\")\n",
    "        return \"generate\"\n",
    "\n",
    "\n",
    "# 생성된 답변의 문서 및 질문과의 관련성 평가\n",
    "def grade_generation_v_documents_and_question(state):\n",
    "    print(\"==== [CHECK HALLUCINATIONS] ====\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    generation = state[\"generation\"]\n",
    "\n",
    "    score = groundedness_grader.invoke(\n",
    "        {\"documents\": documents, \"generation\": generation}\n",
    "    )\n",
    "    grade = score.binary_score\n",
    "\n",
    "    # 환각 여부 확인\n",
    "    if grade == \"yes\":\n",
    "        print(\"==== [DECISION: GENERATION IS GROUNDED IN DOCUMENTS] ====\")\n",
    "        # 질문 해결 여부 확인\n",
    "        print(\"==== [GRADE GENERATION vs QUESTION] ====\")\n",
    "        score = answer_grader.invoke({\"question\": question, \"generation\": generation})\n",
    "        grade = score.binary_score\n",
    "        if grade == \"yes\":\n",
    "            print(\"==== [DECISION: GENERATION ADDRESSES QUESTION] ====\")\n",
    "            return \"relevant\"\n",
    "        else:\n",
    "            print(\"==== [DECISION: GENERATION DOES NOT ADDRESS QUESTION] ====\")\n",
    "            return \"not relevant\"\n",
    "    else:\n",
    "        print(\"==== [DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY] ====\")\n",
    "        return \"hallucination\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# 그래프 상태 초기화\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# 노드 정의\n",
    "workflow.add_node(\"retrieve\", retrieve)  # retrieve\n",
    "workflow.add_node(\"grade_documents\", grade_documents)  # grade documents\n",
    "workflow.add_node(\"generate\", generate)  # generatae\n",
    "workflow.add_node(\"transform_query\", transform_query)  # transform_query\n",
    "\n",
    "# 엣지 정의\n",
    "workflow.add_edge(START, \"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "\n",
    "# 문서 평가 노드에서 조건부 엣지 추가\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"transform_query\": \"transform_query\",\n",
    "        \"generate\": \"generate\",\n",
    "    },\n",
    ")\n",
    "\n",
    "# 엣지 정의\n",
    "workflow.add_edge(\"transform_query\", \"retrieve\")\n",
    "\n",
    "# 답변 생성 노드에서 조건부 엣지 추가\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate\",\n",
    "    grade_generation_v_documents_and_question,\n",
    "    {\n",
    "        \"hallucination\": \"generate\",\n",
    "        \"relevant\": END,\n",
    "        \"not relevant\": \"transform_query\",\n",
    "    },\n",
    ")\n",
    "\n",
    "# 그래프 컴파일\n",
    "app = workflow.compile(checkpointer=MemorySaver())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_teddynote.graphs import visualize_graph\n",
    "\n",
    "visualize_graph(app)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== [RETRIEVE] ====\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mretrieve\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "page_content='SPRi AI Brief |\n",
      "2023-12월호\n",
      "삼성전자, 자체 개발 생성 AI ‘삼성 가우스’ 공개\n",
      "KEY Contents\n",
      "n 삼성전자가 온디바이스에서 작동 가능하며 언어, 코드, 이미지의 3개 모델로 구성된 자체 개발 생성\n",
      "AI 모델 ‘삼성 가우스’를 공개\n",
      "n 삼성전자는 삼성 가우스를 다양한 제품에 단계적으로 탑재할 계획으로, 온디바이스 작동이 가능한\n",
      "삼성 가우스는 외부로 사용자 정보가 유출될 위험이 없다는 장점을 보유\n",
      "£언어, 코드, 이미지의 3개 모델로 구성된 삼성 가우스, 온디바이스 작동 지원' metadata={'source': 'data/SPRI_AI_Brief_2023년12월호_F.pdf', 'file_path': 'data/SPRI_AI_Brief_2023년12월호_F.pdf', 'page': 12, 'total_pages': 23, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13462', 'Producer': 'Hancom PDF 1.3.0.542', 'CreationDate': \"D:20231208132838+09'00'\", 'ModDate': \"D:20231208132838+09'00'\", 'PDFVersion': '1.4'}\n",
      "page_content='▹ 삼성전자, 자체 개발 생성 AI ‘삼성 가우스’ 공개 ···························································10\n",
      "▹ 구글, 앤스로픽에 20억 달러 투자로 생성 AI 협력 강화 ················································11\n",
      "▹ IDC, 2027년 AI 소프트웨어 매출 2,500억 달러 돌파 전망···········································12' metadata={'source': 'data/SPRI_AI_Brief_2023년12월호_F.pdf', 'file_path': 'data/SPRI_AI_Brief_2023년12월호_F.pdf', 'page': 1, 'total_pages': 23, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13462', 'Producer': 'Hancom PDF 1.3.0.542', 'CreationDate': \"D:20231208132838+09'00'\", 'ModDate': \"D:20231208132838+09'00'\", 'PDFVersion': '1.4'}\n",
      "page_content='£언어, 코드, 이미지의 3개 모델로 구성된 삼성 가우스, 온디바이스 작동 지원\n",
      "n 삼성전자가 2023년 11월 8일 열린 ‘삼성 AI 포럼 2023’ 행사에서 자체 개발한 생성 AI 모델\n",
      "‘삼성 가우스’를 최초 공개\n",
      "∙ 정규분포 이론을 정립한 천재 수학자 가우스(Gauss)의 이름을 본뜬 삼성 가우스는 다양한 상황에\n",
      "최적화된 크기의 모델 선택이 가능\n",
      "∙ 삼성 가우스는 라이선스나 개인정보를 침해하지 않는 안전한 데이터를 통해 학습되었으며,\n",
      "온디바이스에서 작동하도록 설계되어 외부로 사용자의 정보가 유출되지 않는 장점을 보유' metadata={'source': 'data/SPRI_AI_Brief_2023년12월호_F.pdf', 'file_path': 'data/SPRI_AI_Brief_2023년12월호_F.pdf', 'page': 12, 'total_pages': 23, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13462', 'Producer': 'Hancom PDF 1.3.0.542', 'CreationDate': \"D:20231208132838+09'00'\", 'ModDate': \"D:20231208132838+09'00'\", 'PDFVersion': '1.4'}\n",
      "page_content='어시스턴트를 적용한 구글 픽셀(Pixel)과 경쟁할 것으로 예상\n",
      "☞ 출처 : 삼성전자, ‘삼성 AI 포럼’서 자체 개발 생성형 AI ‘삼성 가우스’ 공개, 2023.11.08.\n",
      "삼성전자, ‘삼성 개발자 콘퍼런스 코리아 2023’ 개최, 2023.11.14.\n",
      "TechRepublic, Samsung Gauss: Samsung Research Reveals Generative AI, 2023.11.08.\n",
      "10' metadata={'source': 'data/SPRI_AI_Brief_2023년12월호_F.pdf', 'file_path': 'data/SPRI_AI_Brief_2023년12월호_F.pdf', 'page': 12, 'total_pages': 23, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13462', 'Producer': 'Hancom PDF 1.3.0.542', 'CreationDate': \"D:20231208132838+09'00'\", 'ModDate': \"D:20231208132838+09'00'\", 'PDFVersion': '1.4'}\n",
      "page_content='<구글 딥마인드의 범용 AI 분류 프레임워크>\n",
      "성능 특수 AI 예시 범용 AI 예시\n",
      "0단계: AI 아님 계산기 소프트웨어, 컴파일러 아마존 메커니컬 터크\n",
      "1단계: 신진(숙련되지 않은 인간) GOFAI(Good Old Fashioned Artificial Intelligence) 챗GPT, 바드, 라마2\n",
      "스마트 스피커(애플 시리, 아마존 알렉사, 구글\n",
      "2단계: 유능(숙련된 인간의 50% 이상) 미달성\n",
      "어시스턴트), IBM 왓슨\n",
      "3단계: 전문가(숙련된 인간의 90% 이상) 문법 교정기(그래머리), 생성 이미지 모델(달리2) 미달성' metadata={'source': 'data/SPRI_AI_Brief_2023년12월호_F.pdf', 'file_path': 'data/SPRI_AI_Brief_2023년12월호_F.pdf', 'page': 18, 'total_pages': 23, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13462', 'Producer': 'Hancom PDF 1.3.0.542', 'CreationDate': \"D:20231208132838+09'00'\", 'ModDate': \"D:20231208132838+09'00'\", 'PDFVersion': '1.4'}\n",
      "page_content='제작을 포함\n",
      "n 알리바바 클라우드는 급증하는 생성 AI 수요에 대응해 모델 개발과 애플리케이션 구축 절차를\n",
      "간소화하는 올인원 AI 모델 구축 플랫폼 ‘젠AI(GenAI)’도 공개\n",
      "∙ 이 플랫폼은 데이터 관리, 모델 배포와 평가, 신속한 엔지니어링을 위한 종합 도구 모음을 제공하여\n",
      "다양한 기업들이 맞춤형 AI 모델을 한층 쉽게 개발할 수 있도록 지원\n",
      "∙ 생성 AI 개발에 필요한 컴퓨팅과 데이터 처리 요구사항을 지원하기 위해 AI 플랫폼(PAI),\n",
      "데이터베이스 솔루션, 컨테이너 서비스와 같은 클라우드 신제품도 발표' metadata={'source': 'data/SPRI_AI_Brief_2023년12월호_F.pdf', 'file_path': 'data/SPRI_AI_Brief_2023년12월호_F.pdf', 'page': 11, 'total_pages': 23, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13462', 'Producer': 'Hancom PDF 1.3.0.542', 'CreationDate': \"D:20231208132838+09'00'\", 'ModDate': \"D:20231208132838+09'00'\", 'PDFVersion': '1.4'}\n",
      "page_content='처리를 지원\n",
      "∙ 코드 모델 기반의 AI 코딩 어시스턴트 ‘코드아이(code.i)’는 대화형 인터페이스로 서비스를 제공하며\n",
      "사내 소프트웨어 개발에 최적화\n",
      "∙ 이미지 모델은 창의적인 이미지를 생성하고 기존 이미지를 원하는 대로 바꿀 수 있도록 지원하며\n",
      "저해상도 이미지의 고해상도 전환도 지원\n",
      "n IT 전문지 테크리퍼블릭(TechRepublic)은 온디바이스 AI가 주요 기술 트렌드로 부상했다며,\n",
      "2024년부터 가우스를 탑재한 삼성 스마트폰이 메타의 라마(Llama)2를 탑재한 퀄컴 기기 및 구글' metadata={'source': 'data/SPRI_AI_Brief_2023년12월호_F.pdf', 'file_path': 'data/SPRI_AI_Brief_2023년12월호_F.pdf', 'page': 12, 'total_pages': 23, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13462', 'Producer': 'Hancom PDF 1.3.0.542', 'CreationDate': \"D:20231208132838+09'00'\", 'ModDate': \"D:20231208132838+09'00'\", 'PDFVersion': '1.4'}\n",
      "page_content='온디바이스에서 작동하도록 설계되어 외부로 사용자의 정보가 유출되지 않는 장점을 보유\n",
      "∙ 삼성전자는 삼성 가우스를 활용한 온디바이스 AI 기술도 소개했으며, 생성 AI 모델을 다양한 제품에\n",
      "단계적으로 탑재할 계획\n",
      "n 삼성 가우스는 △텍스트를 생성하는 언어모델 △코드를 생성하는 코드 모델 △이미지를 생성하는\n",
      "이미지 모델의 3개 모델로 구성\n",
      "∙ 언어 모델은 클라우드와 온디바이스 대상 다양한 모델로 구성되며, 메일 작성, 문서 요약, 번역 업무의\n",
      "처리를 지원' metadata={'source': 'data/SPRI_AI_Brief_2023년12월호_F.pdf', 'file_path': 'data/SPRI_AI_Brief_2023년12월호_F.pdf', 'page': 12, 'total_pages': 23, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13462', 'Producer': 'Hancom PDF 1.3.0.542', 'CreationDate': \"D:20231208132838+09'00'\", 'ModDate': \"D:20231208132838+09'00'\", 'PDFVersion': '1.4'}\n",
      "page_content='▹ 빌 게이츠, AI 에이전트로 인한 컴퓨터 사용의 패러다임 변화 전망································13\n",
      "▹ 유튜브, 2024년부터 AI 생성 콘텐츠 표시 의무화····························································14\n",
      "3. 기술/연구\n",
      "▹ 영국 과학혁신기술부, AI 안전 연구소 설립 발표······························································15' metadata={'source': 'data/SPRI_AI_Brief_2023년12월호_F.pdf', 'file_path': 'data/SPRI_AI_Brief_2023년12월호_F.pdf', 'page': 1, 'total_pages': 23, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13462', 'Producer': 'Hancom PDF 1.3.0.542', 'CreationDate': \"D:20231208132838+09'00'\", 'ModDate': \"D:20231208132838+09'00'\", 'PDFVersion': '1.4'}\n",
      "page_content='£챗GPT와 구글 바드와 같은 AI 챗봇은 범용 AI 1단계 수준\n",
      "n 구글 딥마인드 연구진은 2023년 11월 4일 범용 AI(Artificial General Intelligence, AGI) 모델을 용도와\n",
      "성능에 따라 분류하는 프레임워크를 제시한 논문을 발표\n",
      "∙ 프레임워크의 목적은 AGI의 성능, 범용성, 자율성 수준을 정의하여 모델 간 비교와 위험 평가, AGI\n",
      "달성까지의 진행 상황을 측정할 수 있는 공통 기준을 제공하기 위함\n",
      "n 연구진은 AGI 개념 정의에 필요한 기준을 수립하기 위한 6가지 원칙을 아래와 같이 도출' metadata={'source': 'data/SPRI_AI_Brief_2023년12월호_F.pdf', 'file_path': 'data/SPRI_AI_Brief_2023년12월호_F.pdf', 'page': 18, 'total_pages': 23, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13462', 'Producer': 'Hancom PDF 1.3.0.542', 'CreationDate': \"D:20231208132838+09'00'\", 'ModDate': \"D:20231208132838+09'00'\", 'PDFVersion': '1.4'}\n",
      "==================================================\n",
      "==== [GRADE DOCUMENTS] ====\n",
      "==== GRADE: DOCUMENT RELEVANT ====\n",
      "==== GRADE: DOCUMENT RELEVANT ====\n",
      "==== GRADE: DOCUMENT RELEVANT ====\n",
      "==== GRADE: DOCUMENT RELEVANT ====\n",
      "==== GRADE: DOCUMENT NOT RELEVANT ====\n",
      "==== GRADE: DOCUMENT NOT RELEVANT ====\n",
      "==== GRADE: DOCUMENT RELEVANT ====\n",
      "==== GRADE: DOCUMENT RELEVANT ====\n",
      "==== GRADE: DOCUMENT NOT RELEVANT ====\n",
      "==== GRADE: DOCUMENT NOT RELEVANT ====\n",
      "==== [ASSESS GRADED DOCUMENTS] ====\n",
      "==== [DECISION: GENERATE] ====\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mgrade_documents\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "page_content='SPRi AI Brief |\n",
      "2023-12월호\n",
      "삼성전자, 자체 개발 생성 AI ‘삼성 가우스’ 공개\n",
      "KEY Contents\n",
      "n 삼성전자가 온디바이스에서 작동 가능하며 언어, 코드, 이미지의 3개 모델로 구성된 자체 개발 생성\n",
      "AI 모델 ‘삼성 가우스’를 공개\n",
      "n 삼성전자는 삼성 가우스를 다양한 제품에 단계적으로 탑재할 계획으로, 온디바이스 작동이 가능한\n",
      "삼성 가우스는 외부로 사용자 정보가 유출될 위험이 없다는 장점을 보유\n",
      "£언어, 코드, 이미지의 3개 모델로 구성된 삼성 가우스, 온디바이스 작동 지원' metadata={'source': 'data/SPRI_AI_Brief_2023년12월호_F.pdf', 'file_path': 'data/SPRI_AI_Brief_2023년12월호_F.pdf', 'page': 12, 'total_pages': 23, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13462', 'Producer': 'Hancom PDF 1.3.0.542', 'CreationDate': \"D:20231208132838+09'00'\", 'ModDate': \"D:20231208132838+09'00'\", 'PDFVersion': '1.4'}\n",
      "page_content='▹ 삼성전자, 자체 개발 생성 AI ‘삼성 가우스’ 공개 ···························································10\n",
      "▹ 구글, 앤스로픽에 20억 달러 투자로 생성 AI 협력 강화 ················································11\n",
      "▹ IDC, 2027년 AI 소프트웨어 매출 2,500억 달러 돌파 전망···········································12' metadata={'source': 'data/SPRI_AI_Brief_2023년12월호_F.pdf', 'file_path': 'data/SPRI_AI_Brief_2023년12월호_F.pdf', 'page': 1, 'total_pages': 23, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13462', 'Producer': 'Hancom PDF 1.3.0.542', 'CreationDate': \"D:20231208132838+09'00'\", 'ModDate': \"D:20231208132838+09'00'\", 'PDFVersion': '1.4'}\n",
      "page_content='£언어, 코드, 이미지의 3개 모델로 구성된 삼성 가우스, 온디바이스 작동 지원\n",
      "n 삼성전자가 2023년 11월 8일 열린 ‘삼성 AI 포럼 2023’ 행사에서 자체 개발한 생성 AI 모델\n",
      "‘삼성 가우스’를 최초 공개\n",
      "∙ 정규분포 이론을 정립한 천재 수학자 가우스(Gauss)의 이름을 본뜬 삼성 가우스는 다양한 상황에\n",
      "최적화된 크기의 모델 선택이 가능\n",
      "∙ 삼성 가우스는 라이선스나 개인정보를 침해하지 않는 안전한 데이터를 통해 학습되었으며,\n",
      "온디바이스에서 작동하도록 설계되어 외부로 사용자의 정보가 유출되지 않는 장점을 보유' metadata={'source': 'data/SPRI_AI_Brief_2023년12월호_F.pdf', 'file_path': 'data/SPRI_AI_Brief_2023년12월호_F.pdf', 'page': 12, 'total_pages': 23, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13462', 'Producer': 'Hancom PDF 1.3.0.542', 'CreationDate': \"D:20231208132838+09'00'\", 'ModDate': \"D:20231208132838+09'00'\", 'PDFVersion': '1.4'}\n",
      "page_content='어시스턴트를 적용한 구글 픽셀(Pixel)과 경쟁할 것으로 예상\n",
      "☞ 출처 : 삼성전자, ‘삼성 AI 포럼’서 자체 개발 생성형 AI ‘삼성 가우스’ 공개, 2023.11.08.\n",
      "삼성전자, ‘삼성 개발자 콘퍼런스 코리아 2023’ 개최, 2023.11.14.\n",
      "TechRepublic, Samsung Gauss: Samsung Research Reveals Generative AI, 2023.11.08.\n",
      "10' metadata={'source': 'data/SPRI_AI_Brief_2023년12월호_F.pdf', 'file_path': 'data/SPRI_AI_Brief_2023년12월호_F.pdf', 'page': 12, 'total_pages': 23, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13462', 'Producer': 'Hancom PDF 1.3.0.542', 'CreationDate': \"D:20231208132838+09'00'\", 'ModDate': \"D:20231208132838+09'00'\", 'PDFVersion': '1.4'}\n",
      "page_content='처리를 지원\n",
      "∙ 코드 모델 기반의 AI 코딩 어시스턴트 ‘코드아이(code.i)’는 대화형 인터페이스로 서비스를 제공하며\n",
      "사내 소프트웨어 개발에 최적화\n",
      "∙ 이미지 모델은 창의적인 이미지를 생성하고 기존 이미지를 원하는 대로 바꿀 수 있도록 지원하며\n",
      "저해상도 이미지의 고해상도 전환도 지원\n",
      "n IT 전문지 테크리퍼블릭(TechRepublic)은 온디바이스 AI가 주요 기술 트렌드로 부상했다며,\n",
      "2024년부터 가우스를 탑재한 삼성 스마트폰이 메타의 라마(Llama)2를 탑재한 퀄컴 기기 및 구글' metadata={'source': 'data/SPRI_AI_Brief_2023년12월호_F.pdf', 'file_path': 'data/SPRI_AI_Brief_2023년12월호_F.pdf', 'page': 12, 'total_pages': 23, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13462', 'Producer': 'Hancom PDF 1.3.0.542', 'CreationDate': \"D:20231208132838+09'00'\", 'ModDate': \"D:20231208132838+09'00'\", 'PDFVersion': '1.4'}\n",
      "page_content='온디바이스에서 작동하도록 설계되어 외부로 사용자의 정보가 유출되지 않는 장점을 보유\n",
      "∙ 삼성전자는 삼성 가우스를 활용한 온디바이스 AI 기술도 소개했으며, 생성 AI 모델을 다양한 제품에\n",
      "단계적으로 탑재할 계획\n",
      "n 삼성 가우스는 △텍스트를 생성하는 언어모델 △코드를 생성하는 코드 모델 △이미지를 생성하는\n",
      "이미지 모델의 3개 모델로 구성\n",
      "∙ 언어 모델은 클라우드와 온디바이스 대상 다양한 모델로 구성되며, 메일 작성, 문서 요약, 번역 업무의\n",
      "처리를 지원' metadata={'source': 'data/SPRI_AI_Brief_2023년12월호_F.pdf', 'file_path': 'data/SPRI_AI_Brief_2023년12월호_F.pdf', 'page': 12, 'total_pages': 23, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13462', 'Producer': 'Hancom PDF 1.3.0.542', 'CreationDate': \"D:20231208132838+09'00'\", 'ModDate': \"D:20231208132838+09'00'\", 'PDFVersion': '1.4'}\n",
      "==================================================\n",
      "==== [GENERATE] ====\n",
      "==== [CHECK HALLUCINATIONS] ====\n",
      "==== [DECISION: GENERATION IS GROUNDED IN DOCUMENTS] ====\n",
      "==== [GRADE GENERATION vs QUESTION] ====\n",
      "==== [DECISION: GENERATION ADDRESSES QUESTION] ====\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mgenerate\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "\u001b[1;32mgeneration\u001b[0m:\n",
      "삼성전자가 개발한 생성형 AI의 이름은 ‘삼성 가우스’이다.\n",
      "\n",
      "**Source**\n",
      "- data/SPRI_AI_Brief_2023년12월호_F.pdf (p.12)\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_teddynote.messages import stream_graph, invoke_graph, random_uuid\n",
    "\n",
    "# config 설정(재귀 최대 횟수, thread_id)\n",
    "config = RunnableConfig(recursion_limit=10, configurable={\"thread_id\": random_uuid()})\n",
    "\n",
    "# 질문 입력\n",
    "inputs = {\n",
    "    \"question\": \"삼성전자가 개발한 생성형 AI 의 이름은?\",\n",
    "}\n",
    "\n",
    "# 그래프 실행\n",
    "invoke_graph(\n",
    "    app, inputs, config, [\"retrieve\", \"transform_query\", \"grade_documents\", \"generate\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== [RETRIEVE] ====\n",
      "==== [GRADE DOCUMENTS] ====\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mgrade_documents\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "{\"binary_score\":\"no\"}==== GRADE: DOCUMENT NOT RELEVANT ====\n",
      "{\"binary_score\":\"no\"}==== GRADE: DOCUMENT NOT RELEVANT ====\n",
      "{\"binary_score\":\"no\"}==== GRADE: DOCUMENT NOT RELEVANT ====\n",
      "{\"binary_score\":\"no\"}==== GRADE: DOCUMENT NOT RELEVANT ====\n",
      "{\"binary_score\":\"no\"}==== GRADE: DOCUMENT NOT RELEVANT ====\n",
      "{\"binary_score\":\"==== GRADE: DOCUMENT NOT RELEVANT ====\n",
      "no\"}{\"binary_score\":\"no==== GRADE: DOCUMENT NOT RELEVANT ====\n",
      "\"}{\"binary_score\":\"no\"}==== GRADE: DOCUMENT NOT RELEVANT ====\n",
      "{\"binary_score\":\"no\"}==== GRADE: DOCUMENT NOT RELEVANT ====\n",
      "{\"binary_score\":\"no\"}==== GRADE: DOCUMENT NOT RELEVANT ====\n",
      "==== [ASSESS GRADED DOCUMENTS] ====\n",
      "==== [DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY] ====\n",
      "==== [TRANSFORM QUERY] ====\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mtransform_query\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "테디노트가 개발한 생성형 인공지능(AI)의 공식 명칭은 무엇인가요?==== [RETRIEVE] ====\n",
      "==== [GRADE DOCUMENTS] ====\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mgrade_documents\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "{\"binary_score\":\"no\"}==== GRADE: DOCUMENT NOT RELEVANT ====\n",
      "{\"binary_score\":\"no\"}==== GRADE: DOCUMENT NOT RELEVANT ====\n",
      "{\"binary_score\":\"no\"}==== GRADE: DOCUMENT NOT RELEVANT ====\n",
      "{\"binary_score\":\"no\"}==== GRADE: DOCUMENT NOT RELEVANT ====\n",
      "{\"binary_score\":\"no\"}==== GRADE: DOCUMENT NOT RELEVANT ====\n",
      "{\"binary_score\":\"no\"}==== GRADE: DOCUMENT NOT RELEVANT ====\n",
      "{\"binary_score\":\"no\"}==== GRADE: DOCUMENT NOT RELEVANT ====\n",
      "{\"binary==== GRADE: DOCUMENT NOT RELEVANT ====_score\n",
      "\":\"no\"}{\"binary_score\":\"no\"}==== GRADE: DOCUMENT NOT RELEVANT ====\n",
      "{\"binary_score\":\"no\"}==== GRADE: DOCUMENT NOT RELEVANT ====\n",
      "==== [ASSESS GRADED DOCUMENTS] ====\n",
      "==== [DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY] ====\n",
      "==== [TRANSFORM QUERY] ====\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mtransform_query\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "What is the official name of the generative artificial intelligence (AI) developed by TeddyNote?==== [RETRIEVE] ====\n",
      "==== [GRADE DOCUMENTS] ====\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mgrade_documents\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "{\"binary_score\":\"no\"}==== GRADE: DOCUMENT NOT RELEVANT ====\n",
      "{\"binary_score\":\"no\"}==== GRADE: DOCUMENT NOT RELEVANT ====\n",
      "{\"binary_score\":\"no\"}==== GRADE: DOCUMENT NOT RELEVANT ====\n",
      "{\"binary_score\":\"no\"}==== GRADE: DOCUMENT NOT RELEVANT ====\n",
      "{\"binary_score\":\"no\"}==== GRADE: DOCUMENT NOT RELEVANT ====\n",
      "{\"binary_score\":\"no\"}==== GRADE: DOCUMENT NOT RELEVANT ====\n",
      "{\"binary_score\":\"no\"}==== GRADE: DOCUMENT NOT RELEVANT ====\n",
      "{\"binary_score\":\"no\"}==== GRADE: DOCUMENT NOT RELEVANT ====\n",
      "{\"binary_score\":\"no\"}==== GRADE: DOCUMENT NOT RELEVANT ====\n",
      "{\"binary_score\":\"no\"}==== GRADE: DOCUMENT NOT RELEVANT ====\n",
      "==== [ASSESS GRADED DOCUMENTS] ====\n",
      "==== [DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY] ====\n",
      "==== [TRANSFORM QUERY] ====\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mtransform_query\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "What is the official name of the generative AI model created by TeddyNote?==== [RETRIEVE] ====\n",
      "GraphRecursionError: Recursion limit of 10 reached without hitting a stop condition. You can increase the limit by setting the `recursion_limit` config key.\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/GRAPH_RECURSION_LIMIT\n"
     ]
    }
   ],
   "source": [
    "from langgraph.errors import GraphRecursionError\n",
    "\n",
    "# config 설정(재귀 최대 횟수, thread_id)\n",
    "config = RunnableConfig(recursion_limit=10, configurable={\"thread_id\": random_uuid()})\n",
    "\n",
    "# 질문 입력\n",
    "inputs = {\n",
    "    \"question\": \"테디노트가 개발한 생성형 AI 의 이름은?\",\n",
    "}\n",
    "\n",
    "try:\n",
    "    # 그래프 실행\n",
    "    stream_graph(\n",
    "        app,\n",
    "        inputs,\n",
    "        config,\n",
    "        [\"retrieve\", \"transform_query\", \"grade_documents\", \"generate\"],\n",
    "    )\n",
    "except GraphRecursionError as recursion_error:\n",
    "    print(f\"GraphRecursionError: {recursion_error}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "05. 계획 후 실행(Plan-and-Execute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-4.1\n"
     ]
    }
   ],
   "source": [
    "from langchain_teddynote.models import get_model_name, LLMs\n",
    "\n",
    "# 모델명 정의\n",
    "MODEL_NAME = get_model_name(LLMs.GPT4_1)\n",
    "print(MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote.tools import TavilySearch\n",
    "\n",
    "# Tavily 검색 도구 초기화\n",
    "tools = [TavilySearch(max_results=3)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# 프롬프트 정의\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer in Korean.\",\n",
    "        ),\n",
    "        (\"human\", \"{messages}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# LLM 정의\n",
    "llm = ChatOpenAI(model=MODEL_NAME, temperature=0)\n",
    "\n",
    "# ReAct 에이전트 생성\n",
    "agent_executor = create_react_agent(llm, tools, prompt=prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='랭체인 한국어 튜토리얼에 대해서 설명해줘', additional_kwargs={}, response_metadata={}, id='ab40a1bb-783c-4835-a938-168e8316ab2f'),\n",
       "  AIMessage(content='랭체인(LangChain)은 자연어 처리(NLP)와 인공지능(AI) 애플리케이션을 쉽게 만들 수 있도록 도와주는 파이썬 기반의 오픈소스 프레임워크입니다. 특히 챗봇, 문서 요약, 질의응답 시스템 등 다양한 LLM(대형 언어 모델) 기반 서비스를 개발할 때 많이 사용됩니다.\\n\\n한국어 튜토리얼에 대해 간단히 설명드리면 다음과 같습니다.\\n\\n1. 랭체인 기본 개념\\n- 랭체인은 LLM(예: GPT-3, GPT-4 등)과 다양한 데이터 소스(문서, 웹, 데이터베이스 등)를 연결해주는 역할을 합니다.\\n- 체인(Chain), 에이전트(Agent), 프롬프트(Prompt) 등 여러 컴포넌트로 구성되어 있습니다.\\n\\n2. 설치 방법\\n- pip install langchain 명령어로 설치할 수 있습니다.\\n- OpenAI API 키 등 LLM을 사용하기 위한 환경설정이 필요합니다.\\n\\n3. 기본 사용법\\n- 프롬프트 템플릿을 만들어 LLM에 입력할 수 있습니다.\\n- 체인을 이용해 여러 작업(예: 질문 → 검색 → 요약)을 순차적으로 처리할 수 있습니다.\\n- 에이전트를 사용하면 LLM이 도구(예: 계산기, 검색엔진 등)를 직접 사용할 수 있습니다.\\n\\n4. 한국어 지원\\n- 랭체인은 한국어도 지원합니다. 다만, 사용하는 LLM이 한국어를 잘 이해해야 하므로, 한국어 지원이 좋은 모델(OpenAI GPT-3.5/4, KoGPT 등)을 선택하는 것이 중요합니다.\\n\\n5. 예제 코드\\n아래는 간단한 랭체인 한국어 예제입니다.\\n\\nfrom langchain.llms import OpenAI\\nfrom langchain.prompts import PromptTemplate\\nfrom langchain.chains import LLMChain\\n\\nllm = OpenAI(model_name=\"gpt-3.5-turbo\", openai_api_key=\"YOUR_API_KEY\")\\nprompt = PromptTemplate(template=\"한국의 수도는 어디인가요?\", input_variables=[])\\nchain = LLMChain(llm=llm, prompt=prompt)\\nresult = chain.run()\\nprint(result)  # \"서울입니다.\"와 같은 답변 출력\\n\\n6. 참고 자료\\n- 공식 문서: https://python.langchain.com/\\n- 한국어 튜토리얼 블로그, 유튜브 등도 검색하면 다양한 자료를 찾을 수 있습니다.\\n\\n혹시 더 구체적인 예제나, 특정 기능(예: 문서 요약, 질의응답 등)에 대한 튜토리얼이 필요하시면 말씀해 주세요!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 578, 'prompt_tokens': 153, 'total_tokens': 731, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-2025-04-14', 'system_fingerprint': 'fp_daf5fcc80a', 'id': 'chatcmpl-C9PDWa5EV88escSMqeomRQFu7sJLE', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--3f549942-196a-4c5a-9556-d81385028a3a-0', usage_metadata={'input_tokens': 153, 'output_tokens': 578, 'total_tokens': 731, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 에이전트 실행\n",
    "agent_executor.invoke(\n",
    "    {\"messages\": [(\"user\", \"랭체인 한국어 튜토리얼에 대해서 설명해줘\")]}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import Annotated, List, Tuple\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "\n",
    "# 상태 정의\n",
    "class PlanExecute(TypedDict):\n",
    "    input: Annotated[str, \"User's input\"]\n",
    "    plan: Annotated[List[str], \"Current plan\"]\n",
    "    past_steps: Annotated[List[Tuple], operator.add]\n",
    "    response: Annotated[str, \"Final response\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "\n",
    "# Plan 모델 정의\n",
    "class Plan(BaseModel):\n",
    "    \"\"\"Sorted steps to execute the plan\"\"\"\n",
    "\n",
    "    steps: Annotated[List[str], \"Different steps to follow, should be in sorted order\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 계획 수립을 위한 프롬프트 템플릿 생성\n",
    "planner_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"For the given objective, come up with a simple step by step plan. \\\n",
    "This plan should involve individual tasks, that if executed correctly will yield the correct answer. Do not add any superfluous steps. \\\n",
    "The result of the final step should be the final answer. Make sure that each step has all the information needed - do not skip steps.\n",
    "Answer in Korean.\"\"\",\n",
    "        ),\n",
    "        (\"placeholder\", \"{messages}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "planner = planner_prompt | ChatOpenAI(\n",
    "    model=MODEL_NAME, temperature=0\n",
    ").with_structured_output(Plan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Plan(steps=['LangGraph의 핵심 장점을 정리한다.', 'LangGraph의 핵심 단점을 정리한다.', 'LangGraph를 사용하는 이유를 정리한다.', '각 항목을 명확하게 구분하여 정리된 내용을 최종 답변으로 작성한다.'])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Planner 실행\n",
    "planner.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            (\n",
    "                \"user\",\n",
    "                \"LangGraph 의 핵심 장단점과 LangGraph 를 사용하는 이유는 무엇인가?\",\n",
    "            )\n",
    "        ]\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "\n",
    "\n",
    "class Response(BaseModel):\n",
    "    \"\"\"Response to user.\"\"\"\n",
    "\n",
    "    # 사용자 응답\n",
    "    response: str\n",
    "\n",
    "\n",
    "class Act(BaseModel):\n",
    "    \"\"\"Action to perform.\"\"\"\n",
    "\n",
    "    # 수행할 작업: \"Response\", \"Plan\". 사용자에게 응답할 경우 Response 사용, 추가 도구 사용이 필요할 경우 Plan 사용\n",
    "    action: Union[Response, Plan] = Field(\n",
    "        description=\"Action to perform. If you want to respond to user, use Response. \"\n",
    "        \"If you need to further use tools to get the answer, use Plan.\"\n",
    "    )\n",
    "\n",
    "\n",
    "# 계획을 재수립하기 위한 프롬프트 정의\n",
    "replanner_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"For the given objective, come up with a simple step by step plan. \\\n",
    "This plan should involve individual tasks, that if executed correctly will yield the correct answer. Do not add any superfluous steps. \\\n",
    "The result of the final step should be the final answer. Make sure that each step has all the information needed - do not skip steps.\n",
    "\n",
    "Your objective was this:\n",
    "{input}\n",
    "\n",
    "Your original plan was this:\n",
    "{plan}\n",
    "\n",
    "You have currently done the follow steps:\n",
    "{past_steps}\n",
    "\n",
    "Update your plan accordingly. If no more steps are needed and you can return to the user, then respond with that. Otherwise, fill out the plan. Only add steps to the plan that still NEED to be done. Do not return previously done steps as part of the plan.\n",
    "\n",
    "Answer in Korean.\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "# Replanner 생성\n",
    "replanner = replanner_prompt | ChatOpenAI(\n",
    "    model=MODEL_NAME, temperature=0\n",
    ").with_structured_output(Act)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "# 사용자 입력을 기반으로 계획을 생성하고 반환\n",
    "def plan_step(state: PlanExecute):\n",
    "    plan = planner.invoke({\"messages\": [(\"user\", state[\"input\"])]})\n",
    "    # 생성된 계획의 단계 리스트 반환\n",
    "    return {\"plan\": plan.steps}\n",
    "\n",
    "\n",
    "# 에이전트 실행기를 사용하여 주어진 작업을 수행하고 결과를 반환\n",
    "def execute_step(state: PlanExecute):\n",
    "    plan = state[\"plan\"]\n",
    "    # 계획을 문자열로 변환하여 각 단계에 번호를 매김\n",
    "    plan_str = \"\\n\".join(f\"{i+1}. {step}\" for i, step in enumerate(plan))\n",
    "    task = plan[0]\n",
    "    # 현재 실행할 작업을 포맷팅하여 에이전트에 전달\n",
    "    task_formatted = f\"\"\"For the following plan:\n",
    "{plan_str}\\n\\nYou are tasked with executing [step 1. {task}].\"\"\"\n",
    "    # 에이전트 실행기를 통해 작업 수행 및 결과 수신\n",
    "    agent_response = agent_executor.invoke({\"messages\": [(\"user\", task_formatted)]})\n",
    "    # 이전 단계와 그 결과를 포함하는 딕셔너리 반환\n",
    "    return {\n",
    "        \"past_steps\": [(task, agent_response[\"messages\"][-1].content)],\n",
    "    }\n",
    "\n",
    "\n",
    "# 이전 단계의 결과를 바탕으로 계획을 업데이트하거나 최종 응답을 반환\n",
    "def replan_step(state: PlanExecute):\n",
    "    output = replanner.invoke(state)\n",
    "    # 응답이 사용자에게 반환될 경우\n",
    "    if isinstance(output.action, Response):\n",
    "        return {\"response\": output.action.response}\n",
    "    # 추가 단계가 필요할 경우 계획의 단계 리스트 반환\n",
    "    else:\n",
    "        next_plan = output.action.steps\n",
    "        if len(next_plan) == 0:\n",
    "            return {\"response\": \"No more steps needed.\"}\n",
    "        else:\n",
    "            return {\"plan\": next_plan}\n",
    "\n",
    "\n",
    "# 에이전트의 실행 종료 여부를 결정하는 함수\n",
    "def should_end(state: PlanExecute):\n",
    "    if \"response\" in state and state[\"response\"]:\n",
    "        return \"final_report\"\n",
    "    else:\n",
    "        return \"execute\"\n",
    "\n",
    "\n",
    "final_report_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"You are given the objective and the previously done steps. Your task is to generate a final report in markdown format.\n",
    "Final report should be written in professional tone.\n",
    "\n",
    "Your objective was this:\n",
    "\n",
    "{input}\n",
    "\n",
    "Your previously done steps(question and answer pairs):\n",
    "\n",
    "{past_steps}\n",
    "\n",
    "Generate a final report in markdown format. Write your response in Korean.\"\"\"\n",
    ")\n",
    "\n",
    "final_report = (\n",
    "    final_report_prompt\n",
    "    | ChatOpenAI(model=MODEL_NAME, temperature=0)\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "\n",
    "def generate_final_report(state: PlanExecute):\n",
    "    past_steps = \"\\n\\n\".join(\n",
    "        [\n",
    "            f\"Question: {past_step[0]}\\n\\nAnswer: {past_step[1]}\\n\\n####\"\n",
    "            for past_step in state[\"past_steps\"]\n",
    "        ]\n",
    "    )\n",
    "    response = final_report.invoke({\"input\": state[\"input\"], \"past_steps\": past_steps})\n",
    "    return {\"response\": response}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "\n",
    "# 그래프 생성\n",
    "workflow = StateGraph(PlanExecute)\n",
    "\n",
    "# 노드 정의\n",
    "workflow.add_node(\"planner\", plan_step)\n",
    "workflow.add_node(\"execute\", execute_step)\n",
    "workflow.add_node(\"replan\", replan_step)\n",
    "workflow.add_node(\"final_report\", generate_final_report)\n",
    "\n",
    "# 엣지 정의\n",
    "workflow.add_edge(START, \"planner\")\n",
    "workflow.add_edge(\"planner\", \"execute\")\n",
    "workflow.add_edge(\"execute\", \"replan\")\n",
    "workflow.add_edge(\"final_report\", END)\n",
    "\n",
    "# 조건부 엣지: replan 후 종료 여부를 결정하는 함수 사용\n",
    "workflow.add_conditional_edges(\n",
    "    \"replan\",\n",
    "    should_end,\n",
    "    {\"execute\": \"execute\", \"final_report\": \"final_report\"},\n",
    ")\n",
    "\n",
    "# 그래프 컴파일\n",
    "app = workflow.compile(checkpointer=MemorySaver())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_teddynote.graphs import visualize_graph\n",
    "\n",
    "visualize_graph(app, xray=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mplanner\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Naive RAG의 구조와 동작 방식을 설명한다.\n",
      "Modular RAG의 구조와 동작 방식을 설명한다.\n",
      "Naive RAG와 Modular RAG의 구조적 차이점을 비교한다.\n",
      "Production level에서 Modular RAG를 사용할 때의 이점을 정리한다.\n",
      "각 이점이 실제 서비스 운영에 어떻게 도움이 되는지 설명한다.\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36magent\u001b[0m in [\u001b[1;33mexecute\u001b[0m] 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Naive RAG(naive Retrieval-Augmented Generation)의 구조와 동작 방식\n",
      "\n",
      "Naive RAG는 정보 검색(Retrieval)과 생성(Generation)을 결합한 자연어 처리(NLP) 프레임워크의 가장 기본적인 형태입니다. 주로 대형 언어 모델(LLM)과 외부 지식 베이스(문서, 데이터베이스 등)를 함께 활용하여, 입력 질문에 대해 더 정확하고 풍부한 답변을 생성하는 데 사용됩니다.\n",
      "\n",
      "구조\n",
      "\n",
      "- Query 입력: 사용자가 질문(질의, Query)을 입력합니다.\n",
      "- Retriever(검색기): 입력된 질문을 바탕으로 외부 지식 베이스에서 관련성이 높은 문서나 정보를 검색합니다.\n",
      "- Generator(생성기): 검색된 문서와 원래 질문을 함께 받아, 대형 언어 모델(예: GPT, BERT 등)이 최종 답변을 생성합니다.\n",
      "\n",
      "동작 방식\n",
      "\n",
      "- 사용자가 질문을 입력합니다.\n",
      "- Retriever가 질문과 유사한 문서(또는 문서의 일부)를 외부 지식 베이스에서 찾아냅니다.\n",
      "- Generator가 검색된 문서와 질문을 함께 입력받아, 이를 바탕으로 자연어로 답변을 생성합니다.\n",
      "- 생성된 답변을 사용자에게 제공합니다.\n",
      "\n",
      "특징\n",
      "\n",
      "- 검색과 생성이 한 번에 순차적으로 이루어집니다.\n",
      "- 검색된 문서가 곧바로 생성 모델에 입력되며, 별도의 전처리나 후처리 과정이 거의 없습니다.\n",
      "- 구조가 단순하여 구현이 쉽지만, 검색과 생성 과정이 밀접하게 결합되어 있어 유연성이 떨어질 수 있습니다.\n",
      "\n",
      "요약\n",
      "Naive RAG는 질문 → 검색 → 생성의 단순한 파이프라인 구조로, 검색된 정보를 바로 생성 모델에 입력하여 답변을 생성하는 방식입니다.\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mexecute\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "('Naive RAG의 구조와 동작 방식을 설명한다.', 'Naive RAG(naive Retrieval-Augmented Generation)의 구조와 동작 방식\\n\\nNaive RAG는 정보 검색(Retrieval)과 생성(Generation)을 결합한 자연어 처리(NLP) 프레임워크의 가장 기본적인 형태입니다. 주로 대형 언어 모델(LLM)과 외부 지식 베이스(문서, 데이터베이스 등)를 함께 활용하여, 입력 질문에 대해 더 정확하고 풍부한 답변을 생성하는 데 사용됩니다.\\n\\n구조\\n\\n- Query 입력: 사용자가 질문(질의, Query)을 입력합니다.\\n- Retriever(검색기): 입력된 질문을 바탕으로 외부 지식 베이스에서 관련성이 높은 문서나 정보를 검색합니다.\\n- Generator(생성기): 검색된 문서와 원래 질문을 함께 받아, 대형 언어 모델(예: GPT, BERT 등)이 최종 답변을 생성합니다.\\n\\n동작 방식\\n\\n- 사용자가 질문을 입력합니다.\\n- Retriever가 질문과 유사한 문서(또는 문서의 일부)를 외부 지식 베이스에서 찾아냅니다.\\n- Generator가 검색된 문서와 질문을 함께 입력받아, 이를 바탕으로 자연어로 답변을 생성합니다.\\n- 생성된 답변을 사용자에게 제공합니다.\\n\\n특징\\n\\n- 검색과 생성이 한 번에 순차적으로 이루어집니다.\\n- 검색된 문서가 곧바로 생성 모델에 입력되며, 별도의 전처리나 후처리 과정이 거의 없습니다.\\n- 구조가 단순하여 구현이 쉽지만, 검색과 생성 과정이 밀접하게 결합되어 있어 유연성이 떨어질 수 있습니다.\\n\\n요약\\nNaive RAG는 질문 → 검색 → 생성의 단순한 파이프라인 구조로, 검색된 정보를 바로 생성 모델에 입력하여 답변을 생성하는 방식입니다.')\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mreplan\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Modular RAG의 구조와 동작 방식을 설명한다.\n",
      "Naive RAG와 Modular RAG의 구조적 차이점을 비교한다.\n",
      "Production level에서 Modular RAG를 사용할 때의 이점을 정리한다.\n",
      "각 이점이 실제 서비스 운영에 어떻게 도움이 되는지 설명한다.\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36magent\u001b[0m in [\u001b[1;33mexecute\u001b[0m] 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Modular RAG(모듈러 RAG, Retrieval-Augmented Generation)의 구조와 동작 방식에 대해 설명하겠습니다.\n",
      "\n",
      "1. Modular RAG의 구조\n",
      "\n",
      "Modular RAG는 전통적인 RAG 시스템을 여러 개의 독립적인 모듈로 분리하여 구성한 구조입니다. 각 모듈은 특정 기능(예: 데이터 전처리, 인덱싱, 검색, 후처리 등)을 담당하며, 전체 파이프라인이 유기적으로 연결되어 있습니다. 주요 구성 요소는 다음과 같습니다.\n",
      "\n",
      "- 데이터 전처리 모듈: 원본 데이터를 정제하고, 문서 단위로 분할하거나 임베딩을 생성할 준비를 합니다.\n",
      "- 인덱싱 모듈: 전처리된 데이터를 벡터 데이터베이스 등에 저장하고, 효율적인 검색이 가능하도록 인덱스를 구축합니다.\n",
      "- 검색(Retrieval) 모듈: 사용자의 쿼리와 유사한 문서를 벡터 검색 등을 통해 찾아냅니다.\n",
      "- 생성(Generation) 모듈: 검색된 문서를 바탕으로 LLM(대형 언어 모델)이 최종 답변을 생성합니다.\n",
      "- 후처리 및 평가 모듈: 생성된 답변을 정제하거나, 품질 평가를 수행합니다.\n",
      "\n",
      "2. Modular RAG의 동작 방식\n",
      "\n",
      "- 사용자가 쿼리를 입력하면, 쿼리는 먼저 전처리 모듈을 거쳐 적절한 형태로 변환됩니다.\n",
      "- 변환된 쿼리는 검색 모듈로 전달되어, 인덱싱된 데이터 중에서 관련성이 높은 문서들을 검색합니다.\n",
      "- 검색된 문서들은 생성 모듈로 전달되어, LLM이 이 문서들을 참고하여 답변을 생성합니다.\n",
      "- 생성된 답변은 후처리 모듈을 통해 추가적인 정제나 평가가 이루어집니다.\n",
      "- 최종적으로 사용자는 정제된 답변을 받게 됩니다.\n",
      "\n",
      "이처럼 Modular RAG는 각 기능별로 모듈이 분리되어 있어, 각 단계의 독립적인 개선 및 유지보수가 용이하며, 필요에 따라 특정 모듈만 교체하거나 확장할 수 있는 유연한 구조를 가지고 있습니다.\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mexecute\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "('Modular RAG의 구조와 동작 방식을 설명한다.', 'Modular RAG(모듈러 RAG, Retrieval-Augmented Generation)의 구조와 동작 방식에 대해 설명하겠습니다.\\n\\n1. Modular RAG의 구조\\n\\nModular RAG는 전통적인 RAG 시스템을 여러 개의 독립적인 모듈로 분리하여 구성한 구조입니다. 각 모듈은 특정 기능(예: 데이터 전처리, 인덱싱, 검색, 후처리 등)을 담당하며, 전체 파이프라인이 유기적으로 연결되어 있습니다. 주요 구성 요소는 다음과 같습니다.\\n\\n- 데이터 전처리 모듈: 원본 데이터를 정제하고, 문서 단위로 분할하거나 임베딩을 생성할 준비를 합니다.\\n- 인덱싱 모듈: 전처리된 데이터를 벡터 데이터베이스 등에 저장하고, 효율적인 검색이 가능하도록 인덱스를 구축합니다.\\n- 검색(Retrieval) 모듈: 사용자의 쿼리와 유사한 문서를 벡터 검색 등을 통해 찾아냅니다.\\n- 생성(Generation) 모듈: 검색된 문서를 바탕으로 LLM(대형 언어 모델)이 최종 답변을 생성합니다.\\n- 후처리 및 평가 모듈: 생성된 답변을 정제하거나, 품질 평가를 수행합니다.\\n\\n2. Modular RAG의 동작 방식\\n\\n- 사용자가 쿼리를 입력하면, 쿼리는 먼저 전처리 모듈을 거쳐 적절한 형태로 변환됩니다.\\n- 변환된 쿼리는 검색 모듈로 전달되어, 인덱싱된 데이터 중에서 관련성이 높은 문서들을 검색합니다.\\n- 검색된 문서들은 생성 모듈로 전달되어, LLM이 이 문서들을 참고하여 답변을 생성합니다.\\n- 생성된 답변은 후처리 모듈을 통해 추가적인 정제나 평가가 이루어집니다.\\n- 최종적으로 사용자는 정제된 답변을 받게 됩니다.\\n\\n이처럼 Modular RAG는 각 기능별로 모듈이 분리되어 있어, 각 단계의 독립적인 개선 및 유지보수가 용이하며, 필요에 따라 특정 모듈만 교체하거나 확장할 수 있는 유연한 구조를 가지고 있습니다.')\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mreplan\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Naive RAG와 Modular RAG의 구조적 차이점을 비교한다.\n",
      "Production level에서 Modular RAG를 사용할 때의 이점을 정리한다.\n",
      "각 이점이 실제 서비스 운영에 어떻게 도움이 되는지 설명한다.\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36magent\u001b[0m in [\u001b[1;33mexecute\u001b[0m] 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "네, 아래는 Naive RAG와 Modular RAG의 구조적 차이점에 대한 비교입니다.\n",
      "\n",
      "Naive RAG와 Modular RAG의 구조적 차이점\n",
      "\n",
      "1. Naive RAG (Retrieval-Augmented Generation)\n",
      "- 구조: 단일 파이프라인으로, 입력 쿼리가 들어오면 바로 검색(검색기) → 생성(생성기) 과정을 거칩니다.\n",
      "- 컴포넌트: 검색기(retriever)와 생성기(generator)가 하나의 흐름으로 연결되어 있습니다.\n",
      "- 유연성: 각 단계가 고정되어 있어, 검색기나 생성기의 교체·확장·조정이 어렵습니다.\n",
      "- 예시: 단순히 벡터DB에서 top-k 문서를 검색한 뒤, 이를 LLM에 넣어 답변을 생성하는 방식.\n",
      "\n",
      "2. Modular RAG\n",
      "- 구조: 각 컴포넌트(검색, 필터링, 랭킹, 생성 등)가 모듈화되어 독립적으로 동작합니다.\n",
      "- 컴포넌트: 검색기, 필터, 랭커, 생성기 등 여러 모듈이 조합되어 파이프라인을 구성합니다.\n",
      "- 유연성: 각 모듈을 독립적으로 교체, 확장, 튜닝할 수 있어 다양한 요구사항에 맞게 시스템을 조정할 수 있습니다.\n",
      "- 예시: 검색 결과에 추가적인 필터링, 랭킹, 후처리 모듈을 삽입하거나, 여러 검색기/생성기를 조합하여 사용.\n",
      "\n",
      "요약 표\n",
      "\n",
      "구분 Naive RAG Modular RAG\n",
      "구조 단일 파이프라인 모듈화된 파이프라인\n",
      "컴포넌트 검색기+생성기 검색기, 필터, 랭커, 생성기 등\n",
      "유연성 낮음 높음\n",
      "확장성 제한적 높음\n",
      "운영 난이도 낮음(단순) 높음(복잡)\n",
      "\n",
      "이렇게 Naive RAG는 단순하고 빠르게 구현할 수 있지만, 구조적으로 유연성이 떨어집니다. 반면 Modular RAG는 각 기능이 모듈화되어 있어, 서비스 요구에 따라 쉽게 확장·변경할 수 있는 구조적 장점이 있습니다.\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mexecute\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "('Naive RAG와 Modular RAG의 구조적 차이점을 비교한다.', '네, 아래는 Naive RAG와 Modular RAG의 구조적 차이점에 대한 비교입니다.\\n\\nNaive RAG와 Modular RAG의 구조적 차이점\\n\\n1. Naive RAG (Retrieval-Augmented Generation)\\n- 구조: 단일 파이프라인으로, 입력 쿼리가 들어오면 바로 검색(검색기) → 생성(생성기) 과정을 거칩니다.\\n- 컴포넌트: 검색기(retriever)와 생성기(generator)가 하나의 흐름으로 연결되어 있습니다.\\n- 유연성: 각 단계가 고정되어 있어, 검색기나 생성기의 교체·확장·조정이 어렵습니다.\\n- 예시: 단순히 벡터DB에서 top-k 문서를 검색한 뒤, 이를 LLM에 넣어 답변을 생성하는 방식.\\n\\n2. Modular RAG\\n- 구조: 각 컴포넌트(검색, 필터링, 랭킹, 생성 등)가 모듈화되어 독립적으로 동작합니다.\\n- 컴포넌트: 검색기, 필터, 랭커, 생성기 등 여러 모듈이 조합되어 파이프라인을 구성합니다.\\n- 유연성: 각 모듈을 독립적으로 교체, 확장, 튜닝할 수 있어 다양한 요구사항에 맞게 시스템을 조정할 수 있습니다.\\n- 예시: 검색 결과에 추가적인 필터링, 랭킹, 후처리 모듈을 삽입하거나, 여러 검색기/생성기를 조합하여 사용.\\n\\n요약 표\\n\\n구분 Naive RAG Modular RAG\\n구조 단일 파이프라인 모듈화된 파이프라인\\n컴포넌트 검색기+생성기 검색기, 필터, 랭커, 생성기 등\\n유연성 낮음 높음\\n확장성 제한적 높음\\n운영 난이도 낮음(단순) 높음(복잡)\\n\\n이렇게 Naive RAG는 단순하고 빠르게 구현할 수 있지만, 구조적으로 유연성이 떨어집니다. 반면 Modular RAG는 각 기능이 모듈화되어 있어, 서비스 요구에 따라 쉽게 확장·변경할 수 있는 구조적 장점이 있습니다.')\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mreplan\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Production level에서 Modular RAG를 사용할 때의 이점을 정리한다.\n",
      "각 이점이 실제 서비스 운영에 어떻게 도움이 되는지 설명한다.\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36magent\u001b[0m in [\u001b[1;33mexecute\u001b[0m] 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Production level에서 Modular RAG(Retrieval-Augmented Generation)를 사용할 때의 이점은 다음과 같이 정리할 수 있습니다.\n",
      "\n",
      "1. 유연한 아키텍처 구성\n",
      "- 다양한 데이터 소스, 벡터DB, 검색 엔진, LLM 등을 모듈 단위로 교체하거나 확장할 수 있어 서비스 요구사항 변화에 빠르게 대응할 수 있습니다.\n",
      "\n",
      "2. 유지보수 및 확장성 용이\n",
      "- 각 모듈이 독립적으로 동작하므로 특정 기능(예: 인덱싱, 검색, 생성 등)만 별도로 개선하거나 확장할 수 있습니다.\n",
      "- 장애 발생 시 문제 모듈만 교체/수정 가능해 전체 서비스 안정성이 높아집니다.\n",
      "\n",
      "3. 성능 최적화\n",
      "- 데이터 전처리, 검색, 생성 등 각 단계별로 최적화된 솔루션을 선택해 적용할 수 있습니다.\n",
      "- 병렬 처리 및 캐싱 등 성능 개선 전략을 모듈 단위로 적용할 수 있습니다.\n",
      "\n",
      "4. 테스트 및 검증 용이\n",
      "- 각 모듈별로 단위 테스트가 가능해 전체 시스템의 신뢰성을 높일 수 있습니다.\n",
      "- 새로운 기술이나 모델을 부분적으로 적용해 실험할 수 있습니다.\n",
      "\n",
      "5. 보안 및 접근 제어 강화\n",
      "- 민감 데이터 접근, 인증, 로깅 등 보안 관련 기능을 별도 모듈로 관리할 수 있습니다.\n",
      "\n",
      "6. 비용 효율성\n",
      "- 필요에 따라 특정 모듈만 클라우드 리소스를 확장하거나 축소할 수 있어 비용을 효율적으로 관리할 수 있습니다.\n",
      "\n",
      "이상으로 Production level에서 Modular RAG를 사용할 때의 주요 이점을 정리했습니다.\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mexecute\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "('Production level에서 Modular RAG를 사용할 때의 이점을 정리한다.', 'Production level에서 Modular RAG(Retrieval-Augmented Generation)를 사용할 때의 이점은 다음과 같이 정리할 수 있습니다.\\n\\n1. 유연한 아키텍처 구성\\n- 다양한 데이터 소스, 벡터DB, 검색 엔진, LLM 등을 모듈 단위로 교체하거나 확장할 수 있어 서비스 요구사항 변화에 빠르게 대응할 수 있습니다.\\n\\n2. 유지보수 및 확장성 용이\\n- 각 모듈이 독립적으로 동작하므로 특정 기능(예: 인덱싱, 검색, 생성 등)만 별도로 개선하거나 확장할 수 있습니다.\\n- 장애 발생 시 문제 모듈만 교체/수정 가능해 전체 서비스 안정성이 높아집니다.\\n\\n3. 성능 최적화\\n- 데이터 전처리, 검색, 생성 등 각 단계별로 최적화된 솔루션을 선택해 적용할 수 있습니다.\\n- 병렬 처리 및 캐싱 등 성능 개선 전략을 모듈 단위로 적용할 수 있습니다.\\n\\n4. 테스트 및 검증 용이\\n- 각 모듈별로 단위 테스트가 가능해 전체 시스템의 신뢰성을 높일 수 있습니다.\\n- 새로운 기술이나 모델을 부분적으로 적용해 실험할 수 있습니다.\\n\\n5. 보안 및 접근 제어 강화\\n- 민감 데이터 접근, 인증, 로깅 등 보안 관련 기능을 별도 모듈로 관리할 수 있습니다.\\n\\n6. 비용 효율성\\n- 필요에 따라 특정 모듈만 클라우드 리소스를 확장하거나 축소할 수 있어 비용을 효율적으로 관리할 수 있습니다.\\n\\n이상으로 Production level에서 Modular RAG를 사용할 때의 주요 이점을 정리했습니다.')\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mreplan\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "\u001b[1;32mresponse\u001b[0m:\n",
      "마지막 단계인 '각 이점이 실제 서비스 운영에 어떻게 도움이 되는지 설명한다.'만 남았습니다. 이 단계를 수행하면 최종 답변이 완성됩니다. 추가로 필요한 단계는 없습니다.\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mfinal_report\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "\u001b[1;32mresponse\u001b[0m:\n",
      "# Modular RAG와 Naive RAG의 구조적 차이 및 Production 환경에서의 이점\n",
      "\n",
      "## 1. 서론\n",
      "\n",
      "Retrieval-Augmented Generation(RAG)은 대형 언어 모델(LLM)에 외부 지식 베이스의 정보를 결합하여, 보다 정확하고 풍부한 답변을 생성하는 대표적인 자연어 처리 프레임워크입니다. 최근에는 단순한 Naive RAG 구조를 넘어, 각 기능별로 모듈화된 Modular RAG가 주목받고 있습니다. 본 보고서에서는 Naive RAG와 Modular RAG의 구조적 차이와, Modular RAG가 실제 서비스(Production level) 환경에서 제공하는 주요 이점에 대해 정리합니다.\n",
      "\n",
      "---\n",
      "\n",
      "## 2. Naive RAG의 구조와 동작 방식\n",
      "\n",
      "Naive RAG는 다음과 같은 단순한 파이프라인 구조를 가집니다.\n",
      "\n",
      "- **Query 입력**: 사용자가 질문을 입력\n",
      "- **Retriever(검색기)**: 외부 지식 베이스에서 관련 문서 검색\n",
      "- **Generator(생성기)**: 검색된 문서와 질문을 함께 받아 LLM이 답변 생성\n",
      "\n",
      "이 방식은 검색과 생성이 순차적으로 한 번에 이루어지며, 별도의 전처리나 후처리 과정이 거의 없습니다. 구조가 단순하여 구현이 쉽지만, 각 단계가 밀접하게 결합되어 있어 유연성과 확장성이 제한적입니다.\n",
      "\n",
      "---\n",
      "\n",
      "## 3. Modular RAG의 구조와 동작 방식\n",
      "\n",
      "Modular RAG는 전체 파이프라인을 여러 개의 독립적인 모듈로 분리하여 구성합니다. 주요 모듈은 다음과 같습니다.\n",
      "\n",
      "- **데이터 전처리 모듈**: 원본 데이터 정제 및 분할, 임베딩 생성 준비\n",
      "- **인덱싱 모듈**: 전처리된 데이터를 벡터DB 등에 저장, 인덱스 구축\n",
      "- **검색(Retrieval) 모듈**: 쿼리와 유사한 문서 검색\n",
      "- **생성(Generation) 모듈**: 검색된 문서를 바탕으로 LLM이 답변 생성\n",
      "- **후처리 및 평가 모듈**: 생성된 답변의 정제 및 품질 평가\n",
      "\n",
      "각 모듈은 독립적으로 동작하며, 필요에 따라 교체·확장·튜닝이 가능합니다.\n",
      "\n",
      "---\n",
      "\n",
      "## 4. Naive RAG와 Modular RAG의 구조적 차이\n",
      "\n",
      "| 구분         | Naive RAG                | Modular RAG                        |\n",
      "|--------------|--------------------------|-------------------------------------|\n",
      "| 구조         | 단일 파이프라인          | 모듈화된 파이프라인                 |\n",
      "| 컴포넌트     | 검색기 + 생성기          | 검색기, 필터, 랭커, 생성기 등       |\n",
      "| 유연성       | 낮음                     | 높음                                |\n",
      "| 확장성       | 제한적                   | 높음                                |\n",
      "| 운영 난이도  | 낮음(단순)               | 높음(복잡)                          |\n",
      "\n",
      "- **Naive RAG**는 단순한 구조로 빠른 구현이 가능하지만, 각 단계의 독립적 개선이나 확장이 어렵습니다.\n",
      "- **Modular RAG**는 각 기능이 모듈화되어 있어, 서비스 요구에 따라 손쉽게 구조를 변경하거나 확장할 수 있습니다.\n",
      "\n",
      "---\n",
      "\n",
      "## 5. Production 환경에서 Modular RAG의 이점\n",
      "\n",
      "Modular RAG는 실제 서비스 환경에서 다음과 같은 장점을 제공합니다.\n",
      "\n",
      "1. **유연한 아키텍처 구성**\n",
      "   - 다양한 데이터 소스, 벡터DB, 검색 엔진, LLM 등을 모듈 단위로 교체·확장 가능\n",
      "   - 서비스 요구사항 변화에 빠르게 대응\n",
      "\n",
      "2. **유지보수 및 확장성 용이**\n",
      "   - 각 모듈이 독립적으로 동작하여, 특정 기능만 별도 개선·확장 가능\n",
      "   - 장애 발생 시 문제 모듈만 교체/수정 가능, 전체 서비스 안정성 향상\n",
      "\n",
      "3. **성능 최적화**\n",
      "   - 각 단계별로 최적화된 솔루션 적용 가능\n",
      "   - 병렬 처리, 캐싱 등 성능 개선 전략을 모듈 단위로 적용\n",
      "\n",
      "4. **테스트 및 검증 용이**\n",
      "   - 각 모듈별 단위 테스트 가능, 시스템 신뢰성 향상\n",
      "   - 새로운 기술이나 모델을 부분적으로 적용해 실험 가능\n",
      "\n",
      "5. **보안 및 접근 제어 강화**\n",
      "   - 민감 데이터 접근, 인증, 로깅 등 보안 기능을 별도 모듈로 관리\n",
      "\n",
      "6. **비용 효율성**\n",
      "   - 필요에 따라 특정 모듈만 클라우드 리소스를 확장/축소하여 비용 효율적 운영 가능\n",
      "\n",
      "---\n",
      "\n",
      "## 6. 결론\n",
      "\n",
      "Modular RAG는 Naive RAG에 비해 구조적 유연성과 확장성이 뛰어나며, 실제 서비스 환경에서의 운영 효율성, 성능 최적화, 보안 강화 등 다양한 이점을 제공합니다. 복잡한 요구사항과 빠른 변화에 대응해야 하는 Production 환경에서는 Modular RAG의 도입이 점차 필수적인 선택이 되고 있습니다.\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "from langchain_teddynote.messages import invoke_graph, random_uuid\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "config = RunnableConfig(recursion_limit=50, configurable={\"thread_id\": random_uuid()})\n",
    "\n",
    "inputs = {\n",
    "    \"input\": \"Modular RAG 가 기존의 Naive RAG 와 어떤 차이가 있는지와 production level 에서 사용하는 이점을 설명해줘\"\n",
    "}\n",
    "\n",
    "invoke_graph(app, inputs, config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Modular RAG와 Naive RAG의 구조적 차이 및 Production 환경에서의 이점\n",
      "\n",
      "## 1. 서론\n",
      "\n",
      "Retrieval-Augmented Generation(RAG)은 대형 언어 모델(LLM)에 외부 지식 베이스의 정보를 결합하여, 보다 정확하고 풍부한 답변을 생성하는 대표적인 자연어 처리 프레임워크입니다. 최근에는 단순한 Naive RAG 구조를 넘어, 각 기능별로 모듈화된 Modular RAG가 주목받고 있습니다. 본 보고서에서는 Naive RAG와 Modular RAG의 구조적 차이와, Modular RAG가 실제 서비스(Production level) 환경에서 제공하는 주요 이점에 대해 정리합니다.\n",
      "\n",
      "---\n",
      "\n",
      "## 2. Naive RAG의 구조와 동작 방식\n",
      "\n",
      "Naive RAG는 다음과 같은 단순한 파이프라인 구조를 가집니다.\n",
      "\n",
      "- **Query 입력**: 사용자가 질문을 입력\n",
      "- **Retriever(검색기)**: 외부 지식 베이스에서 관련 문서 검색\n",
      "- **Generator(생성기)**: 검색된 문서와 질문을 함께 받아 LLM이 답변 생성\n",
      "\n",
      "이 방식은 검색과 생성이 순차적으로 한 번에 이루어지며, 별도의 전처리나 후처리 과정이 거의 없습니다. 구조가 단순하여 구현이 쉽지만, 각 단계가 밀접하게 결합되어 있어 유연성과 확장성이 제한적입니다.\n",
      "\n",
      "---\n",
      "\n",
      "## 3. Modular RAG의 구조와 동작 방식\n",
      "\n",
      "Modular RAG는 전체 파이프라인을 여러 개의 독립적인 모듈로 분리하여 구성합니다. 주요 모듈은 다음과 같습니다.\n",
      "\n",
      "- **데이터 전처리 모듈**: 원본 데이터 정제 및 분할, 임베딩 생성 준비\n",
      "- **인덱싱 모듈**: 전처리된 데이터를 벡터DB 등에 저장, 인덱스 구축\n",
      "- **검색(Retrieval) 모듈**: 쿼리와 유사한 문서 검색\n",
      "- **생성(Generation) 모듈**: 검색된 문서를 바탕으로 LLM이 답변 생성\n",
      "- **후처리 및 평가 모듈**: 생성된 답변의 정제 및 품질 평가\n",
      "\n",
      "각 모듈은 독립적으로 동작하며, 필요에 따라 교체·확장·튜닝이 가능합니다.\n",
      "\n",
      "---\n",
      "\n",
      "## 4. Naive RAG와 Modular RAG의 구조적 차이\n",
      "\n",
      "| 구분         | Naive RAG                | Modular RAG                        |\n",
      "|--------------|--------------------------|-------------------------------------|\n",
      "| 구조         | 단일 파이프라인          | 모듈화된 파이프라인                 |\n",
      "| 컴포넌트     | 검색기 + 생성기          | 검색기, 필터, 랭커, 생성기 등       |\n",
      "| 유연성       | 낮음                     | 높음                                |\n",
      "| 확장성       | 제한적                   | 높음                                |\n",
      "| 운영 난이도  | 낮음(단순)               | 높음(복잡)                          |\n",
      "\n",
      "- **Naive RAG**는 단순한 구조로 빠른 구현이 가능하지만, 각 단계의 독립적 개선이나 확장이 어렵습니다.\n",
      "- **Modular RAG**는 각 기능이 모듈화되어 있어, 서비스 요구에 따라 손쉽게 구조를 변경하거나 확장할 수 있습니다.\n",
      "\n",
      "---\n",
      "\n",
      "## 5. Production 환경에서 Modular RAG의 이점\n",
      "\n",
      "Modular RAG는 실제 서비스 환경에서 다음과 같은 장점을 제공합니다.\n",
      "\n",
      "1. **유연한 아키텍처 구성**\n",
      "   - 다양한 데이터 소스, 벡터DB, 검색 엔진, LLM 등을 모듈 단위로 교체·확장 가능\n",
      "   - 서비스 요구사항 변화에 빠르게 대응\n",
      "\n",
      "2. **유지보수 및 확장성 용이**\n",
      "   - 각 모듈이 독립적으로 동작하여, 특정 기능만 별도 개선·확장 가능\n",
      "   - 장애 발생 시 문제 모듈만 교체/수정 가능, 전체 서비스 안정성 향상\n",
      "\n",
      "3. **성능 최적화**\n",
      "   - 각 단계별로 최적화된 솔루션 적용 가능\n",
      "   - 병렬 처리, 캐싱 등 성능 개선 전략을 모듈 단위로 적용\n",
      "\n",
      "4. **테스트 및 검증 용이**\n",
      "   - 각 모듈별 단위 테스트 가능, 시스템 신뢰성 향상\n",
      "   - 새로운 기술이나 모델을 부분적으로 적용해 실험 가능\n",
      "\n",
      "5. **보안 및 접근 제어 강화**\n",
      "   - 민감 데이터 접근, 인증, 로깅 등 보안 기능을 별도 모듈로 관리\n",
      "\n",
      "6. **비용 효율성**\n",
      "   - 필요에 따라 특정 모듈만 클라우드 리소스를 확장/축소하여 비용 효율적 운영 가능\n",
      "\n",
      "---\n",
      "\n",
      "## 6. 결론\n",
      "\n",
      "Modular RAG는 Naive RAG에 비해 구조적 유연성과 확장성이 뛰어나며, 실제 서비스 환경에서의 운영 효율성, 성능 최적화, 보안 강화 등 다양한 이점을 제공합니다. 복잡한 요구사항과 빠른 변화에 대응해야 하는 Production 환경에서는 Modular RAG의 도입이 점차 필수적인 선택이 되고 있습니다.\n"
     ]
    }
   ],
   "source": [
    "snapshot = app.get_state(config).values\n",
    "print(snapshot[\"response\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Modular RAG와 Naive RAG의 구조적 차이 및 Production 환경에서의 이점\n",
       "\n",
       "## 1. 서론\n",
       "\n",
       "Retrieval-Augmented Generation(RAG)은 대형 언어 모델(LLM)에 외부 지식 베이스의 정보를 결합하여, 보다 정확하고 풍부한 답변을 생성하는 대표적인 자연어 처리 프레임워크입니다. 최근에는 단순한 Naive RAG 구조를 넘어, 각 기능별로 모듈화된 Modular RAG가 주목받고 있습니다. 본 보고서에서는 Naive RAG와 Modular RAG의 구조적 차이와, Modular RAG가 실제 서비스(Production level) 환경에서 제공하는 주요 이점에 대해 정리합니다.\n",
       "\n",
       "---\n",
       "\n",
       "## 2. Naive RAG의 구조와 동작 방식\n",
       "\n",
       "Naive RAG는 다음과 같은 단순한 파이프라인 구조를 가집니다.\n",
       "\n",
       "- **Query 입력**: 사용자가 질문을 입력\n",
       "- **Retriever(검색기)**: 외부 지식 베이스에서 관련 문서 검색\n",
       "- **Generator(생성기)**: 검색된 문서와 질문을 함께 받아 LLM이 답변 생성\n",
       "\n",
       "이 방식은 검색과 생성이 순차적으로 한 번에 이루어지며, 별도의 전처리나 후처리 과정이 거의 없습니다. 구조가 단순하여 구현이 쉽지만, 각 단계가 밀접하게 결합되어 있어 유연성과 확장성이 제한적입니다.\n",
       "\n",
       "---\n",
       "\n",
       "## 3. Modular RAG의 구조와 동작 방식\n",
       "\n",
       "Modular RAG는 전체 파이프라인을 여러 개의 독립적인 모듈로 분리하여 구성합니다. 주요 모듈은 다음과 같습니다.\n",
       "\n",
       "- **데이터 전처리 모듈**: 원본 데이터 정제 및 분할, 임베딩 생성 준비\n",
       "- **인덱싱 모듈**: 전처리된 데이터를 벡터DB 등에 저장, 인덱스 구축\n",
       "- **검색(Retrieval) 모듈**: 쿼리와 유사한 문서 검색\n",
       "- **생성(Generation) 모듈**: 검색된 문서를 바탕으로 LLM이 답변 생성\n",
       "- **후처리 및 평가 모듈**: 생성된 답변의 정제 및 품질 평가\n",
       "\n",
       "각 모듈은 독립적으로 동작하며, 필요에 따라 교체·확장·튜닝이 가능합니다.\n",
       "\n",
       "---\n",
       "\n",
       "## 4. Naive RAG와 Modular RAG의 구조적 차이\n",
       "\n",
       "| 구분         | Naive RAG                | Modular RAG                        |\n",
       "|--------------|--------------------------|-------------------------------------|\n",
       "| 구조         | 단일 파이프라인          | 모듈화된 파이프라인                 |\n",
       "| 컴포넌트     | 검색기 + 생성기          | 검색기, 필터, 랭커, 생성기 등       |\n",
       "| 유연성       | 낮음                     | 높음                                |\n",
       "| 확장성       | 제한적                   | 높음                                |\n",
       "| 운영 난이도  | 낮음(단순)               | 높음(복잡)                          |\n",
       "\n",
       "- **Naive RAG**는 단순한 구조로 빠른 구현이 가능하지만, 각 단계의 독립적 개선이나 확장이 어렵습니다.\n",
       "- **Modular RAG**는 각 기능이 모듈화되어 있어, 서비스 요구에 따라 손쉽게 구조를 변경하거나 확장할 수 있습니다.\n",
       "\n",
       "---\n",
       "\n",
       "## 5. Production 환경에서 Modular RAG의 이점\n",
       "\n",
       "Modular RAG는 실제 서비스 환경에서 다음과 같은 장점을 제공합니다.\n",
       "\n",
       "1. **유연한 아키텍처 구성**\n",
       "   - 다양한 데이터 소스, 벡터DB, 검색 엔진, LLM 등을 모듈 단위로 교체·확장 가능\n",
       "   - 서비스 요구사항 변화에 빠르게 대응\n",
       "\n",
       "2. **유지보수 및 확장성 용이**\n",
       "   - 각 모듈이 독립적으로 동작하여, 특정 기능만 별도 개선·확장 가능\n",
       "   - 장애 발생 시 문제 모듈만 교체/수정 가능, 전체 서비스 안정성 향상\n",
       "\n",
       "3. **성능 최적화**\n",
       "   - 각 단계별로 최적화된 솔루션 적용 가능\n",
       "   - 병렬 처리, 캐싱 등 성능 개선 전략을 모듈 단위로 적용\n",
       "\n",
       "4. **테스트 및 검증 용이**\n",
       "   - 각 모듈별 단위 테스트 가능, 시스템 신뢰성 향상\n",
       "   - 새로운 기술이나 모델을 부분적으로 적용해 실험 가능\n",
       "\n",
       "5. **보안 및 접근 제어 강화**\n",
       "   - 민감 데이터 접근, 인증, 로깅 등 보안 기능을 별도 모듈로 관리\n",
       "\n",
       "6. **비용 효율성**\n",
       "   - 필요에 따라 특정 모듈만 클라우드 리소스를 확장/축소하여 비용 효율적 운영 가능\n",
       "\n",
       "---\n",
       "\n",
       "## 6. 결론\n",
       "\n",
       "Modular RAG는 Naive RAG에 비해 구조적 유연성과 확장성이 뛰어나며, 실제 서비스 환경에서의 운영 효율성, 성능 최적화, 보안 강화 등 다양한 이점을 제공합니다. 복잡한 요구사항과 빠른 변화에 대응해야 하는 Production 환경에서는 Modular RAG의 도입이 점차 필수적인 선택이 되고 있습니다."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "Markdown(snapshot[\"response\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "06. 멀티 에이전트 협업 네트워크(Multi-Agent Collaboration Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-4o\n"
     ]
    }
   ],
   "source": [
    "from langchain_teddynote.models import get_model_name, LLMs\n",
    "\n",
    "# 최신 모델 이름 가져오기\n",
    "MODEL_NAME = get_model_name(LLMs.GPT4o)\n",
    "\n",
    "print(MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import Annotated, Sequence\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "\n",
    "# 상태 정의\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[\n",
    "        Sequence[BaseMessage], operator.add\n",
    "    ]  # Agent 간 공유하는 메시지 목록\n",
    "    sender: Annotated[str, \"The sender of the last message\"]  # 마지막 메시지의 발신자\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "\n",
    "from langchain_teddynote.tools.tavily import TavilySearch\n",
    "from langchain_core.tools import tool\n",
    "from langchain_experimental.utilities import PythonREPL\n",
    "\n",
    "# Tavily 검색 도구 정의\n",
    "tavily_tool = TavilySearch(max_results=5)\n",
    "\n",
    "# Python 코드를 실행하는 도구 정의\n",
    "python_repl = PythonREPL()\n",
    "\n",
    "\n",
    "# Python 코드를 실행하는 도구 정의\n",
    "@tool\n",
    "def python_repl_tool(\n",
    "    code: Annotated[str, \"The python code to execute to generate your chart.\"],\n",
    "):\n",
    "    \"\"\"Use this to execute python code. If you want to see the output of a value,\n",
    "    you should print it out with `print(...)`. This is visible to the user.\"\"\"\n",
    "    try:\n",
    "        # 주어진 코드를 Python REPL에서 실행하고 결과 반환\n",
    "        result = python_repl.run(code)\n",
    "    except BaseException as e:\n",
    "        return f\"Failed to execute code. Error: {repr(e)}\"\n",
    "    # 실행 성공 시 결과와 함께 성공 메시지 반환\n",
    "    result_str = f\"Successfully executed:\\n```python\\n{code}\\n```\\nStdout: {result}\"\n",
    "    return (\n",
    "        result_str + \"\\n\\nIf you have completed all tasks, respond with FINAL ANSWER.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_system_prompt(suffix: str) -> str:\n",
    "    return (\n",
    "        \"You are a helpful AI assistant, collaborating with other assistants.\"\n",
    "        \" Use the provided tools to progress towards answering the question.\"\n",
    "        \" If you are unable to fully answer, that's OK, another assistant with different tools \"\n",
    "        \" will help where you left off. Execute what you can to make progress.\"\n",
    "        \" If you or any of the other assistants have the final answer or deliverable,\"\n",
    "        \" prefix your response with FINAL ANSWER so the team knows to stop.\"\n",
    "        f\"\\n{suffix}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph.graph import MessagesState\n",
    "\n",
    "# LLM 정의\n",
    "llm = ChatOpenAI(model=MODEL_NAME)\n",
    "\n",
    "# Research Agent 생성\n",
    "research_agent = create_react_agent(\n",
    "    llm,\n",
    "    tools=[tavily_tool],\n",
    "    prompt=make_system_prompt(\n",
    "        \"You can only do research. You are working with a chart generator colleague.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "# Research Agent 노드 정의\n",
    "def research_node(state: MessagesState) -> MessagesState:\n",
    "    result = research_agent.invoke(state)\n",
    "\n",
    "    # 마지막 메시지를 HumanMessage 로 변환\n",
    "    last_message = HumanMessage(\n",
    "        content=result[\"messages\"][-1].content, name=\"researcher\"\n",
    "    )\n",
    "    return {\n",
    "        # Research Agent 의 메시지 목록 반환\n",
    "        \"messages\": [last_message],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_generator_system_prompt = \"\"\"\n",
    "You can only generate charts. You are working with a researcher colleague.\n",
    "Be sure to use the following font code in your code when generating charts.\n",
    "\n",
    "##### 폰트 설정 #####\n",
    "import platform\n",
    "\n",
    "# OS 판단\n",
    "current_os = platform.system()\n",
    "\n",
    "if current_os == \"Windows\":\n",
    "    # Windows 환경 폰트 설정\n",
    "    font_path = \"C:/Windows/Fonts/malgun.ttf\"  # 맑은 고딕 폰트 경로\n",
    "    fontprop = fm.FontProperties(fname=font_path, size=12)\n",
    "    plt.rc(\"font\", family=fontprop.get_name())\n",
    "elif current_os == \"Darwin\":  # macOS\n",
    "    # Mac 환경 폰트 설정\n",
    "    plt.rcParams[\"font.family\"] = \"AppleGothic\"\n",
    "else:  # Linux 등 기타 OS\n",
    "    # 기본 한글 폰트 설정 시도\n",
    "    try:\n",
    "        plt.rcParams[\"font.family\"] = \"NanumGothic\"\n",
    "    except:\n",
    "        print(\"한글 폰트를 찾을 수 없습니다. 시스템 기본 폰트를 사용합니다.\")\n",
    "\n",
    "##### 마이너스 폰트 깨짐 방지 #####\n",
    "plt.rcParams[\"axes.unicode_minus\"] = False  # 마이너스 폰트 깨짐 방지\n",
    "\"\"\"\n",
    "\n",
    "# Chart Generator Agent 생성\n",
    "chart_agent = create_react_agent(\n",
    "    llm,\n",
    "    [python_repl_tool],\n",
    "    prompt=make_system_prompt(chart_generator_system_prompt),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chart_node(state: MessagesState) -> MessagesState:\n",
    "    result = chart_agent.invoke(state)\n",
    "\n",
    "    # 마지막 메시지를 HumanMessage 로 변환\n",
    "    last_message = HumanMessage(\n",
    "        content=result[\"messages\"][-1].content, name=\"chart_generator\"\n",
    "    )\n",
    "    return {\n",
    "        # share internal message history of chart agent with other agents\n",
    "        \"messages\": [last_message],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END\n",
    "\n",
    "\n",
    "def router(state: MessagesState):\n",
    "    # This is the router\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    if \"FINAL ANSWER\" in last_message.content:\n",
    "        # Any agent decided the work is done\n",
    "        return END\n",
    "    return \"continue\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, ToolMessage\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "workflow = StateGraph(MessagesState)\n",
    "workflow.add_node(\"researcher\", research_node)\n",
    "workflow.add_node(\"chart_generator\", chart_node)\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"researcher\",\n",
    "    router,\n",
    "    {\"continue\": \"chart_generator\", END: END},\n",
    ")\n",
    "workflow.add_conditional_edges(\n",
    "    \"chart_generator\",\n",
    "    router,\n",
    "    {\"continue\": \"researcher\", END: END},\n",
    ")\n",
    "\n",
    "workflow.add_edge(START, \"researcher\")\n",
    "app = workflow.compile(checkpointer=MemorySaver())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_teddynote.graphs import visualize_graph\n",
    "\n",
    "visualize_graph(app, xray=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36magent\u001b[0m in [\u001b[1;33mresearcher\u001b[0m] 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  tavily_web_search (call_SFrMJLeGNgPNZ9QAt5iV8Glm)\n",
      " Call ID: call_SFrMJLeGNgPNZ9QAt5iV8Glm\n",
      "  Args:\n",
      "    query: 대한민국 2010 to 2023 1인당 GDP 데이터\n",
      "  tavily_web_search (call_FSptEZU8QUx2cOao9RynICRy)\n",
      " Call ID: call_FSptEZU8QUx2cOao9RynICRy\n",
      "  Args:\n",
      "    query: 대한민국 2024 예측 1인당 GDP\n",
      "==================================================\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'Request too large for gpt-4o in organization org-Soffyuxth7vsUnobnzk0MGeR on tokens per min (TPM): Limit 30000, Requested 68416. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[110], line 17\u001b[0m\n\u001b[1;32m      8\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\n\u001b[1;32m     10\u001b[0m         HumanMessage(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m     ],\n\u001b[1;32m     14\u001b[0m }\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# 그래프 실행\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[43minvoke_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresearcher\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mchart_generator\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43magent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_teddynote/messages.py:409\u001b[0m, in \u001b[0;36minvoke_graph\u001b[0;34m(graph, inputs, config, node_names, callback)\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m namespace[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(namespace) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroot graph\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;66;03m# subgraphs=True 를 통해 서브그래프의 출력도 포함\u001b[39;00m\n\u001b[0;32m--> 409\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m namespace, chunk \u001b[38;5;129;01min\u001b[39;00m graph\u001b[38;5;241m.\u001b[39mstream(\n\u001b[1;32m    410\u001b[0m     inputs, config, stream_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupdates\u001b[39m\u001b[38;5;124m\"\u001b[39m, subgraphs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    411\u001b[0m ):\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m node_name, node_chunk \u001b[38;5;129;01min\u001b[39;00m chunk\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    413\u001b[0m         \u001b[38;5;66;03m# node_names가 비어있지 않은 경우에만 필터링\u001b[39;00m\n\u001b[1;32m    414\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(node_names) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m node_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m node_names:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/main.py:2647\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[0m\n\u001b[1;32m   2645\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mmatch_cached_writes():\n\u001b[1;32m   2646\u001b[0m     loop\u001b[38;5;241m.\u001b[39moutput_writes(task\u001b[38;5;241m.\u001b[39mid, task\u001b[38;5;241m.\u001b[39mwrites, cached\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m-> 2647\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mtick(\n\u001b[1;32m   2648\u001b[0m     [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mvalues() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t\u001b[38;5;241m.\u001b[39mwrites],\n\u001b[1;32m   2649\u001b[0m     timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[1;32m   2650\u001b[0m     get_waiter\u001b[38;5;241m=\u001b[39mget_waiter,\n\u001b[1;32m   2651\u001b[0m     schedule_task\u001b[38;5;241m=\u001b[39mloop\u001b[38;5;241m.\u001b[39maccept_push,\n\u001b[1;32m   2652\u001b[0m ):\n\u001b[1;32m   2653\u001b[0m     \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[1;32m   2654\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m _output(\n\u001b[1;32m   2655\u001b[0m         stream_mode, print_mode, subgraphs, stream\u001b[38;5;241m.\u001b[39mget, queue\u001b[38;5;241m.\u001b[39mEmpty\n\u001b[1;32m   2656\u001b[0m     )\n\u001b[1;32m   2657\u001b[0m loop\u001b[38;5;241m.\u001b[39mafter_tick()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/_runner.py:253\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;66;03m# panic on failure or timeout\u001b[39;00m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 253\u001b[0m     \u001b[43m_panic_or_proceed\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfutures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdone\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfutures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpanic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tb \u001b[38;5;241m:=\u001b[39m exc\u001b[38;5;241m.\u001b[39m__traceback__:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/_runner.py:511\u001b[0m, in \u001b[0;36m_panic_or_proceed\u001b[0;34m(futs, timeout_exc_cls, panic)\u001b[0m\n\u001b[1;32m    509\u001b[0m                 interrupts\u001b[38;5;241m.\u001b[39mappend(exc)\n\u001b[1;32m    510\u001b[0m             \u001b[38;5;28;01melif\u001b[39;00m fut \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m SKIP_RERAISE_SET:\n\u001b[0;32m--> 511\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m    512\u001b[0m \u001b[38;5;66;03m# raise combined interrupts\u001b[39;00m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m interrupts:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/_executor.py:81\u001b[0m, in \u001b[0;36mBackgroundExecutor.done\u001b[0;34m(self, task)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Remove the task from the tasks dict when it's done.\"\"\"\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 81\u001b[0m     \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GraphBubbleUp:\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;66;03m# This exception is an interruption signal, not an error\u001b[39;00m\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;66;03m# so we don't want to re-raise it on exit\u001b[39;00m\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mpop(task)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/concurrent/futures/_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/concurrent/futures/_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/_retry.py:42\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[1;32m     40\u001b[0m     task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     44\u001b[0m     ns: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/_internal/_runnable.py:657\u001b[0m, in \u001b[0;36mRunnableSeq.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    655\u001b[0m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[1;32m    656\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[0;32m--> 657\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    659\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/_internal/_runnable.py:401\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    399\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(ret)\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 401\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[1;32m    403\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "Cell \u001b[0;32mIn[104], line 21\u001b[0m, in \u001b[0;36mresearch_node\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mresearch_node\u001b[39m(state: MessagesState) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m MessagesState:\n\u001b[0;32m---> 21\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mresearch_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m# 마지막 메시지를 HumanMessage 로 변환\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     last_message \u001b[38;5;241m=\u001b[39m HumanMessage(\n\u001b[1;32m     25\u001b[0m         content\u001b[38;5;241m=\u001b[39mresult[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mcontent, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresearcher\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     26\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/main.py:3026\u001b[0m, in \u001b[0;36mPregel.invoke\u001b[0;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[0m\n\u001b[1;32m   3023\u001b[0m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m|\u001b[39m Any] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   3024\u001b[0m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m-> 3026\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream(\n\u001b[1;32m   3027\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   3028\u001b[0m     config,\n\u001b[1;32m   3029\u001b[0m     context\u001b[38;5;241m=\u001b[39mcontext,\n\u001b[1;32m   3030\u001b[0m     stream_mode\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupdates\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   3031\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3032\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m stream_mode,\n\u001b[1;32m   3033\u001b[0m     print_mode\u001b[38;5;241m=\u001b[39mprint_mode,\n\u001b[1;32m   3034\u001b[0m     output_keys\u001b[38;5;241m=\u001b[39moutput_keys,\n\u001b[1;32m   3035\u001b[0m     interrupt_before\u001b[38;5;241m=\u001b[39minterrupt_before,\n\u001b[1;32m   3036\u001b[0m     interrupt_after\u001b[38;5;241m=\u001b[39minterrupt_after,\n\u001b[1;32m   3037\u001b[0m     durability\u001b[38;5;241m=\u001b[39mdurability,\n\u001b[1;32m   3038\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3039\u001b[0m ):\n\u001b[1;32m   3040\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   3041\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(chunk) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/main.py:2647\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[0m\n\u001b[1;32m   2645\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mmatch_cached_writes():\n\u001b[1;32m   2646\u001b[0m     loop\u001b[38;5;241m.\u001b[39moutput_writes(task\u001b[38;5;241m.\u001b[39mid, task\u001b[38;5;241m.\u001b[39mwrites, cached\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m-> 2647\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mtick(\n\u001b[1;32m   2648\u001b[0m     [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mvalues() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t\u001b[38;5;241m.\u001b[39mwrites],\n\u001b[1;32m   2649\u001b[0m     timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[1;32m   2650\u001b[0m     get_waiter\u001b[38;5;241m=\u001b[39mget_waiter,\n\u001b[1;32m   2651\u001b[0m     schedule_task\u001b[38;5;241m=\u001b[39mloop\u001b[38;5;241m.\u001b[39maccept_push,\n\u001b[1;32m   2652\u001b[0m ):\n\u001b[1;32m   2653\u001b[0m     \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[1;32m   2654\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m _output(\n\u001b[1;32m   2655\u001b[0m         stream_mode, print_mode, subgraphs, stream\u001b[38;5;241m.\u001b[39mget, queue\u001b[38;5;241m.\u001b[39mEmpty\n\u001b[1;32m   2656\u001b[0m     )\n\u001b[1;32m   2657\u001b[0m loop\u001b[38;5;241m.\u001b[39mafter_tick()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/_runner.py:162\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[0m\n\u001b[1;32m    160\u001b[0m t \u001b[38;5;241m=\u001b[39m tasks[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 162\u001b[0m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m                \u001b[49m\u001b[43m_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m                \u001b[49m\u001b[43mweakref\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m                \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m                \u001b[49m\u001b[43mfutures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweakref\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m                \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschedule_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m                \u001b[49m\u001b[43msubmit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/_retry.py:42\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[1;32m     40\u001b[0m     task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     44\u001b[0m     ns: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/_internal/_runnable.py:657\u001b[0m, in \u001b[0;36mRunnableSeq.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    655\u001b[0m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[1;32m    656\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[0;32m--> 657\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    659\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/_internal/_runnable.py:394\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[1;32m    393\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[0;32m--> 394\u001b[0m         ret \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    396\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/prebuilt/chat_agent_executor.py:627\u001b[0m, in \u001b[0;36mcreate_react_agent.<locals>.call_model\u001b[0;34m(state, runtime, config)\u001b[0m\n\u001b[1;32m    625\u001b[0m     response \u001b[38;5;241m=\u001b[39m cast(AIMessage, dynamic_model\u001b[38;5;241m.\u001b[39minvoke(model_input, config))  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    626\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 627\u001b[0m     response \u001b[38;5;241m=\u001b[39m cast(AIMessage, \u001b[43mstatic_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m)  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;66;03m# add agent name to the AIMessage\u001b[39;00m\n\u001b[1;32m    630\u001b[0m response\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m=\u001b[39m name\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/runnables/base.py:3082\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3080\u001b[0m                 input_ \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, input_, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   3081\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3082\u001b[0m                 input_ \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3083\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   3084\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/runnables/base.py:5495\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   5488\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m   5489\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m   5490\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5493\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   5494\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[0;32m-> 5495\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5496\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5497\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5498\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5499\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:393\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    389\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    390\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    391\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    392\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatGeneration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m--> 393\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    403\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:1019\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m   1011\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m   1012\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1016\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m   1017\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m   1018\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m-> 1019\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:837\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    834\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[1;32m    835\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    836\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 837\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    838\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    840\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    843\u001b[0m         )\n\u001b[1;32m    844\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    845\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:1085\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1083\u001b[0m     result \u001b[38;5;241m=\u001b[39m generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[1;32m   1084\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1085\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1086\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m   1087\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1088\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1089\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_openai/chat_models/base.py:1183\u001b[0m, in \u001b[0;36mBaseChatOpenAI._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1181\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m raw_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(raw_response, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp_response\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1182\u001b[0m         e\u001b[38;5;241m.\u001b[39mresponse \u001b[38;5;241m=\u001b[39m raw_response\u001b[38;5;241m.\u001b[39mhttp_response  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m-> 1183\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m   1184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1185\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minclude_response_headers\n\u001b[1;32m   1186\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m raw_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(raw_response, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1188\u001b[0m ):\n\u001b[1;32m   1189\u001b[0m     generation_info \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(raw_response\u001b[38;5;241m.\u001b[39mheaders)}\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_openai/chat_models/base.py:1178\u001b[0m, in \u001b[0;36mBaseChatOpenAI._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1171\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _construct_lc_result_from_responses_api(\n\u001b[1;32m   1172\u001b[0m             response,\n\u001b[1;32m   1173\u001b[0m             schema\u001b[38;5;241m=\u001b[39moriginal_schema_obj,\n\u001b[1;32m   1174\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mgeneration_info,\n\u001b[1;32m   1175\u001b[0m             output_version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_version,\n\u001b[1;32m   1176\u001b[0m         )\n\u001b[1;32m   1177\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1178\u001b[0m         raw_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwith_raw_response\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1179\u001b[0m         response \u001b[38;5;241m=\u001b[39m raw_response\u001b[38;5;241m.\u001b[39mparse()\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_legacy_response.py:364\u001b[0m, in \u001b[0;36mto_raw_response_wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    360\u001b[0m extra_headers[RAW_RESPONSE_HEADER] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    362\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextra_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m extra_headers\n\u001b[0;32m--> 364\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cast(LegacyAPIResponse[R], \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_utils/_utils.py:287\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 287\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py:1147\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   1101\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m   1102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m   1103\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m   1145\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m   1146\u001b[0m     validate_response_format(response_format)\n\u001b[0;32m-> 1147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1150\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m   1151\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1152\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1153\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maudio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1154\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1155\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1156\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1157\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1158\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1159\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_completion_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1160\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1162\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodalities\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1163\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1165\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1166\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1167\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprompt_cache_key\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1168\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreasoning_effort\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1169\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1170\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msafety_identifier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1171\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1172\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1173\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1174\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1175\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1176\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1177\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1178\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1179\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1181\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mverbosity\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1184\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweb_search_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1185\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1186\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[1;32m   1187\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[1;32m   1188\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1189\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1190\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m   1192\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1196\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_base_client.py:1259\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1245\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1246\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1247\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1254\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1255\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1256\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1257\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1258\u001b[0m     )\n\u001b[0;32m-> 1259\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_base_client.py:1047\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1044\u001b[0m             err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1046\u001b[0m         log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1047\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1051\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould not resolve response (should never happen)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'Request too large for gpt-4o in organization org-Soffyuxth7vsUnobnzk0MGeR on tokens per min (TPM): Limit 30000, Requested 68416. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_teddynote.messages import random_uuid, invoke_graph\n",
    "\n",
    "# config 설정(재귀 최대 횟수, thread_id)\n",
    "config = RunnableConfig(recursion_limit=10, configurable={\"thread_id\": random_uuid()})\n",
    "\n",
    "# 질문 입력\n",
    "inputs = {\n",
    "    \"messages\": [\n",
    "        HumanMessage(\n",
    "            content=\"2010년 ~ 2024년까지의 대한민국의 1인당 GDP 추이를 그래프로 시각화 해주세요.\"\n",
    "        )\n",
    "    ],\n",
    "}\n",
    "\n",
    "# 그래프 실행\n",
    "invoke_graph(app, inputs, config, node_names=[\"researcher\", \"chart_generator\", \"agent\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "07. 멀티 에이전트 감독자(Multi-Agent Supervisor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-4o\n"
     ]
    }
   ],
   "source": [
    "from langchain_teddynote.models import get_model_name, LLMs\n",
    "\n",
    "# 최신 버전의 모델명을 가져옵니다.\n",
    "MODEL_NAME = get_model_name(LLMs.GPT4o)\n",
    "print(MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import Sequence, Annotated\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "\n",
    "# 상태 정의\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]  # 메시지\n",
    "    next: str  # 다음으로 라우팅할 에이전트\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote.tools.tavily import TavilySearch\n",
    "from langchain_experimental.tools import PythonREPLTool\n",
    "\n",
    "# 최대 5개의 검색 결과를 반환하는 Tavily 검색 도구 초기화\n",
    "tavily_tool = TavilySearch(max_results=5)\n",
    "\n",
    "# 로컬에서 코드를 실행하는 Python REPL 도구 초기화 (안전하지 않을 수 있음)\n",
    "python_repl_tool = PythonREPLTool()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "\n",
    "# 지정한 agent와 name을 사용하여 agent 노드를 생성\n",
    "def agent_node(state, agent, name):\n",
    "    # agent 호출\n",
    "    agent_response = agent.invoke(state)\n",
    "    # agent의 마지막 메시지를 HumanMessage로 변환하여 반환\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            HumanMessage(content=agent_response[\"messages\"][-1].content, name=name)\n",
    "        ]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "# Research Agent 생성\n",
    "research_agent = create_react_agent(ChatOpenAI(model=MODEL_NAME), tools=[tavily_tool])\n",
    "\n",
    "# research node 생성\n",
    "research_node = functools.partial(agent_node, agent=research_agent, name=\"Researcher\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='Here\\'s a simple code snippet in Python to print \"Hello, World!\" to the terminal:\\n\\n```python\\nprint(\"Hello, World!\")\\n```\\n\\nYou can run this code in any Python environment or terminal to see the output. If you need a similar example in another programming language, feel free to ask!', additional_kwargs={}, response_metadata={}, name='Researcher')]}"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "research_node(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(content=\"Code hello world and print it to the terminal\")\n",
    "        ]\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import Literal\n",
    "\n",
    "# 멤버 Agent 목록 정의\n",
    "members = [\"Researcher\", \"Coder\"]\n",
    "\n",
    "# 다음 작업자 선택 옵션 목록 정의\n",
    "options_for_next = [\"FINISH\"] + members\n",
    "\n",
    "\n",
    "# 작업자 선택 응답 모델 정의: 다음 작업자를 선택하거나 작업 완료를 나타냄\n",
    "class RouteResponse(BaseModel):\n",
    "    next: Literal[options_for_next]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "# 시스템 프롬프트 정의: 작업자 간의 대화를 관리하는 감독자 역할\n",
    "system_prompt = (\n",
    "    \"You are a supervisor tasked with managing a conversation between the\"\n",
    "    \" following workers:  {members}. Given the following user request,\"\n",
    "    \" respond with the worker to act next. Each worker will perform a\"\n",
    "    \" task and respond with their results and status. When finished,\"\n",
    "    \" respond with FINISH.\"\n",
    ")\n",
    "\n",
    "# ChatPromptTemplate 생성\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Given the conversation above, who should act next? \"\n",
    "            \"Or should we FINISH? Select one of: {options}\",\n",
    "        ),\n",
    "    ]\n",
    ").partial(options=str(options_for_next), members=\", \".join(members))\n",
    "\n",
    "\n",
    "# LLM 초기화\n",
    "llm = ChatOpenAI(model=MODEL_NAME, temperature=0)\n",
    "\n",
    "\n",
    "# Supervisor Agent 생성\n",
    "def supervisor_agent(state):\n",
    "    # 프롬프트와 LLM을 결합하여 체인 구성\n",
    "    supervisor_chain = prompt | llm.with_structured_output(RouteResponse)\n",
    "    \n",
    "    # 여기서 응답을 직접 호출하고 출력합니다.\n",
    "    response = supervisor_chain.invoke(state)\n",
    "    \n",
    "    # 디버깅을 위해 응답 값 출력\n",
    "    print(\"\\n--- LLM 응답 값 ---\")\n",
    "    print(response)\n",
    "    print(\"-------------------\\n\")\n",
    "    \n",
    "    # Agent 호출\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "\n",
    "# Research Agent 생성\n",
    "research_agent = create_react_agent(llm, tools=[tavily_tool])\n",
    "research_node = functools.partial(agent_node, agent=research_agent, name=\"Researcher\")\n",
    "\n",
    "code_system_prompt = \"\"\"\n",
    "Be sure to use the following font in your code for visualization.\n",
    "\n",
    "##### 폰트 설정 #####\n",
    "import platform\n",
    "\n",
    "# OS 판단\n",
    "current_os = platform.system()\n",
    "\n",
    "if current_os == \"Windows\":\n",
    "    # Windows 환경 폰트 설정\n",
    "    font_path = \"C:/Windows/Fonts/malgun.ttf\"  # 맑은 고딕 폰트 경로\n",
    "    fontprop = fm.FontProperties(fname=font_path, size=12)\n",
    "    plt.rc(\"font\", family=fontprop.get_name())\n",
    "elif current_os == \"Darwin\":  # macOS\n",
    "    # Mac 환경 폰트 설정\n",
    "    plt.rcParams[\"font.family\"] = \"AppleGothic\"\n",
    "else:  # Linux 등 기타 OS\n",
    "    # 기본 한글 폰트 설정 시도\n",
    "    try:\n",
    "        plt.rcParams[\"font.family\"] = \"NanumGothic\"\n",
    "    except:\n",
    "        print(\"한글 폰트를 찾을 수 없습니다. 시스템 기본 폰트를 사용합니다.\")\n",
    "\n",
    "##### 마이너스 폰트 깨짐 방지 #####\n",
    "plt.rcParams[\"axes.unicode_minus\"] = False  # 마이너스 폰트 깨짐 방지\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Coder Agent 생성\n",
    "coder_agent = create_react_agent(\n",
    "    llm,\n",
    "    tools=[python_repl_tool],\n",
    "    prompt=code_system_prompt,\n",
    ")\n",
    "coder_node = functools.partial(agent_node, agent=coder_agent, name=\"Coder\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# 그래프 생성\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# 그래프에 노드 추가\n",
    "workflow.add_node(\"Researcher\", research_node)\n",
    "workflow.add_node(\"Coder\", coder_node)\n",
    "workflow.add_node(\"Supervisor\", supervisor_agent)\n",
    "\n",
    "\n",
    "# 멤버 노드 > Supervisor 노드로 엣지 추가\n",
    "for member in members:\n",
    "    workflow.add_edge(member, \"Supervisor\")\n",
    "\n",
    "# 조건부 엣지 추가 (\n",
    "conditional_map = {k: k for k in members}\n",
    "conditional_map[\"FINISH\"] = END\n",
    "\n",
    "\n",
    "def get_next(state):\n",
    "    return state[\"next\"]\n",
    "\n",
    "\n",
    "# Supervisor 노드에서 조건부 엣지 추가\n",
    "workflow.add_conditional_edges(\"Supervisor\", get_next, conditional_map)\n",
    "\n",
    "# 시작점\n",
    "workflow.add_edge(START, \"Supervisor\")\n",
    "\n",
    "# 그래프 컴파일\n",
    "graph = workflow.compile(checkpointer=MemorySaver())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_teddynote.graphs import visualize_graph\n",
    "\n",
    "visualize_graph(graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_openai/chat_models/base.py:425: UserWarning: Invalid schema for OpenAI's structured output feature, which is the default method for `with_structured_output` as of langchain-openai==0.3. Specify `method=\"function_calling\"` instead or update your schema. See supported schemas: https://platform.openai.com/docs/guides/structured-outputs#supported-schemas\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': \"Invalid schema for response_format 'RouteResponse': In context=('properties', 'next'), array schema missing items.\", 'type': 'invalid_request_error', 'param': 'response_format', 'code': None}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[161], line 17\u001b[0m\n\u001b[1;32m      8\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\n\u001b[1;32m     10\u001b[0m         HumanMessage(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m     ],\n\u001b[1;32m     14\u001b[0m }\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# 그래프 실행\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[43minvoke_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_teddynote/messages.py:409\u001b[0m, in \u001b[0;36minvoke_graph\u001b[0;34m(graph, inputs, config, node_names, callback)\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m namespace[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(namespace) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroot graph\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;66;03m# subgraphs=True 를 통해 서브그래프의 출력도 포함\u001b[39;00m\n\u001b[0;32m--> 409\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m namespace, chunk \u001b[38;5;129;01min\u001b[39;00m graph\u001b[38;5;241m.\u001b[39mstream(\n\u001b[1;32m    410\u001b[0m     inputs, config, stream_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupdates\u001b[39m\u001b[38;5;124m\"\u001b[39m, subgraphs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    411\u001b[0m ):\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m node_name, node_chunk \u001b[38;5;129;01min\u001b[39;00m chunk\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    413\u001b[0m         \u001b[38;5;66;03m# node_names가 비어있지 않은 경우에만 필터링\u001b[39;00m\n\u001b[1;32m    414\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(node_names) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m node_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m node_names:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/main.py:2647\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[0m\n\u001b[1;32m   2645\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mmatch_cached_writes():\n\u001b[1;32m   2646\u001b[0m     loop\u001b[38;5;241m.\u001b[39moutput_writes(task\u001b[38;5;241m.\u001b[39mid, task\u001b[38;5;241m.\u001b[39mwrites, cached\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m-> 2647\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mtick(\n\u001b[1;32m   2648\u001b[0m     [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mvalues() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t\u001b[38;5;241m.\u001b[39mwrites],\n\u001b[1;32m   2649\u001b[0m     timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[1;32m   2650\u001b[0m     get_waiter\u001b[38;5;241m=\u001b[39mget_waiter,\n\u001b[1;32m   2651\u001b[0m     schedule_task\u001b[38;5;241m=\u001b[39mloop\u001b[38;5;241m.\u001b[39maccept_push,\n\u001b[1;32m   2652\u001b[0m ):\n\u001b[1;32m   2653\u001b[0m     \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[1;32m   2654\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m _output(\n\u001b[1;32m   2655\u001b[0m         stream_mode, print_mode, subgraphs, stream\u001b[38;5;241m.\u001b[39mget, queue\u001b[38;5;241m.\u001b[39mEmpty\n\u001b[1;32m   2656\u001b[0m     )\n\u001b[1;32m   2657\u001b[0m loop\u001b[38;5;241m.\u001b[39mafter_tick()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/_runner.py:253\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;66;03m# panic on failure or timeout\u001b[39;00m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 253\u001b[0m     \u001b[43m_panic_or_proceed\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfutures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdone\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfutures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpanic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tb \u001b[38;5;241m:=\u001b[39m exc\u001b[38;5;241m.\u001b[39m__traceback__:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/_runner.py:511\u001b[0m, in \u001b[0;36m_panic_or_proceed\u001b[0;34m(futs, timeout_exc_cls, panic)\u001b[0m\n\u001b[1;32m    509\u001b[0m                 interrupts\u001b[38;5;241m.\u001b[39mappend(exc)\n\u001b[1;32m    510\u001b[0m             \u001b[38;5;28;01melif\u001b[39;00m fut \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m SKIP_RERAISE_SET:\n\u001b[0;32m--> 511\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m    512\u001b[0m \u001b[38;5;66;03m# raise combined interrupts\u001b[39;00m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m interrupts:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/_executor.py:81\u001b[0m, in \u001b[0;36mBackgroundExecutor.done\u001b[0;34m(self, task)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Remove the task from the tasks dict when it's done.\"\"\"\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 81\u001b[0m     \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GraphBubbleUp:\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;66;03m# This exception is an interruption signal, not an error\u001b[39;00m\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;66;03m# so we don't want to re-raise it on exit\u001b[39;00m\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mpop(task)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/concurrent/futures/_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/concurrent/futures/_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/_retry.py:42\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[1;32m     40\u001b[0m     task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     44\u001b[0m     ns: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/_internal/_runnable.py:657\u001b[0m, in \u001b[0;36mRunnableSeq.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    655\u001b[0m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[1;32m    656\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[0;32m--> 657\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    659\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/_internal/_runnable.py:401\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    399\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(ret)\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 401\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[1;32m    403\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "Cell \u001b[0;32mIn[157], line 38\u001b[0m, in \u001b[0;36msupervisor_agent\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m     35\u001b[0m supervisor_chain \u001b[38;5;241m=\u001b[39m prompt \u001b[38;5;241m|\u001b[39m llm\u001b[38;5;241m.\u001b[39mwith_structured_output(RouteResponse)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# 여기서 응답을 직접 호출하고 출력합니다.\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43msupervisor_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# 디버깅을 위해 응답 값 출력\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- LLM 응답 값 ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/runnables/base.py:3082\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3080\u001b[0m                 input_ \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, input_, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   3081\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3082\u001b[0m                 input_ \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3083\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   3084\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/runnables/base.py:5495\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   5488\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m   5489\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m   5490\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5493\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   5494\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[0;32m-> 5495\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5496\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5497\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5498\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5499\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:393\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    389\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    390\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    391\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    392\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatGeneration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m--> 393\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    403\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:1019\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m   1011\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m   1012\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1016\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m   1017\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m   1018\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m-> 1019\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:837\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    834\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[1;32m    835\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    836\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 837\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    838\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    840\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    843\u001b[0m         )\n\u001b[1;32m    844\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    845\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:1085\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1083\u001b[0m     result \u001b[38;5;241m=\u001b[39m generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[1;32m   1084\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1085\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1086\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m   1087\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1088\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1089\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_openai/chat_models/base.py:1183\u001b[0m, in \u001b[0;36mBaseChatOpenAI._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1181\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m raw_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(raw_response, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp_response\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1182\u001b[0m         e\u001b[38;5;241m.\u001b[39mresponse \u001b[38;5;241m=\u001b[39m raw_response\u001b[38;5;241m.\u001b[39mhttp_response  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m-> 1183\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m   1184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1185\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minclude_response_headers\n\u001b[1;32m   1186\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m raw_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(raw_response, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1188\u001b[0m ):\n\u001b[1;32m   1189\u001b[0m     generation_info \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(raw_response\u001b[38;5;241m.\u001b[39mheaders)}\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_openai/chat_models/base.py:1157\u001b[0m, in \u001b[0;36mBaseChatOpenAI._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1155\u001b[0m         response \u001b[38;5;241m=\u001b[39m raw_response\u001b[38;5;241m.\u001b[39mparse()\n\u001b[1;32m   1156\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m openai\u001b[38;5;241m.\u001b[39mBadRequestError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1157\u001b[0m         \u001b[43m_handle_openai_bad_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1158\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_responses_api(payload):\n\u001b[1;32m   1159\u001b[0m     original_schema_obj \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_openai/chat_models/base.py:426\u001b[0m, in \u001b[0;36m_handle_openai_bad_request\u001b[0;34m(e)\u001b[0m\n\u001b[1;32m    418\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    419\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid schema for OpenAI\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms structured output feature, which is the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    420\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault method for `with_structured_output` as of langchain-openai==0.3. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    423\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://platform.openai.com/docs/guides/structured-outputs#supported-schemas\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[1;32m    424\u001b[0m     )\n\u001b[1;32m    425\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message)\n\u001b[0;32m--> 426\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    428\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_openai/chat_models/base.py:1151\u001b[0m, in \u001b[0;36mBaseChatOpenAI._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1148\u001b[0m payload\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1150\u001b[0m     raw_response \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m-> 1151\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwith_raw_response\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1152\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpayload\u001b[49m\n\u001b[1;32m   1153\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1154\u001b[0m     )\n\u001b[1;32m   1155\u001b[0m     response \u001b[38;5;241m=\u001b[39m raw_response\u001b[38;5;241m.\u001b[39mparse()\n\u001b[1;32m   1156\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m openai\u001b[38;5;241m.\u001b[39mBadRequestError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_legacy_response.py:364\u001b[0m, in \u001b[0;36mto_raw_response_wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    360\u001b[0m extra_headers[RAW_RESPONSE_HEADER] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    362\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextra_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m extra_headers\n\u001b[0;32m--> 364\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cast(LegacyAPIResponse[R], \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py:183\u001b[0m, in \u001b[0;36mCompletions.parse\u001b[0;34m(self, messages, model, audio, response_format, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, safety_identifier, seed, service_tier, stop, store, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mparser\u001b[39m(raw_completion: ChatCompletion) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ParsedChatCompletion[ResponseFormatT]:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _parse_chat_completion(\n\u001b[1;32m    178\u001b[0m         response_format\u001b[38;5;241m=\u001b[39mresponse_format,\n\u001b[1;32m    179\u001b[0m         chat_completion\u001b[38;5;241m=\u001b[39mraw_completion,\n\u001b[1;32m    180\u001b[0m         input_tools\u001b[38;5;241m=\u001b[39mchat_completion_tools,\n\u001b[1;32m    181\u001b[0m     )\n\u001b[0;32m--> 183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maudio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_completion_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodalities\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprompt_cache_key\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreasoning_effort\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m_type_to_response_format\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msafety_identifier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mverbosity\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweb_search_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpost_parser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# we turn the `ChatCompletion` instance into a `ParsedChatCompletion`\u001b[39;49;00m\n\u001b[1;32m    232\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# in the `parser` function above\u001b[39;49;00m\n\u001b[1;32m    233\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mType\u001b[49m\u001b[43m[\u001b[49m\u001b[43mParsedChatCompletion\u001b[49m\u001b[43m[\u001b[49m\u001b[43mResponseFormatT\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_base_client.py:1259\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1245\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1246\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1247\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1254\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1255\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1256\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1257\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1258\u001b[0m     )\n\u001b[0;32m-> 1259\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_base_client.py:1047\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1044\u001b[0m             err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1046\u001b[0m         log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1047\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1051\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould not resolve response (should never happen)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': \"Invalid schema for response_format 'RouteResponse': In context=('properties', 'next'), array schema missing items.\", 'type': 'invalid_request_error', 'param': 'response_format', 'code': None}}"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_teddynote.messages import random_uuid, invoke_graph\n",
    "\n",
    "# config 설정(재귀 최대 횟수, thread_id)\n",
    "config = RunnableConfig(recursion_limit=10, configurable={\"thread_id\": random_uuid()})\n",
    "\n",
    "# 질문 입력\n",
    "inputs = {\n",
    "    \"messages\": [\n",
    "        HumanMessage(\n",
    "            content=\"2010년 ~ 2024년까지의 대한민국의 1인당 GDP 추이를 그래프로 시각화 해주세요.\"\n",
    "        )\n",
    "    ],\n",
    "}\n",
    "\n",
    "# 그래프 실행\n",
    "invoke_graph(graph, inputs, config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "08. 계층적 멀티 에이전트 팀(Hierarchical Multi-Agent Teams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-4o\n"
     ]
    }
   ],
   "source": [
    "from langchain_teddynote.models import get_model_name, LLMs\n",
    "\n",
    "# 최신 모델 이름 가져오기\n",
    "MODEL_NAME = get_model_name(LLMs.GPT4o)\n",
    "print(MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_teddynote.tools.tavily import TavilySearch\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "# 검색 도구 정의(TavilySearch)\n",
    "tavily_tool = TavilySearch(max_results=5)\n",
    "\n",
    "\n",
    "# 웹 페이지에서 세부 정보를 스크래핑하기 위한 도구 정의\n",
    "@tool\n",
    "def scrape_webpages(urls: List[str]) -> str:\n",
    "    \"\"\"Use requests and bs4 to scrape the provided web pages for detailed information.\"\"\"\n",
    "    # 주어진 URL 목록을 사용하여 웹 페이지 로드\n",
    "    loader = WebBaseLoader(\n",
    "        web_path=urls,\n",
    "        header_template={\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.0.0 Safari/537.36\",\n",
    "        },\n",
    "    )\n",
    "    docs = loader.load()\n",
    "\n",
    "    # 로드된 문서의 제목과 내용을 포함한 문자열 생성\n",
    "    return \"\\n\\n\".join(\n",
    "        [\n",
    "            f'<Document name=\"{doc.metadata.get(\"title\", \"\")}\">\\n{doc.page_content}\\n</Document>'\n",
    "            for doc in docs\n",
    "        ]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Dict, Optional, List\n",
    "from typing_extensions import Annotated\n",
    "\n",
    "\n",
    "# 임시 디렉토리 생성 및 작업 디렉토리 설정\n",
    "WORKING_DIRECTORY = Path(\"./tmp\")\n",
    "\n",
    "# tmp 폴더가 없으면 생성\n",
    "WORKING_DIRECTORY.mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "# 아웃라인 생성 및 파일로 저장\n",
    "@tool\n",
    "def create_outline(\n",
    "    points: Annotated[List[str], \"List of main points or sections.\"],\n",
    "    file_name: Annotated[str, \"File path to save the outline.\"],\n",
    ") -> Annotated[str, \"Path of the saved outline file.\"]:\n",
    "    \"\"\"Create and save an outline.\"\"\"\n",
    "    # 주어진 파일 이름으로 아웃라인을 저장\n",
    "    with (WORKING_DIRECTORY / file_name).open(\"w\") as file:\n",
    "        for i, point in enumerate(points):\n",
    "            file.write(f\"{i + 1}. {point}\\n\")\n",
    "    return f\"Outline saved to {file_name}\"\n",
    "\n",
    "\n",
    "# 문서 읽기\n",
    "@tool\n",
    "def read_document(\n",
    "    file_name: Annotated[str, \"File path to read the document.\"],\n",
    "    start: Annotated[Optional[int], \"The start line. Default is 0\"] = None,\n",
    "    end: Annotated[Optional[int], \"The end line. Default is None\"] = None,\n",
    ") -> str:\n",
    "    \"\"\"Read the specified document.\"\"\"\n",
    "    # 주어진 파일 이름으로 문서 읽기\n",
    "    with (WORKING_DIRECTORY / file_name).open(\"r\") as file:\n",
    "        lines = file.readlines()\n",
    "    # 시작 줄이 지정되지 않은 경우 기본값 설정\n",
    "    if start is not None:\n",
    "        start = 0\n",
    "    return \"\\n\".join(lines[start:end])\n",
    "\n",
    "\n",
    "# 문서 쓰기 및 저장\n",
    "@tool\n",
    "def write_document(\n",
    "    content: Annotated[str, \"Text content to be written into the document.\"],\n",
    "    file_name: Annotated[str, \"File path to save the document.\"],\n",
    ") -> Annotated[str, \"Path of the saved document file.\"]:\n",
    "    \"\"\"Create and save a text document.\"\"\"\n",
    "    # 주어진 파일 이름으로 문서 저장\n",
    "    with (WORKING_DIRECTORY / file_name).open(\"w\") as file:\n",
    "        file.write(content)\n",
    "    return f\"Document saved to {file_name}\"\n",
    "\n",
    "\n",
    "# 문서 편집\n",
    "@tool\n",
    "def edit_document(\n",
    "    file_name: Annotated[str, \"File path of the document to be edited.\"],\n",
    "    inserts: Annotated[\n",
    "        Dict[int, str],\n",
    "        \"Dictionary where key is the line number (1-indexed) and value is the text to be inserted at that line.\",\n",
    "    ],\n",
    ") -> Annotated[str, \"File path of the edited document.\"]:\n",
    "    \"\"\"Edit a document by inserting text at specific line numbers.\"\"\"\n",
    "\n",
    "    # 주어진 파일 이름으로 문서 읽기\n",
    "    with (WORKING_DIRECTORY / file_name).open(\"r\") as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # 삽입할 텍스트를 정렬하여 처리\n",
    "    sorted_inserts = sorted(inserts.items())\n",
    "\n",
    "    # 지정된 줄 번호에 텍스트 삽입\n",
    "    for line_number, text in sorted_inserts:\n",
    "        if 1 <= line_number <= len(lines) + 1:\n",
    "            lines.insert(line_number - 1, text + \"\\n\")\n",
    "        else:\n",
    "            return f\"Error: Line number {line_number} is out of range.\"\n",
    "\n",
    "    # 편집된 문서를 파일에 저장\n",
    "    with (WORKING_DIRECTORY / file_name).open(\"w\") as file:\n",
    "        file.writelines(lines)\n",
    "\n",
    "    return f\"Document edited and saved to {file_name}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.tools import PythonREPLTool\n",
    "\n",
    "# PythonREPL 도구\n",
    "python_repl_tool = PythonREPLTool()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import START, END\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "\n",
    "# 에이전트 팩토리 클래스\n",
    "class AgentFactory:\n",
    "    def __init__(self, model_name):\n",
    "        self.llm = ChatOpenAI(model=model_name, temperature=0)\n",
    "\n",
    "    def create_agent_node(self, agent, name: str):\n",
    "        # 노드 생성 함수\n",
    "        def agent_node(state):\n",
    "            result = agent.invoke(state)\n",
    "            return {\n",
    "                \"messages\": [\n",
    "                    HumanMessage(content=result[\"messages\"][-1].content, name=name)\n",
    "                ]\n",
    "            }\n",
    "\n",
    "        return agent_node\n",
    "\n",
    "\n",
    "# LLM 초기화\n",
    "llm = ChatOpenAI(model=MODEL_NAME, temperature=0)\n",
    "\n",
    "# Agent Factory 인스턴스 생성\n",
    "agent_factory = AgentFactory(MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 에이전트 정의\n",
    "search_agent = create_react_agent(llm, tools=[tavily_tool])\n",
    "# 에이전트 노드 생성\n",
    "search_node = agent_factory.create_agent_node(search_agent, name=\"Searcher\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel\n",
    "from typing import Literal\n",
    "\n",
    "\n",
    "def create_team_supervisor(model_name, system_prompt, members) -> str:\n",
    "    # 다음 작업자 선택 옵션 목록 정의\n",
    "    options_for_next = [\"FINISH\"] + members\n",
    "\n",
    "    # 작업자 선택 응답 모델 정의: 다음 작업자를 선택하거나 작업 완료를 나타냄\n",
    "    class RouteResponse(BaseModel):\n",
    "        next: Literal[options_for_next]\n",
    "\n",
    "    # ChatPromptTemplate 생성\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system_prompt),\n",
    "            MessagesPlaceholder(variable_name=\"messages\"),\n",
    "            (\n",
    "                \"system\",\n",
    "                \"Given the conversation above, who should act next? \"\n",
    "                \"Or should we FINISH? Select one of: {options}\",\n",
    "            ),\n",
    "        ]\n",
    "    ).partial(options=str(options_for_next))\n",
    "\n",
    "    # LLM 초기화\n",
    "    llm = ChatOpenAI(model=model_name, temperature=0)\n",
    "\n",
    "    # 프롬프트와 LLM을 결합하여 체인 구성\n",
    "    supervisor_chain = prompt | llm.with_structured_output(RouteResponse)\n",
    "\n",
    "    return supervisor_chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import List, TypedDict\n",
    "from typing_extensions import Annotated\n",
    "\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "\n",
    "# 상태 정의\n",
    "class ResearchState(TypedDict):\n",
    "    messages: Annotated[List[BaseMessage], operator.add]  # 메시지\n",
    "    team_members: List[str]  # 멤버 에이전트 목록\n",
    "    next: str  # Supervisor 에이전트에게 다음 작업자를 선택하도록 지시\n",
    "\n",
    "\n",
    "# LLM 초기화\n",
    "llm = ChatOpenAI(model=MODEL_NAME, temperature=0)\n",
    "\n",
    "# 검색 노드 생성\n",
    "search_agent = create_react_agent(llm, tools=[tavily_tool])\n",
    "search_node = agent_factory.create_agent_node(search_agent, name=\"Searcher\")\n",
    "\n",
    "# 웹 스크래핑 노드 생성\n",
    "web_scraping_agent = create_react_agent(llm, tools=[scrape_webpages])\n",
    "web_scraping_node = agent_factory.create_agent_node(\n",
    "    web_scraping_agent, name=\"WebScraper\"\n",
    ")\n",
    "\n",
    "# Supervisor 에이전트 생성\n",
    "supervisor_agent = create_team_supervisor(\n",
    "    MODEL_NAME,\n",
    "    \"You are a supervisor tasked with managing a conversation between the\"\n",
    "    \" following workers: Search, WebScraper. Given the following user request,\"\n",
    "    \" respond with the worker to act next. Each worker will perform a\"\n",
    "    \" task and respond with their results and status. When finished,\"\n",
    "    \" respond with FINISH.\",\n",
    "    [\"Searcher\", \"WebScraper\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_node(x):\n",
    "    return x[\"next\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_teddynote.graphs import visualize_graph\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# 그래프 생성\n",
    "web_research_graph = StateGraph(ResearchState)\n",
    "\n",
    "# 노드 추가\n",
    "web_research_graph.add_node(\"Searcher\", search_node)\n",
    "web_research_graph.add_node(\"WebScraper\", web_scraping_node)\n",
    "web_research_graph.add_node(\"Supervisor\", supervisor_agent)\n",
    "\n",
    "# 엣지 추가\n",
    "web_research_graph.add_edge(\"Searcher\", \"Supervisor\")\n",
    "web_research_graph.add_edge(\"WebScraper\", \"Supervisor\")\n",
    "\n",
    "# 조건부 엣지 정의: Supervisor 노드의 결정에 따라 다음 노드로 이동\n",
    "web_research_graph.add_conditional_edges(\n",
    "    \"Supervisor\",\n",
    "    get_next_node,\n",
    "    {\"Searcher\": \"Searcher\", \"WebScraper\": \"WebScraper\", \"FINISH\": END},\n",
    ")\n",
    "\n",
    "# 시작 노드 설정\n",
    "web_research_graph.set_entry_point(\"Supervisor\")\n",
    "\n",
    "# 그래프 컴파일\n",
    "web_research_app = web_research_graph.compile(checkpointer=MemorySaver())\n",
    "\n",
    "# 그래프 시각화\n",
    "visualize_graph(web_research_app, xray=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_teddynote.messages import random_uuid, invoke_graph\n",
    "\n",
    "\n",
    "def run_graph(app, message: str, recursive_limit: int = 50):\n",
    "    # config 설정(재귀 최대 횟수, thread_id)\n",
    "    config = RunnableConfig(\n",
    "        recursion_limit=recursive_limit, configurable={\"thread_id\": random_uuid()}\n",
    "    )\n",
    "\n",
    "    # 질문 입력\n",
    "    inputs = {\n",
    "        \"messages\": [HumanMessage(content=message)],\n",
    "    }\n",
    "\n",
    "    # 그래프 실행\n",
    "    invoke_graph(app, inputs, config)\n",
    "\n",
    "    return app.get_state(config).values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_openai/chat_models/base.py:425: UserWarning: Invalid schema for OpenAI's structured output feature, which is the default method for `with_structured_output` as of langchain-openai==0.3. Specify `method=\"function_calling\"` instead or update your schema. See supported schemas: https://platform.openai.com/docs/guides/structured-outputs#supported-schemas\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': \"Invalid schema for response_format 'RouteResponse': In context=('properties', 'next'), array schema missing items.\", 'type': 'invalid_request_error', 'param': 'response_format', 'code': None}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[174], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mrun_graph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweb_research_app\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhttps://finance.naver.com/news 의 주요 뉴스 정리해서 출력해줘. 출처(URL) 도 함께 출력해줘.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[173], line 17\u001b[0m, in \u001b[0;36mrun_graph\u001b[0;34m(app, message, recursive_limit)\u001b[0m\n\u001b[1;32m     12\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: [HumanMessage(content\u001b[38;5;241m=\u001b[39mmessage)],\n\u001b[1;32m     14\u001b[0m }\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# 그래프 실행\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[43minvoke_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m app\u001b[38;5;241m.\u001b[39mget_state(config)\u001b[38;5;241m.\u001b[39mvalues\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_teddynote/messages.py:409\u001b[0m, in \u001b[0;36minvoke_graph\u001b[0;34m(graph, inputs, config, node_names, callback)\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m namespace[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(namespace) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroot graph\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;66;03m# subgraphs=True 를 통해 서브그래프의 출력도 포함\u001b[39;00m\n\u001b[0;32m--> 409\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m namespace, chunk \u001b[38;5;129;01min\u001b[39;00m graph\u001b[38;5;241m.\u001b[39mstream(\n\u001b[1;32m    410\u001b[0m     inputs, config, stream_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupdates\u001b[39m\u001b[38;5;124m\"\u001b[39m, subgraphs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    411\u001b[0m ):\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m node_name, node_chunk \u001b[38;5;129;01min\u001b[39;00m chunk\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    413\u001b[0m         \u001b[38;5;66;03m# node_names가 비어있지 않은 경우에만 필터링\u001b[39;00m\n\u001b[1;32m    414\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(node_names) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m node_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m node_names:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/main.py:2647\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[0m\n\u001b[1;32m   2645\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mmatch_cached_writes():\n\u001b[1;32m   2646\u001b[0m     loop\u001b[38;5;241m.\u001b[39moutput_writes(task\u001b[38;5;241m.\u001b[39mid, task\u001b[38;5;241m.\u001b[39mwrites, cached\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m-> 2647\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mtick(\n\u001b[1;32m   2648\u001b[0m     [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mvalues() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t\u001b[38;5;241m.\u001b[39mwrites],\n\u001b[1;32m   2649\u001b[0m     timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[1;32m   2650\u001b[0m     get_waiter\u001b[38;5;241m=\u001b[39mget_waiter,\n\u001b[1;32m   2651\u001b[0m     schedule_task\u001b[38;5;241m=\u001b[39mloop\u001b[38;5;241m.\u001b[39maccept_push,\n\u001b[1;32m   2652\u001b[0m ):\n\u001b[1;32m   2653\u001b[0m     \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[1;32m   2654\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m _output(\n\u001b[1;32m   2655\u001b[0m         stream_mode, print_mode, subgraphs, stream\u001b[38;5;241m.\u001b[39mget, queue\u001b[38;5;241m.\u001b[39mEmpty\n\u001b[1;32m   2656\u001b[0m     )\n\u001b[1;32m   2657\u001b[0m loop\u001b[38;5;241m.\u001b[39mafter_tick()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/_runner.py:253\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;66;03m# panic on failure or timeout\u001b[39;00m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 253\u001b[0m     \u001b[43m_panic_or_proceed\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfutures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdone\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfutures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpanic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tb \u001b[38;5;241m:=\u001b[39m exc\u001b[38;5;241m.\u001b[39m__traceback__:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/_runner.py:511\u001b[0m, in \u001b[0;36m_panic_or_proceed\u001b[0;34m(futs, timeout_exc_cls, panic)\u001b[0m\n\u001b[1;32m    509\u001b[0m                 interrupts\u001b[38;5;241m.\u001b[39mappend(exc)\n\u001b[1;32m    510\u001b[0m             \u001b[38;5;28;01melif\u001b[39;00m fut \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m SKIP_RERAISE_SET:\n\u001b[0;32m--> 511\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m    512\u001b[0m \u001b[38;5;66;03m# raise combined interrupts\u001b[39;00m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m interrupts:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/_executor.py:81\u001b[0m, in \u001b[0;36mBackgroundExecutor.done\u001b[0;34m(self, task)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Remove the task from the tasks dict when it's done.\"\"\"\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 81\u001b[0m     \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GraphBubbleUp:\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;66;03m# This exception is an interruption signal, not an error\u001b[39;00m\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;66;03m# so we don't want to re-raise it on exit\u001b[39;00m\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mpop(task)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/concurrent/futures/_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/concurrent/futures/_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/_retry.py:42\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[1;32m     40\u001b[0m     task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     44\u001b[0m     ns: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/_internal/_runnable.py:659\u001b[0m, in \u001b[0;36mRunnableSeq.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    657\u001b[0m                 \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    658\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 659\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    660\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m    661\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/runnables/base.py:5495\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   5488\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m   5489\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m   5490\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5493\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   5494\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[0;32m-> 5495\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5496\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5497\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5498\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5499\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:393\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    389\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    390\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    391\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    392\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatGeneration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m--> 393\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    403\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:1019\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m   1011\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m   1012\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1016\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m   1017\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m   1018\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m-> 1019\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:837\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    834\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[1;32m    835\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    836\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 837\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    838\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    840\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    843\u001b[0m         )\n\u001b[1;32m    844\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    845\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:1085\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1083\u001b[0m     result \u001b[38;5;241m=\u001b[39m generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[1;32m   1084\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1085\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1086\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m   1087\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1088\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1089\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_openai/chat_models/base.py:1183\u001b[0m, in \u001b[0;36mBaseChatOpenAI._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1181\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m raw_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(raw_response, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp_response\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1182\u001b[0m         e\u001b[38;5;241m.\u001b[39mresponse \u001b[38;5;241m=\u001b[39m raw_response\u001b[38;5;241m.\u001b[39mhttp_response  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m-> 1183\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m   1184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1185\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minclude_response_headers\n\u001b[1;32m   1186\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m raw_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(raw_response, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1188\u001b[0m ):\n\u001b[1;32m   1189\u001b[0m     generation_info \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(raw_response\u001b[38;5;241m.\u001b[39mheaders)}\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_openai/chat_models/base.py:1157\u001b[0m, in \u001b[0;36mBaseChatOpenAI._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1155\u001b[0m         response \u001b[38;5;241m=\u001b[39m raw_response\u001b[38;5;241m.\u001b[39mparse()\n\u001b[1;32m   1156\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m openai\u001b[38;5;241m.\u001b[39mBadRequestError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1157\u001b[0m         \u001b[43m_handle_openai_bad_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1158\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_responses_api(payload):\n\u001b[1;32m   1159\u001b[0m     original_schema_obj \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_openai/chat_models/base.py:426\u001b[0m, in \u001b[0;36m_handle_openai_bad_request\u001b[0;34m(e)\u001b[0m\n\u001b[1;32m    418\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    419\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid schema for OpenAI\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms structured output feature, which is the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    420\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault method for `with_structured_output` as of langchain-openai==0.3. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    423\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://platform.openai.com/docs/guides/structured-outputs#supported-schemas\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[1;32m    424\u001b[0m     )\n\u001b[1;32m    425\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message)\n\u001b[0;32m--> 426\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    428\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_openai/chat_models/base.py:1151\u001b[0m, in \u001b[0;36mBaseChatOpenAI._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1148\u001b[0m payload\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1150\u001b[0m     raw_response \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m-> 1151\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwith_raw_response\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1152\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpayload\u001b[49m\n\u001b[1;32m   1153\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1154\u001b[0m     )\n\u001b[1;32m   1155\u001b[0m     response \u001b[38;5;241m=\u001b[39m raw_response\u001b[38;5;241m.\u001b[39mparse()\n\u001b[1;32m   1156\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m openai\u001b[38;5;241m.\u001b[39mBadRequestError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_legacy_response.py:364\u001b[0m, in \u001b[0;36mto_raw_response_wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    360\u001b[0m extra_headers[RAW_RESPONSE_HEADER] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    362\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextra_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m extra_headers\n\u001b[0;32m--> 364\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cast(LegacyAPIResponse[R], \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py:183\u001b[0m, in \u001b[0;36mCompletions.parse\u001b[0;34m(self, messages, model, audio, response_format, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, safety_identifier, seed, service_tier, stop, store, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mparser\u001b[39m(raw_completion: ChatCompletion) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ParsedChatCompletion[ResponseFormatT]:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _parse_chat_completion(\n\u001b[1;32m    178\u001b[0m         response_format\u001b[38;5;241m=\u001b[39mresponse_format,\n\u001b[1;32m    179\u001b[0m         chat_completion\u001b[38;5;241m=\u001b[39mraw_completion,\n\u001b[1;32m    180\u001b[0m         input_tools\u001b[38;5;241m=\u001b[39mchat_completion_tools,\n\u001b[1;32m    181\u001b[0m     )\n\u001b[0;32m--> 183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maudio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_completion_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodalities\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprompt_cache_key\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreasoning_effort\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m_type_to_response_format\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msafety_identifier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mverbosity\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweb_search_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpost_parser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# we turn the `ChatCompletion` instance into a `ParsedChatCompletion`\u001b[39;49;00m\n\u001b[1;32m    232\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# in the `parser` function above\u001b[39;49;00m\n\u001b[1;32m    233\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mType\u001b[49m\u001b[43m[\u001b[49m\u001b[43mParsedChatCompletion\u001b[49m\u001b[43m[\u001b[49m\u001b[43mResponseFormatT\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_base_client.py:1259\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1245\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1246\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1247\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1254\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1255\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1256\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1257\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1258\u001b[0m     )\n\u001b[0;32m-> 1259\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_base_client.py:1047\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1044\u001b[0m             err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1046\u001b[0m         log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1047\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1051\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould not resolve response (should never happen)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': \"Invalid schema for response_format 'RouteResponse': In context=('properties', 'next'), array schema missing items.\", 'type': 'invalid_request_error', 'param': 'response_format', 'code': None}}"
     ]
    }
   ],
   "source": [
    "output = run_graph(\n",
    "    web_research_app,\n",
    "    \"https://finance.naver.com/news 의 주요 뉴스 정리해서 출력해줘. 출처(URL) 도 함께 출력해줘.\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최종 결과 출력\n",
    "print(output[\"messages\"][-1].content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "09. SQL 데이터베이스와 상호작용하는 에이전트\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사용하는 모델명: gpt-4o\n"
     ]
    }
   ],
   "source": [
    "from langchain_teddynote.models import get_model_name, LLMs\n",
    "\n",
    "MODEL_NAME = get_model_name(LLMs.GPT4o)\n",
    "print(f\"사용하는 모델명: {MODEL_NAME}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded and saved as Chinook.db\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://storage.googleapis.com/benchmarks-artifacts/chinook/Chinook.db\"\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    with open(\"Chinook.db\", \"wb\") as file:\n",
    "        file.write(response.content)\n",
    "    print(\"File downloaded and saved as Chinook.db\")\n",
    "else:\n",
    "    print(f\"Failed to download the file. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sqlite\n",
      "['Album', 'Artist', 'Customer', 'Employee', 'Genre', 'Invoice', 'InvoiceLine', 'MediaType', 'Playlist', 'PlaylistTrack', 'Track']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"[(1, 'AC/DC'), (2, 'Accept'), (3, 'Aerosmith'), (4, 'Alanis Morissette'), (5, 'Alice In Chains')]\""
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.utilities import SQLDatabase\n",
    "\n",
    "# SQLite 데이터베이스 파일에서 SQLDatabase 인스턴스 생성\n",
    "db = SQLDatabase.from_uri(\"sqlite:///Chinook.db\")\n",
    "\n",
    "# DB dialect 출력(sqlite)\n",
    "print(db.dialect)\n",
    "\n",
    "# 데이터베이스에서 사용 가능한 테이블 이름 목록 출력\n",
    "print(db.get_usable_table_names())\n",
    "\n",
    "# SQL 쿼리 실행\n",
    "db.run(\"SELECT * FROM Artist LIMIT 5;\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "from langchain_core.messages import ToolMessage\n",
    "from langchain_core.runnables import RunnableLambda, RunnableWithFallbacks\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "\n",
    "# 오류 처리 함수\n",
    "def handle_tool_error(state) -> dict:\n",
    "    # 오류 정보 조회\n",
    "    error = state.get(\"error\")\n",
    "    # 도구 정보 조회\n",
    "    tool_calls = state[\"messages\"][-1].tool_calls\n",
    "    # ToolMessage 로 래핑 후 반환\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            ToolMessage(\n",
    "                content=f\"Here is the error: {repr(error)}\\n\\nPlease fix your mistakes.\",\n",
    "                tool_call_id=tc[\"id\"],\n",
    "            )\n",
    "            for tc in tool_calls\n",
    "        ]\n",
    "    }\n",
    "\n",
    "\n",
    "# 오류를 처리하고 에이전트에 오류를 전달하기 위한 ToolNode 생성\n",
    "def create_tool_node_with_fallback(tools: list) -> RunnableWithFallbacks[Any, dict]:\n",
    "    \"\"\"\n",
    "    Create a ToolNode with a fallback to handle errors and surface them to the agent.\n",
    "    \"\"\"\n",
    "    # 오류 발생 시 대체 동작을 정의하여 ToolNode에 추가\n",
    "    return ToolNode(tools).with_fallbacks(\n",
    "        [RunnableLambda(handle_tool_error)], exception_key=\"error\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[QuerySQLDatabaseTool(description=\"Input to this tool is a detailed and correct SQL query, output is a result from the database. If the query is not correct, an error message will be returned. If an error is returned, rewrite the query, check the query, and try again. If you encounter an issue with Unknown column 'xxxx' in 'field list', use sql_db_schema to query the correct table fields.\", db=<langchain_community.utilities.sql_database.SQLDatabase object at 0x1391cb3a0>),\n",
       " InfoSQLDatabaseTool(description='Input to this tool is a comma-separated list of tables, output is the schema and sample rows for those tables. Be sure that the tables actually exist by calling sql_db_list_tables first! Example Input: table1, table2, table3', db=<langchain_community.utilities.sql_database.SQLDatabase object at 0x1391cb3a0>),\n",
       " ListSQLDatabaseTool(db=<langchain_community.utilities.sql_database.SQLDatabase object at 0x1391cb3a0>),\n",
       " QuerySQLCheckerTool(description='Use this tool to double check if your query is correct before executing it. Always use this tool before executing a query with sql_db_query!', db=<langchain_community.utilities.sql_database.SQLDatabase object at 0x1391cb3a0>, llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x12ed52770>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x12ed51330>, root_client=<openai.OpenAI object at 0x12ed52080>, root_async_client=<openai.AsyncOpenAI object at 0x12ed53c10>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')), llm_chain=LLMChain(verbose=False, prompt=PromptTemplate(input_variables=['dialect', 'query'], input_types={}, partial_variables={}, template='\\n{query}\\nDouble check the {dialect} query above for common mistakes, including:\\n- Using NOT IN with NULL values\\n- Using UNION when UNION ALL should have been used\\n- Using BETWEEN for exclusive ranges\\n- Data type mismatch in predicates\\n- Properly quoting identifiers\\n- Using the correct number of arguments for functions\\n- Casting to the correct data type\\n- Using the proper columns for joins\\n\\nIf there are any of the above mistakes, rewrite the query. If there are no mistakes, just reproduce the original query.\\n\\nOutput the final SQL query only.\\n\\nSQL Query: '), llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x12ed52770>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x12ed51330>, root_client=<openai.OpenAI object at 0x12ed52080>, root_async_client=<openai.AsyncOpenAI object at 0x12ed53c10>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')), output_parser=StrOutputParser(), llm_kwargs={}))]"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.agent_toolkits import SQLDatabaseToolkit\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# SQLDatabaseToolkit 생성\n",
    "toolkit = SQLDatabaseToolkit(db=db, llm=ChatOpenAI(model=MODEL_NAME))\n",
    "\n",
    "# SQLDatabaseToolkit에서 사용 가능한 도구 목록\n",
    "tools = toolkit.get_tools()\n",
    "tools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Album, Artist, Customer, Employee, Genre, Invoice, InvoiceLine, MediaType, Playlist, PlaylistTrack, Track\n",
      "\n",
      "CREATE TABLE \"Artist\" (\n",
      "\t\"ArtistId\" INTEGER NOT NULL, \n",
      "\t\"Name\" NVARCHAR(120), \n",
      "\tPRIMARY KEY (\"ArtistId\")\n",
      ")\n",
      "\n",
      "/*\n",
      "3 rows from Artist table:\n",
      "ArtistId\tName\n",
      "1\tAC/DC\n",
      "2\tAccept\n",
      "3\tAerosmith\n",
      "*/\n"
     ]
    }
   ],
   "source": [
    "# 데이터베이스에서 사용 가능한 테이블을 나열하는 도구 선택\n",
    "list_tables_tool = next(tool for tool in tools if tool.name == \"sql_db_list_tables\")\n",
    "\n",
    "# 특정 테이블의 DDL을 가져오는 도구 선택\n",
    "get_schema_tool = next(tool for tool in tools if tool.name == \"sql_db_schema\")\n",
    "\n",
    "# 데이터베이스의 모든 테이블 목록 출력\n",
    "print(list_tables_tool.invoke(\"\"))\n",
    "\n",
    "# Artist 테이블의 DDL 정보 출력\n",
    "print(get_schema_tool.invoke(\"Artist\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "# Query 실행 도구\n",
    "@tool\n",
    "def db_query_tool(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Run SQL queries against a database and return results\n",
    "    Returns an error message if the query is incorrect\n",
    "    If an error is returned, rewrite the query, check, and retry\n",
    "    \"\"\"\n",
    "    # 쿼리 실행\n",
    "    result = db.run_no_throw(query)\n",
    "\n",
    "    # 오류: 결과가 없으면 오류 메시지 반환\n",
    "    if not result:\n",
    "        return \"Error: Query failed. Please rewrite your query and try again.\"\n",
    "    # 정상: 쿼리 실행 결과 반환\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 'AC/DC'), (2, 'Accept'), (3, 'Aerosmith'), (4, 'Alanis Morissette'), (5, 'Alice In Chains'), (6, 'Antônio Carlos Jobim'), (7, 'Apocalyptica'), (8, 'Audioslave'), (9, 'BackBeat'), (10, 'Billy Cobham')]\n"
     ]
    }
   ],
   "source": [
    "# Artist 테이블에서 상위 10개 행 선택 및 실행 결과 출력\n",
    "print(db_query_tool.invoke(\"SELECT * FROM Artist LIMIT 10;\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: (sqlite3.OperationalError) near \"10\": syntax error\n",
      "[SQL: SELECT * FROM Artist LIMITS 10;]\n",
      "(Background on this error at: https://sqlalche.me/e/20/e3q8)\n"
     ]
    }
   ],
   "source": [
    "# Artist 테이블에서 상위 10개 행 선택 및 실행 결과 출력\n",
    "print(db_query_tool.invoke(\"SELECT * FROM Artist LIMITS 10;\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# SQL 쿼리의 일반적인 실수를 점검하기 위한 시스템 메시지 정의\n",
    "query_check_system = \"\"\"You are a SQL expert with a strong attention to detail.\n",
    "Double check the SQLite query for common mistakes, including:\n",
    "- Using NOT IN with NULL values\n",
    "- Using UNION when UNION ALL should have been used\n",
    "- Using BETWEEN for exclusive ranges\n",
    "- Data type mismatch in predicates\n",
    "- Properly quoting identifiers\n",
    "- Using the correct number of arguments for functions\n",
    "- Casting to the correct data type\n",
    "- Using the proper columns for joins\n",
    "\n",
    "If there are any of the above mistakes, rewrite the query. If there are no mistakes, just reproduce the original query.\n",
    "\n",
    "You will call the appropriate tool to execute the query after running this check.\"\"\"\n",
    "\n",
    "# 프롬프트 생성\n",
    "query_check_prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", query_check_system), (\"placeholder\", \"{messages}\")]\n",
    ")\n",
    "\n",
    "# Query Checker 체인 생성\n",
    "query_check = query_check_prompt | ChatOpenAI(\n",
    "    model=MODEL_NAME, temperature=0\n",
    ").bind_tools([db_query_tool], tool_choice=\"db_query_tool\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'db_query_tool', 'args': {'query': 'SELECT * FROM Artist LIMIT 10;'}, 'id': 'call_Gde0XQ7hxjfhTVkpNi9VMW1t', 'type': 'tool_call'}\n"
     ]
    }
   ],
   "source": [
    "# 사용자 메시지를 사용하여 쿼리 점검 노드 실행\n",
    "response = query_check.invoke(\n",
    "    {\"messages\": [(\"user\", \"SELECT * FROM Artist LIMITS 10;\")]}\n",
    ")\n",
    "print(response.tool_calls[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Literal\n",
    "\n",
    "from langchain_core.messages import AIMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.graph.message import AnyMessage, add_messages\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "\n",
    "# 에이전트의 상태 정의\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], add_messages]\n",
    "\n",
    "\n",
    "# 새로운 그래프 정의\n",
    "workflow = StateGraph(State)\n",
    "\n",
    "\n",
    "# 첫 번째 도구 호출을 위한 노드 추가\n",
    "def first_tool_call(state: State) -> dict[str, list[AIMessage]]:\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            AIMessage(\n",
    "                content=\"\",\n",
    "                tool_calls=[\n",
    "                    {\n",
    "                        \"name\": \"sql_db_list_tables\",\n",
    "                        \"args\": {},\n",
    "                        \"id\": \"initial_tool_call_abc123\",\n",
    "                    }\n",
    "                ],\n",
    "            )\n",
    "        ]\n",
    "    }\n",
    "\n",
    "\n",
    "# 쿼리의 정확성을 모델로 점검하기 위한 함수 정의\n",
    "def model_check_query(state: State) -> dict[str, list[AIMessage]]:\n",
    "    \"\"\"\n",
    "    Use this tool to check that your query is correct before you run it\n",
    "    \"\"\"\n",
    "    return {\"messages\": [query_check.invoke({\"messages\": [state[\"messages\"][-1]]})]}\n",
    "\n",
    "\n",
    "# 첫 번째 도구 호출 노드 추가\n",
    "workflow.add_node(\"first_tool_call\", first_tool_call)\n",
    "\n",
    "# 첫 번째 두 도구를 위한 노드 추가\n",
    "workflow.add_node(\n",
    "    \"list_tables_tool\", create_tool_node_with_fallback([list_tables_tool])\n",
    ")\n",
    "workflow.add_node(\"get_schema_tool\", create_tool_node_with_fallback([get_schema_tool]))\n",
    "\n",
    "# 질문과 사용 가능한 테이블을 기반으로 관련 테이블을 선택하는 모델 노드 추가\n",
    "model_get_schema = ChatOpenAI(model=MODEL_NAME, temperature=0).bind_tools(\n",
    "    [get_schema_tool]\n",
    ")\n",
    "workflow.add_node(\n",
    "    \"model_get_schema\",\n",
    "    lambda state: {\n",
    "        \"messages\": [model_get_schema.invoke(state[\"messages\"])],\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "# 최종 상태를 나타내는 도구 설명\n",
    "class SubmitFinalAnswer(BaseModel):\n",
    "    \"\"\"쿼리 결과를 기반으로 사용자에게 최종 답변 제출\"\"\"\n",
    "\n",
    "    final_answer: str = Field(..., description=\"The final answer to the user\")\n",
    "\n",
    "\n",
    "# 질문과 스키마를 기반으로 쿼리를 생성하기 위한 모델 노드 추가\n",
    "QUERY_GEN_INSTRUCTION = \"\"\"You are a SQL expert with a strong attention to detail.\n",
    "\n",
    "You can define SQL queries, analyze queries results and interpretate query results to response an answer.\n",
    "\n",
    "Read the messages bellow and identify the user question, table schemas, query statement and query result, or error if they exist.\n",
    "\n",
    "1. If there's not any query result that make sense to answer the question, create a syntactically correct SQLite query to answer the user question. DO NOT make any DML statements (INSERT, UPDATE, DELETE, DROP etc.) to the database.\n",
    "\n",
    "2. If you create a query, response ONLY the query statement. For example, \"SELECT id, name FROM pets;\"\n",
    "\n",
    "3. If a query was already executed, but there was an error. Response with the same error message you found. For example: \"Error: Pets table doesn't exist\"\n",
    "\n",
    "4. If a query was already executed successfully interpretate the response and answer the question following this pattern: Answer: <<question answer>>. For example: \"Answer: There three cats registered as adopted\"\n",
    "\"\"\"\n",
    "\n",
    "query_gen_prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", QUERY_GEN_INSTRUCTION), (\"placeholder\", \"{messages}\")]\n",
    ")\n",
    "query_gen = query_gen_prompt | ChatOpenAI(model=MODEL_NAME, temperature=0).bind_tools(\n",
    "    [SubmitFinalAnswer, model_check_query]\n",
    ")\n",
    "\n",
    "\n",
    "# 조건부 에지 정의\n",
    "def should_continue(state: State) -> Literal[END, \"correct_query\", \"query_gen\"]:\n",
    "    messages = state[\"messages\"]\n",
    "\n",
    "    last_message = messages[-1]\n",
    "    if last_message.content.startswith(\"Answer:\"):\n",
    "        return END\n",
    "    if last_message.content.startswith(\"Error:\"):\n",
    "        return \"query_gen\"\n",
    "    else:\n",
    "        return \"correct_query\"\n",
    "\n",
    "\n",
    "# 쿼리 생성 노드 정의\n",
    "def query_gen_node(state: State):\n",
    "    message = query_gen.invoke(state)\n",
    "\n",
    "    # LLM이 잘못된 도구를 호출할 경우 오류 메시지를 반환\n",
    "    tool_messages = []\n",
    "    message.pretty_print()\n",
    "    if message.tool_calls:\n",
    "        for tc in message.tool_calls:\n",
    "            if tc[\"name\"] != \"SubmitFinalAnswer\":\n",
    "                tool_messages.append(\n",
    "                    ToolMessage(\n",
    "                        content=f\"Error: The wrong tool was called: {tc['name']}. Please fix your mistakes. Remember to only call SubmitFinalAnswer to submit the final answer. Generated queries should be outputted WITHOUT a tool call.\",\n",
    "                        tool_call_id=tc[\"id\"],\n",
    "                    )\n",
    "                )\n",
    "    else:\n",
    "        tool_messages = []\n",
    "    return {\"messages\": [message] + tool_messages}\n",
    "\n",
    "\n",
    "# 쿼리 생성 노드 추가\n",
    "workflow.add_node(\"query_gen\", query_gen_node)\n",
    "\n",
    "# 쿼리를 실행하기 전에 모델로 점검하는 노드 추가\n",
    "workflow.add_node(\"correct_query\", model_check_query)\n",
    "\n",
    "# 쿼리를 실행하기 위한 노드 추가\n",
    "workflow.add_node(\"execute_query\", create_tool_node_with_fallback([db_query_tool]))\n",
    "\n",
    "# 노드 간의 엣지 지정\n",
    "workflow.add_edge(START, \"first_tool_call\")\n",
    "workflow.add_edge(\"first_tool_call\", \"list_tables_tool\")\n",
    "workflow.add_edge(\"list_tables_tool\", \"model_get_schema\")\n",
    "workflow.add_edge(\"model_get_schema\", \"get_schema_tool\")\n",
    "workflow.add_edge(\"get_schema_tool\", \"query_gen\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"query_gen\",\n",
    "    should_continue,\n",
    ")\n",
    "workflow.add_edge(\"correct_query\", \"execute_query\")\n",
    "workflow.add_edge(\"execute_query\", \"query_gen\")\n",
    "\n",
    "# 실행 가능한 워크플로우로 컴파일\n",
    "app = workflow.compile(checkpointer=MemorySaver())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaMAAALoCAIAAABagA2RAAAAAXNSR0IArs4c6QAAIABJREFUeJzs3XdAVWUfB/Df3XAve8mUqSCooIIMB7hQEzc50czR0DR7NSvLtDSzYdprmWWWoRXO3HviRERFRRkyZMje3Msd3HvfP26hbykBgod7/H7+OpzxPL9zsW/POefhHo5WqyUAAFbjMl0AAECrQ9IBAPsh6QCA/ZB0AMB+SDoAYD8kHQCwH2/ZsmVM1wDAKoUK2aWyAmmd6k51WWzpA3OByEwgOl2cW798qjg3tvSBpdDAVCB8muWTRTnnSvOthAYmzV22Fhma8IWninNvVZXaGkgMeDymP7zWwme6AAA2UGo0hwuz5Gp1hK3LjYqSnNoaQx6vVl2n1Kir6pSlSvn/L6uUGnWFSiHm8Z9mWaZRKzXqCpXSkCdv3nKlUmHA5cnUdWUqeYFCxuHQolsXXCWms926GPMFTH+oLYmDmcMAT0OhUfM53BNFOcXK2kALWwuBiOmKntatqlJnsbGtSHysKHuknRvT5bQMJB1A850tfRCTnbLCJ5jpQlrFjrx7pcraxR39ORwO07U8LSQdQDOptdpN9++MtXdnupBWVK5StBOJ78uqOptYMl3LU0HSATRHQkWRhdDQhF03s57kj/yMvpb2HY3MmC6k+TDLBKDJVqddl2vUz0nMEdFoO7e82poMaSXThTQfxnQATVOuUtaqVSIuaydkPAmfyzXhC/X0jh3GdABNUKVSFsilz2HMEVGdRjMj4YSejoyQdABNsDz5iqlQyHQVjJnu4r37wT2mq2gOXL0CNFZiZXFVncrb2ILpQpgk5HKN+fqX9Ug6gMaSq+uk6jqmq2BYlqy6WqUMs3ZgupCmwdUrQKOk1VTszc989v1++tHbW376phkHRr4QlHTrWovX4yI23leQqWnxdlsZkg6gUc6U5BkJnvW0ktpa2ZEDO909OjX1wIS487m59zt4dW6NqkbauRbKZa3RcuvB1StAo5wtyetgZNZKT12vXb10/Mie+EuxJSVF3f1DBg0ZOXR45NlThxa9OV23w6AhI1d88f3dO4n7//g9/vK5wvy8Tj6+01/9T2BIKBFptdp+ge6z579//MiepJvXxox/acdvP+kOfHfJ56PHTW3xgi2EBvo13QRJB9AoUnWdvHVu0ikU8ogBfmEDXhj94hSn9q5xl2LfX/jKb7tPu3fo9P26Vfv3bDtw8joRqdXq8SP7WFhazXnzA1s7h59//Hrfrl/3HkuwtrHNz8sZNSTAzd1z8rTX+w0aJpEYv/rSSBtb++WffdcaBefLpXery1908GiNxlsJvrUJ4N8VKmQ78+5NcOzYGo0XFxVWVVaEDhjq3bkbEQ0cPMLNw9PZxYOI7qUld/T00e3G4/HWfb/NwNDQ3MKKiCZNfe2P7dHpacnWNrapKbeIaOz4aRGjJuh2Trl7q09YeGtUS0QCLu9mZQmSDoBtSpTyEqW8lRp3cGwfENT304/envHKW0G9+tk7tndz99Rtupd2J3zIKN1yba1s766tidev5GRnFRfl61ZaWdsQ0b3UZInEeETkZN3K/Lyc2lpZB89WuUlHROYCUT9rx1ZqvJXgiQTAv3MRm46xb61vauNwOJ+t3RQ+ZORvW76PjAj++otlcnktEclk0ge52R6enXWXrrOmDN+7+7fR417648iVuFsFL896k8vltndxJ6L0tLveXboJBX9Oc0tLTSKiTj6+rVQwj8MJsbRrpcZbCZIO4N9JeDw3sWkrti8xfvPtj3bsv/D2B6v27v5txYdvEVFachIReXbqTETxl8+lpSQtWf51+NBRAoFAd2Hr0cFbKBQRUWrybXcPr/rW7qUlW9vYmZi21lePVNYpv0xt+fkrrQpJB/DvZOq6xXcutUbLGo3m9IkDSqVCN7gbHTll0NCRuTlZRJRxL1koFDm1dyUi3eWqu8efV7WF+XlxF8908PQmIqVKmZOd6dHx4UyUjLS77h08W6NanWJFrd49x0TSAfw7MY+v0Wor65Qt3nJNddXSd9/46rMld+8klpYUHT30x7FDfwQG9SWi4uICHo+XEHe+pLjQ2bUDER099AcRJd+9uWLpWxKJkaW1DRGl3r1FRB6e3vVtlpYUyaQ1CXHndQHa4mxEhjNcfFqj5daDWSYAjVKslCs1aoNWmE+XEHd+1Yp3srPSjYxNO3TsNDhi7IjRk3g8XlZm2rvzZ+RkZ6zbuKO7f3DM1h9++PZLaU2Vf1CfpSu+/u+Xy44f2Rf18hzH9i6ffbwoNuF+/X26U8f3f778XS1p9xy9amgobvGChVye3r1PB0kH0FhVdUqVRu/+DqrlfZISv9InpDVCv/VglglAY81LPLu6S58Gdljx4VsatfpvK+UKuYHI4LH7C0Si9z78okVrfCgrMy36x3WP3VRcUmBtZfvYTR6e3pOmvtZAs9cri+0NJPoVcxjTATRBdHayiMsbaOPEdCFMUmjU1iJDAUfPbvEj6QCaQKFR19SpmK6CMSqths/hWgkfP0Rty/QsmAGYVaZU5MulTFfBmNVp10i//rL/L0g6gCawMxCfKcm7VVXCdCEMuFpeNNPFx0qgfwM6XL0CNEd8eaGD2Fikb/eqnkamrKq7qTWXo58jOozpAJohwLydSqNJl1YxXcgzcrjwvlyj1t+YQ9IBNJOzodHJ4ux0GfvDjsfhcInT20LP/qT/b3D1CtB8NyqLO0jMkqrL3CWt+Pf/TDlfmm/E4w9gxawajOkAms/P1FrCF8SW5H2WmsDR06eS/1CuUvA4nEul+ZUqeX9WxBzGdAAtI7mm3MvIvFyl/CT5iovEZKJjx1qN+nZFqYDL8TOzrlXXXa8sEXN5zV424vG7mlpJ61SJVaVPv1xTp7r5yPKd6jIRj+dnYp0rr9mWm9pOJJ7n7qvUaIRc9oyE2HMmAAzyMjInInOB8BXXzh2NzCyFBoZcXnpt5T1ZpYlAyOVyEiuLn365uLho8+ljT9NOZm2ViUDI4dCjy5myqgqlwkIosjMQz3H3nefuq3uDNdMfakvCmA5Ab2RlZS1cuHDnzp1MF6J/WBXbAACPhaQDAPZD0gEA+yHpAID9kHQAwH5IOgBgPyQdALAfkg4A2A9JBwDsh6QDAPZD0gEA+yHpAID9kHQAwH5IOgBgPyQdALAfkg4A2A9JBwDsh6QDAPZD0gEA+yHpAID9kHQAwH5IOgBgPyQdALAfkg5Ab3A4HAsLC6ar0EtIOgC9odVqy8rKmK5CLyHpAID9kHQAwH5IOgBgPyQdALAfkg4A2A9JBwDsh6QDAPZD0gEA+yHpAID9kHQAwH5IOgBgPyQdALAfkg4A2A9JBwDsh6QDAPbjaLVapmsAgIZMmjSpqqqKz+crlcrS0tJ27dpxuVy5XH7kyBGmS9MbGNMBtHXjxo0rKyvLzc0tKipSq9UPHjzIzc01MjJiui59gqQDaOtGjRrl7Oz8t5V9+/ZlqBy9hKQD0AOTJk0SCoX1P7q4uIwdO5bRivQMkg5ADwwfPrx9+/a6ZQ6HExoa6uDgwHRR+gRJB6AfoqKiRCIRETk7O2NA11RIOgD9EBER4eTkxOFwwsLC7O3tmS5Hz/CZLgDgmVJqNZnSyny5TKlRM11Lk3V7eWLZyZM24X2PFWUzXUuTSXiC9mJjJ0NmHhljPh08R44VZR8qyJKp69wlZtV1SqbLeb6Iefzk6rJ2BpK3PPzsDSTPuHckHTwvDhfeP1qYPcGxA9OFPNfKVYqdefeWdgps/2wHd7hPB8+FuLLCgwVZiDnGmQtE0529X7126hn3i6SD58KOB2lDbV2YrgKIiHgczpB27X/NSXmWnSLpgP00RLcrSy0FIqYLgT+ZCUVJVWXPskckHbBfsaLWSWzMdBXwkJnAoFZd9yx7RNLB80BbgyetbYlGq5GqVc+yRyQdALAfkg4A2A9JBwDsh6QDAPZD0gEA+yHpAID9kHQAwH5IOgBgPyQdALAfkg4A2A9JBwDsh6QDeLyTf2xbNOGFqGCvbd99VZCdFRXsFRXsVVNVyXRdjbJ+2dtRwV5b1qxsjcZP7d0WFez1/tTRuh/njQiLCva6evZYa/TVUpB0AI+Rm3nv58+XPrif0W/kOHfvLgIDkVe3AK9uATxec169suDF8DXvzGnMnj988v6sgQHN6AIahjfmADxGVVkpERmKjWa8+7FuzQfrtzSvqfQ7twpzsx3d/v3rjutUqqtnTzSvF2gYkg7g7xJiT+qGYLWymqhgr74RY0dMmbVw/BAi2nA0zsjE9L+L37xy+uikee+U5D84s3/HojUb3Tp1ObZ9S9ypI3lZ6Zbt7Dr7h/R+YaS7d9forz45tmOLrs2oYK+Fq7/3Cwl9bKfxZ459/d483XJUsNewqJkT5yxUKuQx365OvBxbVpBvZGru6O4x9a0P7JxddbuVFj6I/mplVnJSVXmpuU07L7+AqLcWiyVNez9DeXHRlrUrM5Nvl5cUOzi79RwwOCJqFo/HI6LES+cO//5zRnISj8/z8vUfM/MNJ/eOT/3pMgNXrwB/Z9vepffQkUQkEAhHTZ/dvXfY33bgCQREdHL376f2xLh4+oglxlvWrIxZv1oplw8YPcHVq/PxXb+uXvCqXCbrGtTbu0cgEdk5u46aPtvW0flJndq7uPd5YXR9p50DgolozTtzju3YIqupDhvxorG5+a24C0tnTagsLSGimsqKpTMnJMSeMJRI+o0ar1TIYw/uXr3g1aae7MaVi6+cOsIXCELCI/JzMndsWLv9u6+IKCc99csFryRdvdR32CgPH9/4M8c+nz9LpVQ06xNlHsZ0AH/n4OLe94XR5w/v5QtFkbPmEVFBdtY/dystKvx0yz7dCCs1MYGIXlmyyt27CxH5BffVaDR1dSq/kNDM5KQ7CXH2zm66phroNHT4mHOH/qjv9Hb8xVtxF7hc7rKN29o5OKnV6sVTR+VlpB3Z/sv41xccidlcUVJk5+y6/JfdAoFwWNT0t8YMTElMSLx0zje4TyPPNPlG/M3L54UGBss2xkiMTf1DB3719utHt0dHvvrm3evxnn7+rl4+k+e9q6iVvTY4qLykMDXxmk9AcHM/VyYh6QCaqXNAcP2FpG17l7ys9A0fL+reu7+1nWPIkOFNvYr8p5uXzxORW6cu7RyciIjH4/n36Z+XkXY7/uL41xckxp0noh59BwoEQiKytLHz6OyXmpiQFH+x8Ul3O/4SEbl7d5UYmxJR9979tl5K1m0Kj5wcHjlZtywyFJuYW5YW5cuk1U95UkxB0gE0k5Wtff3ypHnv1lRVpty4evD+JiLatn51l8Bec5Z/pbvh1Ty6GS2mllb1a0wsLImoprKSiKS6reaWD7eaWRBRdWVF47soLy4iIrHRY16ykX7nVvTq5el3bja7/jYFSQfQTFzuwxRr5+C05LutGXduZ6cnXz9/OiH25JXTR0POn/YPHdjs9k3MLXT34+rX6FJMl24mZhZFeTnSqodba6oq/paM/0piZExE0uqqf25av3RBYW62t39QRNRMPo//9ftvSvVkLuFj4YkEwNNS1MoOx2ze9NmHbt6dw4ZHvvXZt/1GjSOi4ge5RMThcIhIKZf/azu6PVUqpVarJSK/kL5ElHb7hq6dOpXq6uljROTXK4yIfENCiSj+zPE6lYqIivPzUm9eI6JuvR7/bPexXDy9iSjt1nVdnmamJEUFe70c5ldTXVWYm01EEVEzuwb2NrG00sWcuk79dB8VYzCmA3hafKHozP5deRlpVWVlTh4dayrLLx8/TERe3QOIyMzKmoiSr8dv/+6rrsF9vPyeODHY3LodEdUpFRtXfuDVLaDvC6N69B2QEHty2SsTAvsNSboWl5d5z9LGLnzcFCIaPH7qmX078rLSl0yP9PL1jzt9RKNWd+/T39PXv/GVB/QLd43ZnJmc9OGMF338gy8e209EkTPfMDI2cXBxz8tK371xXerNawmxJzsHhNyOv3j2wE5TC8tGNNzmYEwH8LR4PN5/Vn3To+/Aa+dO7vlp/Zl9O7v0DFmyYaurpw8RBQ14wdG9o0ql3Bf9Q2VZaQPt2Ng76SaaxB7YlXnnFhG9sWLNkAkvyWXSYzu35t/P6NF3wLIft+medYglRks3xnTv0z8vI+34rl9VcsXQidPmfrK2SZULhKL3vvml99ARpYX5p/duF0tMJs5ZOCxqJhFNnLvIw8c3N/Ne6s3rk+YumvKf9929u96Ku1Ccn/vUHxgDOLpxMgCLFSpk82/Gvunux3Qh8Kd8ufRw4f0fuvV/Zj3i6hXg2Tm2c+u924mP3dStV1jwoGEt3uPt+IuxB/947KZ2ju3Hzpzb4j22TUg6gGcnPDIqPDLqWfbYOSCkc0DIs+yxbcJ9OgBgPyQdALAfkg4A2A9JBwDsh6QDAPZD0gEA+yHpAID9kHQAwH5IOgBgPyQdALAfkg4A2A9JB+wn5PKsBIZMVwEPqbVkbyB5lj0i6YD9zAWiImVtdZ2S6ULgTw/kNeZCg2fZI5IOnguDbNrfk+rxaxBY5oFcGmbl+Cx7RNLBc2Gmi3dKdcXd6nKmCwE6UJDpZ2rla/pMv6Ud3zkMzwsNaRfcPO8sNuZzubYisQb/8p8tDWnz5dISpcLH2GKSU8dn3DuSDp4vx4qyk6rKZOq6IoWM6VqaTKVSFRYWOjo+0+u+lmIrkliJDIIsbLuYMPDOHSQdgN7IyspauHDhzp07mS5E/+A+HQCwH5IOANgPSQcA7IekAwD2Q9IBAPsh6QCA/ZB0AMB+SDoAYD8kHQCwH5IOANgPSQcA7IekAwD2Q9IBAPsh6QCA/ZB0AMB+SDoAYD8kHQCwH5IOANgPSQcA7IekAwD2Q9IBAPsh6QCA/ZB0AMB+SDoAvcHhcBwcHJiuQi8h6QD0hlarzcvLY7oKvYSkAwD2Q9IBAPsh6QCA/ZB0AMB+SDoAYD8kHQCwH5IOANgPSQcA7IekAwD2Q9IBAPsh6QCA/ZB0AMB+SDoAYD8kHQCwH5IOANiPo9Vqma4BABoSFRVVUVFBRCqVqry83MbGRrd89OhRpkvTGxjTAbR1ERERZWVlBQUFpaWlGo2moKCgoKCAy8V/vE2ADwugrYuMjHR2dn50jVarDQwMZK4i/YOkA2jr+Hz+mDFjRCJR/RpbW9spU6YwWpSeQdIB6IHRo0c7OjrW/xgQEODu7s5oRXoGSQegB/h8/rhx4/h8PhHZ2NhgQNdUSDoA/TBq1Ch7e3si6tmzJwZ0TcVnugCAVqQhbYFcViiXEnGYrqUFhEweV3XkSM/xY65XFDNdSwvgcbkuYmMTvvAZ9IX5dMBaBwoyDxTcr65TOhhKZKo6psuBv7MSGdysLOlkbDHDxdtNYtqqfSHpgJ1i8tJuVBS/0M5ZyOUxXQs0pEKl2JqTstw7yFVs0nq9IOmAhXbnpSdUFA23c2W6EGisNfdurPMNtREZtlL7eCIBbCPXaE6W5CDm9Msoe/fN9++2XvtIOmCbbFmVQq1mugpoGkuh6FpFUeu1j6QDtilUyBwMjZmuAprGhC80Fgjlmtb6XxSSDthGrdXWqlVMVwFN9qBWym21yUBIOgBgPyQdALAfkg4A2A9JBwDsh6QDAPZD0gEA+yHpAID9kHQAwH5IOgBgPyQdALAfkg4A2A9JB0Drl70dFey1Zc1KIjq1d1tUsNfiqaMYqeTbpQujgr2iv/rksVsfrbMtmzciLCrY6+rZY0wX8hCSDuD/mFlYe3ULcPH0+dc9SwryooK9Dv++uTHNLngxfM07c1qiwFakF0U2D96YA/B/uvfp371P/8bsGXficCPbTL9zqzA329Gtw9OV1rr0oshmQ9IB/J9Te7f9tGpp+w5eK6P3EFFpUf6B6B9vX71YUpDv6ObRtWevQZFRZlbWi6eOyk5LJqJf/7vq1/+u+unMDaHI4LENRn/1ybEdW4goIfZkVLDXwtXf+4WEJl46d/j3nzOSk3h8npev/5iZbzi5d6w/hMvjnj+89+Sebdlpdzt07vbah5+ZWVn/s+XMu7d3/bguMyWJx+V379tv7Mx5xmbmuk2Xjh88vW9HVspdoUjo3SOoe+9+QQNfaOCsH1tkaeGD6K9WZiUnVZWXmtu08/ILiHprsVhipDskNTHh9/WrC7IzlAqFta1DryEjh0+d9XSffSvC1SvAE2m12lXzph/f9au5lc2Q8VMFQtHeX77f+On7RNR/5Hhre0ci6hLYa9T02VzeEwcNXYN6e/cIJCI7Z9dR02fbOjrnpKd+ueCVpKuX+g4b5eHjG3/m2OfzZ6mUivpD7l6L3/XjOif3Dlwu/3b8RV2Pf5NzL2XFnCk3Lp7t0XeAR2ffk7tjls0aXyuVEtGFI/u+/XBBTnpq0MAhwYOGXTl97Jsl/7mXlNjAmf6zyJrKiqUzJyTEnjCUSPqNGq9UyGMP7l694FXd/sk34lfMnpJ285pLR5+ggcMKc7O3fbd65/drn+7zbkUY0wE8UVV5Wf79TAOx+L11mzkcjkajObot2qKdLRENHDvxypljxQ9yuwb2GTpxWgON+IWEZiYn3UmIs3d2i5w1j4iO7fzV08/f1ctn8rx3FbWy1wYHlZcUpiZe8wkI1h1SUpC3evtRI1Ozbr36rV74WuLF2IqS4r8N6w5vi1bU1gaEhU9f9BERbVmz8uj26POH9w6KnJR66xoRRUyaPixqJhF17zMgPzvD2MSsSUXu/H5tRUmRnbPr8l92CwTCYVHT3xozMCUxIfHSOd/gPjs2rNVoNH2HjXnlg5VE1Dkg+NsPFxz47edhUTMMJW3xC5+RdABPJDExNTI1q6msWDF7Sic/f0cPr/BxU3i8p32tYnjk5PDIybplkaHYxNyytChfJq2u36Fnv8FGpmZE1KVnL92ajOTb3Xv3e7SR+6l3icjdu4vuR91CSmL8oMhJto4uRHTw100FedlWtg4h4RGdugU0tcjEuPNE1KPvQIFASESWNnYenf1SExOS4i927No9JTGBiAL7D9Ht7B82iIjqlIq71+IbeZfzGUPSATwRn8+f8/HqTas+TLlxNeXGVSKybGcXNjxy9IynekCZfudW9Orl6XduPmkHw7/uhfEFAkOxUa2sRi6t+ds+clkNEcWsXx2zfnX9ysK8HCIaMHp89r2U84f3nN6zXTc68/Tzn/X+ynYOTo0vUlpVSUSm5pb1a0zMLIiourJCVlP55xrLP7cKBEKJiam0qrKmqqLxXTxLSDqAhnTp2evL7UdTExMykm/HnTiUmZy0Z/N3A8ZO1P1n3zzrly4ozM329g+KiJrJ5/G/fv9NXazUq/0r11RKRa2shogk/7j2FBubENHAsZN6hoXXrxSJxUQkNDB8dcmno6bPzk67e+vy+XNH9iVfjz++c2vUm+81vkgTM4uivBzpI8mlSzFTSysjEzMOh6PVauvLVqmUcpmUiEzNrZr1kbQ6PJEAeKLiB7m7flx3fMdW7x6BEZNnLP95l52zq7qurrQwn4g4xCEipVL+r+1wOBwiUsrlRKTRaApzs4koImpm18DeJpZWurxQ1z18LdbNuPPS6koiunHxLBFxuVzXTn+f3+fh7UtEFSXF3v5B3v5BfKEwP+c+XyAgossnDm34+B0DA8OAsPDp7348a/EK3bk0vkgi8g0JJaL4M8frVCoiKs7PS715jYi69QoVGYo79Qgiost/zbO5cvKIuq5ObGTi6eff9I/5WcCYDqAh+7f8yOVycjPvmVvbFOZm59/PNDEzd3LrSETm1tZEdHb/LrlMNvjFKY+dCKKj25R8PX77d191De7j4OKel5W+e+O61JvXEmJPdg4IuR1/8eyBnaYWllqtlojkMumSaZFdgnpdOLKPiPzDwv85hAx/MerknpirZ4+veWeOvYv7qT3bpFWV81etc+7Q6c61uPOH9+bcS+3WK1StUevyyMvvX27V/a3IweOnntm3Iy8rfcn0SC9f/7jTRzRqdfc+/T19/Ylo3Gvzl78Wf2bfjvKSQhNTi0snDhLR6OmzDcTiFvrgWxjGdABPZG3vuPDL7xxc3c/u37nnp/WJl84NHDtx2abtuqHT4PFTxUYmRXk5B3/dpNFqGmgnaMALju4dVSrlvugfKstKJ85d5OHjm5t5L/Xm9UlzF035z/vu3l1vxV0ozs/V1NUR0aAxEz19e5zcHSOXyboE9pr+zkf/bNPO2XXBFxs6dOmWEHtyf/QPdk4ucz9Z6x86iIgmvfFO+ItTCvPu79m8YX/0RnPrdq8u+WzIhJcaPtm/FSmWGC3dGNO9T/+8jLTju35VyRVDJ06b+8mf80g8fHyXbPjVxdMn8WLsucN7TM2tZr3/ScPPoJnF0f0/BIA1zpTkHSrIGmPvznQh0DTLk+P/CBom5LbK8AtXrwAtYPOXH8lq/v54VCdi8oz2HbyeeUWPoRdFthIkHUALmLZwKdMl/Du9KLKV4D4dALAfkg4A2A9JBwDsh6QDAPZD0gEA+yHpAID9kHQAwH5IOgBgPyQdALAfkg4A2A9JBwDsh6QDtjHg8o34QqargCZzlZjwuJxWahxJB2zjLDZOri5jugpomkKFTK6u4xGSDqBx7AzEDoZGVXUqpguBJsiTS3tb2rde+0g6YKG57l1/y0lhugporDRpxfWK4mnOnVqvC3znMLBTbq301esnR9i5mwuE5kIDLeHfeZvDIU6BXFqpUt6oLP6h+4BWHXYh6YC1VFrNz1l3bleXKtTqCpWS6XJagEajkcvl4rb6VpqmchYbc4gTaNHuGXwVPpIOQG9kZWUtXLhw586dTBeif3CfDgDYD0kHAOyHpAMA9kPSAQD7IekAgP2QdADAfkg6AGA/JB0AsB+SDgDYD0kHAOyHpAMA9kPSAQD7IekAgP2QdADAfkg6AGA/JB0AsB+SDgDYD0kHAOyHpAMA9kPSAQD7IekAgP2QdADAfkg6AGA/JB2A3uBwOG5ubkxXoZeQdAB6Q6vVZmRkMF2FXkLSAQD7IekAgP2QdADAfkjUR2IGAAAgAElEQVQ6AGA/JB0AsB+SDgDYD0kHAOyHpAMA9kPSAQD7IekAgP2QdADAfkg6AGA/JB0AsB+SDgDYD0kHAOzH0Wq1TNcAAA15+eWXCwoKOByOSqWqqKiwsrLicDhKpfLEiRNMl6Y3MKYDaOvCwsLKysqKiorKy8u1Wm1xcXFRUZFIJGK6Ln2CpANo68aMGePk5PToGo1G061bN+Yq0j9IOoC2ztjYOCIiQiAQ1K+xs7ObPHkyo0XpGSQdgB4YO3aso6Nj/Y++vr6dOnVitCI9g6QD0APGxsbDhg3j8/m6Ad2kSZOYrkjPIOkA9MPYsWN1d+u6du3auXNnpsvRM3ymCwBomjqttlAh4zBdBgME3F7DXyjbvz98wosP5FKmq2ECh+xFkmYeivl0oC8ulxVsy0tLrip3lhhVqeqYLgeeNQdD8d2q8iBL2zfd/Yz5gkYc8RCSDvTDqZK8XXlpEbau5gLMI3t+qbSaQoVsa3bKph4DLAUGjT8QSQd64GRRzp6CzMmOHZkuBNqK5clX/giKEHIb+6QBTySgrdMQ7c1HzMH/mejkuT7jZuP3R9JBW5chraypUzJdBbQt1kLDK+WFjd8fSQdtXV6t1FViynQV0LaYCoTWQkOZprEPppB00NbVadVStYrpKqDNyZRVcRr9lAFJBwDsh6QDAPZD0gEA+yHpAID9kHQAwH5IOgBgPyQdALAfkg4A2A9JBwDsh6QDAPZD0gEA+yHpABorIfZkVLDX3OGhDe9WmHM/KtgrKthLWl35rEprSQXZWbr6a6r0sv7HQtIB6J/YQ3uigr0yU5KYLkRvIOkA9M+Vk4eZLkHP4N1gwEL/XfzmldNHp7y1mMcTHN0RzeVwQ0dEDp0w7fdvvjj020+WNnZDJrw0dOI03c6piQm/r19dkJ2hVCisbR16DRk5fOos3SatVvv7us8vHT9UXlLYobNfyOARj/aiVqv3bv7u6tkThbn33X38+o8cFzRwaJPqrJVWb/j4vTsJcTYOTgPHTFAq5FvWrAwaOPSN5Wue1H5NVeVrgwN1hy+ZNtbdu+tHm7Y30EVpUf6B6B9vX71YUpDv6ObRtWevQZFRZlbWRKTRaH79etXtKxeK8vPMraw79wwZM/0N3SYdaWXFb+s+v37+pNjIZMj4aYMi/3zJbObd27t+XJeZksTj8rv37Td25jxjM3MiOrZza/TqFf6hg8IjJ+/Z/F1u5j3/0IGT575z7fzpTauWcIjrHzpw1gcruVwuEeVlpf/x0/q0m9dqqipcvToPGf+Sf+jAJn16TcJbtmxZ67UO8PQyZVUZ0iovY/PGH3I19mRuempNZUVuZpqJucW9pMRbcReyUu4UP8i1cXDKvpd8K+58r8HDjUzNkm/EfzJnamnBA8+uPTp07Z587crNuPMadZ23fxARHd+5defG/8pl0qCBQ5UKxaVjB1VKhaHY6IVJLxPRDyveO7p9i8TYpPfQUck34s/u32lqYeXWqbO0qvLYjq1ENHzKLKGoobe6fL/83fjTx/gCvl9I3/OH96bfuSmtqnTy8OrZL7yB9jlcbvL1eCLqP3KcX+9QV68nvvtVq9UunTku8VJsew/Pnv0Gl5cUXTy6P+9+eq/Bw4lof/TGfb9s0Gg0weHDHmRm3L0Wl5KYEDYiUlpVeWznViLKy8zISkmqldZUlpUmXjobOHCoiZlFzr2Uj1+fnJd5r9eQ4UYmZucP740/c6zvsDECofB+2t3r589wOJzES7EOrh5pN69lJidl30u5cupIl54hGXdvZd9LtrS1c/H0VikVy2ZNSE1M6BLY28vXP+7UkcsnDvUIHWBmaf2kc/mnc6UPXnTwEDTuVRIY0wFrVVWUfbntCJfHWzhucFFeTmlh/vKfd2k1mvljBlSUFN29Ed/OyXnHhrUajabvsDGvfLCSiDoHBH/74YIDv/08LGqGocT41N4dRDR4/NQp8xcT0adzpyddvahrvKQg7/zhvUQ0d8UaJw/PFya+/MbwPrs2rRswZkJjyysvu3LqKBG99uEq/9BwaXXlfyLD67c20H7krHn7Nm/QaDT9Ro939fRpuIv8+5kGYvF76zZzOByNRnN0W7RFO1sikstk+6N/IKI3ln/VpWevqlfK5o3ql37nZvKNq+Z/xY2Ftc17634qLcpfPGWUtKoyKf6ig4v74W3RitragLDw6Ys+IqIta1Ye3R59/vDe+hFfXlb6FzGH7ZxdiejcoT9uXj639o9TFtbtlArllVNHkq/Hhw2PzLx728rW3s7JZfZHX/L5/Ptpd1MSE66cPubcoVNzf9v/AvfpgLW69OzFFwi4XK6jWwci8vIL4PF4fIHA3sWNiGoqymul0pTEBCIK7D9Ed4h/2CAiqlMq7l6LVyrkuempRNSjd3/d1tCI0fWNZ6XcISKByMDJw5OITMwt2jm2ryorzctKb2R5uempGo2Gx+f7hoQRkcTYNHjQsBZsn4gkJqZGpmZymWzF7Ck7v1975fSx8HFTdCebcedmrayGw+F49wjStb/5bOLWS8mdugXUHz586itEZGlj5+UXQESV5WVEdD/1LhG5e3fR7aNbSEmMrz/Ktr2LLuacPDoSkaNbBwvrdkTk5NaBiKory4moo2+PD9ZveefrTXw+n4is7OyJqLampvGn1lQY0wFrCYR/vhlWKBIRkYGh4Z8/CkVEpNZoZDV/zqIwsbT88xCBUGJiKq2qrKmqqJVKdSsNJUa6BbGxSX3jtVIZEakU8qhgr0c7LczNdnB2a0x5VRVluiIFAqFujYH44QvqG2rfxb2RnwCfz5/z8epNqz5MuXE15cZVIrJsZxc2PHL0jDmlRYVEZCA24vF4TzrcxOKvj0UoICKtWk1EclkNEcWsXx2zfvXDqvJy6pfrL9iFQgMiEhmK/1ovIiKNWk1ElaUlW9Z+EnfyyDN7CyuSDp5fRiZmHA5Hq9VK/5o4plIp5TIpEZmaW9Uno7SmSrcgq66qP1ZiYkJEBmLxfz5b/2ibDm4d5NJGjU10ESCXSVVKhS6UZY/MX2ug/SadY5eevb7cfjQ1MSEj+XbciUOZyUl7Nn83YOxEibGxLrbUanUDYfdPurgfOHZSz7CH19oisbhJVW3/fu3lE4ctbe0nvrHIxNRszy8b7ly93KQWmgpXr/D8EhmKO/UIIqLLJ/6ctHHl5BF1XZ3YyMTTz19kKNaNnm5eOqe7ux936mj9se6duujudhmZmnn7B3XqEXj/XnJFWYmhRPLkDv9P/S22xEtniaimqjIx7nxj2+dwiEilUDTcRfGD3F0/rju+Y6t3j8CIyTOW/7zLztlVXVdXWpjv4uWtO6nbcReISKVUzB7WKyrY63b8xYbb9PD2JaKKkmJv/yBv/yC+UJifc58vEDTyrHUKsjOJKCQ8ImjAkI6+PfKzMolI0+gXfTUDxnTwXBv32vzlr8Wf2bejvKTQxNTi0omDRDR6+mwDsZiI+kSMifnmi0O//1xRVlJa8KAwL7v+QFNLqz4vjD536I+Vc6f1HzkuOz31xoUzzh29Q8IjGtm1mZW1X6+wGxfObPj4vZDBF27HXVTJaxvZvrl1u9KCBzs3ruvULWD09NkN9LJ/y49cLic38565tU1hbnb+/UwTM3Mnt458gWDQ2MnHd/36zYcLQgYPu3Extqqs1Dekb+eAkILsrAYaDH8x6uSemKtnj695Z469i/upPdukVZXzV61r0sMER/cOKYkJsQd38/j8zOTb7Tt6lpcUJl29HHtwd99hYxrfTuNhTAfPNQ8f3yUbfnXx9Em8GHvu8B5Tc6tZ739SP9XuhYkv9xs5joguHt3P4XKnLVxKRHV/vZLxpQVLBo6dpNFo90X/kHL96qCxk99e/X2Ten/l/ZUdunaXy6Rn9u306hYQPHj4o/e5Gmhfl253rl6KO3Wkgfat7R0Xfvmdg6v72f079/y0PvHSuYFjJy7btF03BHtp4ZIXX5vP4XBO7o6pqSwPG/Hi3OVr/7VmO2fXBV9s6NClW0Lsyf3RP9g5ucz9ZK1/6KAmnXjE5Jne/sF1KtWNC2d9g/rM/3Sdf+igwpz7yTcSmtRO43Ge2R1BgOY5WZxzvChnlF2jbvPrl7vX47VqtZOHp27m7WfzZ9yKuzD+9QX1U5ehAStTrm7rOcSQ16gLU1y9ArSizV9+JHvC5ImIyTMObN2YeDHWxt4peHBEUV7OrbgLQgOD7n37N779suLCmG+/fNLW2cu+aFbVLIQxHbR1LB7TSasrf/psaWFuTl5Gmpa0XQJ7T5izsPGTSJ5zGNMB6AeJsencFf9+awyeHp5IAAD7IekAgP2QdADAfkg6AGA/JB0AsB+SDgDYD0kHAOyHpAMA9kPSAQD7IekAgP2QdNDWCTk8Y76Q6SqgzXGTmOq+kbQxkHTQ1rUXG6dWlzNdBbQtpUp5uVJuyG3s98Ij6aCtcxYbW4oMlPjSHXhEgUIWbGnX+P2RdKAHprT3ir5/h+kqoK2orlPteZDxmusTX+n9T/h+OtAPyTUVnyRfGWrraiUUmeC23fOqSFFbppTvyc/YHjhUyGnCQA1JB3ojt7bmt5zU6xVFfC63RCFnuhxmaLRabqNvw7OMp4l5tUoZYmk3w9m7qcci6UD/1JGW81z+s83Kylq0aNH27duZLoQZWg6n2V8djO8cBv3DJw49l8MaHofD0Wp5z+uY7mngiQQAsB+SDgDYD0kHAOyHpAMA9kPSAQD7IekAgP2QdADAfkg6AGA/JB0AsB+SDgDYD0kHAOyHpAMA9kPSAQD7IekAgP2QdADAfkg6AGA/JB0AsB+SDgDYD0kHAOyHpAMA9kPSAQD7IekAgP2QdAB6g8PhuLu7M12FXkLSAegNrVabnp7OdBV6CUkHAOyHpAMA9kPSAQD7IekAgP2QdADAfkg6AGA/JB0AsB+SDgDYD0kHAOyHpAMA9kPSAQD7IekAgP2QdADAfkg6AGA/JB0AsB9Hq9UyXQMANOT1118vKyvjcrkymaygoMDFxYXL5SoUit27dzNdmt7gM10AAPwLHx+fzZs31/+o+zJOtVrNaFF6BlevAG1dZGSks7Pzo2s0Gk1ISAhzFekfJB1AW2dra9u/f38Oh1O/xszMbNq0aYwWpWeQdAB6YNy4ce3bt6//sWPHjgEBAYxWpGeQdAB6wNraul+/frplExOTGTNmMF2RnkHSAeiHcePGubi4EJGXlxcGdE2FZ6/AKhoiInZOnLKyse4T2re0vGzKS1M1LD1H3S+PR5xG7Ng0mE8HbHC7qnRbbtrd6jKlRqPSYPqFHnM3MqtUKQLM273u1kXAabGLTiQd6L1zpQ+2Zif3t3ayEhlKeLhM0XulSnmpUrEtN2VrQLi5wKBF2kTSgX47Uph9qDBrkmNHpguBlvdpytVfew424gmevik8kQA9VqNWHSu8j5hjq8ntPb9Nv9UiTSHpQI+lVlcotRqmq4DWYmcgOVeS1yJNIelAj+XLZS5iY6argNYi4HB9TCxya2uevikkHeixWrWqFn/ozmr5clmLPElA0gEA+yHpAID9kHQAwH5IOgBgPyQdALAfkg4A2A9JBwDsh6QDAPZD0gEA+yHpAID9kHQAwH5IOgDmFWRnRQV7RQV71VRVMl1La1m/7O2oYK8ta1Yy0juSDqBRYg/tiQr2ykxJYrqQZ23Bi+Fr3pnDdBVPC0kH0ChXTh5mugQGpN+5VZibzXQVLQBfug/Pl1pp9YaP37uTEGfj4DRwzASlQr5lzcqggUPfWL6GiKoqyn5f90VK4tXqiorOAcEjXnrF1atzTVXla4MDdYcvmTbW3bvrR5u2N9BFaVH+gegfb1+9WFKQ7+jm0bVnr0GRUWZW1kRUXly0Ze3KzOTb5SXFDs5uPQcMjoiaxePx6o+VVlb8tu7z6+dPio1MhoyfNihykm595t3bu35cl5mSxOPyu/ftN3bmPGMzcyL67+I3r5w+OuWtxTye4OiOaC6HGzoicuiEab9/88Wh336ytLEbMuGloROn6RpJvHTu8O8/ZyQn8fg8L1//MTPfcHJv6Ouao7/65NiOLUSUEHsyKthr4erv/UJClQp5zLerEy/HlhXkG5maO7p7TH3rAztn1z/PvfBB9Fcrs5KTqspLzW3aefkFRL21WCwxeurf29PCmA6eL5tWfZgQe0KrVXt07npg64/HdmwlIg6XR0RKee3y1yafO/SHXXuX4PAXbl258NGrkzOTbwtFolHTZ+sO7z9yXOiIsQ20r9VqV82bfnzXr+ZWNkPGTxUIRXt/+X7jp+/rtm5cufjKqSN8gSAkPCI/J3PHhrXbv/vq0cN/+vyjOwmXlQpFYW72L6s/zstKJ6Kceykr5ky5cfFsj74DPDr7ntwds2zW+FqplIh4AgERXTiy/8LRfVa29nlZ6b/997OvFs3OuHvLu0dgaVH+r/9dVZhzn4hy0lO/XPBK0tVLfYeN8vDxjT9z7PP5s1RKRQPn0jWot3ePQCKyc3YdNX22raMzEa15Z86xHVtkNdVhI140Nje/FXdh6awJlaUlRFRTWbF05oSE2BOGEkm/UeOVCnnswd2rF7zaEr+3p4UxHTxHqirKrpw6SkSvfbjKPzRcWl35n8jw+q2XTx3Nv59pamn11mfr+QJB18Bea9+dd2Drj3NXrI2cNW/f5g0ajabf6PGunj4NdVFeln8/00Asfm/dZg6Ho9Fojm6LtmhnS0TJN+JvXj4vNDBYtjFGYmzqHzrwq7dfP7o9OvLVN+sPt7C2eW/dT6VF+YunjJJWVSbFX3RwcT+8LVpRWxsQFj590UdEtGXNyqPbo88f3ls/4quqKPty2xEuj7dw3OCivJzSwvzlP+/SajTzxwyoKCm6eyO+nZPz3evxnn7+rl4+k+e9q6iVvTY4qLykMDXxmk9A8JPOxS8kNDM56U5CnL2zW+SseUR0O/7irbgLXC532cZt7Ryc1Gr14qmj8jLSjmz/ZfzrC47EbK4oKbJzdl3+y26BQDgsavpbYwamJCYkXjrnG9ynhX6HzYQxHTxH8jLuaTQaHp/vGxJGRBJj0+BBw+q33k+9Q0Qunt58gYCI3L39iOju9fgmdSExMTUyNZPLZCtmT9n5/dorp4+Fj5sS2H8IEd2Ov0RE7t5dJcamRNS9d7+tl5I3x94SCIT1hw+f+goRWdrYefkFEFFleRkR3U+9S0Tu3l10++gWUhIfFtalZy++QMDlch3dOhCRl18Aj8fjCwT2Lm5EVFNRTkThkZM/WL9l8rx3iUhkKDYxtyQimbS6SWd38/J5InLr1KWdgxMR8Xg8/z79dQlIRIlx54moR9+BujOytLHz6OxHREnxF5vUS2vAmA6eI1UVZUQkEIrqw8VALKnfKpfJiCjxYmxUsNfDQ8pKFbUykaG4kV3w+fw5H6/etOrDlBtXU25cJSLLdnZhwyNHz5hTXlxERGKjhl58YWJhqVsQCAVEpFWriUguqyGimPWrY9avrt+zMC+nflkgFOkWhCIRERkYGv75o1BERGqNRvdsIXr18vQ7Nxt5Io+lmwRjamn1t4JrKiuJSKrbam75cKuZBRFVV1Y8TactAkkHzxEDQwkRyWVSlVKhSwfZI/PXJMYmRNTRt0fkzLmPHsXjN+19o1169vpy+9HUxISM5NtxJw5lJift2fzdgLETJUbGRCStrmpq2WJjEyIaOHZSz7CH19oicWPDV2f90gWFudne/kERUTP5PP7X778pbfrcPRNzC939uPo1uhTTpZuJmUVRXo606uHWmqqKvyUjU3D1Cs8Rl46ddAuJl87qRii6Cy4dd5+uRFRa8MDNu6u3f5Cts0t2eopGq9FdzBKHQ0QqRUO38Imo+EHurh/XHd+x1btHYMTkGct/3mXn7KquqystzHfx9CaitFvXdUmRmZIUFez1cpifTPov777y8PYlooqSYm//IG//IL5QmJ9z/8+qGkej0egmi0REzewa2NvE0koXc+q6f3nfEIfDISKlXK770S+kLxGl3b5R/CCXiOpUqqunjxGRX68wIvINCSWi+DPH61QqIirOz0u9eY2IuvUKbXyprQRjOniOmFpa+fUKu3HhzIaP3wsZfOF23EWVvLZ+a48+A9o5ti/Mzf74tUnd+/S/evpYbua9weOndg4IISJz63alBQ92blzXqVvA6L8exT7W/i0/crmc3Mx75tY2hbnZ+fczTczMndw6Orp6uMZszkxO+nDGiz7+wReP7SeiyJlviCVGVaUlDTQY/mLUyT0xV88eX/POHHsX91N7tkmrKuevWufcoVMjT5zL5Tq4uOdlpe/euC715rWE2JOdA0Jux188e2CnqYVlp+49n3SgbnJM8vX47d991TW4j5dfQI++AxJiTy57ZUJgvyFJ1+LyMu9Z2tiFj5tCRIPHTz2zb0deVvqS6ZFevv5xp49o1Oruffp7+vo3ss7WgzEdPF9eeX9lh67d5TLpmX07vboFBA8eTkRCkQER8QWCt1f/4B866EFW+p6f1isU8olzF02Ys1B3oC7d7ly9FHfqSAPtW9s7LvzyOwdX97P7d+75aX3ipXMDx05ctmk7XyAQCEXvffNL76EjSgvzT+/dLpaYTJyzcFjUzH+t2c7ZdcEXGzp06ZYQe3J/9A92Ti5zP1nrHzqoSSc+ce4iDx/f3Mx7qTevT5q7aMp/3nf37nor7kJxfm4DRwUNeMHRvaNKpdwX/UNlWSkRvbFizZAJL8ll0mM7t+bfz+jRd8CyH7fpZsyJJUZLN8Z079M/LyPt+K5fVXLF0InT5n6ytkl1thKOVtsib1MEYMDOvHt3q8sH2Tg1/pC71+O1arWTh6du5u1n82fcirsw/vUFw6fOas1KoZnWZ9z6xCfYyfBp5x7j6hWeLwe2bky8GGtj7xQ8OKIoL+dW3AWhgUH3vv0b30JZcWHMt18+aevsZV+0UKXPyLGdW+/dTnzspm69wh6dhaPXMKYDPdaMMZ20uvKnz5YW5ubkZaRpSdslsPeEOQsdXNxbs0xoPozpAJpDYmw6d0WbuHMEzxKeSAAA+yHpAID9kHQAwH5IOgBgPyQdALAfkg4A2A9JBwDsh6QDAPZD0gEA+yHpAID9kHSgxwx4PMNHXiEI7GNvKGnEXv8OSQd6zEYkzpY17Z0voEe0RDcqip/+z/uRdKDf3CSmIh6+pYK1ChWyEEu7FmkKSQd6zEpo4G9mszc/k+lCoFVsy02d6dLQ23UbD99PB3rvt5zU+IrCQTbtLQQipmuBFqDWaosUtTG5qZ92DnYTm7ZIm0g6YIMzJXm78tIzpZUWIgO1hrX/pDVajUqpFIkMmC6kFdkaiO9Wl4VY2k139rY3aJnHEUg6YBWFRl2qlLP4H3Rubu6qVau++eYbpgtpXQ4tF3D1cDcX2EPE5bXgKKANUvFFvIrq1ggC1sMTCQBgPyQdALAfkg4A2A9JBwDsh6QDAPZD0gEA+yHpAID9kHQAwH5IOgBgPyQdALAfkg4A2A9JBwDsh6QDAPZD0gEA+yHpAID9kHQAwH5IOgBgPyQdALAfkg4A2A9JBwDsh6QDAPZD0gEA+yHpAPQGh8Nxc3Njugq9hKQD0BtarTYjI4PpKvQSkg4A2A9JBwDsh6QDAPZD0gEA+yHpAID9kHQAwH5IOgBgPyQdALAfkg4A2A9JBwDsh6QDAPZD0gEA+yHpAID9kHQAwH5IOgBgPyQdALAfR6vVMl0DADTko48+2rt3L4fD0Wq1Wq2Wy+USkUajuX79OtOl6Q2M6QDauqlTp7Zv357D4XC5XB6Px+FwOByOv78/03XpEyQdQFvn6uoaGBj46BpTU9OpU6cyV5H+QdIB6IGJEyc6OTnV/+jm5tanTx9GK9IzSDoAPeDi4lI/rDMzM4uKimK6Ij2DpAPQD/XDOjc3t9DQUKbL0TNIOgD94Ozs3LNnT4lEMnnyZKZr0T+YZQLwf1JrKnbkpeXVSksUtUzX8ndqjUYmkxkbGTFdyN/ZGIg5RF1MrV5x8WG6lsdD0gE8dLYkb/P9u4EW7ewNjQy5fKbL0Rs8DhUr5OUqxZ4H6VsDBlsJDZiu6O+QdAB/OlSQdawoZ4JjB6YL0WMaonXpid/6hlq0sbDDfToAIqJipRwx9/S4ROMcO3ydnsh0IX+HpAMgIrpRUWTA5TFdBRvYicQ3KoqlahXThfwfJB0AEVGBXOYkNma6CpboamqZIa1muor/g3uuAEREFSqlhjRMV8ES5SqlSqNmuor/gzEdALAfkg4A2A9JBwDsh6QDAPZD0gEA+yHpAID9kHQAwH5IOgBgPyQdALAfkg4A2A9JBwDsh6QDAPZD0gEA+yHpAID9kHQAwH74fjqAZqqVVm/4+L07CXE2Dk4Dx0yQVlXGrF8dNHDoG8vX3Lpy4bM3ZxhKjDeeiNftvOTlsZnJSdPf/aj/yPFElHn39q4f12WmJPG4/O59+42dOc/YzJyI/rv4zSunj06a905J/oMz+3e8tGDJxk/eJ6L1hy6amFsQUWnhgzdH9Seir/eetrSxa6C837/54tKxg0TUa+iIgNDwD2dEmlhYrj94gYjUavXezd9dPXuiMPe+u49f/5HjggYO1R31+tCg6oqK5T/vvHHh7MXjB6TVVf59BkxbtIzL1e9RkX5XD8CgTas+TIg9odWqPTp3PbD1xxN/xBARpxFf0Z5zL2XFnCk3Lp7t0XeAR2ffk7tjls0aXyuVEhFPICCik7t/P7UnxsXTx9XTx927KxFdjT2uO/bGhVgi8u4R2HDMndj1+8FfN5UVF3T07ZZ26/q3S/9DRLy/3na28ZPFu3/8pk6pHBQZVZh7/5slb53cHaPbxOeLiCh69Yq7N6629/CsKis9tXf7mX07WuIDYxKSDqA5qsrLrpw6SkSvfbjq5beXffzTjlppTSOPPbwtWlFbGxAWPn3RR/NWfh3+4pTC3Ozzh/fW71BaVPjpln0fbvi1fQev0BFjifjs3P8AACAASURBVOj6+TO6TTfjzhNRryEjGu7i1N5tRDR43NQ3lq/5YP0WK1vH+k0lBXm6vuauWDNh9oLlP+3k8ni7Nq179HCJqdnidT/PXbE2aNAwIroVd6Epn01bhKQDaI7M5CSNRsPj831DwohIYmwa2G9wI4+9n3qXiNy9u+h+1C2kJMbX79A5INjO2VW3HDJouMjQ8FbceZVKWVdXdzv+Al8o6tlvSAPt19XV5dxLIaIeffrr1vQbNa5+a1bKHSISiAycPDyJyMTcop1j+6qy0rys9Pp9evb/s323Tp2JqKqirNEfTBuF+3QAzVErrSYigVAkEAh1awyNTRp5rFxWQ0Qx61fHrF9dv7IwL6d+2crWvn7ZQCwOHhRxZt+OxIuxEhNTRW1tryHDDSWSBtqXVlXq3uNsKDH6szZD8SOVy4hIpZBHBXs9elRhbraDi/uf+4v/3F8oNCAijUbv37CBpANoDpGhmIjkMqlKqRAIRURUW11Vv5XD4RCRSqWsXyN9ZKvY2ISIBo6d1DMs/GGD4odhxP3/m339Rr54Zt+OGxfPmpiZE1GfF0Y3XJuBoaFuoaqi/K/eK+u3SkxMdAH6n8/WP3qUgxub33WLq1eA5nD19NEtJF46S0Q1VZWJcefrt0pMTImoTqnIzUgjorys9OIHufVbPbx9iaiipNjbP8jbP4gvFObn3OcLBE/qy927q6N7x4TY47fiL5hbtfPxD264NpGhWDc6u3n5HBFptdq4U0cfttapCxHJZTIjUzNv/6BOPQLv30uuKCtpeJyo7zCmA2gOMytrv15hNy6c2fDxe72GXLx1+YJSIa/f6uTW0cLatqy44IsFr/YePPzS8UMObh1y01N1W8NfjDq5J+bq2eNr3plj7+J+as82aVXl/FXrnDt0elJ3/Ue+GP3VJ9UVFcOnztINGBvWJ2JMzDdfHN0eXVleWl5UkH8/o36TqaVVnxdGnzv0x8q50/qPHJednnrjwhnnjt4h4RFP/am0XRjTATTTK++v7NClm1wmPb13R0ff7r0GD6/fxBcIZn/8RTvH9pWlxVmpd2Z//KW9sxsR1anqiMjO2XXBFxs6dOmWEHtyf/QPdk4ucz9Z6x86qIG+uv/1bCF02NjG1PbCxJf7jRzH4XAuHz9IHE7ka/OJSGgg0m19acGSgWMnaTTafdE/pFy/Omjs5LdXf/90H0Zbx9HduQR4zq1Lv6khTaC5bbNb+P3bLw9u/TE4PGLOR1+2aGlEREe2/bJ17ad+vcIWfrmhMftnpyXXVFZY2NrbOrYnon2//LB9w1e+wX3e/mpji9f2T1tzUqY7e3c3s34GfTUSrl4B2rQdP3ydfD0+5cZVLpc7Yc7C+vXHdm69dzvxsYd06xVWWvAgZv1qA7Gk/6jxpNUe2R5NRIEDhz3DwtsWJB1Am/YgMz3lxlXnjt6jp7/u6OpRvz48Mio8MqqBA6urKlNvXD29d0ettNrDx3fES6/WXwI/h3D1CkAtcvUK9drg1SueSAAA+yHp4Hmn0WjWrVt34eJFpguBVoSkg+dLRUWFSqUionfffXfw4MFqtVqj0ZiYmHh27Mh0adCKkHTAckqlMiEhoby8nIheffXVyMjI2tpaIhoxYsRvv/3G4/H4fP5LL71kZWXFdKXQivDsFVgoJyfn0qVL/v7+bm5u8+bN02q1K1asIKLly5fb2Njo9gkJCWG6THh2kHTAEomJiYcPH+7bt29ISMgff/whl8v79+9PRBs2PJxqWx9z8LxB0oFeqqioMDMzu3DhwqZNmwYPHjx+/Pjs7GwPDw8fHx8imjdvHtMFQtuC+3SgHwoLC1NSUojo1KlTYWFhBw4cICJjY+P58+ePGzeOiIYPHx4ZGWlqasp0pdAWYUwHbVRdXd2VK1dqamrCw8PPnj37+eefv/zyy56enj4+PgcPHpRIJETUtWtXpssE/YCkgzZELpfv3Lmzurr69ddfv3nzZkxMzNChQ4koKCjo4MGDun3atWvXGl2LeXyFtq41Wn4OmQpEjflqqWcJSQcMUygUq1atKi4u/uabb0pLS0tKSnRPRbt37969e3fdPiKRqLXLMBeKblZWE659W0JGTYW9gbgROz47SDp4ptLT011cXHg83ksvvZSZmRkbG1tXV9e9e3dfX18icnBwmD9/PiOFuUpMEitLGOmaZVRajY2B2EbUtpIOf+EPrauoqCgxMTEwMNDExCQiIkIikWzZskUoFCYlJemek7Ydy5PjLYQGPdrS36Xro+15acNsXQdYOzZi32cHSQct7+bNmxcvXhw6dKizs/O8efMkEskHH3wgkUgUCsUzuA59Gh/cuWwuFAVb2HKpbd1m0gsKjXp3XvpgW+cX2jkzXcvfIengadXV1fH5/P+1d99xTZ3dA8BPAgkQZgABWaIoCKKgIIqiUhWss47EvWqX1VrrqJ1ubfXna23V2tZa9bX6ak1cVMXiKDhYKooiArKXbAJkz98f11JrIRJMcpNwvp/+Ee9N7nMM9fDcc59x/fr18+fPs1is8PDwffv20en0mTNn2tjYkB2dxn4szDxXUdCNYUulviTZ1dfXOzo66iuujhMKRQwrKx2l7vLycgCgyxQiWwZDIveuavTkiWxsbN5++22dtNdRmOmQxhQKRX19fZcuXS5cuHDw4MF33nnn9ddfj4+PNzc3Hzp0qIH32tqpSNjcKJOoecPq1asXLlwYFBSkx6AgISHh/PnzK1eudHd3b8fbn8nNzf39999XrVql9XgeP378888/Nzc3UyigqGlQNfGVCgWFQlEqlffv39d6c68CMx1ql5ycHJlMFhQUdPbs2W3btq1bt27cuHFZWVnW1tbduhncrYruCASCnJycAQMG6P9OXCaTzZ07Ny8vb/z48Zs2bdLos48ePbK0tPT19dV6VIcPHz548KBQKHz+oFKpTE9P13pbrwLnSKDWiUSiuLi4ixcvAsCZM2c2bdpUVVUFAMOGDUtJSRk3bhwABAYGdqo0V1ZWNm7cOA8PD/0MfHnB6dOny8rKKBRKenp6Xl6eRp/t06ePLtIcACxcuDAsLOz5DpMBpjnMdOgfeDzenj179u7dCwAPHz5MSkpiMpkAMHny5GPHjo0aNQoAnJycyA6TBGVlZUSHLjExUUdDl9WTy+Vnz56VSCQAUFlZ+euvv2p6hbKyMhaLpYvYtmzZ4uXl1fJHOzu7jz76qLKyUhdtdRhmus6Lx+MBQGNj4/LlyxcvXky8tre3j46OBoDw8PDNmzdHREQAgKGNd9ezS5cuEaP8/P39yYrhzJkzJSUlLX+8c+dOfn6+Rlfw9PRcuXIl0UnXLgaDsWrVKgcHB6JDl5CQwGKxiDktdXV1Wm+uY7BO14mIRKKsrKzQ0FCxWMxisRwcHI4ePdrU1JSZmdm3b19bW1uyAzQ4JSUl3t7e8fHxMTExJIYhl8vnzZv35MmTliMqlWrixIkbNmwgMaoX7Nixg8vlMpnMS5cutRzcu3dvTU3NunXrzMzMSI0OM52py8nJuXfv3uTJky0tLUePHu3v7//999/L5fLa2lo3N9wHS52tW7d6eXnNnz+f7EDg5MmTu3fvFovFzx90cXHpQAdNqVSuXLny22+/1WqAz7DZbA6H88LBixcv9uvXz8nJSSqVkrjSDGY6E3T9+vWbN28uXLjQ3d191apVXbt2/eijj8zNceZfezU2NhLLQ02ZMoXsWAAAZs+e3dzcrFQqpVJpU1OTi4uLUqmUSCRXrlzpwNWSkpL++OOPjRs36iDSNkkkkvHjxy9btuyNN97QZ7t/UyEjJxAIVCrVqVOn3nrrrfv376tUqgMHDpw6dUoikZAdmvGRyWQrV64sLCwkO5DWFRYWTps2jewoOi4hIUGlUqWkpOi/aXwiYXyqq6srKioA4Ndff33ttdcyMjIAwNHRcdmyZcR6bW+99dbUqVPpdDrZkRqfs2fPTpo0ycfHh+xAdO706dMvDILTgxEjRgCAmZnZwIED9fxwFjOdEZBKpSkpKffu3QOAAwcOLFy4sLi4mNjzJTY2lng8GhUVFRwc3MkfknZYVVXVmjVrAIDFYhH/Gk1enz593nnnHVKaDgsLu337tkKhAIDffvtNP41ipjNQ9fX1x44d+/3334mOxtGjR+VyOQDMmjXr4sWLRHbz9fXFB6ZasX379rfeeovsKPTK399/9+7dJI4CIQZgV1dXL126VA/N4RMJA1JVVbV//34bG5sVK1YkJSWlpqaOHTu2d+/eZMdlsjIyMrKzs2fMmEF2IO1VVFS0evVqLperrQuWlJS4uLhYWlpq64IdwOfzbWxszp8/T6VSibk3uoB9OtIQi0BUVlYuXLiQGLgrEAiCg4NnzZpF3JmuWLEC05zuVFVV7d69+/XXXyc7EDLZ29uPHz+e3BiIBW9GjhyZkpKSlJSko1awT6c/FRUVeXl5w4cP5/F4EydODA0N/fbbbxsaGioqKgxtTUrT9ueffwYEBFhZWRndRmJa79MR13z69ClRDyGdQCCwtrZevnz5nDlzwsPDtXhl7NPpVkpKyv79+4mRUIsXL05MTCRmz8THxxOjN5lMJqY5fYqNjb148aKrq6vRpTkd8fHxMZA0BwDElm8rVqyIi4sDgIaGBm1dGTOdNhGPk7hc7qpVq4gfUmxsLDFvlE6nx8bGrl27FgDodLqVlRXZwXY6CQkJxPorO3bswIfUL5g4caJIJCI7imd8fHzWr18PAIWFhe+//75WHptgpnslUqmUz+cDwP79+6dMmVJUVEQcnDRpEtFl+Oqrr9599138d0W6pUuXPn36FAB69uxJdiyG6Lvvvjtw4ADZUbxowIABixYtunv3bstyMh2GdTqNZWdnMxgMb2/vb775hsvl/vLLLwEBAWlpaV27dn1+7RpkIHJycvz9/bOysgIDA8mO5VXpok5nLNatW6dUKrds2dKxj2Of7uV4PN6FCxeIXyzbt2/fsmVLc3MzMbQtKSkpICCAWOMI05yhaWxsfOONN4gOtQmkOT3YvXs3sd6qAdq0aVNUVBQxMqa2VuP9KjHTta6srGzPnj1nz54llidLS0sjxuiuWbPm6NGjxDOErl27kh0mUqewsHDfvn1+fn5kB2I0ZsyY8f7775MdRZtGjx5NjEqZO3fu9evXNfosZjognm0Tmy5/9NFHu3btIn5v2NvbDxw4EABmzpy5ceNG4h8MVtwM36NHjyIjIwEgJCSEGIiP2snV1fX06dNkR/ESjo6Oly5dIlbDPn/+vEwma8+nOmmmE4lE2dnZAFBQUDBp0qRt27YRx1ks1nvvvUcM3J0/fz7+OzEuUqkUAO7evdux5YwQITk52WDvYVv07dsXAGxtbYcPH96upQr0v3wKWTIzM8+fP69SqUpKSiIjI7dv365Sqerr68vLy8kODWkBh8PZvHkz2VHoln5WbVIqlcQmOMZCKBSWlpYeOnRIzXs6S5+usbFxx44dxAQsV1fXGzduEGtXMJlMjbbORIZJIpHk5eV9+eWXZAeiWxQKxdPTUw+tXL16taCgQNcNaYuVlZWnpyefz9+8eXNb7zH9USZPnjzZvXv3nj17yA4E6ZBSqaRSTf/XdmceZdIecrm8rbW1Tf9/DrlcTmyChUyVXC5fsGAB2VGYlOrq6rfffpvsKDSWkJDw+PHjVk+Zfqbr1asXduhMm1Kp1HRLQKSeQqEw/IcS/3br1q229vw2/V1UzM3NiZ0okami0WhHjx4lOwqT4uLiYoCTw15q1KhRbe14Z/p9utzc3CVLlpAdBdIhCoXSo0cPsqMwKWZmZq6urmRHobHIyMi25jWbfqZTKBTE5C1kqoiNn8mOwqQYaZ3uypUrnbdO5+fn98MPP5AdBdIhrNNpHdbpjI+ZmRmxfDMyVVin0zqs0xkfrNOZPKzTaR3W6YwP1ulMHtbptA7rdMYH63QmD+t0Wod1OuODdTqTh3U6rcM6nfHBOp3Jwzqd1mGdzvhgnc7kYZ1O66qrq998802yo9AY1umwTmfKsE6ndQqFogN7NZAO63RYpzNlWKfTOhcXl0OHDpEdhcawTod1OlOGdTqtMzMzc3Z2JjsKjXXGOt3ixYtDQkJCQ0NnzpyZkpIyYMCA0NDQ/v37kx0X0j6s02mdkdbpLl++nJWV1eopU850bm5uFAqF+helUomZziRhnU7rjLROl5SU1Nb/CSab6UJCQog9p1s4OTnNmTOHvIiQrmCdTuuMtE4XHR3d1hbmJpvpAGDBggXP1xp8fHxGjRpFakRIJ7BOp3VGWqcbMmSIr69vq6dMOdM9361zcHCYNWsW2REhnZDL5fjD1S6s0xmZBQsWODk5YYfOtCmVyuLiYrKjMClYpzMyISEhffv2tbKymjlzJtmxIF2h0WjHjx8nOwqTYnp1upeMHJaplCfLnuTyeXVSsW5i0zkRa5RdZNDZLrSzGYlkx9IRrpYMK6p5sL1ztIsX2bEYliVLlhQVFZmZmRHdOgqFQqFQ5HJ5XFwc2aEZPeOt07V1Sl2me8LnLX9wfYSzR3eGXZCdo25i0wuv1gcTGgUqhVopFqQ31lyoLPym3zAqUMiOyFDMnj173bp1TU1Nzx80+Z3a9aO6uvqTTz4xum7d5cuXPTw8Wu3WtZnpHjXV7yt8sK53uI5jQy/nYWkNAFnNDR89uLG733CywzEUkZGR/v7+t2/fbjmiUqkGDRpEalAmwnjrdAMGDGg107Vep1OoVLvzM2Z4+Ok+NtRegbbMADvHX4pbf7TUOc2ZM+f5zXzt7e0XLFhAakQmwvTqdK1nunuNNRZUKp1q4s8rjE4PK7ur1aVkR2FAIiMjnx8/FRgYiH06rTDeOp1m4+lKRfxuDDsdR4U0Zk+jM2kWTXIp2YEYkHnz5hHdOjs7u/nz55MdjonoLOPpmmRSmUqp46hQR9RKxRKlguwoDEhLty4gICA8HMvK2mG8dbq2xtOZ/vp0yNBIlIoCQZNUqZAplY50yx7WdvUySQG/scOvI+bOKBA0LliwoF4qLhA0OVlYdmfYafq6TiouFDQ5W1j6MOxqpKIaidjLysbWnEb2t0UO463TtbUoPGY6pA8SpeJCZdGjpvpuDNt8QWO+oFGhUkkUChtzcxcLBl8uq5aIOv6aJu760aKTFlL+o6Rqifiv41KNXjfLpTUSsa05rYuFFV8hE8plDjTLcEdXJagcaZYju3gyzDrRPxbjrdO1daoT/fAQKQqFzQqV8teS7HRejUSpuFH3j7N8haxSIjKo108lQuJ1pUSUzW8AAEuq2eXqkpU9+1uZmbtYWGn/OzI8nWg8HUKvbm/+gxt1FQ0yCdmBvBKxUvG4ueHde9c8rGyC7Z2X+waTHZHOGW+drq3xdJjpkE40y2XXakp/ryw0mSkLKoAyEb9cxO9p7RDj6kWjmPIYLKzTIfRyYqXis0dJuXwe2YFonwrgu/z76bzqlb1CrM1M9nmF6dXpTPn3EiJFmZi/KP2qSaa5FjfqKhbdvVolFZEdiK5UVVUZ4+DEzrs+HdK/tPqqWonJpoAWDTLJ7foqsqPQFaVS2dDQQHYUGsPxdEhP1j9OTamvJDsKPfm+4AFPJp7r1ZvsQLTP1dX1yJEjZEehMazTIX3glOfda6w1mUcQL6VQqU6XFzhZWI116UZ2LFpGpVKZTCbZUWgM63RIHxQqlVghJzsKveIrZAX8pna80cgYaZ3u0qVLmZmZrZ7CTIe0o0TEP16WS3YUJLheWy5VmtokcSOt06WmphYWFrZ6CjMd0o5Nj1NFht2ha8otuBw5mfcoR7uXbZBJ3r//p3avSTojrdONGTOmT58+rZ4y5UxXW1k+N6J33PHDZAdi+oqFzXKDX9a8ObcAAGx9fbR/ZbmsSGhS97BGWqcbPHhwWzv/mnKmS72CO6foibuVNc/gp3w15xXa+HYzs7TQ+pUpFIq7lY3WL0si06vTae3Zq1KpPPbdtsy0W9VPy5nOXYLCh0xd9IGDcxfi7GXu//6MPVlVVkyjW7h6eE9964PgIcMBoDQ/97O5k6zt7Fds33vsu21WNnbzPvrshSOf7zmkUCjOHf7hTuKVqrJi3z4hI9+YPnj0WPXtfj5/csmTbAA4tnvbsd3bDibcp1tYthW8SND846bPsu6munh4jZ46U9DUeGLfzsGjx36wedfDtFvbl79lZW3785Vn+xWsfXNaYfajRZ9uHPnGDABo4tUf37MjJ+NOM48XNDBi0oJ3u/cOAoB47tEjO7eEjYgOjhh26sDeqEnsq6ePNfN4SzftjIgeT1zqw0lR9TWVH2ze1fLXMVJ3GqqFurx1VUikxSfO1dxKE5ZW2Pn79lgwndk/CAAas3LT3l0Tvv//Cg79Vpt816Znd6+pYz0nxRCfehqfWHr6oqC4jBncp/vC6fyCEtue3XURXoNUzJfLHGnaz6FkMd46nUwmCwoK+vcprfXpzv964I+TR3j1tZFjJ8kk0mtnftv1yVJio6bY/+7/785N5YV54VFjevYJzs96sGPVuw/TbgEAjUYHAJGA/8PGT4T8Zu+efv8+AgA/b/389IG9cqk0mjW3qqx479oVV0+fUN/uyDdmdHH3BIC+g4ZOXrSEqnbJnV+2rbt7/YpKpegZ1O/80QNXzpwAAArV7KV/a6lYtHnxnBsXz3T19omIGfcw7dbG9+YUZmcCgDmNBgAledm/bFvH7NLF1cNz+AQWAKTffFbTKS/Kr6+ptGQwQoeP1NIPgTTc8jzdXVzWxL+z5LPqxGS/9xdE/LrHwol5//Nt4tp6ABCUlANA8fFzPnOmRl065hjaN3vXfoVEAgDViSmZm3Y5DQwZevwHj4nRmZt2NT8ptOmp/VtXwoasVB1dmRRYp2udWCj8/ch+APhg8zdvfbJp65Ez5nSL/KwH2ffviATN5/77AwC8/dnm99ZtW73zp5FTZgDA6Z/3tHxcqVAEDhi0kxM/d/ln/z5SW1l+M+4cACzbsmvmklWbD3KpZmanftmjvt3R02Z1cfcCgH6DhrHe+dDcvM1M19RQn3btDwBYvG7bmx9v2HSQIxLw2/kXT7n2x9PiQnsn5xXb9y1as3Hxuq/lUsn5owda3lBdXvru2m2bD54aNm7K6CkzAeD+zQSFQgEAD1NvAsDAETE0utH3Bcx0Od09/+AJSV1DyPYvmP2DLJ0dAz/7kGJmVnM9BQCEpRVUOr3XkgXMkD40G2vH0H4qmVxa3wgAhUc4ToP6+741i+5g12XowK4xI+TNfJvuutoz1/Bv3jWCdbrWFWQ9EAn5FAolMHQwANgxHQ8nZhxNzg7oP/DxvTsSkQgABo16doMW/toYAHiSeV/4XEKJnjbnhWu2HCnKyQIAmoWlV09/4uKunt5N9XXlRflq2m1/8IXZj5RKpZm5efCQKACwtrUf9NqYdn62ODcLAHz8A4kenG9gCAA8vvf3vnxWDJuhYyYSr7u4ewYMCBcJ+Tn377RkumHjJrc/VIM12LH1gemvTqVQVF657joq0rKLE3GEam5GZ9pL6nkAwM8vdo4IterqQpwSV9cClWrhxBRX1Tbl5LtF/71jJJ1pDwA2url7BYC53v46uvLzKBSKt7e3Hhqqra1dv369HhrSLp3X6eqqqwDAkmFDbKj+PH5TIwBYWdu2lMnsHJ5tki1oamx5m7O7+wsfbDkiEggBQCYRz434x7SbqrISQVNTW+22n0jQDAA0ugVx4wwAVrbt3S1ILBQCQEbS9edja6qvk4ierebo6Nb1+dhGTJj2OD0t/ea1Xn1Dsu6kMp1dA0JNYS+rOqmuejSiiioZr6nkt9iS32KfP06bMBoAmvOKPCaMbjnIzy+29nan0mmNWbkA4NDn7+wjrqq1cGJaOutqg3aGXtY1UalUJSUlemhIJpOlp6froSHtUlOn006ms7a1BQCxkK9QKF5IOvYOjgAgEQmkEjGR7PiNPOK3k50Ds6Gmmngb9V9FsZYj1nZ2AGDJYKzcvu/5N3j06JX/KKOtdtvPwooBAGKhQCaVEDeSoua/RwxQKBQAkMn+3o5L8NxZa1s7APALDmW9vez5a5r9tf/AC1ENGvX6f3duuXv9asiQETKZdMSEKcT1jV2pqFlHV5YLRQAQ+OkHVh5uzx+39vaQNQvEldU2Pf6eicUvLCF6beKaOgCwdO3ScqoxK8fWr/X7Gq24WFUU6dRVd9fXMyaT+fHHH5MdhcbGjBnj4uLS6int3L369A4kfuFkpt4CAJlUsmT80LkRvTNvJ/mHhFlZ2yqVSqIWBgDJVy4CQGBoBJFiXso3oC/Re7KxdwgMGxwQOqg4L5tXX2tlba2mXQCgAAUApFKx+ut3939WwsxITiQ6oRmpN1vOWtvZA4BcKikreEI8RqipKPs7tj79AKCusqJHYL/AsMFu3XxK8nOUKiVxM/tvNLrFsHGTayrKLv12BACGTZjWnm/A8A12dGvHuzqC7sQEAKuuLo79g4j/GB5udv6+Fk7Mpuw8ALB97iEDP6/I1rcbAIBSCQBK+bPHwc1PCuvvPLDRwUi6Fr1sHNrxLqNhaWk5fPjwdrzRsKip02mnT+fYxS162pzLp47tXbdqyJjx95OuN9XXBQ8ZHjRwCABMfXvpse+2Hfj6y6z0NF5t1YOUm1Qqlb14eTsvbu/kPGzclBsXz3y1bOHIN6aX5Ofev5XQzS9wSMwE9e0yu3QBgMTfT4mFwjHseS1DXl7g4NwlZGjU/VsJP276bOjrSQ9TbkklfydHrx5+jl3c6msqd6x6L3LMxOTLFz169CrLfzbtKXTYKFdP76qykk2LZw8YNvLOn/FlhXljZswnAmjV6Kmz4jm/3r+V0CsoxNVDVwVyPRvr2o1T9qRcLND6lS2dHV2iIor+d8Y+0E8hkdalpuf/ctzvg0Uuwwfx84vMrRlW7s9KhMLyp3K+gEhnjqH9AKDo2GnHsGBZU3N5bDwA2Pjqah6+p5XNm94BOro4KXg8nTScPAAAGGRJREFU3s8//2x03bpLly55enrqdpTJgtVr2Ys/olAoV0+f4Dc2RE1iL9v8LXFq7MyF73yx1dbe8fr5Uw9Sbnbv3Wfd/uM9+2iwGP+CVWtHT5utVKpij+zPuXcnetqcj3f+9NJ2x8yYz7Cxqy4vvXDsF6Xa7Wvf/eKrXn37i4WCP89x/IIHtDxDIAaLLNm0w9XTu7Gupig3a8mm/7h36wEAcpmcOPvxzv1hI6IrivLPHtwnkYhnLVszc+lqNW25+/RwcfcCgMhxU9r/DRg4sULuatmuHnoHBH7yAd3B7s9xcxMnzC87+4fPnKkuwwcBQHN+sV3vni1v4+eXtHTxbHt1773qvfLY+LvLviw4+JvHhGgA0N2D1zAHFxOb+CoSia5fv052FBpTM++VomptEs9/S7KfigVRzh66j80QHf/+PxeOHoiImbB043+0fvHCnEdrF06zdXD47syfdEuNN5ralXd/b8iILnSD26Fq/ePU5E6zMt0LIp3c1/XW4HF/hxUVFa1evZrL5eq6IbFYnJaWZnQ3sCkpKS4uLq3ewHaW9ekO/2ejkN/6KLkJc97y7qWPxRTT/vwjnnO0ODcbANiLV3QgzRmyd7sH1UjEeYI2F1Uv//0y8Uj0BXKhyJzR+lfRfT67ZQTJq6u+mVZ763Y73vgPNj26ebMnqHmDt5XtHC+/VwvN4Bhvna6tU50l0y1cTf7gIF5tdfa9205u7uPnLCJmkpkSD0vrVb36q1nVw2NitMfEaP0G9Q8ukeEukeHavSYVYEufwW4WurpzJ4vp1ek6S6bTyKylq2eprbV1TAx7Xgx7ntYvazioFHCzYFT+tTN0Z9Db1pHejlmDRoeo0xldptPHvFeEujPsYty8bc1Ndm/AF1ib0yKcuprSxP4Wxjuerq15r9inQ9o019N/ENPtw4xEhcEvV/eKLKhm63qH97c3vk1R28P06nTYp0Na1svafoyrqe0g82+LfAJMNc0RdbodO3aQHYXGcB8JpFcf+QYPZLramOhtLMPMfLiTx5SuvmQHokOmN54O716RTmwNHFwgbNrwOLVSbFIPKHpY2833Dhiis9lvBsJI63Rjx45ta94rZjqkKz0YdnuDo9ZlJTfLZaWi9i75Z7Ac6RauFtbf9I00M4lFGdQz0jpdeHibo4gw0yEdsjOnfdtveLGw2dqctvNJeomIXysREY8qlKCiAIUCoFIBUFSg8WugAEVHr1WgolIoxLgZJ5rVaBevye49ykT8vnZOZH+jemKk4+ni4uK8vLxwPB0iRzeGLQCsDxiU1lBlb063p9F/LckpEjX3ZNh5WNmk8arKRfxwB1eNXg9iurlbWqc0VD4VCzr2Orm+slIiiGC6uf3rtVKlfM3FaxDTNZ1X42rBGOLkRgUK0xRHk7TFSMfTpaWlyeXyVjMdzns1MgY77xXpAc57VS8tLc3FxcXHp5XluVrv05lTKLROUIwwRrY0CxXgjwbplunV6VofZeJIt6zX2XrZqMNUAKXCJhd6m9s5IqQVRjqeLi4uTrPxdN2t7SVKhY6jQhqrlYqDTXe0KjIcRjqeLi0tra3xdK1nut42DjbmtIzGWh0HhjQTV1XE9uhFdhTI9DGZzE8//ZTsKDQ2duzYvn37tnqqzTkSGwIG5fJ59xprdBgX0sSx0hy2R8+BTK2t14ZQWywtLYcOHUp2FBoLDw9v9XHES0aZ7OwbuS337t6CB440S4aZsY5HUaqUcrmC3sYWNobP2pxWLGyyMae/4d7jNWdPssNBnQKPx/vpp58++eQTsgPRTMfH033qF1orFRcJm+olL9lhy2BVVFScP3/p3XffJTuQDjKnUqd79uphbY9Pw5HeiESimzdvGl2mUzOe7uU9NWe6pbMxP+x7XC+4mlsc46qPnc8RMg3GW6fDea8IofYy3jpdW6dw1SaE0It4PN727dvJjkJjGo+nQwh1ZkSdjuwoNKZmPB3evSKEXoR1OoSQ6cM6HULI9GGdDiFk+rBOhxAyfVinQwiZPqzTIYRMH9bpEEKmD+t0CCHTZ6R1uvHjxzs7t75ULWY6hNCLjLROFxYW1tYpvHtFCL3ISOt058+ff/DgQaunMNMhhF5kpHW6u3fvFhcXt3oK714RQi/COh1CyPRhnQ4hZPoaGhq2bt1KdhQawzodQkgDYrE4JSWF7Cg0hnU6hJAGmEzmF198QXYUGsM6HUJIA5aWloMHDyY7Co1hnQ4hpAGs0xkfOp2uUqlKS0vJDgShV5WRkWFtba2HhhobGwsKCvTQkHbl5OSUlJS0eoqiUqn0Ho++JSYmfvvtt+7u7mw2OyoqiuxwENKMSCTicDgcDqdHjx5r165tqxSlXVVVVTQazdHRUQ9tacu1a9cGDRrU6i+DTpHpCKmpqRwO59GjR2w2m8Vi2dnZkR0RQi+RmZnJ4XCuXbs2ffp0FovVtWtXfbaenZ3t6urKZDL12aiOdKJMR6ipqeFyuRwOJyIigsVi9e/fn+yIEGrFuXPnOByOubk5m80eP348WWHMmDHjq6++8vX1JSuA9jt9+rSDg8PIkSNbPdvpMl2L+Ph4DofT3NzMZrOnTZtGdjgIAQCUlpZyudyTJ0+OGzeOzWb37t2b7Ijg0aNHAQEBVKqh1/RjYmJOnDjR1u125810hPz8/JMnT54+fZq4pe3RowfZEaFOKiEhgcPhVFRUsNlsNptNo9HIjugZqVSalZUVEhJCdiDqSCSSpqamLl26tPWGzp7pCCqVisPhcLlcJpPJYrGio6PJjgh1Fk1NTcTThqCgIDabPWjQILIjakV6evqPP/64f/9+sgPpOMx0/3D37l0ul3v79m2ii+fk5ER2RMhk3bt3j8vlJicnE504/TxR7bC6ujo+n9+tWzeyA2ldVFTUlStXzM3bnAqBma4VjY2NJ0+e5HK5wcHBLBZLzYZDCHUAl8vlcrm2trZsNjsmJobscNrr6dOnKpXK3d2d7EBedPPmzYSEhC+//FLNezDTqXPt2jUul1tVVcVisdhstprfGAi9VEFBAVEkmTZtGpvNNooHmi9Yv359eHg4ic+COwwz3csVFxcTA1MmTpzIYrH8/f3JjggZmcuXL3O53IaGBuJGlexwXklWVpa3t7eNjQ3ZgTyjUqlycnJe+pAaM50Gzpw5w+VyLSwsWCzWuHHjyA4HGbq6ujqiEzdw4EAWixUaGkp2RNqRnZ3ds2dPA7nFOXfu3IMHD9auXav+bZjpNPbgwQMul5uYmEjc0rq5uZEdETI4aWlpXC73wYMHxP8k9vb2ZEekTRKJZOTIkbdu3SI7EACAH3/8MTo6+qWlAMx0HSQQCIhb2l69erHZ7CFDhpAdESKfXC4nOnGurq4sFqut8fomQCgUPnnyJDg4mOxA2gsz3au6ceMGl8stLCwkfntbWVmRHREiQU5ODofDOX/+PDE+yWBHY2gRn8/n8Xienp4kxpCTkyMUCtszpxMznXZUVFQQXbyRI0ey2eygoCCyI0J6cvHiRS6XK5VKWSzW5MmTyQ5Hr06cOFFWVrZ69WqyApg7d+6XX37ZnjlzmOm07MKFCxwOR6FQsNnsSZMmkR0O0pXKykriRnXEiBEsFqtfv35kR0SOgoICCwsLDw8P/Tfd2NiYmJjYzn9lmOl04vHjx1wuNy4ujriX8fLyIjsipDVJSUknT57My8sjfrj6WRrTkJWXlzs4OBj494CZToekUilxS4uLgJoAYjlMLpfbvXt3NpsdGRlJdkQGZObMmVu3btXzWOgdO3YsWrSonVM2MdPpQ3sWAd2zZ8+yZcvIiA69RMtymMSPzwCnQxmChISEYcOGmZmZ6ae5R48e7dix4/Dhw+18P2Y6/VGzCOiwYcMAYNmyZdOnTyc1RvQPsbGxHA7HzMyM3OUwjYJCoSgtLfXx8dFPc0+fPqXT6e1fgwMzHQlaFgFlsVgsFgsAQkNDKRSKg4PD+vXriayHSEQsh8nhcMaOHctisQICAsiOyDg8fPjwm2++OXToENmBtAIzHWny8/M5HM6pU6fodLpEIiEOOjk5/fTTT3r7xYhe8PxymCwWi06nkx2RkXn69GldXZ2uR1mlp6efOXNm8+bN7f8IZjqSqVSqAQMGPF/dcHFxiY2NNZBJhZ1EU1MT0Ynr06ePwS6HaSwaGxspFIpOd6TavHlzaGioRnPPMdORbNKkSRUVFc8fUalUvXv3PnbsGHlBdSLPL4fJYrHULM+N2m/Dhg1hYWETJkwgO5C/YaYjk1ylGrHifaqLI9XeFlQAFAAVmJlRKRSqvb29ga/cbwLS09MBVB6enq4ursQRBo1ma0bzs2W+5kzCUFhTkpqa2qtXL11sFysWi0UikaZ7M2KmI83Dprr1j1PFeUWWAgmNSqXRaAwGw8rKik6nm5ubd+/enewATV9TU9MLN1nmVGqdVCyQy2ql4l39htEohr4hliGrqqpydnbW+riT1atXT5gwQdPRqVgMIse9xppDxY/X9BoAvQaQHUsnZt/m1g1FouYP7yfuDYkyo1D0G5PpcHZ2HjJkSGpqqhavKZPJJBJJBwbhY5+OBHKVakZq3Md+mOMM2qPm+iqx8HP/MLIDMWICgSA1NdUQVq/CzjkJ/qgqDrTTfv0CaVcfW8fk+qcihZzsQIyYtbX10KFDGxsbtXXBtLS0urq6DnwQMx0JSoTNXS0Nejo0IvjbOD7h88iOwrhZWFj88ccfO3bsePVL1dTUrF+/vmN7k2KmI0GdVGyO1R9joAJVs1xGdhRGb/r06TExMXl5ea94ndLS0pfuF9EWfCKBENK54ODg5uZmqVT6KtNOBgzoeGkb+3QIIX2wtbVduHBhbm5uxz5eV1e3f//+DreOmQ4hpCf/+9//MjMz5fKOPOQ5efLkqwzNw7tXhJD+TJkyRSgUdmBa98CBA/v27dvhdrFPhxDSHwqFUlhYuHDhQk0/GBYWZmFh0eF2MdMhhPQqKCho48aNSUlJ7f/IwYMH4+PjX6VRzHQIIX3r1q1beHi4TNbeETxHjx4dPHjwq7SImQ4hRAJzc/Nt27bFxsa+9J0KheLChQuvuOAdZjqEEDnWrl3LYDCqqqrUv02hULz64s+Y6RBCpBk9erSjo6P6dUa0srMKZjqEEJnMzMzULGefnJw8e/bsV1/kDjMdQohMVCr1ypUrHA6n1bMRERHLly/XQiuvfgmEEHoVdnZ2LBZLqVS+cFyhUKSlpWmlCcx0SDOr2DG7PllKdhTI1FAolLNnz3799dfPHzx37tzVq1e1cn3MdEgD+VkPq8pKyI4CmaapU6eOHDkyIyOj5YhQKJw3b55WLo6rq5NgS/ZtFwurfm1vYvBvTbz643t25GTcaebxggZGTFrwbvfeQQCwd+2KlCtxA6Niln+9GwBSrsTtXbvCzoH59bHz9o5OhY8zTx3YU5jzyIxqPmD4a9Pe/tDW4dmOSrkZd7kH9pYXPpGIhD7+fUZPnT149FgAOPnDN7FH9ocOH7Vi+/cAIBII3hkdCgD/+e1SPPdYPOfXlpBW7/wpZMgINU2ocXzvjuT4CwAwdOykkCHDNy+ea+fotO/CLblcvnBYEABsPnyqu38fADi2e1vc8cMRMROWbvyPmu+hND/3s7mTrO3sV2zfe+y7bVY2dgxrmzuJl0dOmbFozUai0Z82fXoj7uyM91dNnP9OO792TnneVHffoU5d2/+TQoYJ+3RGQCoWbV4858bFM129fSJixj1Mu7XxvTmF2ZkA8Nanmxycu9xOiM+8nSQRCY9+9xUAvLd2u72jU2lezpal8+4nJYYOH9UzKPjq6RMb3pkhEggAgN/U+N0Xy7PuJHt079UnbEj2vdt71664dytBfRj9BkcGhg4CgK7duk9etMTNs5uaJtS4cur4hWO/1NdU+gX3f/Lw3g8bPwEAM+rLp3yr+R5oNDoAiAT8HzZ+IuQ3e/f0i5rEAoD0G3+2fDwjOREA+g4e2u4vHpFj/vz5ubm59+/fT05O1tY1MdMZgZRrfzwtLrR3cl6xfd+iNRsXr/taLpWcP3oAAKysbZes3wEAB7etP/XzHl5tzfAJ04KHDAeAuN+OSESigVExi9Zs/PCr72LY86rKSm7GnQOAy9xjjXW1vfoN+HzPoRXb946bvQgA4k4cVh9GyJARAQMGAYB7tx6sdz508/ZR04Qa1879BgBjps//YPOuL/f96urR7dW/B4JSoQgcMGgnJ37u8s+CI4Y7OHfh1VYXPs4EgOInj5t4DUxnVx+/wHY2h8hy5MiRmzdvbt++vWMLqbcKM50RKM7NAgAf/0BzGg0AfANDAODxvdvE2cCwwTGsudUVpRePH3Jy7Tp/xRd/feoxAPgGPlvohniRk3EbAB7dSQaAoIERxKnZy9YcTc7+fM9LMl1rgbXZRFukEnHJk2wACB32bL+oEROmaOV7IERPm0O8oFAor70xHQDSb/0JAA+SbwLAiEnTNP07IlKw2ezFixf7+flp64K4Pp0REAuFAJCRdH1uRO+Wg031dRKR0MKKAQBhUdHx3KMAEBQ+1JLB+OtTfAA4sW/niX07Wz5VVV4KAA01VQDAsLZ95cDabKItLfe2VtY2xAuGbXvnM6r5Hlr+6Ozu3vL6tTemn/nl+3s3E6a9vezh7VsAMGxce7MqIpetre2IESO0eEHMdEbA2tYOAPyCQ1lvL3v+uJk5DQBkMumhHRsAgGZhmfg7d8T4KX7BoS0ZZPS02eFRMS0fsWAwAIBhawsAAn7Tv9uiUqkAIJdJiT8K+er2r1PTRFssrayIF028hmdNNP8dBtE6ACj+WpZW0Nzczu/hryv8PZjesYtr8JDhGUnXnxYXZt+77R8c6urhpSY2ZMLw7tUI+PbpBwB1lRU9AvsFhg126+ZTkp+jVCmJm7hzB/c9LS4MDB305sfrAOCnLZ/JZFIA6BkYDAC82prAsMGBYYPN6fSnpcXER7r79QGAzNRbxJP3uOOH50b03rx4DgBY2zkAQMmTXJlUAgAZKTeej4RCoQCAVCwm/qimibZYWDE8fHwB4EHKDQBQqVSp1/5oOUulUq2sbQGg4HEmAEhEwux7fw8cVf89tCpqIgsAju3ZrlQoho6d9Go/B2TEzDZs2EB2DJ3O9doKa3Oaq6W6vs/z3Dy7JV++UPO0/H5yYkNtNeeHXTcvxVrb2QcPHlaan/vDxjXm5rSPdx3oEzb4YdqtkrwcuUzWN3yIq4f31TMnygvzip88LsnL+d+e7WnXLvUbFOnu4+vm5XPrj/NVZcVZd1IepNyIO3GYama2ZP3/Obm5WzIYV08fFwsFuQ/Si7If3buZ0MyrVyqVMey5NvYOlWXF6TeuNVRXScUiM5p5n9DBbTWh5q8jkYgz05LyH2VUlBTGnzxS9OSxVCyyYtiMm/0mAJQ8eVxWkJeVniYWCrj7d9vY2ddVPfXy9Qt/LUbN98Bv5BH37xPmvUO3sGxpq6t39yunj5U8yTanWyxeu52m4ZIYWc31AbaO3oxXvc1HpMM+nREwp9E+3rk/bER0RVH+2YP7JBLxrGVrZi5drVAoftz0qVKhmLjgXTdPbwB4+/MtVCo17n8HC3Mede3WfdWOH3v17X/3+tXfj+zv6uWzbOu3YSOiiWEiG38+4R8SlpNxN/XqJR//Piu2f0/c83r5+i1cvd7K2rb4SY5IwF+x/XuGtQ0AyOUyABg8apynr59MJo09sr+xvk5NE2qMm/Um8awg5fIFoFDY7/1jVuOcDz8LDBssk4iz7qSMmT5/0OhxLTezbX0PatqiUqn9h44EgLDho6yscTfxzgtHDpOgAyOHTdj9pMT/rHqP6ey65/dErV9cKOCvmTmOV1u96Rduj8AgTT+OI4dNBj6RQNoXzz2al5nR6qn+Q6MiosfrIYaq8tKft35e87ScV1s9ZMzEDqQ5ZEow0yHti2HNjWHNJTcGqUiYfe82w8Zu+Pipsz/8hNxgEOkw0yGShQwZcTQ5W+uX9erpr4vLIiOFTyQQQqYPMx1CyPRhpkMImT7MdAgh04eZDiFk+jDTIYRMH2Y6hJDpw0yHEDJ9mOkQQqYPMx1CyPRhpiMBk24hUMjJjgK9XLNc6kS3bMcbkaHDTEcCPxtmg1RMdhTo5Rqkkh7W9mRHgbQAMx0Jol28spsb+HIZ2YEgdW7VV4xy8aRT8d+IKcCfIjn2hkSdrsivw56doUqurxQpFIu79yU7EKQduOYwaaolog2PU1Wg8mbYAv4QDAOdalYrFSlVKlcLxspe/ckOB2kNZjqSZTbVFQqaeXIJ2YEgAAA6UJwsrLpb2/liec60YKZDCJk+rNMhhEwfZjqEkOnDTIcQMn2Y6RBCpg8zHULI9GGmQwiZvv8HBrFA/66v+bcAAAAASUVORK5CYII=",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_teddynote.graphs import visualize_graph\n",
    "\n",
    "visualize_graph(app, xray=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_teddynote.messages import random_uuid, invoke_graph, stream_graph\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langgraph.errors import GraphRecursionError\n",
    "\n",
    "\n",
    "def run_graph(\n",
    "    message: str, recursive_limit: int = 30, node_names=[], stream: bool = False\n",
    "):\n",
    "    # config 설정(재귀 최대 횟수, thread_id)\n",
    "    config = RunnableConfig(\n",
    "        recursion_limit=recursive_limit, configurable={\"thread_id\": random_uuid()}\n",
    "    )\n",
    "\n",
    "    # 질문 입력\n",
    "    inputs = {\n",
    "        \"messages\": [HumanMessage(content=message)],\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        if stream:\n",
    "            # 그래프 실행\n",
    "            stream_graph(app, inputs, config, node_names=node_names)\n",
    "        else:\n",
    "            invoke_graph(app, inputs, config, node_names=node_names)\n",
    "        output = app.get_state(config).values\n",
    "        return output\n",
    "    except GraphRecursionError as recursion_error:\n",
    "        print(f\"GraphRecursionError: {recursion_error}\")\n",
    "        output = app.get_state(config).values\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mfirst_tool_call\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  sql_db_list_tables (initial_tool_call_abc123)\n",
      " Call ID: initial_tool_call_abc123\n",
      "  Args:\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mlist_tables_tool\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: sql_db_list_tables\n",
      "\n",
      "Album, Artist, Customer, Employee, Genre, Invoice, InvoiceLine, MediaType, Playlist, PlaylistTrack, Track\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mmodel_get_schema\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  sql_db_schema (call_O3yTy4KRztcnxRsE6gPpuX7Y)\n",
      " Call ID: call_O3yTy4KRztcnxRsE6gPpuX7Y\n",
      "  Args:\n",
      "    table_names: Employee\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mget_schema_tool\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: sql_db_schema\n",
      "\n",
      "\n",
      "CREATE TABLE \"Employee\" (\n",
      "\t\"EmployeeId\" INTEGER NOT NULL, \n",
      "\t\"LastName\" NVARCHAR(20) NOT NULL, \n",
      "\t\"FirstName\" NVARCHAR(20) NOT NULL, \n",
      "\t\"Title\" NVARCHAR(30), \n",
      "\t\"ReportsTo\" INTEGER, \n",
      "\t\"BirthDate\" DATETIME, \n",
      "\t\"HireDate\" DATETIME, \n",
      "\t\"Address\" NVARCHAR(70), \n",
      "\t\"City\" NVARCHAR(40), \n",
      "\t\"State\" NVARCHAR(40), \n",
      "\t\"Country\" NVARCHAR(40), \n",
      "\t\"PostalCode\" NVARCHAR(10), \n",
      "\t\"Phone\" NVARCHAR(24), \n",
      "\t\"Fax\" NVARCHAR(24), \n",
      "\t\"Email\" NVARCHAR(60), \n",
      "\tPRIMARY KEY (\"EmployeeId\"), \n",
      "\tFOREIGN KEY(\"ReportsTo\") REFERENCES \"Employee\" (\"EmployeeId\")\n",
      ")\n",
      "\n",
      "/*\n",
      "3 rows from Employee table:\n",
      "EmployeeId\tLastName\tFirstName\tTitle\tReportsTo\tBirthDate\tHireDate\tAddress\tCity\tState\tCountry\tPostalCode\tPhone\tFax\tEmail\n",
      "1\tAdams\tAndrew\tGeneral Manager\tNone\t1962-02-18 00:00:00\t2002-08-14 00:00:00\t11120 Jasper Ave NW\tEdmonton\tAB\tCanada\tT5K 2N1\t+1 (780) 428-9482\t+1 (780) 428-3457\tandrew@chinookcorp.com\n",
      "2\tEdwards\tNancy\tSales Manager\t1\t1958-12-08 00:00:00\t2002-05-01 00:00:00\t825 8 Ave SW\tCalgary\tAB\tCanada\tT2P 2T3\t+1 (403) 262-3443\t+1 (403) 262-3322\tnancy@chinookcorp.com\n",
      "3\tPeacock\tJane\tSales Support Agent\t2\t1973-08-29 00:00:00\t2002-04-01 00:00:00\t1111 6 Ave SW\tCalgary\tAB\tCanada\tT2P 5M5\t+1 (403) 262-3443\t+1 (403) 262-6712\tjane@chinookcorp.com\n",
      "*/\n",
      "==================================================\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  model_check_query (call_PuoVwdB63oH0wZrarD5eYkrQ)\n",
      " Call ID: call_PuoVwdB63oH0wZrarD5eYkrQ\n",
      "  Args:\n",
      "    state: {'messages': [{'content': 'Andrew Adam 직원의 인적정보를 모두 조회해줘', 'type': 'human'}, {'content': 'Album, Artist, Customer, Employee, Genre, Invoice, InvoiceLine, MediaType, Playlist, PlaylistTrack, Track', 'type': 'function', 'name': 'sql_db_list_tables'}, {'content': '\\nCREATE TABLE \"Employee\" (\\n\\t\"EmployeeId\" INTEGER NOT NULL, \\n\\t\"LastName\" NVARCHAR(20) NOT NULL, \\n\\t\"FirstName\" NVARCHAR(20) NOT NULL, \\n\\t\"Title\" NVARCHAR(30), \\n\\t\"ReportsTo\" INTEGER, \\n\\t\"BirthDate\" DATETIME, \\n\\t\"HireDate\" DATETIME, \\n\\t\"Address\" NVARCHAR(70), \\n\\t\"City\" NVARCHAR(40), \\n\\t\"State\" NVARCHAR(40), \\n\\t\"Country\" NVARCHAR(40), \\n\\t\"PostalCode\" NVARCHAR(10), \\n\\t\"Phone\" NVARCHAR(24), \\n\\t\"Fax\" NVARCHAR(24), \\n\\t\"Email\" NVARCHAR(60), \\n\\tPRIMARY KEY (\"EmployeeId\"), \\n\\tFOREIGN KEY(\"ReportsTo\") REFERENCES \"Employee\" (\"EmployeeId\")\\n)\\n\\n/*\\n3 rows from Employee table:\\nEmployeeId\\tLastName\\tFirstName\\tTitle\\tReportsTo\\tBirthDate\\tHireDate\\tAddress\\tCity\\tState\\tCountry\\tPostalCode\\tPhone\\tFax\\tEmail\\n1\\tAdams\\tAndrew\\tGeneral Manager\\tNone\\t1962-02-18 00:00:00\\t2002-08-14 00:00:00\\t11120 Jasper Ave NW\\tEdmonton\\tAB\\tCanada\\tT5K 2N1\\t+1 (780) 428-9482\\t+1 (780) 428-3457\\tandrew@chinookcorp.com\\n2\\tEdwards\\tNancy\\tSales Manager\\t1\\t1958-12-08 00:00:00\\t2002-05-01 00:00:00\\t825 8 Ave SW\\tCalgary\\tAB\\tCanada\\tT2P 2T3\\t+1 (403) 262-3443\\t+1 (403) 262-3322\\tnancy@chinookcorp.com\\n3\\tPeacock\\tJane\\tSales Support Agent\\t2\\t1973-08-29 00:00:00\\t2002-04-01 00:00:00\\t1111 6 Ave SW\\tCalgary\\tAB\\tCanada\\tT2P 5M5\\t+1 (403) 262-3443\\t+1 (403) 262-6712\\tjane@chinookcorp.com\\n*/', 'type': 'function', 'name': 'sql_db_schema'}]}\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mquery_gen\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  model_check_query (call_PuoVwdB63oH0wZrarD5eYkrQ)\n",
      " Call ID: call_PuoVwdB63oH0wZrarD5eYkrQ\n",
      "  Args:\n",
      "    state: {'messages': [{'content': 'Andrew Adam 직원의 인적정보를 모두 조회해줘', 'type': 'human'}, {'content': 'Album, Artist, Customer, Employee, Genre, Invoice, InvoiceLine, MediaType, Playlist, PlaylistTrack, Track', 'type': 'function', 'name': 'sql_db_list_tables'}, {'content': '\\nCREATE TABLE \"Employee\" (\\n\\t\"EmployeeId\" INTEGER NOT NULL, \\n\\t\"LastName\" NVARCHAR(20) NOT NULL, \\n\\t\"FirstName\" NVARCHAR(20) NOT NULL, \\n\\t\"Title\" NVARCHAR(30), \\n\\t\"ReportsTo\" INTEGER, \\n\\t\"BirthDate\" DATETIME, \\n\\t\"HireDate\" DATETIME, \\n\\t\"Address\" NVARCHAR(70), \\n\\t\"City\" NVARCHAR(40), \\n\\t\"State\" NVARCHAR(40), \\n\\t\"Country\" NVARCHAR(40), \\n\\t\"PostalCode\" NVARCHAR(10), \\n\\t\"Phone\" NVARCHAR(24), \\n\\t\"Fax\" NVARCHAR(24), \\n\\t\"Email\" NVARCHAR(60), \\n\\tPRIMARY KEY (\"EmployeeId\"), \\n\\tFOREIGN KEY(\"ReportsTo\") REFERENCES \"Employee\" (\"EmployeeId\")\\n)\\n\\n/*\\n3 rows from Employee table:\\nEmployeeId\\tLastName\\tFirstName\\tTitle\\tReportsTo\\tBirthDate\\tHireDate\\tAddress\\tCity\\tState\\tCountry\\tPostalCode\\tPhone\\tFax\\tEmail\\n1\\tAdams\\tAndrew\\tGeneral Manager\\tNone\\t1962-02-18 00:00:00\\t2002-08-14 00:00:00\\t11120 Jasper Ave NW\\tEdmonton\\tAB\\tCanada\\tT5K 2N1\\t+1 (780) 428-9482\\t+1 (780) 428-3457\\tandrew@chinookcorp.com\\n2\\tEdwards\\tNancy\\tSales Manager\\t1\\t1958-12-08 00:00:00\\t2002-05-01 00:00:00\\t825 8 Ave SW\\tCalgary\\tAB\\tCanada\\tT2P 2T3\\t+1 (403) 262-3443\\t+1 (403) 262-3322\\tnancy@chinookcorp.com\\n3\\tPeacock\\tJane\\tSales Support Agent\\t2\\t1973-08-29 00:00:00\\t2002-04-01 00:00:00\\t1111 6 Ave SW\\tCalgary\\tAB\\tCanada\\tT2P 5M5\\t+1 (403) 262-3443\\t+1 (403) 262-6712\\tjane@chinookcorp.com\\n*/', 'type': 'function', 'name': 'sql_db_schema'}]}\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "\n",
      "Error: The wrong tool was called: model_check_query. Please fix your mistakes. Remember to only call SubmitFinalAnswer to submit the final answer. Generated queries should be outputted WITHOUT a tool call.\n",
      "==================================================\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "SELECT * FROM Employee WHERE FirstName = 'Andrew' AND LastName = 'Adams';\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mquery_gen\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "SELECT * FROM Employee WHERE FirstName = 'Andrew' AND LastName = 'Adams';\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mcorrect_query\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  db_query_tool (call_OEElD5MHC7UFGK0OmV4HTqmo)\n",
      " Call ID: call_OEElD5MHC7UFGK0OmV4HTqmo\n",
      "  Args:\n",
      "    query: SELECT * FROM Employee WHERE FirstName = 'Andrew' AND LastName = 'Adams';\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mexecute_query\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: db_query_tool\n",
      "\n",
      "[(1, 'Adams', 'Andrew', 'General Manager', None, '1962-02-18 00:00:00', '2002-08-14 00:00:00', '11120 Jasper Ave NW', 'Edmonton', 'AB', 'Canada', 'T5K 2N1', '+1 (780) 428-9482', '+1 (780) 428-3457', 'andrew@chinookcorp.com')]\n",
      "==================================================\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Answer: Andrew Adams의 인적정보는 다음과 같습니다:\n",
      "- 직책: General Manager\n",
      "- 생년월일: 1962-02-18\n",
      "- 고용일: 2002-08-14\n",
      "- 주소: 11120 Jasper Ave NW, Edmonton, AB, Canada, T5K 2N1\n",
      "- 전화번호: +1 (780) 428-9482\n",
      "- 팩스: +1 (780) 428-3457\n",
      "- 이메일: andrew@chinookcorp.com\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mquery_gen\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Answer: Andrew Adams의 인적정보는 다음과 같습니다:\n",
      "- 직책: General Manager\n",
      "- 생년월일: 1962-02-18\n",
      "- 고용일: 2002-08-14\n",
      "- 주소: 11120 Jasper Ave NW, Edmonton, AB, Canada, T5K 2N1\n",
      "- 전화번호: +1 (780) 428-9482\n",
      "- 팩스: +1 (780) 428-3457\n",
      "- 이메일: andrew@chinookcorp.com\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "output = run_graph(\n",
    "    \"Andrew Adam 직원의 인적정보를 모두 조회해줘\",\n",
    "    stream=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mfirst_tool_call\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  sql_db_list_tables (initial_tool_call_abc123)\n",
      " Call ID: initial_tool_call_abc123\n",
      "  Args:\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mlist_tables_tool\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: sql_db_list_tables\n",
      "\n",
      "Album, Artist, Customer, Employee, Genre, Invoice, InvoiceLine, MediaType, Playlist, PlaylistTrack, Track\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mmodel_get_schema\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  sql_db_schema (call_JMeyAhbiQQ6Hi8GhO56ir2Ds)\n",
      " Call ID: call_JMeyAhbiQQ6Hi8GhO56ir2Ds\n",
      "  Args:\n",
      "    table_names: Customer, Invoice\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mget_schema_tool\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: sql_db_schema\n",
      "\n",
      "\n",
      "CREATE TABLE \"Customer\" (\n",
      "\t\"CustomerId\" INTEGER NOT NULL, \n",
      "\t\"FirstName\" NVARCHAR(40) NOT NULL, \n",
      "\t\"LastName\" NVARCHAR(20) NOT NULL, \n",
      "\t\"Company\" NVARCHAR(80), \n",
      "\t\"Address\" NVARCHAR(70), \n",
      "\t\"City\" NVARCHAR(40), \n",
      "\t\"State\" NVARCHAR(40), \n",
      "\t\"Country\" NVARCHAR(40), \n",
      "\t\"PostalCode\" NVARCHAR(10), \n",
      "\t\"Phone\" NVARCHAR(24), \n",
      "\t\"Fax\" NVARCHAR(24), \n",
      "\t\"Email\" NVARCHAR(60) NOT NULL, \n",
      "\t\"SupportRepId\" INTEGER, \n",
      "\tPRIMARY KEY (\"CustomerId\"), \n",
      "\tFOREIGN KEY(\"SupportRepId\") REFERENCES \"Employee\" (\"EmployeeId\")\n",
      ")\n",
      "\n",
      "/*\n",
      "3 rows from Customer table:\n",
      "CustomerId\tFirstName\tLastName\tCompany\tAddress\tCity\tState\tCountry\tPostalCode\tPhone\tFax\tEmail\tSupportRepId\n",
      "1\tLuís\tGonçalves\tEmbraer - Empresa Brasileira de Aeronáutica S.A.\tAv. Brigadeiro Faria Lima, 2170\tSão José dos Campos\tSP\tBrazil\t12227-000\t+55 (12) 3923-5555\t+55 (12) 3923-5566\tluisg@embraer.com.br\t3\n",
      "2\tLeonie\tKöhler\tNone\tTheodor-Heuss-Straße 34\tStuttgart\tNone\tGermany\t70174\t+49 0711 2842222\tNone\tleonekohler@surfeu.de\t5\n",
      "3\tFrançois\tTremblay\tNone\t1498 rue Bélanger\tMontréal\tQC\tCanada\tH2G 1A7\t+1 (514) 721-4711\tNone\tftremblay@gmail.com\t3\n",
      "*/\n",
      "\n",
      "\n",
      "CREATE TABLE \"Invoice\" (\n",
      "\t\"InvoiceId\" INTEGER NOT NULL, \n",
      "\t\"CustomerId\" INTEGER NOT NULL, \n",
      "\t\"InvoiceDate\" DATETIME NOT NULL, \n",
      "\t\"BillingAddress\" NVARCHAR(70), \n",
      "\t\"BillingCity\" NVARCHAR(40), \n",
      "\t\"BillingState\" NVARCHAR(40), \n",
      "\t\"BillingCountry\" NVARCHAR(40), \n",
      "\t\"BillingPostalCode\" NVARCHAR(10), \n",
      "\t\"Total\" NUMERIC(10, 2) NOT NULL, \n",
      "\tPRIMARY KEY (\"InvoiceId\"), \n",
      "\tFOREIGN KEY(\"CustomerId\") REFERENCES \"Customer\" (\"CustomerId\")\n",
      ")\n",
      "\n",
      "/*\n",
      "3 rows from Invoice table:\n",
      "InvoiceId\tCustomerId\tInvoiceDate\tBillingAddress\tBillingCity\tBillingState\tBillingCountry\tBillingPostalCode\tTotal\n",
      "1\t2\t2009-01-01 00:00:00\tTheodor-Heuss-Straße 34\tStuttgart\tNone\tGermany\t70174\t1.98\n",
      "2\t4\t2009-01-02 00:00:00\tUllevålsveien 14\tOslo\tNone\tNorway\t0171\t3.96\n",
      "3\t8\t2009-01-03 00:00:00\tGrétrystraat 63\tBrussels\tNone\tBelgium\t1000\t5.94\n",
      "*/\n",
      "==================================================\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  model_check_query (call_jrHV3YKiROan9fx812HrLnka)\n",
      " Call ID: call_jrHV3YKiROan9fx812HrLnka\n",
      "  Args:\n",
      "    state: {'messages': [{'content': '2009년도에 어느 국가의 고객이 가장 많이 지출했을까요? 그리고 얼마를 지출했을까요? 한글로 답변하세요.', 'type': 'human'}, {'content': 'Album, Artist, Customer, Employee, Genre, Invoice, InvoiceLine, MediaType, Playlist, PlaylistTrack, Track', 'type': 'function', 'name': 'sql_db_list_tables'}, {'content': '{\"table_names\":\"Customer, Invoice\"}', 'type': 'function', 'name': 'sql_db_schema'}, {'content': '\\nCREATE TABLE \"Customer\" (\\n\\t\"CustomerId\" INTEGER NOT NULL, \\n\\t\"FirstName\" NVARCHAR(40) NOT NULL, \\n\\t\"LastName\" NVARCHAR(20) NOT NULL, \\n\\t\"Company\" NVARCHAR(80), \\n\\t\"Address\" NVARCHAR(70), \\n\\t\"City\" NVARCHAR(40), \\n\\t\"State\" NVARCHAR(40), \\n\\t\"Country\" NVARCHAR(40), \\n\\t\"PostalCode\" NVARCHAR(10), \\n\\t\"Phone\" NVARCHAR(24), \\n\\t\"Fax\" NVARCHAR(24), \\n\\t\"Email\" NVARCHAR(60) NOT NULL, \\n\\t\"SupportRepId\" INTEGER, \\n\\tPRIMARY KEY (\"CustomerId\"), \\n\\tFOREIGN KEY(\"SupportRepId\") REFERENCES \"Employee\" (\"EmployeeId\")\\n)\\n\\n/*\\n3 rows from Customer table:\\nCustomerId\\tFirstName\\tLastName\\tCompany\\tAddress\\tCity\\tState\\tCountry\\tPostalCode\\tPhone\\tFax\\tEmail\\tSupportRepId\\n1\\tLuís\\tGonçalves\\tEmbraer - Empresa Brasileira de Aeronáutica S.A.\\tAv. Brigadeiro Faria Lima, 2170\\tSão José dos Campos\\tSP\\tBrazil\\t12227-000\\t+55 (12) 3923-5555\\t+55 (12) 3923-5566\\tluisg@embraer.com.br\\t3\\n2\\tLeonie\\tKöhler\\tNone\\tTheodor-Heuss-Straße 34\\tStuttgart\\tNone\\tGermany\\t70174\\t+49 0711 2842222\\tNone\\tleonekohler@surfeu.de\\t5\\n3\\tFrançois\\tTremblay\\tNone\\t1498 rue Bélanger\\tMontréal\\tQC\\tCanada\\tH2G 1A7\\t+1 (514) 721-4711\\tNone\\tftremblay@gmail.com\\t3\\n*/\\n\\n\\nCREATE TABLE \"Invoice\" (\\n\\t\"InvoiceId\" INTEGER NOT NULL, \\n\\t\"CustomerId\" INTEGER NOT NULL, \\n\\t\"InvoiceDate\" DATETIME NOT NULL, \\n\\t\"BillingAddress\" NVARCHAR(70), \\n\\t\"BillingCity\" NVARCHAR(40), \\n\\t\"BillingState\" NVARCHAR(40), \\n\\t\"BillingCountry\" NVARCHAR(40), \\n\\t\"BillingPostalCode\" NVARCHAR(10), \\n\\t\"Total\" NUMERIC(10, 2) NOT NULL, \\n\\tPRIMARY KEY (\"InvoiceId\"), \\n\\tFOREIGN KEY(\"CustomerId\") REFERENCES \"Customer\" (\"CustomerId\")\\n)\\n\\n/*\\n3 rows from Invoice table:\\nInvoiceId\\tCustomerId\\tInvoiceDate\\tBillingAddress\\tBillingCity\\tBillingState\\tBillingCountry\\tBillingPostalCode\\tTotal\\n1\\t2\\t2009-01-01 00:00:00\\tTheodor-Heuss-Straße 34\\tStuttgart\\tNone\\tGermany\\t70174\\t1.98\\n2\\t4\\t2009-01-02 00:00:00\\tUllevålsveien 14\\tOslo\\tNone\\tNorway\\t0171\\t3.96\\n3\\t8\\t2009-01-03 00:00:00\\tGrétrystraat 63\\tBrussels\\tNone\\tBelgium\\t1000\\t5.94\\n*/', 'type': 'function', 'name': 'sql_db_schema'}]}\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mquery_gen\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  model_check_query (call_jrHV3YKiROan9fx812HrLnka)\n",
      " Call ID: call_jrHV3YKiROan9fx812HrLnka\n",
      "  Args:\n",
      "    state: {'messages': [{'content': '2009년도에 어느 국가의 고객이 가장 많이 지출했을까요? 그리고 얼마를 지출했을까요? 한글로 답변하세요.', 'type': 'human'}, {'content': 'Album, Artist, Customer, Employee, Genre, Invoice, InvoiceLine, MediaType, Playlist, PlaylistTrack, Track', 'type': 'function', 'name': 'sql_db_list_tables'}, {'content': '{\"table_names\":\"Customer, Invoice\"}', 'type': 'function', 'name': 'sql_db_schema'}, {'content': '\\nCREATE TABLE \"Customer\" (\\n\\t\"CustomerId\" INTEGER NOT NULL, \\n\\t\"FirstName\" NVARCHAR(40) NOT NULL, \\n\\t\"LastName\" NVARCHAR(20) NOT NULL, \\n\\t\"Company\" NVARCHAR(80), \\n\\t\"Address\" NVARCHAR(70), \\n\\t\"City\" NVARCHAR(40), \\n\\t\"State\" NVARCHAR(40), \\n\\t\"Country\" NVARCHAR(40), \\n\\t\"PostalCode\" NVARCHAR(10), \\n\\t\"Phone\" NVARCHAR(24), \\n\\t\"Fax\" NVARCHAR(24), \\n\\t\"Email\" NVARCHAR(60) NOT NULL, \\n\\t\"SupportRepId\" INTEGER, \\n\\tPRIMARY KEY (\"CustomerId\"), \\n\\tFOREIGN KEY(\"SupportRepId\") REFERENCES \"Employee\" (\"EmployeeId\")\\n)\\n\\n/*\\n3 rows from Customer table:\\nCustomerId\\tFirstName\\tLastName\\tCompany\\tAddress\\tCity\\tState\\tCountry\\tPostalCode\\tPhone\\tFax\\tEmail\\tSupportRepId\\n1\\tLuís\\tGonçalves\\tEmbraer - Empresa Brasileira de Aeronáutica S.A.\\tAv. Brigadeiro Faria Lima, 2170\\tSão José dos Campos\\tSP\\tBrazil\\t12227-000\\t+55 (12) 3923-5555\\t+55 (12) 3923-5566\\tluisg@embraer.com.br\\t3\\n2\\tLeonie\\tKöhler\\tNone\\tTheodor-Heuss-Straße 34\\tStuttgart\\tNone\\tGermany\\t70174\\t+49 0711 2842222\\tNone\\tleonekohler@surfeu.de\\t5\\n3\\tFrançois\\tTremblay\\tNone\\t1498 rue Bélanger\\tMontréal\\tQC\\tCanada\\tH2G 1A7\\t+1 (514) 721-4711\\tNone\\tftremblay@gmail.com\\t3\\n*/\\n\\n\\nCREATE TABLE \"Invoice\" (\\n\\t\"InvoiceId\" INTEGER NOT NULL, \\n\\t\"CustomerId\" INTEGER NOT NULL, \\n\\t\"InvoiceDate\" DATETIME NOT NULL, \\n\\t\"BillingAddress\" NVARCHAR(70), \\n\\t\"BillingCity\" NVARCHAR(40), \\n\\t\"BillingState\" NVARCHAR(40), \\n\\t\"BillingCountry\" NVARCHAR(40), \\n\\t\"BillingPostalCode\" NVARCHAR(10), \\n\\t\"Total\" NUMERIC(10, 2) NOT NULL, \\n\\tPRIMARY KEY (\"InvoiceId\"), \\n\\tFOREIGN KEY(\"CustomerId\") REFERENCES \"Customer\" (\"CustomerId\")\\n)\\n\\n/*\\n3 rows from Invoice table:\\nInvoiceId\\tCustomerId\\tInvoiceDate\\tBillingAddress\\tBillingCity\\tBillingState\\tBillingCountry\\tBillingPostalCode\\tTotal\\n1\\t2\\t2009-01-01 00:00:00\\tTheodor-Heuss-Straße 34\\tStuttgart\\tNone\\tGermany\\t70174\\t1.98\\n2\\t4\\t2009-01-02 00:00:00\\tUllevålsveien 14\\tOslo\\tNone\\tNorway\\t0171\\t3.96\\n3\\t8\\t2009-01-03 00:00:00\\tGrétrystraat 63\\tBrussels\\tNone\\tBelgium\\t1000\\t5.94\\n*/', 'type': 'function', 'name': 'sql_db_schema'}]}\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "\n",
      "Error: The wrong tool was called: model_check_query. Please fix your mistakes. Remember to only call SubmitFinalAnswer to submit the final answer. Generated queries should be outputted WITHOUT a tool call.\n",
      "==================================================\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "```sql\n",
      "SELECT c.Country, SUM(i.Total) AS TotalSpent\n",
      "FROM Customer c\n",
      "JOIN Invoice i ON c.CustomerId = i.CustomerId\n",
      "WHERE strftime('%Y', i.InvoiceDate) = '2009'\n",
      "GROUP BY c.Country\n",
      "ORDER BY TotalSpent DESC\n",
      "LIMIT 1;\n",
      "```\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mquery_gen\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "```sql\n",
      "SELECT c.Country, SUM(i.Total) AS TotalSpent\n",
      "FROM Customer c\n",
      "JOIN Invoice i ON c.CustomerId = i.CustomerId\n",
      "WHERE strftime('%Y', i.InvoiceDate) = '2009'\n",
      "GROUP BY c.Country\n",
      "ORDER BY TotalSpent DESC\n",
      "LIMIT 1;\n",
      "```\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mcorrect_query\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  db_query_tool (call_XEefsOTcslitTwO71dnvvmJ0)\n",
      " Call ID: call_XEefsOTcslitTwO71dnvvmJ0\n",
      "  Args:\n",
      "    query: SELECT c.Country, SUM(i.Total) AS TotalSpent\n",
      "FROM Customer c\n",
      "JOIN Invoice i ON c.CustomerId = i.CustomerId\n",
      "WHERE strftime('%Y', i.InvoiceDate) = '2009'\n",
      "GROUP BY c.Country\n",
      "ORDER BY TotalSpent DESC\n",
      "LIMIT 1;\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mexecute_query\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: db_query_tool\n",
      "\n",
      "[('USA', 103.95)]\n",
      "==================================================\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Answer: 2009년도에 미국(USA) 고객이 가장 많이 지출했으며, 총 지출액은 103.95입니다.\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mquery_gen\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Answer: 2009년도에 미국(USA) 고객이 가장 많이 지출했으며, 총 지출액은 103.95입니다.\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "output = run_graph(\n",
    "    \"2009년도에 어느 국가의 고객이 가장 많이 지출했을까요? 그리고 얼마를 지출했을까요? 한글로 답변하세요.\",\n",
    "    stream=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "# 클라이언트 초기화\n",
    "client = Client()\n",
    "\n",
    "# 데이터셋 생성 및 업로드\n",
    "examples = [\n",
    "    (\n",
    "        \"Which country's customers spent the most? And how much did they spend?\",\n",
    "        \"The country whose customers spent the most is the USA, with a total spending of 523.06.\",\n",
    "    ),\n",
    "    (\n",
    "        \"What was the most purchased track of 2013?\",\n",
    "        \"The most purchased track of 2013 was Hot Girl.\",\n",
    "    ),\n",
    "    (\n",
    "        \"How many albums does the artist Led Zeppelin have?\",\n",
    "        \"Led Zeppelin has 14 albums\",\n",
    "    ),\n",
    "    (\n",
    "        \"What is the total price for the album “Big Ones”?\",\n",
    "        \"The total price for the album 'Big Ones' is 14.85\",\n",
    "    ),\n",
    "    (\n",
    "        \"Which sales agent made the most in sales in 2009?\",\n",
    "        \"Steve Johnson made the most sales in 2009\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "dataset_name = \"SQL Agent Response\"\n",
    "\n",
    "if not client.has_dataset(dataset_name=dataset_name):\n",
    "    dataset = client.create_dataset(dataset_name=dataset_name)\n",
    "    inputs, outputs = zip(\n",
    "        *[({\"input\": text}, {\"output\": label}) for text, label in examples]\n",
    "    )\n",
    "    client.create_examples(inputs=inputs, outputs=outputs, dataset_id=dataset.id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 에이전트의 SQL 쿼리 응답을 예측하기 위한 함수 정의\n",
    "def predict_sql_agent_answer(example: dict):\n",
    "    \"\"\"Use this for answer evaluation\"\"\"\n",
    "    config = RunnableConfig(configurable={\"thread_id\": random_uuid()})\n",
    "\n",
    "    inputs = {\n",
    "        \"messages\": [HumanMessage(content=example[\"input\"])],\n",
    "    }\n",
    "    # 그래프를 실행하여 메시지 결과 조회\n",
    "    messages = app.invoke(inputs, config)\n",
    "    answer = messages[\"messages\"][-1].content\n",
    "    # 결과 반환\n",
    "    return {\"response\": answer}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Grade prompt\n",
    "grade_prompt_answer_accuracy = hub.pull(\"langchain-ai/rag-answer-vs-reference\")\n",
    "\n",
    "\n",
    "# 답변 평가자 LLM-as-judge 정의\n",
    "def answer_evaluator(run, example) -> dict:\n",
    "    # input: 질문\n",
    "    input_question = example.inputs[\"input\"]\n",
    "    # output: 참조 답변\n",
    "    reference = example.outputs[\"output\"]\n",
    "    # 예측 답변\n",
    "    prediction = run.outputs[\"response\"]\n",
    "\n",
    "    # LLM 평가자 초기화\n",
    "    llm = ChatOpenAI(model=MODEL_NAME, temperature=0)\n",
    "    answer_grader = grade_prompt_answer_accuracy | llm\n",
    "\n",
    "    # 평가자 실행\n",
    "    score = answer_grader.invoke(\n",
    "        {\n",
    "            \"question\": input_question,\n",
    "            \"correct_answer\": reference,\n",
    "            \"student_answer\": prediction,\n",
    "        }\n",
    "    )\n",
    "    score = score[\"Score\"]\n",
    "\n",
    "    # 점수 반환\n",
    "    return {\"key\": \"answer_v_reference_score\", \"score\": score}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'sql-agent-eval-b55dacd7' at:\n",
      "https://smith.langchain.com/o/53db61eb-d386-4c02-9de8-5b28476b0edc/datasets/4c01e9c9-dfee-427a-8251-cebd69bbc095/compare?selectedSessions=f1513783-462d-43a7-8642-be3dfb0a3e9b\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad37ab8e1ca9441fa02d8dde0b0b0cd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  model_check_query (call_BVj9xlvpkd5bAp6j6YFo1ZOF)\n",
      " Call ID: call_BVj9xlvpkd5bAp6j6YFo1ZOF\n",
      "  Args:\n",
      "    state: {'messages': [{'content': 'What was the most purchased track of 2013?', 'type': 'human'}, {'content': 'CREATE TABLE \"Invoice\" (\\n\\t\"InvoiceId\" INTEGER NOT NULL, \\n\\t\"CustomerId\" INTEGER NOT NULL, \\n\\t\"InvoiceDate\" DATETIME NOT NULL, \\n\\t\"BillingAddress\" NVARCHAR(70), \\n\\t\"BillingCity\" NVARCHAR(40), \\n\\t\"BillingState\" NVARCHAR(40), \\n\\t\"BillingCountry\" NVARCHAR(40), \\n\\t\"BillingPostalCode\" NVARCHAR(10), \\n\\t\"Total\" NUMERIC(10, 2) NOT NULL, \\n\\tPRIMARY KEY (\"InvoiceId\"), \\n\\tFOREIGN KEY(\"CustomerId\") REFERENCES \"Customer\" (\"CustomerId\")\\n)\\n\\n/*\\n3 rows from Invoice table:\\nInvoiceId\\tCustomerId\\tInvoiceDate\\tBillingAddress\\tBillingCity\\tBillingState\\tBillingCountry\\tBillingPostalCode\\tTotal\\n1\\t2\\t2009-01-01 00:00:00\\tTheodor-Heuss-Straße 34\\tStuttgart\\tNone\\tGermany\\t70174\\t1.98\\n2\\t4\\t2009-01-02 00:00:00\\tUllevålsveien 14\\tOslo\\tNone\\tNorway\\t0171\\t3.96\\n3\\t8\\t2009-01-03 00:00:00\\tGrétrystraat 63\\tBrussels\\tNone\\tBelgium\\t1000\\t5.94\\n]', 'type': 'ai'}, {'content': 'CREATE TABLE \"InvoiceLine\" (\\n\\t\"InvoiceLineId\" INTEGER NOT NULL, \\n\\t\"InvoiceId\" INTEGER NOT NULL, \\n\\t\"TrackId\" INTEGER NOT NULL, \\n\\t\"UnitPrice\" NUMERIC(10, 2) NOT NULL, \\n\\t\"Quantity\" INTEGER NOT NULL, \\n\\tPRIMARY KEY (\"InvoiceLineId\"), \\n\\tFOREIGN KEY(\"TrackId\") REFERENCES \"Track\" (\"TrackId\"), \\n\\tFOREIGN KEY(\"InvoiceId\") REFERENCES \"Invoice\" (\"InvoiceId\")\\n)\\n\\n/*\\n3 rows from InvoiceLine table:\\nInvoiceLineId\\tInvoiceId\\tTrackId\\tUnitPrice\\tQuantity\\n1\\t1\\t2\\t0.99\\t1\\n2\\t1\\t4\\t0.99\\t1\\n3\\t2\\t6\\t0.99\\t1\\n]', 'type': 'ai'}, {'content': 'CREATE TABLE \"Track\" (\\n\\t\"TrackId\" INTEGER NOT NULL, \\n\\t\"Name\" NVARCHAR(200) NOT NULL, \\n\\t\"AlbumId\" INTEGER, \\n\\t\"MediaTypeId\" INTEGER NOT NULL, \\n\\t\"GenreId\" INTEGER, \\n\\t\"Composer\" NVARCHAR(220), \\n\\t\"Milliseconds\" INTEGER NOT NULL, \\n\\t\"Bytes\" INTEGER, \\n\\t\"UnitPrice\" NUMERIC(10, 2) NOT NULL, \\n\\tPRIMARY KEY (\"TrackId\"), \\n\\tFOREIGN KEY(\"MediaTypeId\") REFERENCES \"MediaType\" (\"MediaTypeId\"), \\n\\tFOREIGN KEY(\"GenreId\") REFERENCES \"Genre\" (\"GenreId\"), \\n\\tFOREIGN KEY(\"AlbumId\") REFERENCES \"Album\" (\"AlbumId\")\\n)\\n\\n/*\\n3 rows from Track table:\\nTrackId\\tName\\tAlbumId\\tMediaTypeId\\tGenreId\\tComposer\\tMilliseconds\\tBytes\\tUnitPrice\\n1\\tFor Those About To Rock (We Salute You)\\t1\\t1\\t1\\tAngus Young, Malcolm Young, Brian Johnson\\t343719\\t11170334\\t0.99\\n2\\tBalls to the Wall\\t2\\t2\\t1\\tNone\\t342562\\t5510424\\t0.99\\n3\\tFast As a Shark\\t3\\t2\\t1\\tF. Baltes, S. Kaufman, U. Dirkscneider & W. Hoffman\\t230619\\t3990994\\t0.99\\n]', 'type': 'ai'}]}\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "```sql\n",
      "SELECT \n",
      "    t.Name, \n",
      "    SUM(il.Quantity) AS TotalPurchases\n",
      "FROM \n",
      "    Track t\n",
      "JOIN \n",
      "    InvoiceLine il ON t.TrackId = il.TrackId\n",
      "JOIN \n",
      "    Invoice i ON il.InvoiceId = i.InvoiceId\n",
      "WHERE \n",
      "    strftime('%Y', i.InvoiceDate) = '2013'\n",
      "GROUP BY \n",
      "    t.TrackId\n",
      "ORDER BY \n",
      "    TotalPurchases DESC\n",
      "LIMIT 1;\n",
      "```\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Answer: The most purchased track of 2013 was \"Hot Girl\" with 1 purchase.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  model_check_query (call_tk3zfYh3Kc0VeCJnNOYNpAn5)\n",
      " Call ID: call_tk3zfYh3Kc0VeCJnNOYNpAn5\n",
      "  Args:\n",
      "    state: {'messages': [{'content': \"Which country's customers spent the most? And how much did they spend?\", 'type': 'human'}, {'content': 'SELECT Customer.Country, SUM(Invoice.Total) as TotalSpent FROM Customer JOIN Invoice ON Customer.CustomerId = Invoice.CustomerId GROUP BY Customer.Country ORDER BY TotalSpent DESC LIMIT 1;', 'type': 'ai'}]}\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "SELECT Customer.Country, SUM(Invoice.Total) as TotalSpent FROM Customer JOIN Invoice ON Customer.CustomerId = Invoice.CustomerId GROUP BY Customer.Country ORDER BY TotalSpent DESC LIMIT 1;\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Answer: The country's customers who spent the most are from the USA, with a total spending of 523.06.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  model_check_query (call_TQ16mXLb3XDpunzOBcdNBBAp)\n",
      " Call ID: call_TQ16mXLb3XDpunzOBcdNBBAp\n",
      "  Args:\n",
      "    state: {'messages': [{'content': 'How many albums does the artist Led Zeppelin have?', 'type': 'human'}, {'content': 'Album, Artist, Customer, Employee, Genre, Invoice, InvoiceLine, MediaType, Playlist, PlaylistTrack, Track', 'type': 'function', 'name': 'sql_db_list_tables'}, {'content': '\\nCREATE TABLE \"Album\" (\\n\\t\"AlbumId\" INTEGER NOT NULL, \\n\\t\"Title\" NVARCHAR(160) NOT NULL, \\n\\t\"ArtistId\" INTEGER NOT NULL, \\n\\tPRIMARY KEY (\"AlbumId\"), \\n\\tFOREIGN KEY(\"ArtistId\") REFERENCES \"Artist\" (\"ArtistId\")\\n)\\n\\n/*\\n3 rows from Album table:\\nAlbumId\\tTitle\\tArtistId\\n1\\tFor Those About To Rock We Salute You\\t1\\n2\\tBalls to the Wall\\t2\\n3\\tRestless and Wild\\t2\\n*/\\n\\n\\nCREATE TABLE \"Artist\" (\\n\\t\"ArtistId\" INTEGER NOT NULL, \\n\\t\"Name\" NVARCHAR(120), \\n\\tPRIMARY KEY (\"ArtistId\")\\n)\\n\\n/*\\n3 rows from Artist table:\\nArtistId\\tName\\n1\\tAC/DC\\n2\\tAccept\\n3\\tAerosmith\\n*/', 'type': 'function', 'name': 'sql_db_schema'}]}\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "SELECT COUNT(*) \n",
      "FROM Album \n",
      "JOIN Artist ON Album.ArtistId = Artist.ArtistId \n",
      "WHERE Artist.Name = 'Led Zeppelin';\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Answer: Led Zeppelin has 14 albums.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "To determine which sales agent made the most in sales in 2009, we need to join the `Employee`, `Invoice`, and `InvoiceLine` tables. We will filter the invoices by the year 2009 and sum the total sales for each sales agent. Here's the query to achieve that:\n",
      "\n",
      "```sql\n",
      "SELECT \n",
      "    e.FirstName || ' ' || e.LastName AS SalesAgent,\n",
      "    SUM(i.Total) AS TotalSales\n",
      "FROM \n",
      "    Employee e\n",
      "JOIN \n",
      "    Customer c ON e.EmployeeId = c.SupportRepId\n",
      "JOIN \n",
      "    Invoice i ON c.CustomerId = i.CustomerId\n",
      "WHERE \n",
      "    strftime('%Y', i.InvoiceDate) = '2009'\n",
      "GROUP BY \n",
      "    e.EmployeeId\n",
      "ORDER BY \n",
      "    TotalSales DESC\n",
      "LIMIT 1;\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error running target function: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-Soffyuxth7vsUnobnzk0MGeR on tokens per min (TPM): Limit 30000, Used 28954, Requested 1073. Please try again in 54ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/evaluation/_runner.py\", line 1924, in _forward\n",
      "    fn(*args, langsmith_extra=langsmith_extra)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/run_helpers.py\", line 703, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"/var/folders/sj/3r49bs6178v7nfhcxh7lpzd80000gn/T/ipykernel_43129/2799409917.py\", line 10, in predict_sql_agent_answer\n",
      "    messages = app.invoke(inputs, config)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/main.py\", line 3026, in invoke\n",
      "    for chunk in self.stream(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/main.py\", line 2647, in stream\n",
      "    for _ in runner.tick(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/_runner.py\", line 162, in tick\n",
      "    run_with_retry(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/_retry.py\", line 42, in run_with_retry\n",
      "    return task.proc.invoke(task.input, config)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/_internal/_runnable.py\", line 657, in invoke\n",
      "    input = context.run(step.invoke, input, config, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/_internal/_runnable.py\", line 401, in invoke\n",
      "    ret = self.func(*args, **kwargs)\n",
      "  File \"/var/folders/sj/3r49bs6178v7nfhcxh7lpzd80000gn/T/ipykernel_43129/266772150.py\", line 116, in query_gen_node\n",
      "    message = query_gen.invoke(state)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3082, in invoke\n",
      "    input_ = context.run(step.invoke, input_, config)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 5495, in invoke\n",
      "    return self.bound.invoke(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 393, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 1019, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 837, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 1085, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_openai/chat_models/base.py\", line 1183, in _generate\n",
      "    raise e\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_openai/chat_models/base.py\", line 1178, in _generate\n",
      "    raw_response = self.client.with_raw_response.create(**payload)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_legacy_response.py\", line 364, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], func(*args, **kwargs))\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1147, in create\n",
      "    return self._post(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-Soffyuxth7vsUnobnzk0MGeR on tokens per min (TPM): Limit 30000, Used 28954, Requested 1073. Please try again in 54ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running evaluator <DynamicRunEvaluator answer_evaluator> on run da51e121-9705-4207-bada-8ead922c3367: KeyError('response')\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/evaluation/_runner.py\", line 1620, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/evaluation/evaluator.py\", line 351, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/run_helpers.py\", line 703, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"/var/folders/sj/3r49bs6178v7nfhcxh7lpzd80000gn/T/ipykernel_43129/316870680.py\", line 15, in answer_evaluator\n",
      "    prediction = run.outputs[\"response\"]\n",
      "KeyError: 'response'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  model_check_query (call_OHFP0VpCv4485aDnVHL91kiS)\n",
      " Call ID: call_OHFP0VpCv4485aDnVHL91kiS\n",
      "  Args:\n",
      "    state: {'messages': [{'content': 'What is the total price for the album “Big Ones”?', 'type': 'human'}, {'content': 'Album, Artist, Customer, Employee, Genre, Invoice, InvoiceLine, MediaType, Playlist, PlaylistTrack, Track', 'type': 'function', 'name': 'sql_db_list_tables'}, {'content': 'CREATE TABLE \"Album\" (\\n\\t\"AlbumId\" INTEGER NOT NULL, \\n\\t\"Title\" NVARCHAR(160) NOT NULL, \\n\\t\"ArtistId\" INTEGER NOT NULL, \\n\\tPRIMARY KEY (\"AlbumId\"), \\n\\tFOREIGN KEY(\"ArtistId\") REFERENCES \"Artist\" (\"ArtistId\")\\n)\\n\\n/*\\n3 rows from Album table:\\nAlbumId\\tTitle\\tArtistId\\n1\\tFor Those About To Rock We Salute You\\t1\\n2\\tBalls to the Wall\\t2\\n3\\tRestless and Wild\\t2\\n*/', 'type': 'function', 'name': 'sql_db_schema'}, {'content': 'CREATE TABLE \"InvoiceLine\" (\\n\\t\"InvoiceLineId\" INTEGER NOT NULL, \\n\\t\"InvoiceId\" INTEGER NOT NULL, \\n\\t\"TrackId\" INTEGER NOT NULL, \\n\\t\"UnitPrice\" NUMERIC(10, 2) NOT NULL, \\n\\t\"Quantity\" INTEGER NOT NULL, \\n\\tPRIMARY KEY (\"InvoiceLineId\"), \\n\\tFOREIGN KEY(\"TrackId\") REFERENCES \"Track\" (\"TrackId\"), \\n\\tFOREIGN KEY(\"InvoiceId\") REFERENCES \"Invoice\" (\"InvoiceId\")\\n)\\n\\n/*\\n3 rows from InvoiceLine table:\\nInvoiceLineId\\tInvoiceId\\tTrackId\\tUnitPrice\\tQuantity\\n1\\t1\\t2\\t0.99\\t1\\n2\\t1\\t4\\t0.99\\t1\\n3\\t2\\t6\\t0.99\\t1\\n*/', 'type': 'function', 'name': 'sql_db_schema'}]}\n",
      "  model_check_query (call_sQEFPg8QnNFNiG76jfUDYzha)\n",
      " Call ID: call_sQEFPg8QnNFNiG76jfUDYzha\n",
      "  Args:\n",
      "    state: {'messages': [{'content': 'What is the total price for the album “Big Ones”?', 'type': 'human'}, {'content': 'Album, Artist, Customer, Employee, Genre, Invoice, InvoiceLine, MediaType, Playlist, PlaylistTrack, Track', 'type': 'function', 'name': 'sql_db_list_tables'}, {'content': 'CREATE TABLE \"Album\" (\\n\\t\"AlbumId\" INTEGER NOT NULL, \\n\\t\"Title\" NVARCHAR(160) NOT NULL, \\n\\t\"ArtistId\" INTEGER NOT NULL, \\n\\tPRIMARY KEY (\"AlbumId\"), \\n\\tFOREIGN KEY(\"ArtistId\") REFERENCES \"Artist\" (\"ArtistId\")\\n)\\n\\n/*\\n3 rows from Album table:\\nAlbumId\\tTitle\\tArtistId\\n1\\tFor Those About To Rock We Salute You\\t1\\n2\\tBalls to the Wall\\t2\\n3\\tRestless and Wild\\t2\\n*/', 'type': 'function', 'name': 'sql_db_schema'}, {'content': 'CREATE TABLE \"InvoiceLine\" (\\n\\t\"InvoiceLineId\" INTEGER NOT NULL, \\n\\t\"InvoiceId\" INTEGER NOT NULL, \\n\\t\"TrackId\" INTEGER NOT NULL, \\n\\t\"UnitPrice\" NUMERIC(10, 2) NOT NULL, \\n\\t\"Quantity\" INTEGER NOT NULL, \\n\\tPRIMARY KEY (\"InvoiceLineId\"), \\n\\tFOREIGN KEY(\"TrackId\") REFERENCES \"Track\" (\"TrackId\"), \\n\\tFOREIGN KEY(\"InvoiceId\") REFERENCES \"Invoice\" (\"InvoiceId\")\\n)\\n\\n/*\\n3 rows from InvoiceLine table:\\nInvoiceLineId\\tInvoiceId\\tTrackId\\tUnitPrice\\tQuantity\\n1\\t1\\t2\\t0.99\\t1\\n2\\t1\\t4\\t0.99\\t1\\n3\\t2\\t6\\t0.99\\t1\\n*/', 'type': 'function', 'name': 'sql_db_schema'}]}\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "To find the total price for the album \"Big Ones\", we need to join the Album, Track, and InvoiceLine tables. Here's the query to achieve that:\n",
      "\n",
      "```sql\n",
      "SELECT SUM(InvoiceLine.UnitPrice * InvoiceLine.Quantity) AS TotalPrice\n",
      "FROM Album\n",
      "JOIN Track ON Album.AlbumId = Track.AlbumId\n",
      "JOIN InvoiceLine ON Track.TrackId = InvoiceLine.TrackId\n",
      "WHERE Album.Title = 'Big Ones';\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error running target function: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-Soffyuxth7vsUnobnzk0MGeR on tokens per min (TPM): Limit 30000, Used 30000, Requested 706. Please try again in 1.412s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/evaluation/_runner.py\", line 1924, in _forward\n",
      "    fn(*args, langsmith_extra=langsmith_extra)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/run_helpers.py\", line 703, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"/var/folders/sj/3r49bs6178v7nfhcxh7lpzd80000gn/T/ipykernel_43129/2799409917.py\", line 10, in predict_sql_agent_answer\n",
      "    messages = app.invoke(inputs, config)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/main.py\", line 3026, in invoke\n",
      "    for chunk in self.stream(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/main.py\", line 2647, in stream\n",
      "    for _ in runner.tick(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/_runner.py\", line 162, in tick\n",
      "    run_with_retry(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/_retry.py\", line 42, in run_with_retry\n",
      "    return task.proc.invoke(task.input, config)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/_internal/_runnable.py\", line 657, in invoke\n",
      "    input = context.run(step.invoke, input, config, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/_internal/_runnable.py\", line 401, in invoke\n",
      "    ret = self.func(*args, **kwargs)\n",
      "  File \"/var/folders/sj/3r49bs6178v7nfhcxh7lpzd80000gn/T/ipykernel_43129/266772150.py\", line 116, in query_gen_node\n",
      "    message = query_gen.invoke(state)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3082, in invoke\n",
      "    input_ = context.run(step.invoke, input_, config)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 5495, in invoke\n",
      "    return self.bound.invoke(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 393, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 1019, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 837, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 1085, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_openai/chat_models/base.py\", line 1183, in _generate\n",
      "    raise e\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_openai/chat_models/base.py\", line 1178, in _generate\n",
      "    raw_response = self.client.with_raw_response.create(**payload)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_legacy_response.py\", line 364, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], func(*args, **kwargs))\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1147, in create\n",
      "    return self._post(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-Soffyuxth7vsUnobnzk0MGeR on tokens per min (TPM): Limit 30000, Used 30000, Requested 706. Please try again in 1.412s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running evaluator <DynamicRunEvaluator answer_evaluator> on run 643b50b9-f8ef-47d4-9251-b3aef268cb0b: KeyError('response')\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/evaluation/_runner.py\", line 1620, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/evaluation/evaluator.py\", line 351, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/run_helpers.py\", line 703, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"/var/folders/sj/3r49bs6178v7nfhcxh7lpzd80000gn/T/ipykernel_43129/316870680.py\", line 15, in answer_evaluator\n",
      "    prediction = run.outputs[\"response\"]\n",
      "KeyError: 'response'\n",
      "Error running target function: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-Soffyuxth7vsUnobnzk0MGeR on tokens per min (TPM): Limit 30000, Used 30000, Requested 41. Please try again in 82ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/evaluation/_runner.py\", line 1924, in _forward\n",
      "    fn(*args, langsmith_extra=langsmith_extra)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/run_helpers.py\", line 703, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"/var/folders/sj/3r49bs6178v7nfhcxh7lpzd80000gn/T/ipykernel_43129/2799409917.py\", line 10, in predict_sql_agent_answer\n",
      "    messages = app.invoke(inputs, config)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/main.py\", line 3026, in invoke\n",
      "    for chunk in self.stream(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/main.py\", line 2647, in stream\n",
      "    for _ in runner.tick(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/_runner.py\", line 162, in tick\n",
      "    run_with_retry(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/_retry.py\", line 42, in run_with_retry\n",
      "    return task.proc.invoke(task.input, config)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/_internal/_runnable.py\", line 657, in invoke\n",
      "    input = context.run(step.invoke, input, config, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/_internal/_runnable.py\", line 401, in invoke\n",
      "    ret = self.func(*args, **kwargs)\n",
      "  File \"/var/folders/sj/3r49bs6178v7nfhcxh7lpzd80000gn/T/ipykernel_43129/266772150.py\", line 65, in <lambda>\n",
      "    \"messages\": [model_get_schema.invoke(state[\"messages\"])],\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 5495, in invoke\n",
      "    return self.bound.invoke(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 393, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 1019, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 837, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 1085, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_openai/chat_models/base.py\", line 1183, in _generate\n",
      "    raise e\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_openai/chat_models/base.py\", line 1178, in _generate\n",
      "    raw_response = self.client.with_raw_response.create(**payload)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_legacy_response.py\", line 364, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], func(*args, **kwargs))\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1147, in create\n",
      "    return self._post(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-Soffyuxth7vsUnobnzk0MGeR on tokens per min (TPM): Limit 30000, Used 30000, Requested 41. Please try again in 82ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running evaluator <DynamicRunEvaluator answer_evaluator> on run dfc83b27-63ba-40a2-ac36-32a593c30323: KeyError('response')\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/evaluation/_runner.py\", line 1620, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/evaluation/evaluator.py\", line 351, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/run_helpers.py\", line 703, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"/var/folders/sj/3r49bs6178v7nfhcxh7lpzd80000gn/T/ipykernel_43129/316870680.py\", line 15, in answer_evaluator\n",
      "    prediction = run.outputs[\"response\"]\n",
      "KeyError: 'response'\n",
      "Error running target function: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-Soffyuxth7vsUnobnzk0MGeR on tokens per min (TPM): Limit 30000, Used 30000, Requested 48. Please try again in 96ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/evaluation/_runner.py\", line 1924, in _forward\n",
      "    fn(*args, langsmith_extra=langsmith_extra)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/run_helpers.py\", line 703, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"/var/folders/sj/3r49bs6178v7nfhcxh7lpzd80000gn/T/ipykernel_43129/2799409917.py\", line 10, in predict_sql_agent_answer\n",
      "    messages = app.invoke(inputs, config)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/main.py\", line 3026, in invoke\n",
      "    for chunk in self.stream(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/main.py\", line 2647, in stream\n",
      "    for _ in runner.tick(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/_runner.py\", line 162, in tick\n",
      "    run_with_retry(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/_retry.py\", line 42, in run_with_retry\n",
      "    return task.proc.invoke(task.input, config)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/_internal/_runnable.py\", line 657, in invoke\n",
      "    input = context.run(step.invoke, input, config, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/_internal/_runnable.py\", line 401, in invoke\n",
      "    ret = self.func(*args, **kwargs)\n",
      "  File \"/var/folders/sj/3r49bs6178v7nfhcxh7lpzd80000gn/T/ipykernel_43129/266772150.py\", line 65, in <lambda>\n",
      "    \"messages\": [model_get_schema.invoke(state[\"messages\"])],\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 5495, in invoke\n",
      "    return self.bound.invoke(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 393, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 1019, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 837, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 1085, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_openai/chat_models/base.py\", line 1183, in _generate\n",
      "    raise e\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_openai/chat_models/base.py\", line 1178, in _generate\n",
      "    raw_response = self.client.with_raw_response.create(**payload)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_legacy_response.py\", line 364, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], func(*args, **kwargs))\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1147, in create\n",
      "    return self._post(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-Soffyuxth7vsUnobnzk0MGeR on tokens per min (TPM): Limit 30000, Used 30000, Requested 48. Please try again in 96ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running evaluator <DynamicRunEvaluator answer_evaluator> on run 4088f72c-55a2-4d4a-b859-8fcf0f319732: KeyError('response')\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/evaluation/_runner.py\", line 1620, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/evaluation/evaluator.py\", line 351, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/run_helpers.py\", line 703, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"/var/folders/sj/3r49bs6178v7nfhcxh7lpzd80000gn/T/ipykernel_43129/316870680.py\", line 15, in answer_evaluator\n",
      "    prediction = run.outputs[\"response\"]\n",
      "KeyError: 'response'\n",
      "Error running target function: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-Soffyuxth7vsUnobnzk0MGeR on tokens per min (TPM): Limit 30000, Used 29979, Requested 43. Please try again in 44ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/evaluation/_runner.py\", line 1924, in _forward\n",
      "    fn(*args, langsmith_extra=langsmith_extra)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/run_helpers.py\", line 703, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"/var/folders/sj/3r49bs6178v7nfhcxh7lpzd80000gn/T/ipykernel_43129/2799409917.py\", line 10, in predict_sql_agent_answer\n",
      "    messages = app.invoke(inputs, config)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/main.py\", line 3026, in invoke\n",
      "    for chunk in self.stream(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/main.py\", line 2647, in stream\n",
      "    for _ in runner.tick(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/_runner.py\", line 162, in tick\n",
      "    run_with_retry(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/_retry.py\", line 42, in run_with_retry\n",
      "    return task.proc.invoke(task.input, config)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/_internal/_runnable.py\", line 657, in invoke\n",
      "    input = context.run(step.invoke, input, config, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/_internal/_runnable.py\", line 401, in invoke\n",
      "    ret = self.func(*args, **kwargs)\n",
      "  File \"/var/folders/sj/3r49bs6178v7nfhcxh7lpzd80000gn/T/ipykernel_43129/266772150.py\", line 65, in <lambda>\n",
      "    \"messages\": [model_get_schema.invoke(state[\"messages\"])],\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 5495, in invoke\n",
      "    return self.bound.invoke(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 393, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 1019, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 837, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 1085, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_openai/chat_models/base.py\", line 1183, in _generate\n",
      "    raise e\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_openai/chat_models/base.py\", line 1178, in _generate\n",
      "    raw_response = self.client.with_raw_response.create(**payload)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_legacy_response.py\", line 364, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], func(*args, **kwargs))\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1147, in create\n",
      "    return self._post(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-Soffyuxth7vsUnobnzk0MGeR on tokens per min (TPM): Limit 30000, Used 29979, Requested 43. Please try again in 44ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running evaluator <DynamicRunEvaluator answer_evaluator> on run 625bfa86-60e8-4952-84bf-4117fd3a08ba: KeyError('response')\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/evaluation/_runner.py\", line 1620, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/evaluation/evaluator.py\", line 351, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/run_helpers.py\", line 703, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"/var/folders/sj/3r49bs6178v7nfhcxh7lpzd80000gn/T/ipykernel_43129/316870680.py\", line 15, in answer_evaluator\n",
      "    prediction = run.outputs[\"response\"]\n",
      "KeyError: 'response'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  model_check_query (call_YrcMwHj4LaZWfUQiTPvf9jOI)\n",
      " Call ID: call_YrcMwHj4LaZWfUQiTPvf9jOI\n",
      "  Args:\n",
      "    state: {'messages': [{'content': 'Which sales agent made the most in sales in 2009?', 'type': 'human'}, {'content': 'Album, Artist, Customer, Employee, Genre, Invoice, InvoiceLine, MediaType, Playlist, PlaylistTrack, Track', 'type': 'function', 'name': 'sql_db_list_tables', 'id': 'b1b0b3b3-0b0b-4b0b-9b0b-0b0b0b0b0b0b'}, {'content': 'parallel', 'type': 'tool_use', 'tool_calls': [{'name': 'functions.sql_db_schema', 'args': '{\"table_names\":\"Employee\"}', 'id': 'b1b0b3b3-0b0b-4b0b-9b0b-0b0b0b0b0b0b'}, {'name': 'functions.sql_db_schema', 'args': '{\"table_names\":\"Invoice\"}', 'id': 'b1b0b3b3-0b0b-4b0b-9b0b-0b0b0b0b0b0b'}], 'id': 'b1b0b3b3-0b0b-4b0b-9b0b-0b0b0b0b0b0b'}, {'content': '\\nCREATE TABLE \"Employee\" (\\n\\t\"EmployeeId\" INTEGER NOT NULL, \\n\\t\"LastName\" NVARCHAR(20) NOT NULL, \\n\\t\"FirstName\" NVARCHAR(20) NOT NULL, \\n\\t\"Title\" NVARCHAR(30), \\n\\t\"ReportsTo\" INTEGER, \\n\\t\"BirthDate\" DATETIME, \\n\\t\"HireDate\" DATETIME, \\n\\t\"Address\" NVARCHAR(70), \\n\\t\"City\" NVARCHAR(40), \\n\\t\"State\" NVARCHAR(40), \\n\\t\"Country\" NVARCHAR(40), \\n\\t\"PostalCode\" NVARCHAR(10), \\n\\t\"Phone\" NVARCHAR(24), \\n\\t\"Fax\" NVARCHAR(24), \\n\\t\"Email\" NVARCHAR(60), \\n\\tPRIMARY KEY (\"EmployeeId\"), \\n\\tFOREIGN KEY(\"ReportsTo\") REFERENCES \"Employee\" (\"EmployeeId\")\\n)\\n\\n/*\\n3 rows from Employee table:\\nEmployeeId\\tLastName\\tFirstName\\tTitle\\tReportsTo\\tBirthDate\\tHireDate\\tAddress\\tCity\\tState\\tCountry\\tPostalCode\\tPhone\\tFax\\tEmail\\n1\\tAdams\\tAndrew\\tGeneral Manager\\tNone\\t1962-02-18 00:00:00\\t2002-08-14 00:00:00\\t11120 Jasper Ave NW\\tEdmonton\\tAB\\tCanada\\tT5K 2N1\\t+1 (780) 428-9482\\t+1 (780) 428-3457\\tandrew@chinookcorp.com\\n2\\tEdwards\\tNancy\\tSales Manager\\t1\\t1958-12-08 00:00:00\\t2002-05-01 00:00:00\\t825 8 Ave SW\\tCalgary\\tAB\\tCanada\\tT2P 2T3\\t+1 (403) 262-3443\\t+1 (403) 262-3322\\tnancy@chinookcorp.com\\n3\\tPeacock\\tJane\\tSales Support Agent\\t2\\t1973-08-29 00:00:00\\t2002-04-01 00:00:00\\t1111 6 Ave SW\\tCalgary\\tAB\\tCanada\\tT2P 5M5\\t+1 (403) 262-3443\\t+1 (403) 262-6712\\tjane@chinookcorp.com\\n*/', 'type': 'function', 'name': 'sql_db_schema', 'id': 'b1b0b3b3-0b0b-4b0b-9b0b-0b0b0b0b0b0'}, {'content': '\\nCREATE TABLE \"Invoice\" (\\n\\t\"InvoiceId\" INTEGER NOT NULL, \\n\\t\"CustomerId\" INTEGER NOT NULL, \\n\\t\"InvoiceDate\" DATETIME NOT NULL, \\n\\t\"BillingAddress\" NVARCHAR(70), \\n\\t\"BillingCity\" NVARCHAR(40), \\n\\t\"BillingState\" NVARCHAR(40), \\n\\t\"BillingCountry\" NVARCHAR(40), \\n\\t\"BillingPostalCode\" NVARCHAR(10), \\n\\t\"Total\" NUMERIC(10, 2) NOT NULL, \\n\\tPRIMARY KEY (\"InvoiceId\"), \\n\\tFOREIGN KEY(\"CustomerId\") REFERENCES \"Customer\" (\"CustomerId\")\\n)\\n\\n/*\\n3 rows from Invoice table:\\nInvoiceId\\tCustomerId\\tInvoiceDate\\tBillingAddress\\tBillingCity\\tBillingState\\tBillingCountry\\tBillingPostalCode\\tTotal\\n1\\t2\\t2009-01-01 00:00:00\\tTheodor-Heuss-Straße 34\\tStuttgart\\tNone\\tGermany\\t70174\\t1.98\\n2\\t4\\t2009-01-02 00:00:00\\tUllevålsveien 14\\tOslo\\tNone\\tNorway\\t0171\\t3.96\\n3\\t8\\t2009-01-03 00:00:00\\tGrétrystraat 63\\tBrussels\\tNone\\tBelgium\\t1000\\t5.94\\n*/', 'type': 'function', 'name': 'sql_db_schema', 'id': 'b1b0b3b3-0b0b-4b0b-9b0b-0b0b0b0b0b0'}]}\n",
      "    query: SELECT e.FirstName, e.LastName, SUM(i.Total) as TotalSales FROM Employee e JOIN Customer c ON e.EmployeeId = c.SupportRepId JOIN Invoice i ON c.CustomerId = i.CustomerId WHERE strftime('%Y', i.InvoiceDate) = '2009' GROUP BY e.EmployeeId ORDER BY TotalSales DESC LIMIT 1;\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "SELECT e.FirstName, e.LastName, SUM(i.Total) as TotalSales\n",
      "FROM Employee e\n",
      "JOIN Customer c ON e.EmployeeId = c.SupportRepId\n",
      "JOIN Invoice i ON c.CustomerId = i.CustomerId\n",
      "WHERE strftime('%Y', i.InvoiceDate) = '2009'\n",
      "GROUP BY e.EmployeeId\n",
      "ORDER BY TotalSales DESC\n",
      "LIMIT 1;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error running target function: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-Soffyuxth7vsUnobnzk0MGeR on tokens per min (TPM): Limit 30000, Used 30000, Requested 229. Please try again in 458ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/evaluation/_runner.py\", line 1924, in _forward\n",
      "    fn(*args, langsmith_extra=langsmith_extra)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/run_helpers.py\", line 703, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"/var/folders/sj/3r49bs6178v7nfhcxh7lpzd80000gn/T/ipykernel_43129/2799409917.py\", line 10, in predict_sql_agent_answer\n",
      "    messages = app.invoke(inputs, config)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/main.py\", line 3026, in invoke\n",
      "    for chunk in self.stream(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/main.py\", line 2647, in stream\n",
      "    for _ in runner.tick(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/_runner.py\", line 162, in tick\n",
      "    run_with_retry(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/_retry.py\", line 42, in run_with_retry\n",
      "    return task.proc.invoke(task.input, config)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/_internal/_runnable.py\", line 657, in invoke\n",
      "    input = context.run(step.invoke, input, config, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/_internal/_runnable.py\", line 401, in invoke\n",
      "    ret = self.func(*args, **kwargs)\n",
      "  File \"/var/folders/sj/3r49bs6178v7nfhcxh7lpzd80000gn/T/ipykernel_43129/266772150.py\", line 46, in model_check_query\n",
      "    return {\"messages\": [query_check.invoke({\"messages\": [state[\"messages\"][-1]]})]}\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3082, in invoke\n",
      "    input_ = context.run(step.invoke, input_, config)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 5495, in invoke\n",
      "    return self.bound.invoke(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 393, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 1019, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 837, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 1085, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_openai/chat_models/base.py\", line 1183, in _generate\n",
      "    raise e\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_openai/chat_models/base.py\", line 1178, in _generate\n",
      "    raw_response = self.client.with_raw_response.create(**payload)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_legacy_response.py\", line 364, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], func(*args, **kwargs))\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1147, in create\n",
      "    return self._post(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-Soffyuxth7vsUnobnzk0MGeR on tokens per min (TPM): Limit 30000, Used 30000, Requested 229. Please try again in 458ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running evaluator <DynamicRunEvaluator answer_evaluator> on run 94efafea-3ca4-4da2-9dcc-67f46e2d87ce: KeyError('response')\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/evaluation/_runner.py\", line 1620, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/evaluation/evaluator.py\", line 351, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/run_helpers.py\", line 703, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"/var/folders/sj/3r49bs6178v7nfhcxh7lpzd80000gn/T/ipykernel_43129/316870680.py\", line 15, in answer_evaluator\n",
      "    prediction = run.outputs[\"response\"]\n",
      "KeyError: 'response'\n",
      "Error running target function: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-Soffyuxth7vsUnobnzk0MGeR on tokens per min (TPM): Limit 30000, Used 30000, Requested 44. Please try again in 88ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/evaluation/_runner.py\", line 1924, in _forward\n",
      "    fn(*args, langsmith_extra=langsmith_extra)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/run_helpers.py\", line 703, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"/var/folders/sj/3r49bs6178v7nfhcxh7lpzd80000gn/T/ipykernel_43129/2799409917.py\", line 10, in predict_sql_agent_answer\n",
      "    messages = app.invoke(inputs, config)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/main.py\", line 3026, in invoke\n",
      "    for chunk in self.stream(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/main.py\", line 2647, in stream\n",
      "    for _ in runner.tick(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/_runner.py\", line 162, in tick\n",
      "    run_with_retry(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/_retry.py\", line 42, in run_with_retry\n",
      "    return task.proc.invoke(task.input, config)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/_internal/_runnable.py\", line 657, in invoke\n",
      "    input = context.run(step.invoke, input, config, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/_internal/_runnable.py\", line 401, in invoke\n",
      "    ret = self.func(*args, **kwargs)\n",
      "  File \"/var/folders/sj/3r49bs6178v7nfhcxh7lpzd80000gn/T/ipykernel_43129/266772150.py\", line 65, in <lambda>\n",
      "    \"messages\": [model_get_schema.invoke(state[\"messages\"])],\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 5495, in invoke\n",
      "    return self.bound.invoke(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 393, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 1019, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 837, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 1085, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_openai/chat_models/base.py\", line 1183, in _generate\n",
      "    raise e\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_openai/chat_models/base.py\", line 1178, in _generate\n",
      "    raw_response = self.client.with_raw_response.create(**payload)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_legacy_response.py\", line 364, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], func(*args, **kwargs))\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1147, in create\n",
      "    return self._post(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-Soffyuxth7vsUnobnzk0MGeR on tokens per min (TPM): Limit 30000, Used 30000, Requested 44. Please try again in 88ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running evaluator <DynamicRunEvaluator answer_evaluator> on run e2debb50-3f49-4afb-8b6c-cf1841105bf7: KeyError('response')\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/evaluation/_runner.py\", line 1620, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/evaluation/evaluator.py\", line 351, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/run_helpers.py\", line 703, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"/var/folders/sj/3r49bs6178v7nfhcxh7lpzd80000gn/T/ipykernel_43129/316870680.py\", line 15, in answer_evaluator\n",
      "    prediction = run.outputs[\"response\"]\n",
      "KeyError: 'response'\n",
      "Error running target function: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-Soffyuxth7vsUnobnzk0MGeR on tokens per min (TPM): Limit 30000, Used 30000, Requested 41. Please try again in 82ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/evaluation/_runner.py\", line 1924, in _forward\n",
      "    fn(*args, langsmith_extra=langsmith_extra)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/run_helpers.py\", line 703, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"/var/folders/sj/3r49bs6178v7nfhcxh7lpzd80000gn/T/ipykernel_43129/2799409917.py\", line 10, in predict_sql_agent_answer\n",
      "    messages = app.invoke(inputs, config)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/main.py\", line 3026, in invoke\n",
      "    for chunk in self.stream(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/main.py\", line 2647, in stream\n",
      "    for _ in runner.tick(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/_runner.py\", line 162, in tick\n",
      "    run_with_retry(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/_retry.py\", line 42, in run_with_retry\n",
      "    return task.proc.invoke(task.input, config)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/_internal/_runnable.py\", line 657, in invoke\n",
      "    input = context.run(step.invoke, input, config, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/_internal/_runnable.py\", line 401, in invoke\n",
      "    ret = self.func(*args, **kwargs)\n",
      "  File \"/var/folders/sj/3r49bs6178v7nfhcxh7lpzd80000gn/T/ipykernel_43129/266772150.py\", line 65, in <lambda>\n",
      "    \"messages\": [model_get_schema.invoke(state[\"messages\"])],\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 5495, in invoke\n",
      "    return self.bound.invoke(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 393, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 1019, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 837, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 1085, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_openai/chat_models/base.py\", line 1183, in _generate\n",
      "    raise e\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_openai/chat_models/base.py\", line 1178, in _generate\n",
      "    raw_response = self.client.with_raw_response.create(**payload)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_legacy_response.py\", line 364, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], func(*args, **kwargs))\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1147, in create\n",
      "    return self._post(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-Soffyuxth7vsUnobnzk0MGeR on tokens per min (TPM): Limit 30000, Used 30000, Requested 41. Please try again in 82ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running evaluator <DynamicRunEvaluator answer_evaluator> on run 9daf147d-e828-4a07-b626-92e435e16157: KeyError('response')\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/evaluation/_runner.py\", line 1620, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/evaluation/evaluator.py\", line 351, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/run_helpers.py\", line 703, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"/var/folders/sj/3r49bs6178v7nfhcxh7lpzd80000gn/T/ipykernel_43129/316870680.py\", line 15, in answer_evaluator\n",
      "    prediction = run.outputs[\"response\"]\n",
      "KeyError: 'response'\n",
      "Error running target function: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-Soffyuxth7vsUnobnzk0MGeR on tokens per min (TPM): Limit 30000, Used 30000, Requested 48. Please try again in 96ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/evaluation/_runner.py\", line 1924, in _forward\n",
      "    fn(*args, langsmith_extra=langsmith_extra)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/run_helpers.py\", line 703, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"/var/folders/sj/3r49bs6178v7nfhcxh7lpzd80000gn/T/ipykernel_43129/2799409917.py\", line 10, in predict_sql_agent_answer\n",
      "    messages = app.invoke(inputs, config)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/main.py\", line 3026, in invoke\n",
      "    for chunk in self.stream(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/main.py\", line 2647, in stream\n",
      "    for _ in runner.tick(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/_runner.py\", line 162, in tick\n",
      "    run_with_retry(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/_retry.py\", line 42, in run_with_retry\n",
      "    return task.proc.invoke(task.input, config)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/_internal/_runnable.py\", line 657, in invoke\n",
      "    input = context.run(step.invoke, input, config, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/_internal/_runnable.py\", line 401, in invoke\n",
      "    ret = self.func(*args, **kwargs)\n",
      "  File \"/var/folders/sj/3r49bs6178v7nfhcxh7lpzd80000gn/T/ipykernel_43129/266772150.py\", line 65, in <lambda>\n",
      "    \"messages\": [model_get_schema.invoke(state[\"messages\"])],\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 5495, in invoke\n",
      "    return self.bound.invoke(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 393, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 1019, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 837, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 1085, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_openai/chat_models/base.py\", line 1183, in _generate\n",
      "    raise e\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_openai/chat_models/base.py\", line 1178, in _generate\n",
      "    raw_response = self.client.with_raw_response.create(**payload)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_legacy_response.py\", line 364, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], func(*args, **kwargs))\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1147, in create\n",
      "    return self._post(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-Soffyuxth7vsUnobnzk0MGeR on tokens per min (TPM): Limit 30000, Used 30000, Requested 48. Please try again in 96ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running evaluator <DynamicRunEvaluator answer_evaluator> on run 198465a6-314e-4da1-9909-a955d09e158f: KeyError('response')\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/evaluation/_runner.py\", line 1620, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/evaluation/evaluator.py\", line 351, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/run_helpers.py\", line 703, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"/var/folders/sj/3r49bs6178v7nfhcxh7lpzd80000gn/T/ipykernel_43129/316870680.py\", line 15, in answer_evaluator\n",
      "    prediction = run.outputs[\"response\"]\n",
      "KeyError: 'response'\n",
      "Error running target function: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-Soffyuxth7vsUnobnzk0MGeR on tokens per min (TPM): Limit 30000, Used 30000, Requested 43. Please try again in 86ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/evaluation/_runner.py\", line 1924, in _forward\n",
      "    fn(*args, langsmith_extra=langsmith_extra)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/run_helpers.py\", line 703, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"/var/folders/sj/3r49bs6178v7nfhcxh7lpzd80000gn/T/ipykernel_43129/2799409917.py\", line 10, in predict_sql_agent_answer\n",
      "    messages = app.invoke(inputs, config)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/main.py\", line 3026, in invoke\n",
      "    for chunk in self.stream(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/main.py\", line 2647, in stream\n",
      "    for _ in runner.tick(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/_runner.py\", line 162, in tick\n",
      "    run_with_retry(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/_retry.py\", line 42, in run_with_retry\n",
      "    return task.proc.invoke(task.input, config)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/_internal/_runnable.py\", line 657, in invoke\n",
      "    input = context.run(step.invoke, input, config, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/_internal/_runnable.py\", line 401, in invoke\n",
      "    ret = self.func(*args, **kwargs)\n",
      "  File \"/var/folders/sj/3r49bs6178v7nfhcxh7lpzd80000gn/T/ipykernel_43129/266772150.py\", line 65, in <lambda>\n",
      "    \"messages\": [model_get_schema.invoke(state[\"messages\"])],\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 5495, in invoke\n",
      "    return self.bound.invoke(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 393, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 1019, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 837, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 1085, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_openai/chat_models/base.py\", line 1183, in _generate\n",
      "    raise e\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_openai/chat_models/base.py\", line 1178, in _generate\n",
      "    raw_response = self.client.with_raw_response.create(**payload)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_legacy_response.py\", line 364, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], func(*args, **kwargs))\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1147, in create\n",
      "    return self._post(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-Soffyuxth7vsUnobnzk0MGeR on tokens per min (TPM): Limit 30000, Used 30000, Requested 43. Please try again in 86ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running evaluator <DynamicRunEvaluator answer_evaluator> on run fe89b6bf-abb3-403e-a2f1-5d336bb7fdde: KeyError('response')\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/evaluation/_runner.py\", line 1620, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/evaluation/evaluator.py\", line 351, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/run_helpers.py\", line 703, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"/var/folders/sj/3r49bs6178v7nfhcxh7lpzd80000gn/T/ipykernel_43129/316870680.py\", line 15, in answer_evaluator\n",
      "    prediction = run.outputs[\"response\"]\n",
      "KeyError: 'response'\n",
      "Error running target function: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-Soffyuxth7vsUnobnzk0MGeR on tokens per min (TPM): Limit 30000, Used 30000, Requested 43. Please try again in 86ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/evaluation/_runner.py\", line 1924, in _forward\n",
      "    fn(*args, langsmith_extra=langsmith_extra)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/run_helpers.py\", line 703, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"/var/folders/sj/3r49bs6178v7nfhcxh7lpzd80000gn/T/ipykernel_43129/2799409917.py\", line 10, in predict_sql_agent_answer\n",
      "    messages = app.invoke(inputs, config)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/main.py\", line 3026, in invoke\n",
      "    for chunk in self.stream(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/main.py\", line 2647, in stream\n",
      "    for _ in runner.tick(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/_runner.py\", line 162, in tick\n",
      "    run_with_retry(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/_retry.py\", line 42, in run_with_retry\n",
      "    return task.proc.invoke(task.input, config)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/_internal/_runnable.py\", line 657, in invoke\n",
      "    input = context.run(step.invoke, input, config, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/_internal/_runnable.py\", line 401, in invoke\n",
      "    ret = self.func(*args, **kwargs)\n",
      "  File \"/var/folders/sj/3r49bs6178v7nfhcxh7lpzd80000gn/T/ipykernel_43129/266772150.py\", line 65, in <lambda>\n",
      "    \"messages\": [model_get_schema.invoke(state[\"messages\"])],\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 5495, in invoke\n",
      "    return self.bound.invoke(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 393, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 1019, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 837, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 1085, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_openai/chat_models/base.py\", line 1183, in _generate\n",
      "    raise e\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_openai/chat_models/base.py\", line 1178, in _generate\n",
      "    raw_response = self.client.with_raw_response.create(**payload)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_legacy_response.py\", line 364, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], func(*args, **kwargs))\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1147, in create\n",
      "    return self._post(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-Soffyuxth7vsUnobnzk0MGeR on tokens per min (TPM): Limit 30000, Used 30000, Requested 43. Please try again in 86ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running evaluator <DynamicRunEvaluator answer_evaluator> on run 8e773812-02df-40ff-be20-f7fedacf6ebb: KeyError('response')\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/evaluation/_runner.py\", line 1620, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/evaluation/evaluator.py\", line 351, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/run_helpers.py\", line 703, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"/var/folders/sj/3r49bs6178v7nfhcxh7lpzd80000gn/T/ipykernel_43129/316870680.py\", line 15, in answer_evaluator\n",
      "    prediction = run.outputs[\"response\"]\n",
      "KeyError: 'response'\n",
      "Error running target function: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-Soffyuxth7vsUnobnzk0MGeR on tokens per min (TPM): Limit 30000, Used 30000, Requested 44. Please try again in 88ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/evaluation/_runner.py\", line 1924, in _forward\n",
      "    fn(*args, langsmith_extra=langsmith_extra)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/run_helpers.py\", line 703, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"/var/folders/sj/3r49bs6178v7nfhcxh7lpzd80000gn/T/ipykernel_43129/2799409917.py\", line 10, in predict_sql_agent_answer\n",
      "    messages = app.invoke(inputs, config)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/main.py\", line 3026, in invoke\n",
      "    for chunk in self.stream(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/main.py\", line 2647, in stream\n",
      "    for _ in runner.tick(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/_runner.py\", line 162, in tick\n",
      "    run_with_retry(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/_retry.py\", line 42, in run_with_retry\n",
      "    return task.proc.invoke(task.input, config)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/_internal/_runnable.py\", line 657, in invoke\n",
      "    input = context.run(step.invoke, input, config, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/_internal/_runnable.py\", line 401, in invoke\n",
      "    ret = self.func(*args, **kwargs)\n",
      "  File \"/var/folders/sj/3r49bs6178v7nfhcxh7lpzd80000gn/T/ipykernel_43129/266772150.py\", line 65, in <lambda>\n",
      "    \"messages\": [model_get_schema.invoke(state[\"messages\"])],\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 5495, in invoke\n",
      "    return self.bound.invoke(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 393, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 1019, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 837, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 1085, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_openai/chat_models/base.py\", line 1183, in _generate\n",
      "    raise e\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_openai/chat_models/base.py\", line 1178, in _generate\n",
      "    raw_response = self.client.with_raw_response.create(**payload)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_legacy_response.py\", line 364, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], func(*args, **kwargs))\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1147, in create\n",
      "    return self._post(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-Soffyuxth7vsUnobnzk0MGeR on tokens per min (TPM): Limit 30000, Used 30000, Requested 44. Please try again in 88ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running evaluator <DynamicRunEvaluator answer_evaluator> on run 78983cfa-c42f-4f24-962f-8d6cf57637e8: KeyError('response')\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/evaluation/_runner.py\", line 1620, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/evaluation/evaluator.py\", line 351, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langsmith/run_helpers.py\", line 703, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"/var/folders/sj/3r49bs6178v7nfhcxh7lpzd80000gn/T/ipykernel_43129/316870680.py\", line 15, in answer_evaluator\n",
      "    prediction = run.outputs[\"response\"]\n",
      "KeyError: 'response'\n"
     ]
    }
   ],
   "source": [
    "from langsmith.evaluation import evaluate\n",
    "\n",
    "# 평가용 데이터셋 이름\n",
    "dataset_name = \"SQL Agent Response\"\n",
    "\n",
    "try:\n",
    "    # 평가 진행\n",
    "    experiment_results = evaluate(\n",
    "        predict_sql_agent_answer,  # 평가시 활용할 예측 함수\n",
    "        data=dataset_name,  # 평가용 데이터셋 이름\n",
    "        evaluators=[answer_evaluator],  # 평가자 목록\n",
    "        num_repetitions=3,  # 실험 반복 횟수 설정\n",
    "        experiment_prefix=\"sql-agent-eval\",\n",
    "        metadata={\"version\": \"chinook db, sql-agent-eval: gpt-4o\"},  # 실험 메타데이터\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. STORM 개념을 도입한 연구를 위한 멀티 에이전트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote.models import get_model_name, LLMs\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 최신 모델 가져오기\n",
    "GPT4o = get_model_name(LLMs.GPT4o)\n",
    "\n",
    "# 모델 초기화\n",
    "llm = ChatOpenAI(model=GPT4o)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from typing_extensions import TypedDict\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_teddynote.graphs import visualize_graph\n",
    "\n",
    "\n",
    "# 분석가의 속성과 메타데이터를 정의하는 클래스\n",
    "class Analyst(BaseModel):\n",
    "    # 주요 소속 정보\n",
    "    affiliation: str = Field(\n",
    "        description=\"Primary affiliation of the analyst.\",\n",
    "    )\n",
    "    # 이름\n",
    "    name: str = Field(description=\"Name of the analyst.\")\n",
    "\n",
    "    # 역할\n",
    "    role: str = Field(\n",
    "        description=\"Role of the analyst in the context of the topic.\",\n",
    "    )\n",
    "    # 중점, 우려 사항 및 동기에 대한 설명\n",
    "    description: str = Field(\n",
    "        description=\"Description of the analyst focus, concerns, and motives.\",\n",
    "    )\n",
    "\n",
    "    # 분석가의 인적 정보를 문자열로 반환하는 속성\n",
    "    @property\n",
    "    def persona(self) -> str:\n",
    "        return f\"Name: {self.name}\\nRole: {self.role}\\nAffiliation: {self.affiliation}\\nDescription: {self.description}\\n\"\n",
    "\n",
    "\n",
    "# 분석가들의 집합\n",
    "class Perspectives(BaseModel):\n",
    "    # 분석가 목록\n",
    "    analysts: List[Analyst] = Field(\n",
    "        description=\"Comprehensive list of analysts with their roles and affiliations.\",\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 상태 정의\n",
    "class GenerateAnalystsState(TypedDict):\n",
    "    # 연구 주제\n",
    "    topic: str\n",
    "    # 생성할 분석가의 최대 수\n",
    "    max_analysts: int\n",
    "    # 사람 피드백\n",
    "    human_analyst_feedback: str\n",
    "    # 분석가 목록\n",
    "    analysts: List[Analyst]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "# 분석가 생성 프롬프트\n",
    "analyst_instructions = \"\"\"You are tasked with creating a set of AI analyst personas. \n",
    "\n",
    "Follow these instructions carefully:\n",
    "1. First, review the research topic:\n",
    "\n",
    "{topic}\n",
    "\n",
    "2. Examine any editorial feedback that has been optionally provided to guide creation of the analysts: \n",
    "\n",
    "{human_analyst_feedback}\n",
    "\n",
    "3. Determine the most interesting themes based upon documents and / or feedback above.\n",
    "\n",
    "4. Pick the top {max_analysts} themes.\n",
    "\n",
    "5. Assign one analyst to each theme.\"\"\"\n",
    "\n",
    "\n",
    "# 분석가 생성 노드\n",
    "def create_analysts(state: GenerateAnalystsState):\n",
    "    \"\"\"분석가 페르소나를 생성하는 함수\"\"\"\n",
    "\n",
    "    topic = state[\"topic\"]\n",
    "    max_analysts = state[\"max_analysts\"]\n",
    "    human_analyst_feedback = state.get(\"human_analyst_feedback\", \"\")\n",
    "\n",
    "    # LLM에 구조화된 출력 형식을 적용\n",
    "    structured_llm = llm.with_structured_output(Perspectives)\n",
    "\n",
    "    # 분석가 생성을 위한 시스템 프롬프트 구성\n",
    "    system_message = analyst_instructions.format(\n",
    "        topic=topic,\n",
    "        human_analyst_feedback=human_analyst_feedback,\n",
    "        max_analysts=max_analysts,\n",
    "    )\n",
    "\n",
    "    # LLM을 호출하여 분석가 페르소나 생성\n",
    "    analysts = structured_llm.invoke(\n",
    "        [SystemMessage(content=system_message)]\n",
    "        + [HumanMessage(content=\"Generate the set of analysts.\")]\n",
    "    )\n",
    "\n",
    "    # 생성된 분석가 목록을 상태에 저장\n",
    "    return {\"analysts\": analysts.analysts}\n",
    "\n",
    "\n",
    "# 사용자 피드백 노드(상태 업데이트를 진행할 예정이므로, 내용은 비워 두어도 무방)\n",
    "def human_feedback(state: GenerateAnalystsState):\n",
    "    \"\"\"사용자 피드백을 받기 위한 중단점 노드\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "# 인간 피드백 여부에 따라 워크플로우의 다음 단계를 결정하는 함수\n",
    "def should_continue(state: GenerateAnalystsState):\n",
    "    \"\"\"워크플로우의 다음 단계를 결정하는 함수\"\"\"\n",
    "\n",
    "    human_analyst_feedback = state.get(\"human_analyst_feedback\", None)\n",
    "    if human_analyst_feedback:\n",
    "        return \"create_analysts\"\n",
    "\n",
    "    return END\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langgraph.graph import START, END, StateGraph\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "# 그래프 생성\n",
    "builder = StateGraph(GenerateAnalystsState)\n",
    "\n",
    "# 노드 추가\n",
    "builder.add_node(\"create_analysts\", create_analysts)\n",
    "builder.add_node(\"human_feedback\", human_feedback)\n",
    "\n",
    "# 엣지 연결\n",
    "builder.add_edge(START, \"create_analysts\")\n",
    "builder.add_edge(\"create_analysts\", \"human_feedback\")\n",
    "\n",
    "# 조건부 엣지 추가: 사람 피드백이 있을 경우 다시 분석가 생성 노드로 돌아갑니다.\n",
    "builder.add_conditional_edges(\n",
    "    \"human_feedback\", should_continue, [\"create_analysts\", END]\n",
    ")\n",
    "\n",
    "# 메모리 생성\n",
    "memory = MemorySaver()\n",
    "\n",
    "# 그래프 컴파일(중단점 설정)\n",
    "graph = builder.compile(interrupt_before=[\"human_feedback\"], checkpointer=memory)\n",
    "\n",
    "# 그래프 시각화\n",
    "visualize_graph(graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mcreate_analysts\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "affiliation='AI Research Lab' name='Dr. Emily Tan' role='AI Systems Researcher' description='Dr. Tan focuses on the architectural and performance improvements of AI models. Her major interest is in how Modular RAG can improve flexibility and scalability compared to Naive RAG, particularly in large-scale AI systems. She analyzes how the modular approach allows for more efficient resource management and adaptability in various production environments.'\n",
      "affiliation='Tech Industry Observer' name='Michael Chen' role='Tech Industry Analyst' description=\"Michael observes trends and innovations in AI technologies as they apply to industry use. He studies the commercial advantages that Modular RAG offers over Naive RAG, such as ease of integration into existing workflows and the potential for cost reduction. Michael's interest lies in understanding how these features can lead to increased adoption in business settings.\"\n",
      "affiliation='Enterprise Software Solutions' name='Laura Martínez' role='Enterprise Solutions Architect' description='Laura provides insights into how companies transition AI models into production environments. She evaluates the practical advantages of using Modular RAG over Naive RAG in real-world applications, such as increased reliability, improved maintenance workflows, and reduced downtime. Her focus is on how enterprise clients can leverage these advantages to enhance their AI deployment strategies effectively.'\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36m__interrupt__\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_teddynote.messages import random_uuid, invoke_graph\n",
    "\n",
    "config = RunnableConfig(\n",
    "    recursion_limit=10,\n",
    "    configurable={\"thread_id\": random_uuid()},\n",
    ")\n",
    "\n",
    "# 분석가 수 설정\n",
    "max_analysts = 3\n",
    "\n",
    "# 연구 주제 설정\n",
    "topic = \"Modular RAG 가 기존의 Naive RAG 와 어떤 차이가 있는지와 production level 에서 사용하는 이점\"\n",
    "\n",
    "# 입력 데이터 설정\n",
    "inputs = {\n",
    "    \"topic\": topic,\n",
    "    \"max_analysts\": max_analysts,\n",
    "}\n",
    "\n",
    "# 그래프 실행\n",
    "invoke_graph(graph, inputs, config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('human_feedback',)\n"
     ]
    }
   ],
   "source": [
    "# 그래프의 현재 상태 가져오기\n",
    "state = graph.get_state(config)\n",
    "\n",
    "# 다음 실행할 노드 확인\n",
    "print(state.next)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'configurable': {'thread_id': '0f224407-101d-45c7-b3fc-89073ff26589',\n",
       "  'checkpoint_ns': '',\n",
       "  'checkpoint_id': '1f083d30-5f58-60cc-8002-d5d9fef4e090'}}"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 그래프 상태를 업데이트하여 human_feedback 노드의 역할 수행\n",
    "graph.update_state(\n",
    "    config,\n",
    "    {\n",
    "        \"human_analyst_feedback\": \"Add in someone named Teddy Lee from a startup to add an entrepreneur perspective\"\n",
    "    },\n",
    "    as_node=\"human_feedback\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mcreate_analysts\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "affiliation='Independent Researcher' name='Dr. Hannah Kim' role='Academic Specialist' description='Dr. Kim is an academic researcher with a focus on machine learning frameworks and their applications in real-world scenarios. She is particularly interested in comparing Modular RAG and Naive RAG in terms of flexibility and efficiency for handling complex data requirements in production settings.'\n",
      "affiliation='TechVision Analytics' name='Rahul Patel' role='Data Engineer' description='Rahul works as a data engineer who has extensive experience implementing and optimizing retrieval-augmented generation systems in various production environments. His interest lies in the practical aspects of how Modular RAG can enhance scalability and robustness compared to Naive RAG for enterprise-level applications.'\n",
      "affiliation='Startup Founder' name='Teddy Lee' role='Entrepreneurial Thinker' description='Teddy is a co-founder of an AI-driven startup looking to leverage cutting-edge technologies to innovate new services. He focuses on the entrepreneurial benefits of using Modular RAG over Naive RAG, such as cost-effectiveness and faster deployment capabilities that are crucial for startup survival and competition in fast-moving markets.'\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36m__interrupt__\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# 이어서 진행\n",
    "invoke_graph(graph, None, config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'configurable': {'thread_id': '0f224407-101d-45c7-b3fc-89073ff26589',\n",
       "  'checkpoint_ns': '',\n",
       "  'checkpoint_id': '1f083d30-e5ca-6b00-8004-7965721ba1e3'}}"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 추가 피드백이 없을 경우 None 값을 할당하여 상태 업데이트\n",
    "human_feedback_input = None\n",
    "\n",
    "# 그래프 상태를 업데이트하여 human_feedback 노드의 역할 수행\n",
    "graph.update_state(\n",
    "    config, {\"human_analyst_feedback\": human_feedback_input}, as_node=\"human_feedback\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이어서 진행\n",
    "invoke_graph(graph, None, config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "생성된 분석가 수: 3\n",
      "================================\n",
      "Name: Dr. Hannah Kim\n",
      "Role: Academic Specialist\n",
      "Affiliation: Independent Researcher\n",
      "Description: Dr. Kim is an academic researcher with a focus on machine learning frameworks and their applications in real-world scenarios. She is particularly interested in comparing Modular RAG and Naive RAG in terms of flexibility and efficiency for handling complex data requirements in production settings.\n",
      "\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Name: Rahul Patel\n",
      "Role: Data Engineer\n",
      "Affiliation: TechVision Analytics\n",
      "Description: Rahul works as a data engineer who has extensive experience implementing and optimizing retrieval-augmented generation systems in various production environments. His interest lies in the practical aspects of how Modular RAG can enhance scalability and robustness compared to Naive RAG for enterprise-level applications.\n",
      "\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Name: Teddy Lee\n",
      "Role: Entrepreneurial Thinker\n",
      "Affiliation: Startup Founder\n",
      "Description: Teddy is a co-founder of an AI-driven startup looking to leverage cutting-edge technologies to innovate new services. He focuses on the entrepreneurial benefits of using Modular RAG over Naive RAG, such as cost-effectiveness and faster deployment capabilities that are crucial for startup survival and competition in fast-moving markets.\n",
      "\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n"
     ]
    }
   ],
   "source": [
    "# 그래프의 최종 상태 가져오기\n",
    "final_state = graph.get_state(config)\n",
    "\n",
    "# 최종 상태에서 생성된 분석가 목록 가져오기\n",
    "analysts = final_state.values.get(\"analysts\")\n",
    "\n",
    "# 생성된 분석가 수 출력\n",
    "print(f\"생성된 분석가 수: {len(analysts)}\", end=\"\\n================================\\n\")\n",
    "\n",
    "# 각 분석가의 페르소나 출력\n",
    "for analyst in analysts:\n",
    "    print(analyst.persona)\n",
    "    print(\"- \" * 30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "()\n"
     ]
    }
   ],
   "source": [
    "# 그래프의 다음 실행할 노드 상태 가져오기\n",
    "print(final_state.next)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import Annotated\n",
    "from langgraph.graph import MessagesState\n",
    "\n",
    "\n",
    "# 인터뷰 상태 정의\n",
    "class InterviewState(MessagesState):\n",
    "    # 대화 턴수\n",
    "    max_num_turns: int\n",
    "    # 소스 문서를 포함하는 컨텍스트 리스트\n",
    "    context: Annotated[list, operator.add]\n",
    "    # 지정된 분석가\n",
    "    analyst: Analyst\n",
    "    # 인터뷰 내용을 저장하는 문자열\n",
    "    interview: str\n",
    "    # 보고서 섹션 리스트\n",
    "    sections: list\n",
    "\n",
    "\n",
    "# 검색 쿼리 데이터 클래스 정의\n",
    "class SearchQuery(BaseModel):\n",
    "    search_query: str = Field(None, description=\"Search query for retrieval.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_instructions = \"\"\"You are an analyst tasked with interviewing an expert to learn about a specific topic. \n",
    "\n",
    "Your goal is boil down to interesting and specific insights related to your topic.\n",
    "\n",
    "1. Interesting: Insights that people will find surprising or non-obvious.\n",
    "\n",
    "2. Specific: Insights that avoid generalities and include specific examples from the expert.\n",
    "\n",
    "Here is your topic of focus and set of goals: {goals}\n",
    "\n",
    "Begin by introducing yourself using a name that fits your persona, and then ask your question.\n",
    "\n",
    "Continue to ask questions to drill down and refine your understanding of the topic.\n",
    "\n",
    "When you are satisfied with your understanding, complete the interview with: \"Thank you so much for your help!\"\n",
    "\n",
    "Remember to stay in character throughout your response, reflecting the persona and goals provided to you.\"\"\"\n",
    "\n",
    "\n",
    "# 질문을 생성하는 노드 정의\n",
    "def generate_question(state: InterviewState):\n",
    "    # 상태에서 분석가와 메시지 가져오기\n",
    "    analyst = state[\"analyst\"]\n",
    "    messages = state[\"messages\"]\n",
    "\n",
    "    # 질문 생성\n",
    "    # 분석가의 목표를 기반으로 시스템 메시지 생성\n",
    "    system_message = question_instructions.format(goals=analyst.persona)\n",
    "    # LLM을 사용하여 질문 생성\n",
    "    question = llm.invoke([SystemMessage(content=system_message)] + messages)\n",
    "\n",
    "    # 상태에 메시지 기록\n",
    "    return {\"messages\": [question]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 웹 검색 도구 초기화\n",
    "from langchain_teddynote.tools.tavily import TavilySearch\n",
    "\n",
    "# 웹 검색을 위한 TavilySearch 인스턴스 생성\n",
    "tavily_search = TavilySearch(max_results=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'Published': '2024-07-26', 'Title': 'Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks', 'Authors': 'Yunfan Gao, Yun Xiong, Meng Wang, Haofen Wang', 'Summary': 'Retrieval-augmented Generation (RAG) has markedly enhanced the capabilities\\nof Large Language Models (LLMs) in tackling knowledge-intensive tasks. The\\nincreasing demands of application scenarios have driven the evolution of RAG,\\nleading to the integration of advanced retrievers, LLMs and other complementary\\ntechnologies, which in turn has amplified the intricacy of RAG systems.\\nHowever, the rapid advancements are outpacing the foundational RAG paradigm,\\nwith many methods struggling to be unified under the process of\\n\"retrieve-then-generate\". In this context, this paper examines the limitations\\nof the existing RAG paradigm and introduces the modular RAG framework. By\\ndecomposing complex RAG systems into independent modules and specialized\\noperators, it facilitates a highly reconfigurable framework. Modular RAG\\ntranscends the traditional linear architecture, embracing a more advanced\\ndesign that integrates routing, scheduling, and fusion mechanisms. Drawing on\\nextensive research, this paper further identifies prevalent RAG\\npatterns-linear, conditional, branching, and looping-and offers a comprehensive\\nanalysis of their respective implementation nuances. Modular RAG presents\\ninnovative opportunities for the conceptualization and deployment of RAG\\nsystems. Finally, the paper explores the potential emergence of new operators\\nand paradigms, establishing a solid theoretical foundation and a practical\\nroadmap for the continued evolution and practical deployment of RAG\\ntechnologies.', 'entry_id': 'http://arxiv.org/abs/2407.21059v1', 'published_first_time': '2024-07-26', 'comment': None, 'journal_ref': None, 'doi': None, 'primary_category': 'cs.CL', 'categories': ['cs.CL', 'cs.AI', 'cs.IR'], 'links': ['http://arxiv.org/abs/2407.21059v1', 'http://arxiv.org/pdf/2407.21059v1']}, page_content='1\\nModular RAG: Transforming RAG Systems into\\nLEGO-like Reconfigurable Frameworks\\nYunfan Gao, Yun Xiong, Meng Wang, Haofen Wang\\nAbstract—Retrieval-augmented\\nGeneration\\n(RAG)\\nhas\\nmarkedly enhanced the capabilities of Large Language Models\\n(LLMs) in tackling knowledge-intensive tasks. The increasing\\ndemands of application scenarios have driven the evolution\\nof RAG, leading to the integration of advanced retrievers,\\nLLMs and other complementary technologies, which in turn\\nhas amplified the intricacy of RAG systems. However, the rapid\\nadvancements are outpacing the foundational RAG paradigm,\\nwith many methods struggling to be unified under the process\\nof “retrieve-then-generate”. In this context, this paper examines\\nthe limitations of the existing RAG paradigm and introduces\\nthe modular RAG framework. By decomposing complex RAG\\nsystems into independent modules and specialized operators, it\\nfacilitates a highly reconfigurable framework. Modular RAG\\ntranscends the traditional linear architecture, embracing a\\nmore advanced design that integrates routing, scheduling, and\\nfusion mechanisms. Drawing on extensive research, this paper\\nfurther identifies prevalent RAG patterns—linear, conditional,\\nbranching, and looping—and offers a comprehensive analysis\\nof their respective implementation nuances. Modular RAG\\npresents\\ninnovative\\nopportunities\\nfor\\nthe\\nconceptualization\\nand deployment of RAG systems. Finally, the paper explores\\nthe potential emergence of new operators and paradigms,\\nestablishing a solid theoretical foundation and a practical\\nroadmap for the continued evolution and practical deployment\\nof RAG technologies.\\nIndex Terms—Retrieval-augmented generation, large language\\nmodel, modular system, information retrieval\\nI. INTRODUCTION\\nL\\nARGE Language Models (LLMs) have demonstrated\\nremarkable capabilities, yet they still face numerous\\nchallenges, such as hallucination and the lag in information up-\\ndates [1]. Retrieval-augmented Generation (RAG), by access-\\ning external knowledge bases, provides LLMs with important\\ncontextual information, significantly enhancing their perfor-\\nmance on knowledge-intensive tasks [2]. Currently, RAG, as\\nan enhancement method, has been widely applied in various\\npractical application scenarios, including knowledge question\\nanswering, recommendation systems, customer service, and\\npersonal assistants. [3]–[6]\\nDuring the nascent stages of RAG , its core framework is\\nconstituted by indexing, retrieval, and generation, a paradigm\\nreferred to as Naive RAG [7]. However, as the complexity\\nof tasks and the demands of applications have escalated, the\\nYunfan Gao is with Shanghai Research Institute for Intelligent Autonomous\\nSystems, Tongji University, Shanghai, 201210, China.\\nYun Xiong is with Shanghai Key Laboratory of Data Science, School of\\nComputer Science, Fudan University, Shanghai, 200438, China.\\nMeng Wang and Haofen Wang are with College of Design and Innovation,\\nTongji University, Shanghai, 20092, China. (Corresponding author: Haofen\\nWang. E-mail: carter.whfcarter@gmail.com)\\nlimitations of Naive RAG have become increasingly apparent.\\nAs depicted in Figure 1, it predominantly hinges on the\\nstraightforward similarity of chunks, result in poor perfor-\\nmance when confronted with complex queries and chunks with\\nsubstantial variability. The primary challenges of Naive RAG\\ninclude: 1) Shallow Understanding of Queries. The semantic\\nsimilarity between a query and document chunk is not always\\nhighly consistent. Relying solely on similarity calculations\\nfor retrieval lacks an in-depth exploration of the relationship\\nbetween the query and the document [8]. 2) Retrieval Re-\\ndundancy and Noise. Feeding all retrieved chunks directly\\ninto LLMs is not always beneficial. Research indicates that\\nan excess of redundant and noisy information may interfere\\nwith the LLM’s identification of key information, thereby\\nincreasing the risk of generating erroneous and hallucinated\\nresponses. [9]\\nTo overcome the aforementioned limitations, '), Document(metadata={'Published': '2024-03-27', 'Title': 'Retrieval-Augmented Generation for Large Language Models: A Survey', 'Authors': 'Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, Haofen Wang', 'Summary': \"Large Language Models (LLMs) showcase impressive capabilities but encounter\\nchallenges like hallucination, outdated knowledge, and non-transparent,\\nuntraceable reasoning processes. Retrieval-Augmented Generation (RAG) has\\nemerged as a promising solution by incorporating knowledge from external\\ndatabases. This enhances the accuracy and credibility of the generation,\\nparticularly for knowledge-intensive tasks, and allows for continuous knowledge\\nupdates and integration of domain-specific information. RAG synergistically\\nmerges LLMs' intrinsic knowledge with the vast, dynamic repositories of\\nexternal databases. This comprehensive review paper offers a detailed\\nexamination of the progression of RAG paradigms, encompassing the Naive RAG,\\nthe Advanced RAG, and the Modular RAG. It meticulously scrutinizes the\\ntripartite foundation of RAG frameworks, which includes the retrieval, the\\ngeneration and the augmentation techniques. The paper highlights the\\nstate-of-the-art technologies embedded in each of these critical components,\\nproviding a profound understanding of the advancements in RAG systems.\\nFurthermore, this paper introduces up-to-date evaluation framework and\\nbenchmark. At the end, this article delineates the challenges currently faced\\nand points out prospective avenues for research and development.\", 'entry_id': 'http://arxiv.org/abs/2312.10997v5', 'published_first_time': '2023-12-18', 'comment': 'Ongoing Work', 'journal_ref': None, 'doi': None, 'primary_category': 'cs.CL', 'categories': ['cs.CL', 'cs.AI'], 'links': ['http://arxiv.org/abs/2312.10997v5', 'http://arxiv.org/pdf/2312.10997v5']}, page_content='1\\nRetrieval-Augmented Generation for Large\\nLanguage Models: A Survey\\nYunfan Gaoa, Yun Xiongb, Xinyu Gaob, Kangxiang Jiab, Jinliu Panb, Yuxi Bic, Yi Daia, Jiawei Suna, Meng\\nWangc, and Haofen Wang a,c\\naShanghai Research Institute for Intelligent Autonomous Systems, Tongji University\\nbShanghai Key Laboratory of Data Science, School of Computer Science, Fudan University\\ncCollege of Design and Innovation, Tongji University\\nAbstract—Large Language Models (LLMs) showcase impres-\\nsive capabilities but encounter challenges like hallucination,\\noutdated knowledge, and non-transparent, untraceable reasoning\\nprocesses. Retrieval-Augmented Generation (RAG) has emerged\\nas a promising solution by incorporating knowledge from external\\ndatabases. This enhances the accuracy and credibility of the\\ngeneration, particularly for knowledge-intensive tasks, and allows\\nfor continuous knowledge updates and integration of domain-\\nspecific information. RAG synergistically merges LLMs’ intrin-\\nsic knowledge with the vast, dynamic repositories of external\\ndatabases. This comprehensive review paper offers a detailed\\nexamination of the progression of RAG paradigms, encompassing\\nthe Naive RAG, the Advanced RAG, and the Modular RAG.\\nIt meticulously scrutinizes the tripartite foundation of RAG\\nframeworks, which includes the retrieval, the generation and the\\naugmentation techniques. The paper highlights the state-of-the-\\nart technologies embedded in each of these critical components,\\nproviding a profound understanding of the advancements in RAG\\nsystems. Furthermore, this paper introduces up-to-date evalua-\\ntion framework and benchmark. At the end, this article delineates\\nthe challenges currently faced and points out prospective avenues\\nfor research and development 1.\\nIndex Terms—Large language model, retrieval-augmented gen-\\neration, natural language processing, information retrieval\\nI. INTRODUCTION\\nL\\nARGE language models (LLMs) have achieved remark-\\nable success, though they still face significant limitations,\\nespecially in domain-specific or knowledge-intensive tasks [1],\\nnotably producing “hallucinations” [2] when handling queries\\nbeyond their training data or requiring current information. To\\novercome challenges, Retrieval-Augmented Generation (RAG)\\nenhances LLMs by retrieving relevant document chunks from\\nexternal knowledge base through semantic similarity calcu-\\nlation. By referencing external knowledge, RAG effectively\\nreduces the problem of generating factually incorrect content.\\nIts integration into LLMs has resulted in widespread adoption,\\nestablishing RAG as a key technology in advancing chatbots\\nand enhancing the suitability of LLMs for real-world applica-\\ntions.\\nRAG technology has rapidly developed in recent years, and\\nthe technology tree summarizing related research is shown\\nCorresponding Author.Email:haofen.wang@tongji.edu.cn\\n1Resources\\nare\\navailable\\nat\\nhttps://github.com/Tongji-KGLLM/\\nRAG-Survey\\nin Figure 1. The development trajectory of RAG in the era\\nof large models exhibits several distinct stage characteristics.\\nInitially, RAG’s inception coincided with the rise of the\\nTransformer architecture, focusing on enhancing language\\nmodels by incorporating additional knowledge through Pre-\\nTraining Models (PTM). This early stage was characterized\\nby foundational work aimed at refining pre-training techniques\\n[3]–[5].The subsequent arrival of ChatGPT [6] marked a\\npivotal moment, with LLM demonstrating powerful in context\\nlearning (ICL) capabilities. RAG research shifted towards\\nproviding better information for LLMs to answer more com-\\nplex and knowledge-intensive tasks during the inference stage,\\nleading to rapid development in RAG studies. As research\\nprogressed, the enhancement of RAG was no longer limited\\nto the inference stage but began to incorporate more with LLM\\nfine-tuning techniques.\\nThe burgeoning field of RAG has experienced swift growth,\\nyet it has not been accompanied by a systematic synthesis that\\ncould clarify its broader trajectory. Thi'), Document(metadata={'Published': '2025-02-04', 'Title': 'Plan*RAG: Efficient Test-Time Planning for Retrieval Augmented Generation', 'Authors': 'Prakhar Verma, Sukruta Prakash Midigeshi, Gaurav Sinha, Arno Solin, Nagarajan Natarajan, Amit Sharma', 'Summary': \"We introduce Plan*RAG, a novel framework that enables structured multi-hop\\nreasoning in retrieval-augmented generation (RAG) through test-time reasoning\\nplan generation. While existing approaches such as ReAct maintain reasoning\\nchains within the language model's context window, we observe that this often\\nleads to plan fragmentation and execution failures. Our key insight is that by\\nisolating the reasoning plan as a directed acyclic graph (DAG) outside the LM's\\nworking memory, we can enable (1) systematic exploration of reasoning paths,\\n(2) atomic subqueries enabling precise retrievals and grounding, and (3)\\nefficiency through parallel execution and bounded context window utilization.\\nMoreover, Plan*RAG's modular design allows it to be integrated with existing\\nRAG methods, thus providing a practical solution to improve current RAG\\nsystems. On standard multi-hop reasoning benchmarks, Plan*RAG consistently\\nachieves improvements over recently proposed methods such as RQ-RAG and\\nSelf-RAG, while maintaining comparable computational costs.\", 'entry_id': 'http://arxiv.org/abs/2410.20753v2', 'published_first_time': '2024-10-28', 'comment': '19 pages, preprint', 'journal_ref': None, 'doi': None, 'primary_category': 'cs.CL', 'categories': ['cs.CL', 'cs.LG'], 'links': ['http://arxiv.org/abs/2410.20753v2', 'http://arxiv.org/pdf/2410.20753v2']}, page_content='Plan∗RAG: Efficient Test-Time Planning for Retrieval Augmented Generation\\nPrakhar Verma † 1 Sukruta Prakash Midigeshi 2 Gaurav Sinha 2 Arno Solin 1\\nNagarajan Natarajan 2 Amit Sharma 2\\nAbstract\\nWe introduce Plan∗RAG, a novel framework\\nthat enables structured multi-hop reasoning in\\nretrieval-augmented generation (RAG) through\\ntest-time reasoning plan generation. While exist-\\ning approaches such as ReAct maintain reason-\\ning chains within the language model’s context\\nwindow, we observe that this often leads to plan\\nfragmentation and execution failures. Our key\\ninsight is that by isolating the reasoning plan as\\na directed acyclic graph (DAG) outside the LM’s\\nworking memory, we can enable (1) systematic\\nexploration of reasoning paths, (2) atomic sub-\\nqueries enabling precise retrievals and ground-\\ning, and (3) efficiency through parallel execution\\nand bounded context window utilization. More-\\nover, Plan∗RAG’s modular design allows it to\\nbe integrated with existing RAG methods, thus\\nproviding a practical solution to improve current\\nRAG systems. On standard multi-hop reasoning\\nbenchmarks, Plan∗RAG consistently achieves im-\\nprovements over recently proposed methods such\\nas RQ-RAG and Self-RAG, while maintaining\\ncomparable computational costs.\\n1. Introduction\\nRetrieval-Augmented Generation (RAG, Lewis et al., 2020;\\nPetroni et al., 2020; Guu et al., 2020) has emerged as a\\npromising approach for grounding language model (LM)\\nresponses in external knowledge. However, RAG systems\\nstruggle with multi-hop queries that require reasoning across\\nmultiple retrieved documents (Tang & Yang, 2024; Wei\\net al., 2022). A key challenge lies in the initial retrieval step,\\nwhich often fails to retrieve sufficient relevant documents\\ndue to the query’s lack of full contextual information (Ma\\net al., 2023). This limitation has been highlighted in recent\\nsurveys (Torfi et al., 2020; Zhao et al., 2023) as a funda-\\n† Work done during an internship with Microsoft Research.\\n1Aalto University, Finland 2Microsoft Research. Correspondence\\nto: <prakhar.verma@aalto.fi>, <amshar@microsoft.com>.\\nRAG\\nSelf-RAG\\nReAct\\n20\\n25\\n30\\n35\\n40\\n45\\nTest-time planning improves RAG\\n25.51\\n34.09\\n33.15\\n31.12\\n37.31\\n40.44\\nAccuracy (%)\\nVanilla\\nWith Plan∗\\nFigure 1. Plan∗RAG improves performance on the HotpotQA\\nbenchmark substantially compared to various existing RAG meth-\\nods, demonstrating the value of externalizing planning as a directed\\nacyclic graph (DAG) outside of the LLM’s context.\\nmental barrier to reliable AI systems, particularly given the\\nwidespread deployment of large language models (Brown\\net al., 2020) across critical domains. Consider the query:\\n“Rumble Fish was a novel by the author of the coming-of-age\\nnovel published in what year by Viking Press?” Answering\\nthis requires an iterative retrieval process: identifying the\\nRumble Fish’s author, connecting to their coming-of-age\\nnovel, and determining its publication year. Single-step\\nretrieval in RAG systems often fails in such cases, as it\\nmay retrieve documents about Rumble Fish’s author and\\nViking Press without recognizing the intermediate fact—\\nthe author’s coming-of-age novel—must first be established.\\nFurthermore, Leng et al. (2024); Shuster et al. (2021) demon-\\nstrate that even when relevant documents are retrieved, LMs\\nstruggle to reason across them due to fixed context win-\\ndows, leading to information loss and broken reasoning\\nchain. These limitations pose risks in critical domains such\\nas healthcare and finance (Pal et al., 2023; Zhao et al., 2024),\\nwhere accurate multi-step reasoning is essential.\\nRecent research has attempted to address these limita-\\ntions through structured reasoning frameworks. Chain-of-\\nThought (CoT) prompting (Wei et al., 2022) and systematic\\nquery decomposition (Patel et al., 2022) have introduced\\nexplicit reasoning steps, enabling more granular thought\\nprocesses and targeted retrievals. Building upon these foun-\\ndations, Yao et al. (2023) proposed ReAct—a framework\\nthat creates a reasoning chain ')]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.retrievers import ArxivRetriever\n",
    "\n",
    "# Arxiv 검색을 위한 ArxivRetriever 인스턴스 생성\n",
    "arxiv_retriever = ArxivRetriever(\n",
    "    load_max_docs=3,\n",
    "    load_all_available_meta=True,\n",
    "    get_full_documents=True,\n",
    ")\n",
    "\n",
    "# 검색 결과 출력\n",
    "arxiv_search_results = arxiv_retriever.invoke(\"Modular RAG vs Naive RAG\")\n",
    "print(arxiv_search_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Published': '2024-07-26',\n",
       " 'Title': 'Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks',\n",
       " 'Authors': 'Yunfan Gao, Yun Xiong, Meng Wang, Haofen Wang',\n",
       " 'Summary': 'Retrieval-augmented Generation (RAG) has markedly enhanced the capabilities\\nof Large Language Models (LLMs) in tackling knowledge-intensive tasks. The\\nincreasing demands of application scenarios have driven the evolution of RAG,\\nleading to the integration of advanced retrievers, LLMs and other complementary\\ntechnologies, which in turn has amplified the intricacy of RAG systems.\\nHowever, the rapid advancements are outpacing the foundational RAG paradigm,\\nwith many methods struggling to be unified under the process of\\n\"retrieve-then-generate\". In this context, this paper examines the limitations\\nof the existing RAG paradigm and introduces the modular RAG framework. By\\ndecomposing complex RAG systems into independent modules and specialized\\noperators, it facilitates a highly reconfigurable framework. Modular RAG\\ntranscends the traditional linear architecture, embracing a more advanced\\ndesign that integrates routing, scheduling, and fusion mechanisms. Drawing on\\nextensive research, this paper further identifies prevalent RAG\\npatterns-linear, conditional, branching, and looping-and offers a comprehensive\\nanalysis of their respective implementation nuances. Modular RAG presents\\ninnovative opportunities for the conceptualization and deployment of RAG\\nsystems. Finally, the paper explores the potential emergence of new operators\\nand paradigms, establishing a solid theoretical foundation and a practical\\nroadmap for the continued evolution and practical deployment of RAG\\ntechnologies.',\n",
       " 'entry_id': 'http://arxiv.org/abs/2407.21059v1',\n",
       " 'published_first_time': '2024-07-26',\n",
       " 'comment': None,\n",
       " 'journal_ref': None,\n",
       " 'doi': None,\n",
       " 'primary_category': 'cs.CL',\n",
       " 'categories': ['cs.CL', 'cs.AI', 'cs.IR'],\n",
       " 'links': ['http://arxiv.org/abs/2407.21059v1',\n",
       "  'http://arxiv.org/pdf/2407.21059v1']}"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Arxiv 메타데이터 출력\n",
    "arxiv_search_results[0].metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Modular RAG: Transforming RAG Systems into\n",
      "LEGO-like Reconfigurable Frameworks\n",
      "Yunfan Gao, Yun Xiong, Meng Wang, Haofen Wang\n",
      "Abstract—Retrieval-augmented\n",
      "Generation\n",
      "(RAG)\n",
      "has\n",
      "markedly enhanced the capabilities of Large Language Models\n",
      "(LLMs) in tackling knowledge-intensive tasks. The increasing\n",
      "demands of application scenarios have driven the evolution\n",
      "of RAG, leading to the integration of advanced retrievers,\n",
      "LLMs and other complementary technologies, which in turn\n",
      "has amplified the intricacy of RAG systems. However, the rapid\n",
      "advancements are outpacing the foundational RAG paradigm,\n",
      "with many methods struggling to be unified under the process\n",
      "of “retrieve-then-generate”. In this context, this paper examines\n",
      "the limitations of the existing RAG paradigm and introduces\n",
      "the modular RAG framework. By decomposing complex RAG\n",
      "systems into independent modules and specialized operators, it\n",
      "facilitates a highly reconfigurable framework. Modular RAG\n",
      "transcends the traditional linear architecture, embracing a\n",
      "more advanced design that integrates routing, scheduling, and\n",
      "fusion mechanisms. Drawing on extensive research, this paper\n",
      "further identifies prevalent RAG patterns—linear, conditional,\n",
      "branching, and looping—and offers a comprehensive analysis\n",
      "of their respective implementation nuances. Modular RAG\n",
      "presents\n",
      "innovative\n",
      "opportunities\n",
      "for\n",
      "the\n",
      "conceptualization\n",
      "and deployment of RAG systems. Finally, the paper explores\n",
      "the potential emergence of new operators and paradigms,\n",
      "establishing a solid theoretical foundation and a practical\n",
      "roadmap for the continued evolution and practical deployment\n",
      "of RAG technologies.\n",
      "Index Terms—Retrieval-augmented generation, large language\n",
      "model, modular system, information retrieval\n",
      "I. INTRODUCTION\n",
      "L\n",
      "ARGE Language Models (LLMs) have demonstrated\n",
      "remarkable capabilities, yet they still face numerous\n",
      "challenges, such as hallucination and the lag in information up-\n",
      "dates [1]. Retrieval-augmented Generation (RAG), by access-\n",
      "ing external knowledge bases, provides LLMs with important\n",
      "contextual information, significantly enhancing their perfor-\n",
      "mance on knowledge-intensive tasks [2]. Currently, RAG, as\n",
      "an enhancement method, has been widely applied in various\n",
      "practical application scenarios, including knowledge question\n",
      "answering, recommendation systems, customer service, and\n",
      "personal assistants. [3]–[6]\n",
      "During the nascent stages of RAG , its core framework is\n",
      "constituted by indexing, retrieval, and generation, a paradigm\n",
      "referred to as Naive RAG [7]. However, as the complexity\n",
      "of tasks and the demands of applications have escalated, the\n",
      "Yunfan Gao is with Shanghai Research Institute for Intelligent Autonomous\n",
      "Systems, Tongji University, Shanghai, 201210, China.\n",
      "Yun Xiong is with Shanghai Key Laboratory of Data Science, School of\n",
      "Computer Science, Fudan University, Shanghai, 200438, China.\n",
      "Meng Wang and Haofen Wang are with College of Design and Innovation,\n",
      "Tongji University, Shanghai, 20092, China. (Corresponding author: Haofen\n",
      "Wang. E-mail: carter.whfcarter@gmail.com)\n",
      "limitations of Naive RAG have become increasingly apparent.\n",
      "As depicted in Figure 1, it predominantly hinges on the\n",
      "straightforward similarity of chunks, result in poor perfor-\n",
      "mance when confronted with complex queries and chunks with\n",
      "substantial variability. The primary challenges of Naive RAG\n",
      "include: 1) Shallow Understanding of Queries. The semantic\n",
      "similarity between a query and document chunk is not always\n",
      "highly consistent. Relying solely on similarity calculations\n",
      "for retrieval lacks an in-depth exploration of the relationship\n",
      "between the query and the document [8]. 2) Retrieval Re-\n",
      "dundancy and Noise. Feeding all retrieved chunks directly\n",
      "into LLMs is not always beneficial. Research indicates that\n",
      "an excess of redundant and noisy information may interfere\n",
      "with the LLM’s identification of key information, thereby\n",
      "increasing the risk of generating erroneous and hallucinated\n",
      "responses. [9]\n",
      "To overcome the aforementioned limitations, \n"
     ]
    }
   ],
   "source": [
    "# Arxiv 내용 출력\n",
    "print(arxiv_search_results[0].page_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문서 검색 결과를 포맷팅\n",
    "formatted_search_docs = \"\\n\\n---\\n\\n\".join(\n",
    "    [\n",
    "        f'<Document source=\"{doc.metadata[\"entry_id\"]}\" date=\"{doc.metadata.get(\"Published\", \"\")}\" authors=\"{doc.metadata.get(\"Authors\", \"\")}\"/>\\n<Title>\\n{doc.metadata[\"Title\"]}\\n</Title>\\n\\n<Summary>\\n{doc.metadata[\"Summary\"]}\\n</Summary>\\n\\n<Content>\\n{doc.page_content}\\n</Content>\\n</Document>'\n",
    "        for doc in arxiv_search_results\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Document source=\"http://arxiv.org/abs/2407.21059v1\" date=\"2024-07-26\" authors=\"Yunfan Gao, Yun Xiong, Meng Wang, Haofen Wang\"/>\n",
      "<Title>\n",
      "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks\n",
      "</Title>\n",
      "\n",
      "<Summary>\n",
      "Retrieval-augmented Generation (RAG) has markedly enhanced the capabilities\n",
      "of Large Language Models (LLMs) in tackling knowledge-intensive tasks. The\n",
      "increasing demands of application scenarios have driven the evolution of RAG,\n",
      "leading to the integration of advanced retrievers, LLMs and other complementary\n",
      "technologies, which in turn has amplified the intricacy of RAG systems.\n",
      "However, the rapid advancements are outpacing the foundational RAG paradigm,\n",
      "with many methods struggling to be unified under the process of\n",
      "\"retrieve-then-generate\". In this context, this paper examines the limitations\n",
      "of the existing RAG paradigm and introduces the modular RAG framework. By\n",
      "decomposing complex RAG systems into independent modules and specialized\n",
      "operators, it facilitates a highly reconfigurable framework. Modular RAG\n",
      "transcends the traditional linear architecture, embracing a more advanced\n",
      "design that integrates routing, scheduling, and fusion mechanisms. Drawing on\n",
      "extensive research, this paper further identifies prevalent RAG\n",
      "patterns-linear, conditional, branching, and looping-and offers a comprehensive\n",
      "analysis of their respective implementation nuances. Modular RAG presents\n",
      "innovative opportunities for the conceptualization and deployment of RAG\n",
      "systems. Finally, the paper explores the potential emergence of new operators\n",
      "and paradigms, establishing a solid theoretical foundation and a practical\n",
      "roadmap for the continued evolution and practical deployment of RAG\n",
      "technologies.\n",
      "</Summary>\n",
      "\n",
      "<Content>\n",
      "1\n",
      "Modular RAG: Transforming RAG Systems into\n",
      "LEGO-like Reconfigurable Frameworks\n",
      "Yunfan Gao, Yun Xiong, Meng Wang, Haofen Wang\n",
      "Abstract—Retrieval-augmented\n",
      "Generation\n",
      "(RAG)\n",
      "has\n",
      "markedly enhanced the capabilities of Large Language Models\n",
      "(LLMs) in tackling knowledge-intensive tasks. The increasing\n",
      "demands of application scenarios have driven the evolution\n",
      "of RAG, leading to the integration of advanced retrievers,\n",
      "LLMs and other complementary technologies, which in turn\n",
      "has amplified the intricacy of RAG systems. However, the rapid\n",
      "advancements are outpacing the foundational RAG paradigm,\n",
      "with many methods struggling to be unified under the process\n",
      "of “retrieve-then-generate”. In this context, this paper examines\n",
      "the limitations of the existing RAG paradigm and introduces\n",
      "the modular RAG framework. By decomposing complex RAG\n",
      "systems into independent modules and specialized operators, it\n",
      "facilitates a highly reconfigurable framework. Modular RAG\n",
      "transcends the traditional linear architecture, embracing a\n",
      "more advanced design that integrates routing, scheduling, and\n",
      "fusion mechanisms. Drawing on extensive research, this paper\n",
      "further identifies prevalent RAG patterns—linear, conditional,\n",
      "branching, and looping—and offers a comprehensive analysis\n",
      "of their respective implementation nuances. Modular RAG\n",
      "presents\n",
      "innovative\n",
      "opportunities\n",
      "for\n",
      "the\n",
      "conceptualization\n",
      "and deployment of RAG systems. Finally, the paper explores\n",
      "the potential emergence of new operators and paradigms,\n",
      "establishing a solid theoretical foundation and a practical\n",
      "roadmap for the continued evolution and practical deployment\n",
      "of RAG technologies.\n",
      "Index Terms—Retrieval-augmented generation, large language\n",
      "model, modular system, information retrieval\n",
      "I. INTRODUCTION\n",
      "L\n",
      "ARGE Language Models (LLMs) have demonstrated\n",
      "remarkable capabilities, yet they still face numerous\n",
      "challenges, such as hallucination and the lag in information up-\n",
      "dates [1]. Retrieval-augmented Generation (RAG), by access-\n",
      "ing external knowledge bases, provides LLMs with important\n",
      "contextual information, significantly enhancing their perfor-\n",
      "mance on knowledge-intensive tasks [2]. Currently, RAG, as\n",
      "an enhancement method, has been widely applied in various\n",
      "practical application scenarios, including knowledge question\n",
      "answering, recommendation systems, customer service, and\n",
      "personal assistants. [3]–[6]\n",
      "During the nascent stages of RAG , its core framework is\n",
      "constituted by indexing, retrieval, and generation, a paradigm\n",
      "referred to as Naive RAG [7]. However, as the complexity\n",
      "of tasks and the demands of applications have escalated, the\n",
      "Yunfan Gao is with Shanghai Research Institute for Intelligent Autonomous\n",
      "Systems, Tongji University, Shanghai, 201210, China.\n",
      "Yun Xiong is with Shanghai Key Laboratory of Data Science, School of\n",
      "Computer Science, Fudan University, Shanghai, 200438, China.\n",
      "Meng Wang and Haofen Wang are with College of Design and Innovation,\n",
      "Tongji University, Shanghai, 20092, China. (Corresponding author: Haofen\n",
      "Wang. E-mail: carter.whfcarter@gmail.com)\n",
      "limitations of Naive RAG have become increasingly apparent.\n",
      "As depicted in Figure 1, it predominantly hinges on the\n",
      "straightforward similarity of chunks, result in poor perfor-\n",
      "mance when confronted with complex queries and chunks with\n",
      "substantial variability. The primary challenges of Naive RAG\n",
      "include: 1) Shallow Understanding of Queries. The semantic\n",
      "similarity between a query and document chunk is not always\n",
      "highly consistent. Relying solely on similarity calculations\n",
      "for retrieval lacks an in-depth exploration of the relationship\n",
      "between the query and the document [8]. 2) Retrieval Re-\n",
      "dundancy and Noise. Feeding all retrieved chunks directly\n",
      "into LLMs is not always beneficial. Research indicates that\n",
      "an excess of redundant and noisy information may interfere\n",
      "with the LLM’s identification of key information, thereby\n",
      "increasing the risk of generating erroneous and hallucinated\n",
      "responses. [9]\n",
      "To overcome the aforementioned limitations, \n",
      "</Content>\n",
      "</Document>\n",
      "\n",
      "---\n",
      "\n",
      "<Document source=\"http://arxiv.org/abs/2312.10997v5\" date=\"2024-03-27\" authors=\"Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, Haofen Wang\"/>\n",
      "<Title>\n",
      "Retrieval-Augmented Generation for Large Language Models: A Survey\n",
      "</Title>\n",
      "\n",
      "<Summary>\n",
      "Large Language Models (LLMs) showcase impressive capabilities but encounter\n",
      "challenges like hallucination, outdated knowledge, and non-transparent,\n",
      "untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has\n",
      "emerged as a promising solution by incorporating knowledge from external\n",
      "databases. This enhances the accuracy and credibility of the generation,\n",
      "particularly for knowledge-intensive tasks, and allows for continuous knowledge\n",
      "updates and integration of domain-specific information. RAG synergistically\n",
      "merges LLMs' intrinsic knowledge with the vast, dynamic repositories of\n",
      "external databases. This comprehensive review paper offers a detailed\n",
      "examination of the progression of RAG paradigms, encompassing the Naive RAG,\n",
      "the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the\n",
      "tripartite foundation of RAG frameworks, which includes the retrieval, the\n",
      "generation and the augmentation techniques. The paper highlights the\n",
      "state-of-the-art technologies embedded in each of these critical components,\n",
      "providing a profound understanding of the advancements in RAG systems.\n",
      "Furthermore, this paper introduces up-to-date evaluation framework and\n",
      "benchmark. At the end, this article delineates the challenges currently faced\n",
      "and points out prospective avenues for research and development.\n",
      "</Summary>\n",
      "\n",
      "<Content>\n",
      "1\n",
      "Retrieval-Augmented Generation for Large\n",
      "Language Models: A Survey\n",
      "Yunfan Gaoa, Yun Xiongb, Xinyu Gaob, Kangxiang Jiab, Jinliu Panb, Yuxi Bic, Yi Daia, Jiawei Suna, Meng\n",
      "Wangc, and Haofen Wang a,c\n",
      "aShanghai Research Institute for Intelligent Autonomous Systems, Tongji University\n",
      "bShanghai Key Laboratory of Data Science, School of Computer Science, Fudan University\n",
      "cCollege of Design and Innovation, Tongji University\n",
      "Abstract—Large Language Models (LLMs) showcase impres-\n",
      "sive capabilities but encounter challenges like hallucination,\n",
      "outdated knowledge, and non-transparent, untraceable reasoning\n",
      "processes. Retrieval-Augmented Generation (RAG) has emerged\n",
      "as a promising solution by incorporating knowledge from external\n",
      "databases. This enhances the accuracy and credibility of the\n",
      "generation, particularly for knowledge-intensive tasks, and allows\n",
      "for continuous knowledge updates and integration of domain-\n",
      "specific information. RAG synergistically merges LLMs’ intrin-\n",
      "sic knowledge with the vast, dynamic repositories of external\n",
      "databases. This comprehensive review paper offers a detailed\n",
      "examination of the progression of RAG paradigms, encompassing\n",
      "the Naive RAG, the Advanced RAG, and the Modular RAG.\n",
      "It meticulously scrutinizes the tripartite foundation of RAG\n",
      "frameworks, which includes the retrieval, the generation and the\n",
      "augmentation techniques. The paper highlights the state-of-the-\n",
      "art technologies embedded in each of these critical components,\n",
      "providing a profound understanding of the advancements in RAG\n",
      "systems. Furthermore, this paper introduces up-to-date evalua-\n",
      "tion framework and benchmark. At the end, this article delineates\n",
      "the challenges currently faced and points out prospective avenues\n",
      "for research and development 1.\n",
      "Index Terms—Large language model, retrieval-augmented gen-\n",
      "eration, natural language processing, information retrieval\n",
      "I. INTRODUCTION\n",
      "L\n",
      "ARGE language models (LLMs) have achieved remark-\n",
      "able success, though they still face significant limitations,\n",
      "especially in domain-specific or knowledge-intensive tasks [1],\n",
      "notably producing “hallucinations” [2] when handling queries\n",
      "beyond their training data or requiring current information. To\n",
      "overcome challenges, Retrieval-Augmented Generation (RAG)\n",
      "enhances LLMs by retrieving relevant document chunks from\n",
      "external knowledge base through semantic similarity calcu-\n",
      "lation. By referencing external knowledge, RAG effectively\n",
      "reduces the problem of generating factually incorrect content.\n",
      "Its integration into LLMs has resulted in widespread adoption,\n",
      "establishing RAG as a key technology in advancing chatbots\n",
      "and enhancing the suitability of LLMs for real-world applica-\n",
      "tions.\n",
      "RAG technology has rapidly developed in recent years, and\n",
      "the technology tree summarizing related research is shown\n",
      "Corresponding Author.Email:haofen.wang@tongji.edu.cn\n",
      "1Resources\n",
      "are\n",
      "available\n",
      "at\n",
      "https://github.com/Tongji-KGLLM/\n",
      "RAG-Survey\n",
      "in Figure 1. The development trajectory of RAG in the era\n",
      "of large models exhibits several distinct stage characteristics.\n",
      "Initially, RAG’s inception coincided with the rise of the\n",
      "Transformer architecture, focusing on enhancing language\n",
      "models by incorporating additional knowledge through Pre-\n",
      "Training Models (PTM). This early stage was characterized\n",
      "by foundational work aimed at refining pre-training techniques\n",
      "[3]–[5].The subsequent arrival of ChatGPT [6] marked a\n",
      "pivotal moment, with LLM demonstrating powerful in context\n",
      "learning (ICL) capabilities. RAG research shifted towards\n",
      "providing better information for LLMs to answer more com-\n",
      "plex and knowledge-intensive tasks during the inference stage,\n",
      "leading to rapid development in RAG studies. As research\n",
      "progressed, the enhancement of RAG was no longer limited\n",
      "to the inference stage but began to incorporate more with LLM\n",
      "fine-tuning techniques.\n",
      "The burgeoning field of RAG has experienced swift growth,\n",
      "yet it has not been accompanied by a systematic synthesis that\n",
      "could clarify its broader trajectory. Thi\n",
      "</Content>\n",
      "</Document>\n",
      "\n",
      "---\n",
      "\n",
      "<Document source=\"http://arxiv.org/abs/2410.20753v2\" date=\"2025-02-04\" authors=\"Prakhar Verma, Sukruta Prakash Midigeshi, Gaurav Sinha, Arno Solin, Nagarajan Natarajan, Amit Sharma\"/>\n",
      "<Title>\n",
      "Plan*RAG: Efficient Test-Time Planning for Retrieval Augmented Generation\n",
      "</Title>\n",
      "\n",
      "<Summary>\n",
      "We introduce Plan*RAG, a novel framework that enables structured multi-hop\n",
      "reasoning in retrieval-augmented generation (RAG) through test-time reasoning\n",
      "plan generation. While existing approaches such as ReAct maintain reasoning\n",
      "chains within the language model's context window, we observe that this often\n",
      "leads to plan fragmentation and execution failures. Our key insight is that by\n",
      "isolating the reasoning plan as a directed acyclic graph (DAG) outside the LM's\n",
      "working memory, we can enable (1) systematic exploration of reasoning paths,\n",
      "(2) atomic subqueries enabling precise retrievals and grounding, and (3)\n",
      "efficiency through parallel execution and bounded context window utilization.\n",
      "Moreover, Plan*RAG's modular design allows it to be integrated with existing\n",
      "RAG methods, thus providing a practical solution to improve current RAG\n",
      "systems. On standard multi-hop reasoning benchmarks, Plan*RAG consistently\n",
      "achieves improvements over recently proposed methods such as RQ-RAG and\n",
      "Self-RAG, while maintaining comparable computational costs.\n",
      "</Summary>\n",
      "\n",
      "<Content>\n",
      "Plan∗RAG: Efficient Test-Time Planning for Retrieval Augmented Generation\n",
      "Prakhar Verma † 1 Sukruta Prakash Midigeshi 2 Gaurav Sinha 2 Arno Solin 1\n",
      "Nagarajan Natarajan 2 Amit Sharma 2\n",
      "Abstract\n",
      "We introduce Plan∗RAG, a novel framework\n",
      "that enables structured multi-hop reasoning in\n",
      "retrieval-augmented generation (RAG) through\n",
      "test-time reasoning plan generation. While exist-\n",
      "ing approaches such as ReAct maintain reason-\n",
      "ing chains within the language model’s context\n",
      "window, we observe that this often leads to plan\n",
      "fragmentation and execution failures. Our key\n",
      "insight is that by isolating the reasoning plan as\n",
      "a directed acyclic graph (DAG) outside the LM’s\n",
      "working memory, we can enable (1) systematic\n",
      "exploration of reasoning paths, (2) atomic sub-\n",
      "queries enabling precise retrievals and ground-\n",
      "ing, and (3) efficiency through parallel execution\n",
      "and bounded context window utilization. More-\n",
      "over, Plan∗RAG’s modular design allows it to\n",
      "be integrated with existing RAG methods, thus\n",
      "providing a practical solution to improve current\n",
      "RAG systems. On standard multi-hop reasoning\n",
      "benchmarks, Plan∗RAG consistently achieves im-\n",
      "provements over recently proposed methods such\n",
      "as RQ-RAG and Self-RAG, while maintaining\n",
      "comparable computational costs.\n",
      "1. Introduction\n",
      "Retrieval-Augmented Generation (RAG, Lewis et al., 2020;\n",
      "Petroni et al., 2020; Guu et al., 2020) has emerged as a\n",
      "promising approach for grounding language model (LM)\n",
      "responses in external knowledge. However, RAG systems\n",
      "struggle with multi-hop queries that require reasoning across\n",
      "multiple retrieved documents (Tang & Yang, 2024; Wei\n",
      "et al., 2022). A key challenge lies in the initial retrieval step,\n",
      "which often fails to retrieve sufficient relevant documents\n",
      "due to the query’s lack of full contextual information (Ma\n",
      "et al., 2023). This limitation has been highlighted in recent\n",
      "surveys (Torfi et al., 2020; Zhao et al., 2023) as a funda-\n",
      "† Work done during an internship with Microsoft Research.\n",
      "1Aalto University, Finland 2Microsoft Research. Correspondence\n",
      "to: <prakhar.verma@aalto.fi>, <amshar@microsoft.com>.\n",
      "RAG\n",
      "Self-RAG\n",
      "ReAct\n",
      "20\n",
      "25\n",
      "30\n",
      "35\n",
      "40\n",
      "45\n",
      "Test-time planning improves RAG\n",
      "25.51\n",
      "34.09\n",
      "33.15\n",
      "31.12\n",
      "37.31\n",
      "40.44\n",
      "Accuracy (%)\n",
      "Vanilla\n",
      "With Plan∗\n",
      "Figure 1. Plan∗RAG improves performance on the HotpotQA\n",
      "benchmark substantially compared to various existing RAG meth-\n",
      "ods, demonstrating the value of externalizing planning as a directed\n",
      "acyclic graph (DAG) outside of the LLM’s context.\n",
      "mental barrier to reliable AI systems, particularly given the\n",
      "widespread deployment of large language models (Brown\n",
      "et al., 2020) across critical domains. Consider the query:\n",
      "“Rumble Fish was a novel by the author of the coming-of-age\n",
      "novel published in what year by Viking Press?” Answering\n",
      "this requires an iterative retrieval process: identifying the\n",
      "Rumble Fish’s author, connecting to their coming-of-age\n",
      "novel, and determining its publication year. Single-step\n",
      "retrieval in RAG systems often fails in such cases, as it\n",
      "may retrieve documents about Rumble Fish’s author and\n",
      "Viking Press without recognizing the intermediate fact—\n",
      "the author’s coming-of-age novel—must first be established.\n",
      "Furthermore, Leng et al. (2024); Shuster et al. (2021) demon-\n",
      "strate that even when relevant documents are retrieved, LMs\n",
      "struggle to reason across them due to fixed context win-\n",
      "dows, leading to information loss and broken reasoning\n",
      "chain. These limitations pose risks in critical domains such\n",
      "as healthcare and finance (Pal et al., 2023; Zhao et al., 2024),\n",
      "where accurate multi-step reasoning is essential.\n",
      "Recent research has attempted to address these limita-\n",
      "tions through structured reasoning frameworks. Chain-of-\n",
      "Thought (CoT) prompting (Wei et al., 2022) and systematic\n",
      "query decomposition (Patel et al., 2022) have introduced\n",
      "explicit reasoning steps, enabling more granular thought\n",
      "processes and targeted retrievals. Building upon these foun-\n",
      "dations, Yao et al. (2023) proposed ReAct—a framework\n",
      "that creates a reasoning chain \n",
      "</Content>\n",
      "</Document>\n"
     ]
    }
   ],
   "source": [
    "print(formatted_search_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import get_buffer_string\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "# 검색 쿼리 작성\n",
    "search_instructions = SystemMessage(\n",
    "    content=f\"\"\"You will be given a conversation between an analyst and an expert. \n",
    "\n",
    "Your goal is to generate a well-structured query for use in retrieval and / or web-search related to the conversation.\n",
    "\n",
    "First, analyze the full conversation.\n",
    "\n",
    "Pay particular attention to the final question posed by the analyst.\n",
    "\n",
    "Convert this final question into a well-structured web search query\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "# 웹 검색 수행 함수 정의\n",
    "def search_web(state: InterviewState):\n",
    "    \"\"\"웹 검색을 통한 문서 검색\"\"\"\n",
    "\n",
    "    # 검색 쿼리 생성\n",
    "    structured_llm = llm.with_structured_output(SearchQuery)\n",
    "    search_query = structured_llm.invoke([search_instructions] + state[\"messages\"])\n",
    "\n",
    "    # 검색 수행\n",
    "    search_docs = tavily_search.invoke(search_query.search_query)\n",
    "\n",
    "    # 검색 결과 형식 지정\n",
    "    formatted_search_docs = \"\\n\\n---\\n\\n\".join(\n",
    "        [\n",
    "            f'<Document href=\"{doc[\"url\"]}\"/>\\n{doc[\"content\"]}\\n</Document>'\n",
    "            for doc in search_docs\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return {\"context\": [formatted_search_docs]}\n",
    "\n",
    "\n",
    "# Arxiv 검색 노드 생성\n",
    "def search_arxiv(state: InterviewState):\n",
    "    \"\"\"Arxiv 검색 노드\"\"\"\n",
    "\n",
    "    # 검색 쿼리 생성\n",
    "    structured_llm = llm.with_structured_output(SearchQuery)\n",
    "    search_query = structured_llm.invoke([search_instructions] + state[\"messages\"])\n",
    "\n",
    "    try:\n",
    "        # 검색 수행\n",
    "        arxiv_search_results = arxiv_retriever.invoke(\n",
    "            search_query.search_query,\n",
    "            load_max_docs=2,\n",
    "            load_all_available_meta=True,\n",
    "            get_full_documents=True,\n",
    "        )\n",
    "\n",
    "        # 검색 결과 형식 지정\n",
    "        formatted_search_docs = \"\\n\\n---\\n\\n\".join(\n",
    "            [\n",
    "                f'<Document source=\"{doc.metadata[\"entry_id\"]}\" date=\"{doc.metadata.get(\"Published\", \"\")}\" authors=\"{doc.metadata.get(\"Authors\", \"\")}\"/>\\n<Title>\\n{doc.metadata[\"Title\"]}\\n</Title>\\n\\n<Summary>\\n{doc.metadata[\"Summary\"]}\\n</Summary>\\n\\n<Content>\\n{doc.page_content}\\n</Content>\\n</Document>'\n",
    "                for doc in arxiv_search_results\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        return {\"context\": [formatted_search_docs]}\n",
    "    except Exception as e:\n",
    "        print(f\"Arxiv 검색 중 오류 발생: {str(e)}\")\n",
    "        return {\n",
    "            \"context\": [\"<Error>Arxiv 검색 결과를 가져오는데 실패했습니다.</Error>\"]\n",
    "        }\n",
    "\n",
    "\n",
    "answer_instructions = \"\"\"You are an expert being interviewed by an analyst.\n",
    "\n",
    "Here is analyst area of focus: {goals}. \n",
    "\n",
    "You goal is to answer a question posed by the interviewer.\n",
    "\n",
    "To answer question, use this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "When answering questions, follow these guidelines:\n",
    "\n",
    "1. Use only the information provided in the context. \n",
    "\n",
    "2. Do not introduce external information or make assumptions beyond what is explicitly stated in the context.\n",
    "\n",
    "3. The context contain sources at the topic of each individual document.\n",
    "\n",
    "4. Include these sources your answer next to any relevant statements. For example, for source # 1 use [1]. \n",
    "\n",
    "5. List your sources in order at the bottom of your answer. [1] Source 1, [2] Source 2, etc\n",
    "\n",
    "6. If the source is: <Document source=\"assistant/docs/llama3_1.pdf\" page=\"7\"/>' then just list: \n",
    "\n",
    "[1] assistant/docs/llama3_1.pdf, page 7 \n",
    "\n",
    "And skip the addition of the brackets as well as the Document source preamble in your citation.\"\"\"\n",
    "\n",
    "\n",
    "# 질문에 대한 답변 생성 함수 정의\n",
    "def generate_answer(state: InterviewState):\n",
    "    \"\"\"질문에 대한 답변 생성 노드\"\"\"\n",
    "\n",
    "    # 상태에서 분석가와 메시지 가져오기\n",
    "    analyst = state[\"analyst\"]\n",
    "    messages = state[\"messages\"]\n",
    "    context = state[\"context\"]\n",
    "\n",
    "    # 질문에 대한 답변 생성\n",
    "    system_message = answer_instructions.format(goals=analyst.persona, context=context)\n",
    "    answer = llm.invoke([SystemMessage(content=system_message)] + messages)\n",
    "\n",
    "    # 메시지를 전문가의 답변으로 명명\n",
    "    answer.name = \"expert\"\n",
    "\n",
    "    # 상태에 메시지 추가\n",
    "    return {\"messages\": [answer]}\n",
    "\n",
    "\n",
    "# 인터뷰 저장 함수 정의\n",
    "def save_interview(state: InterviewState):\n",
    "    \"\"\"인터뷰 저장\"\"\"\n",
    "\n",
    "    # 메시지 가져오기\n",
    "    messages = state[\"messages\"]\n",
    "\n",
    "    # 인터뷰를 문자열로 변환\n",
    "    interview = get_buffer_string(messages)\n",
    "\n",
    "    # 인터뷰 키에 저장\n",
    "    return {\"interview\": interview}\n",
    "\n",
    "\n",
    "# 메시지 라우팅 함수 정의\n",
    "def route_messages(state: InterviewState, name: str = \"expert\"):\n",
    "    \"\"\"질문과 답변 사이의 라우팅\"\"\"\n",
    "\n",
    "    # 메시지 가져오기\n",
    "    messages = state[\"messages\"]\n",
    "    max_num_turns = state.get(\"max_num_turns\", 2)\n",
    "\n",
    "    # 전문가의 답변 수 확인\n",
    "    num_responses = len(\n",
    "        [m for m in messages if isinstance(m, AIMessage) and m.name == name]\n",
    "    )\n",
    "\n",
    "    # 전문가가 최대 턴 수 이상 답변한 경우 종료\n",
    "    if num_responses >= max_num_turns:\n",
    "        return \"save_interview\"\n",
    "\n",
    "    # 이 라우터는 각 질문-답변 쌍 후에 실행됨\n",
    "    # 논의 종료를 신호하는 마지막 질문 가져오기\n",
    "    last_question = messages[-2]\n",
    "\n",
    "    if \"Thank you so much for your help\" in last_question.content:\n",
    "        return \"save_interview\"\n",
    "    return \"ask_question\"\n",
    "\n",
    "\n",
    "# 세션 작성 지시사항\n",
    "section_writer_instructions = \"\"\"You are an expert technical writer. \n",
    "\n",
    "Your task is to create a detailed and comprehensive section of a report, thoroughly analyzing a set of source documents.\n",
    "This involves extracting key insights, elaborating on relevant points, and providing in-depth explanations to ensure clarity and understanding. Your writing should include necessary context, supporting evidence, and examples to enhance the reader's comprehension. Maintain a logical and well-organized structure, ensuring that all critical aspects are covered in detail and presented in a professional tone.\n",
    "\n",
    "Please follow these instructions:\n",
    "1. Analyze the content of the source documents: \n",
    "- The name of each source document is at the start of the document, with the <Document tag.\n",
    "\n",
    "2. Create a report structure using markdown formatting:\n",
    "- Use ## for the section title\n",
    "- Use ### for sub-section headers\n",
    "\n",
    "3. Write the report following this structure:\n",
    "a. Title (## header)\n",
    "b. Summary (### header)\n",
    "c. Comprehensive analysis (### header)\n",
    "d. Sources (### header)\n",
    "\n",
    "4. Make your title engaging based upon the focus area of the analyst: \n",
    "{focus}\n",
    "\n",
    "5. For the summary section:\n",
    "- Set up summary with general background / context related to the focus area of the analyst\n",
    "- Emphasize what is novel, interesting, or surprising about insights gathered from the interview\n",
    "- Create a numbered list of source documents, as you use them\n",
    "- Do not mention the names of interviewers or experts\n",
    "- Aim for approximately 400 words maximum\n",
    "- Use numbered sources in your report (e.g., [1], [2]) based on information from source documents\n",
    "\n",
    "6. For the Comprehensive analysis section:\n",
    "- Provide a detailed examination of the information from the source documents.\n",
    "- Break down complex ideas into digestible segments, ensuring a logical flow of ideas.\n",
    "- Use sub-sections where necessary to cover multiple perspectives or dimensions of the analysis.\n",
    "- Support your analysis with data, direct quotes, and examples from the source documents.\n",
    "- Clearly explain the relevance of each point to the overall focus of the report.\n",
    "- Use bullet points or numbered lists for clarity when presenting multiple related ideas.\n",
    "- Ensure the tone remains professional and objective, avoiding bias or unsupported opinions.\n",
    "- Aim for at least 800 words to ensure the analysis is thorough.\n",
    "\n",
    "7. In the Sources section:\n",
    "- Include all sources used in your report\n",
    "- Provide full links to relevant websites or specific document paths\n",
    "- Separate each source by a newline. Use two spaces at the end of each line to create a newline in Markdown.\n",
    "- It will look like:\n",
    "\n",
    "### Sources\n",
    "[1] Link or Document name\n",
    "[2] Link or Document name\n",
    "\n",
    "8. Be sure to combine sources. For example this is not correct:\n",
    "\n",
    "[3] https://ai.meta.com/blog/meta-llama-3-1/\n",
    "[4] https://ai.meta.com/blog/meta-llama-3-1/\n",
    "\n",
    "There should be no redundant sources. It should simply be:\n",
    "\n",
    "[3] https://ai.meta.com/blog/meta-llama-3-1/\n",
    "\n",
    "9. Final review:\n",
    "- Ensure the report follows the required structure\n",
    "- Include no preamble before the title of the report\n",
    "- Check that all guidelines have been followed\"\"\"\n",
    "\n",
    "\n",
    "# 섹션 작성 함수 정의\n",
    "def write_section(state: InterviewState):\n",
    "    \"\"\"질문에 대한 답변 생성 노드\"\"\"\n",
    "\n",
    "    # 상태에서 컨텍스트, 분석가 가져오기\n",
    "    context = state[\"context\"]\n",
    "    analyst = state[\"analyst\"]\n",
    "\n",
    "    # 섹션 작성을 위한 시스템 프롬프트 정의\n",
    "    system_message = section_writer_instructions.format(focus=analyst.description)\n",
    "    section = llm.invoke(\n",
    "        [SystemMessage(content=system_message)]\n",
    "        + [HumanMessage(content=f\"Use this source to write your section: {context}\")]\n",
    "    )\n",
    "\n",
    "    # 상태에 섹션 추가\n",
    "    return {\"sections\": [section.content]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# 노드 및 엣지 추가\n",
    "interview_builder = StateGraph(InterviewState)\n",
    "interview_builder.add_node(\"ask_question\", generate_question)\n",
    "interview_builder.add_node(\"search_web\", search_web)\n",
    "interview_builder.add_node(\"search_arxiv\", search_arxiv)\n",
    "interview_builder.add_node(\"answer_question\", generate_answer)\n",
    "interview_builder.add_node(\"save_interview\", save_interview)\n",
    "interview_builder.add_node(\"write_section\", write_section)\n",
    "\n",
    "# 흐름 설정\n",
    "interview_builder.add_edge(START, \"ask_question\")\n",
    "interview_builder.add_edge(\"ask_question\", \"search_web\")\n",
    "interview_builder.add_edge(\"ask_question\", \"search_arxiv\")\n",
    "interview_builder.add_edge(\"search_web\", \"answer_question\")\n",
    "interview_builder.add_edge(\"search_arxiv\", \"answer_question\")\n",
    "interview_builder.add_conditional_edges(\n",
    "    \"answer_question\", route_messages, [\"ask_question\", \"save_interview\"]\n",
    ")\n",
    "interview_builder.add_edge(\"save_interview\", \"write_section\")\n",
    "interview_builder.add_edge(\"write_section\", END)\n",
    "\n",
    "# 인터뷰 그래프 생성\n",
    "memory = MemorySaver()\n",
    "interview_graph = interview_builder.compile(checkpointer=memory).with_config(\n",
    "    run_name=\"Conduct Interviews\"\n",
    ")\n",
    "\n",
    "# 그래프 시각화\n",
    "visualize_graph(interview_graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Analyst(affiliation='Independent Researcher', name='Dr. Hannah Kim', role='Academic Specialist', description='Dr. Kim is an academic researcher with a focus on machine learning frameworks and their applications in real-world scenarios. She is particularly interested in comparing Modular RAG and Naive RAG in terms of flexibility and efficiency for handling complex data requirements in production settings.')"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 분석가 목록에서 첫 번째 분석가 선택\n",
    "analysts[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mask_question\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello, Dr. Kim. My name is Alex Thompson, and I'm an analyst deeply interested in the practical applications of machine learning frameworks, particularly Modular RAG and Naive RAG. I'm eager to understand the differences between these two frameworks, especially in terms of their flexibility and efficiency in handling complex data requirements in production settings. Could you please elaborate on these aspects for us?\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36msearch_arxiv\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "<Document source=\"http://arxiv.org/abs/2508.05650v1\" date=\"2025-07-26\" authors=\"Jiaxuan Liang, Shide Zhou, Kailong Wang\"/>\n",
      "<Title>\n",
      "OmniBench-RAG: A Multi-Domain Evaluation Platform for Retrieval-Augmented Generation Tools\n",
      "</Title>\n",
      "\n",
      "<Summary>\n",
      "While Retrieval Augmented Generation (RAG) is now widely adopted to enhance\n",
      "LLMs, evaluating its true performance benefits in a reproducible and\n",
      "interpretable way remains a major hurdle. Existing methods often fall short:\n",
      "they lack domain coverage, employ coarse metrics that miss sub document\n",
      "precision, and fail to capture computational trade offs. Most critically, they\n",
      "provide no standardized framework for comparing RAG effectiveness across\n",
      "different models and domains.\n",
      "  We introduce OmniBench RAG, a novel automated platform for multi domain\n",
      "evaluation of RAG systems. The platform quantifies performance gains across\n",
      "accuracy and efficiency dimensions, spanning nine knowledge fields including\n",
      "culture, geography, and health. We introduce two standardized metrics:\n",
      "Improvements (accuracy gains) and Transformation (efficiency differences\n",
      "between pre RAG and post RAG models), enabling reproducible comparisons across\n",
      "models and tasks. The platform features dynamic test generation, modular\n",
      "evaluation pipelines, and automated knowledge base construction. Our evaluation\n",
      "reveals striking variability in RAG effectiveness, from significant gains in\n",
      "culture to declines in mathematics, highlighting the critical importance of\n",
      "systematic, domain aware assessment. A demonstration video is available at:\n",
      "https://www.youtube.com/watch?v=BZx83QFcTCI. Code and datasets:\n",
      "https://github.com/Garnett-Liang/Omnibench-RAG.\n",
      "</Summary>\n",
      "\n",
      "<Content>\n",
      "OmniBench-RAG: A Multi-Domain Evaluation\n",
      "Platform for Retrieval-Augmented Generation Tools\n",
      "Jiaxuan Liang*, Shide Zhou*, and Kailong Wang†\n",
      "Huazhong University of Science and Technology\n",
      "{liangjx, shidez, wangkl}@hust.edu.cn\n",
      "Abstract—While Retrieval Augmented Generation (RAG) is\n",
      "now widely adopted to enhance LLMs, evaluating its true\n",
      "performance benefits in a reproducible and interpretable way\n",
      "remains a major hurdle. Existing methods often fall short: they\n",
      "lack domain coverage, employ coarse metrics that miss sub\n",
      "document precision, and fail to capture computational trade\n",
      "offs. Most critically, they provide no standardized framework\n",
      "for comparing RAG effectiveness across different models and\n",
      "domains.\n",
      "We introduce OmniBench RAG, a novel automated platform\n",
      "for multi domain evaluation of RAG systems. The platform\n",
      "quantifies performance gains across accuracy and efficiency\n",
      "dimensions, spanning nine knowledge fields including culture,\n",
      "geography, and health. We introduce two standardized metrics:\n",
      "Improvements (accuracy gains) and Transformation (efficiency\n",
      "differences between pre RAG and post RAG models), enabling\n",
      "reproducible comparisons across models and tasks. The platform\n",
      "features dynamic test generation, modular evaluation pipelines,\n",
      "and automated knowledge base construction. Our evaluation\n",
      "reveals striking variability in RAG effectiveness, from significant\n",
      "gains in culture to declines in mathematics, highlighting the\n",
      "critical importance of systematic, domain aware assessment. A\n",
      "demonstration video is available at: https://www.youtube.com/\n",
      "watch?v=BZx83QFcTCI. Code and datasets: https://github.com/\n",
      "Garnett-Liang/Omnibench-RAG.\n",
      "I. INTRODUCTION\n",
      "Retrieval-Augmented Generation (RAG) is a key technique\n",
      "for enhancing Large Language Models (LLMs) [1], [2]. By\n",
      "grounding model responses in external, verifiable knowledge,\n",
      "RAG promises to mitigate hallucinations [3], improve factual\n",
      "accuracy [4], and provide up-to-date information [5]. However,\n",
      "the true effectiveness of RAG is far from uniform. Recent\n",
      "studies [6], [7] reveal a significant disparity: while RAG\n",
      "can boost the accuracy of smaller models like Llama-3.2-3B-\n",
      "Instruct by as much as 38.12%, its impact on state-of-the-art\n",
      "models such as GPT-4o, which excel with extended context\n",
      "windows, is often less pronounced. This variability, which\n",
      "depends not only on the model’s scale but also heavily on\n",
      "the knowledge domain, underscores a critical challenge: the\n",
      "lack of a systematic platform to quantify the value of RAG\n",
      "across these diverse contexts.\n",
      "Current RAG evaluation approaches suffer from fundamen-\n",
      "tal limitations that impede reproducible and comprehensive as-\n",
      "sessment: First, they lack automated multi-domain evalua-\n",
      "tion capabilities and rely on non-deterministic components.\n",
      "*Jiaxuan Liang and Shide Zhou are co-first authors.\n",
      "†Kailong Wang is the corresponding author.\n",
      "Existing benchmarks typically require manual configuration\n",
      "for each knowledge domain and fail to provide unified assess-\n",
      "ment across diverse fields like finance, healthcare, or culture,\n",
      "making cross-domain performance analysis labor-intensive and\n",
      "inconsistent. Moreover, key metrics in leading frameworks\n",
      "(e.g., LLM-based scoring in Ragas [8]) inadvertently introduce\n",
      "randomness due to reliance on large language models in the\n",
      "evaluation loop, undermining result reproducibility. Second,\n",
      "they employ static datasets and coarse-grained metrics.\n",
      "Most frameworks rely on fixed benchmarks and document-\n",
      "level retrieval metrics (e.g., MRR@k[9]), missing the critical\n",
      "sub-document precision needed to assess whether models\n",
      "extract specific facts accurately. They also lack the ability to\n",
      "dynamically generate test cases that probe complex reasoning\n",
      "patterns. Third, they fail to capture the computational\n",
      "trade-offs inherent in RAG systems. Without automated\n",
      "profiling of resource utilization and efficiency metrics, practi-\n",
      "tioners cannot make informed decisions about the cost-benefit\n",
      "trade-offs of deploying RAG in production environments.\n",
      "Our \n",
      "</Content>\n",
      "</Document>\n",
      "\n",
      "---\n",
      "\n",
      "<Document source=\"http://arxiv.org/abs/2508.13828v1\" date=\"2025-08-19\" authors=\"Yifei Chen, Guanting Dong, Yutao Zhu, Zhicheng Dou\"/>\n",
      "<Title>\n",
      "Revisiting RAG Ensemble: A Theoretical and Mechanistic Analysis of Multi-RAG System Collaboration\n",
      "</Title>\n",
      "\n",
      "<Summary>\n",
      "Retrieval-Augmented Generation (RAG) technology has been widely applied in\n",
      "recent years. However, despite the emergence of various RAG frameworks, a\n",
      "single RAG framework still cannot adapt well to a broad range of downstream\n",
      "tasks. Therefore, how to leverage the advantages of multiple RAG systems has\n",
      "become an area worth exploring. To address this issue, we have conducted a\n",
      "comprehensive and systematic investigation into ensemble methods based on RAG\n",
      "systems. Specifically, we have analyzed the RAG ensemble framework from both\n",
      "theoretical and mechanistic analysis perspectives. From the theoretical\n",
      "analysis, we provide the first explanation of the RAG ensemble framework from\n",
      "the perspective of information entropy. In terms of mechanism analysis, we have\n",
      "explored the RAG ensemble framework from both the pipeline and module levels.\n",
      "We carefully select four different pipelines (Branching, Iterative, Loop, and\n",
      "Agentic) and three different modules (Generator, Retriever, and Reranker) to\n",
      "solve seven different research questions. The experiments show that aggregating\n",
      "multiple RAG systems is both generalizable and robust, whether at the pipeline\n",
      "level or the module level. Our work lays the foundation for similar research on\n",
      "the multi-RAG system ensemble.\n",
      "</Summary>\n",
      "\n",
      "<Content>\n",
      "Revisiting RAG Ensemble: A Theoretical and Mechanistic\n",
      "Analysis of Multi-RAG System Collaboration\n",
      "Yifei Chen\n",
      "zhangboguodong@ruc.edu.cn\n",
      "Gaoling School of Artificial Intelligence, Renmin\n",
      "University of China\n",
      "Haidian Qu, Beijing Shi, China\n",
      "Guanting Dong\n",
      "Gaoling School of Artificial Intelligence, Renmin\n",
      "University of China\n",
      "Haidian Qu, Beijing Shi, China\n",
      "Yutao Zhu\n",
      "Gaoling School of Artificial Intelligence, Renmin\n",
      "University of China\n",
      "Haidian Qu, Beijing Shi, China\n",
      "Zhicheng Dou\n",
      "Gaoling School of Artificial Intelligence, Renmin\n",
      "University of China\n",
      "Haidian Qu, Beijing Shi, China\n",
      "Abstract\n",
      "Retrieval-Augmented Generation (RAG) technology has been widely\n",
      "applied in recent years. However, despite the emergence of various\n",
      "RAG frameworks, a single RAG framework still cannot adapt well\n",
      "to a broad range of downstream tasks. Therefore, how to leverage\n",
      "the advantages of multiple RAG systems has become an area worth\n",
      "exploring. To address this issue, we have conducted a comprehen-\n",
      "sive and systematic investigation into ensemble methods based on\n",
      "RAG systems. Specifically, we have analyzed the RAG ensemble\n",
      "framework from both theoretical and mechanistic analysis perspec-\n",
      "tives. From the theoretical analysis, we provide the first explanation\n",
      "of the RAG ensemble framework from the perspective of informa-\n",
      "tion entropy. In terms of mechanism analysis, we have explored\n",
      "the RAG ensemble framework from both the pipeline and module\n",
      "levels. We carefully select four different pipelines (Branching, Iter-\n",
      "ative, Loop, and Agentic) and three different modules (Generator,\n",
      "Retriever, and Reranker) to solve seven different research questions.\n",
      "The experiments show that aggregating multiple RAG systems is\n",
      "both generalizable and robust, whether at the pipeline level or the\n",
      "module level. Our work lays the foundation for similar research on\n",
      "the multi-RAG system ensemble.\n",
      "CCS Concepts\n",
      "• Information systems →Information integration.\n",
      "Keywords\n",
      "Retrieval-Augmented Generation, Pipeline Ensemble, Module En-\n",
      "semble, Model Preference\n",
      "ACM Reference Format:\n",
      "Yifei Chen, Guanting Dong, Yutao Zhu, and Zhicheng Dou. 2025. Revisiting\n",
      "RAG Ensemble: A Theoretical and Mechanistic Analysis of Multi-RAG\n",
      "Permission to make digital or hard copies of all or part of this work for personal or\n",
      "classroom use is granted without fee provided that copies are not made or distributed\n",
      "for profit or commercial advantage and that copies bear this notice and the full citation\n",
      "on the first page. Copyrights for components of this work owned by others than the\n",
      "author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\n",
      "republish, to post on servers or to redistribute to lists, requires prior specific permission\n",
      "and/or a fee. Request permissions from permissions@acm.org.\n",
      "CIKM’25, Seoul, Korea\n",
      "© 2025 Copyright held by the owner/author(s). Publication rights licensed to ACM.\n",
      "ACM ISBN 978-1-4503-XXXX-X/2018/06\n",
      "https://doi.org/XXXXXXX.XXXXXXX\n",
      "System Collaboration. In Proceedings of CIKM’25. ACM, New York, NY, USA,\n",
      "13 pages. https://doi.org/XXXXXXX.XXXXXXX\n",
      "1\n",
      "Introduction\n",
      "The emergence of Large Language Models (LLMs) has profoundly\n",
      "revolutionized many real-world tasks that rely on natural lan-\n",
      "guage [4, 45, 70]. However, when dealing with knowledge-intensive\n",
      "tasks, LLMs relying solely on their parametric knowledge often suf-\n",
      "fer from factual inconsistencies or hallucinations. To address these\n",
      "limitations, Retrieval-Augmented Generation (RAG) methods have\n",
      "been proposed, augmenting LLMs with dynamically retrieved exter-\n",
      "nal knowledge. This integration enhances response accuracy and\n",
      "reliability by grounding outputs in verifiable information sources.\n",
      "As research in this field advances, more and more RAG methods\n",
      "have been proposed. Component Module RAG inserts various mod-\n",
      "ules into the standard pipeline to better complete the retrieval task.\n",
      "For instance, the LongLLMLingua and RECOMP methods refine\n",
      "the retrieved knowledge with a refiner, and the SKR and Adaptive\n",
      "RAG methods distinguish the diffi\n",
      "</Content>\n",
      "</Document>\n",
      "\n",
      "---\n",
      "\n",
      "<Document source=\"http://arxiv.org/abs/2507.18515v1\" date=\"2025-07-24\" authors=\"Zezhou Yang, Ting Peng, Cuiyun Gao, Chaozheng Wang, Hailiang Huang, Yuetang Deng\"/>\n",
      "<Title>\n",
      "A Deep Dive into Retrieval-Augmented Generation for Code Completion: Experience on WeChat\n",
      "</Title>\n",
      "\n",
      "<Summary>\n",
      "Code completion, a crucial task in software engineering that enhances\n",
      "developer productivity, has seen substantial improvements with the rapid\n",
      "advancement of large language models (LLMs). In recent years,\n",
      "retrieval-augmented generation (RAG) has emerged as a promising method to\n",
      "enhance the code completion capabilities of LLMs, which leverages relevant\n",
      "context from codebases without requiring model retraining. While existing\n",
      "studies have demonstrated the effectiveness of RAG on public repositories and\n",
      "benchmarks, the potential distribution shift between open-source and\n",
      "closed-source codebases presents unique challenges that remain unexplored. To\n",
      "mitigate the gap, we conduct an empirical study to investigate the performance\n",
      "of widely-used RAG methods for code completion in the industrial-scale codebase\n",
      "of WeChat, one of the largest proprietary software systems. Specifically, we\n",
      "extensively explore two main types of RAG methods, namely identifier-based RAG\n",
      "and similarity-based RAG, across 26 open-source LLMs ranging from 0.5B to 671B\n",
      "parameters. For a more comprehensive analysis, we employ different retrieval\n",
      "techniques for similarity-based RAG, including lexical and semantic retrieval.\n",
      "Based on 1,669 internal repositories, we achieve several key findings: (1) both\n",
      "RAG methods demonstrate effectiveness in closed-source repositories, with\n",
      "similarity-based RAG showing superior performance, (2) the effectiveness of\n",
      "similarity-based RAG improves with more advanced retrieval techniques, where\n",
      "BM25 (lexical retrieval) and GTE-Qwen (semantic retrieval) achieve superior\n",
      "performance, and (3) the combination of lexical and semantic retrieval\n",
      "techniques yields optimal results, demonstrating complementary strengths.\n",
      "Furthermore, we conduct a developer survey to validate the practical utility of\n",
      "RAG methods in real-world development environments.\n",
      "</Summary>\n",
      "\n",
      "<Content>\n",
      "A Deep Dive into Retrieval-Augmented Generation\n",
      "for Code Completion: Experience on WeChat\n",
      "Zezhou Yang\n",
      "Tencent\n",
      "Guangzhou, China\n",
      "zezhouyang@tencent.com\n",
      "Ting Peng\n",
      "Tencent\n",
      "Guangzhou, China\n",
      "sakurapeng@tencent.com\n",
      "Cuiyun Gao∗\n",
      "The Chinese University of Hong Kong\n",
      "Hong Kong, China\n",
      "cuiyungao@outlook.com\n",
      "Chaozheng Wang\n",
      "The Chinese University of Hong Kong\n",
      "Hong Kong, China\n",
      "adf111178@gmail.com\n",
      "Hailiang Huang\n",
      "Tencent\n",
      "Guangzhou, China\n",
      "eraserhuang@tencent.com\n",
      "Yuetang Deng\n",
      "Tencent\n",
      "Guangzhou, China\n",
      "yuetangdeng@tencent.com\n",
      "Abstract—Code completion, a crucial task in software engi-\n",
      "neering that enhances developer productivity, has seen substantial\n",
      "improvements with the rapid advancement of large language\n",
      "models (LLMs). In recent years, retrieval-augmented generation\n",
      "(RAG) has emerged as a promising method to enhance the code\n",
      "completion capabilities of LLMs, which leverages relevant con-\n",
      "text from codebases without requiring model retraining. While\n",
      "existing studies have demonstrated the effectiveness of RAG on\n",
      "public repositories and benchmarks, the potential distribution\n",
      "shift between open-source and closed-source codebases presents\n",
      "unique challenges that remain unexplored. To mitigate the gap,\n",
      "we conduct an empirical study to investigate the performance of\n",
      "widely-used RAG methods for code completion in the industrial-\n",
      "scale codebase of WeChat, one of the largest proprietary software\n",
      "systems. Specifically, we extensively explore two main types\n",
      "of RAG methods, namely identifier-based RAG and similarity-\n",
      "based RAG, across 26 open-source LLMs ranging from 0.5B\n",
      "to 671B parameters. For a more comprehensive analysis, we\n",
      "employ different retrieval techniques for similarity-based RAG,\n",
      "including lexical and semantic retrieval. Based on 1,669 internal\n",
      "repositories, we achieve several key findings: (1) both RAG\n",
      "methods demonstrate effectiveness in closed-source repositories,\n",
      "with similarity-based RAG showing superior performance, (2)\n",
      "the effectiveness of similarity-based RAG improves with more\n",
      "advanced retrieval techniques, where BM25 (lexical retrieval)\n",
      "and GTE-Qwen (semantic retrieval) achieve superior perfor-\n",
      "mance, and (3) the combination of lexical and semantic retrieval\n",
      "techniques yields optimal results, demonstrating complemen-\n",
      "tary strengths. Furthermore, we conduct a developer survey\n",
      "to validate the practical utility of RAG methods in real-world\n",
      "development environments.\n",
      "Index Terms—large language model, retrieval-augmented gen-\n",
      "eration, code completion\n",
      "I. INTRODUCTION\n",
      "Code completion, which automatically predicts and sug-\n",
      "gests code fragments based on the surrounding programming\n",
      "context, has evolved from simple token-level suggestions to\n",
      "∗Cuiyun Gao is the corresponding author.\n",
      "This research is supported by National Key R&D Program of China\n",
      "(No. 2022YFB3103900), National Natural Science Foundation of China\n",
      "under project (No. 62472126), Natural Science Foundation of Guangdong\n",
      "Province (Project No. 2023A1515011959), Shenzhen-Hong Kong Jointly\n",
      "Funded Project (Category A, No. SGDX20230116091246007), and Shenzhen\n",
      "Basic Research (General Project No. JCYJ20220531095214031).\n",
      "generating entire code blocks [1], [2]. Studies have demon-\n",
      "strated that code completion tools substantially enhance de-\n",
      "veloper productivity in real-world software development [3],\n",
      "[4]. Notably, 87% of professional developers report significant\n",
      "improvements in their coding efficiency when utilizing code\n",
      "completion tools in industrial settings [5]. Recent advances\n",
      "in large language models (LLMs) have further transformed\n",
      "various software engineering tasks [6]–[10], demonstrating\n",
      "unprecedented capabilities in code understanding and gen-\n",
      "eration. These models have achieved particularly impressive\n",
      "performance in code completion tasks [11]–[13].\n",
      "To enhance LLMs’ performance on domain-specific tasks,\n",
      "researchers have explored Retrieval-Augmented Generation\n",
      "(RAG), which augments model inference by retrieving and in-\n",
      "corporating relevant context from the target codebase without\n",
      "requiring param\n",
      "</Content>\n",
      "</Document>\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36msearch_web\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "<Document href=\"https://rabiloo.com/blog/the-3-types-of-rag-models-naive-rag-modular-rag-and-advanced-rag\"/>\n",
      "Building AI systems requires choosing the right tools for the job, and Retrieval-Augmented Generation (RAG) offers various models, each designed to serve different needs. Essentially, a RAG model retrieves information from external databases, knowledge bases, or the web and then uses that data to generate relevant, meaningful responses. Naive RAG begins by indexing the data source for quick retrieval of relevant information. Modular RAG builds on the basic principles of Naive RAG by breaking down the retrieval and generation processes into separate, specialized modules. Unlike Naive RAG or Modular RAG, this stage is more precise and contextually aware, ensuring that the system retrieves only the most relevant and contextually appropriate data. The generation module in Advanced RAG then processes the refined data using sophisticated models like T5 or GPT-3.\n",
      "</Document>\n",
      "\n",
      "---\n",
      "\n",
      "<Document href=\"https://medium.com/@drjulija/what-are-naive-rag-advanced-rag-modular-rag-paradigms-edff410c202e\"/>\n",
      "I share the differences between Naive RAG, Advanced RAG and Modular RAG frameworks. Retrieval-Augmented Generation (RAG) framework is designed to help mitigate these challenges. RAG enhances LLMs’ knowledge base with additional, domain-specific data. RAG systems can be split into 3 categories: See the comparison between all three paradigms of RAG — Naive RAG, Advanced RAG and Modular RAG below.\n",
      "</Document>\n",
      "\n",
      "---\n",
      "\n",
      "<Document href=\"https://blog.searce.com/unlocking-the-power-of-llm-rag-discover-naive-advanced-and-modular-retrieval-augmented-6922b353d8d3\"/>\n",
      "Aug 5, 2024·Modular RAG takes the concept further by enabling the customization and modularization of the retrieval process. This allows for tailored\n",
      "</Document>\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36manswer_question\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: expert\n",
      "\n",
      "Certainly, Dr. Kim. The primary difference between Modular RAG and Naive RAG lies in their structure and approach to information retrieval and generation.\n",
      "\n",
      "Naive RAG starts by indexing data sources to enable quick retrieval of relevant information. It maintains a straightforward approach where the retrieval and generation processes occur in a single, undivided pipeline. This simplicity can sometimes limit its flexibility and precision, particularly in handling complex or specialized data requirements in production environments [4].\n",
      "\n",
      "Modular RAG, on the other hand, builds upon Naive RAG by compartmentalizing the retrieval and generation processes into distinct, specialized modules. This modularity allows for greater flexibility as each module can be independently optimized or replaced to better suit specific data characteristics or processing needs. For example, different retrieval techniques can be employed within the same system, depending on the data type and retrieval demands. This separation allows for more precise and context-aware data retrieval, which enhances the overall responsiveness and relevance of the generated outputs [4][5].\n",
      "\n",
      "In production settings, using Modular RAG offers several benefits:\n",
      "\n",
      "1. **Customization**: By separating the retrieval and generation processes, organizations can tailor each component to better align with their data and application requirements, ensuring higher performance and efficiency.\n",
      "\n",
      "2. **Scalability**: Modular systems can be easily scaled by adapting or swapping individual modules without overhauling the entire architecture, which is particularly useful as data complexity grows.\n",
      "\n",
      "3. **Improved Accuracy**: With specialized modules, Modular RAG systems can achieve better accuracy in retrieving relevant data and generating coherent outputs, as each module is tuned to handle specific tasks optimally.\n",
      "\n",
      "These features make Modular RAG a more robust choice for handling complex and dynamic data requirements in production environments compared to Naive RAG [4].\n",
      "\n",
      "Sources:\n",
      "[4] medium.com/@drjulija/what-are-naive-rag-advanced-rag-modular-rag-paradigms-edff410c202e\n",
      "[5] blog.searce.com/unlocking-the-power-of-llm-rag-discover-naive-advanced-and-modular-retrieval-augmented-6922b353d8d3\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mask_question\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Thank you for that detailed explanation, Dr. Kim. Could you please provide a specific example or case study where Modular RAG was effectively implemented in a production setting, highlighting the challenges faced and how the modular approach was beneficial?\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36msearch_web\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "<Document href=\"https://rabiloo.com/blog/the-3-types-of-rag-models-naive-rag-modular-rag-and-advanced-rag\"/>\n",
      "Building AI systems requires choosing the right tools for the job, and Retrieval-Augmented Generation (RAG) offers various models, each designed to serve different needs. Essentially, a RAG model retrieves information from external databases, knowledge bases, or the web and then uses that data to generate relevant, meaningful responses. Naive RAG begins by indexing the data source for quick retrieval of relevant information. Modular RAG builds on the basic principles of Naive RAG by breaking down the retrieval and generation processes into separate, specialized modules. Unlike Naive RAG or Modular RAG, this stage is more precise and contextually aware, ensuring that the system retrieves only the most relevant and contextually appropriate data. The generation module in Advanced RAG then processes the refined data using sophisticated models like T5 or GPT-3.\n",
      "</Document>\n",
      "\n",
      "---\n",
      "\n",
      "<Document href=\"https://adasci.org/how-does-modular-rag-improve-upon-naive-rag/\"/>\n",
      "Naive RAG, the initial implementation of Retrieval-Augmented Generation, operates on a straightforward principle: retrieve relevant documents from an external knowledge base and use these documents to inform the generative process. The retrieval process in Naive RAG is relatively static and lacks flexibility, often leading to inefficiencies and suboptimal integration with the generative model. By adopting a modular architecture, this approach addresses the limitations of Naive RAG, offering enhanced flexibility, scalability, and efficiency. Unlike Naive RAG, which operates as a monolithic entity, Modular RAG breaks down the retrieval and generation processes into distinct, interchangeable modules. * *Seamless Integration*: Generative models in Modular RAG are designed to seamlessly integrate with various retrieval modules, enhancing the coherence and relevance of generated responses.\n",
      "</Document>\n",
      "\n",
      "---\n",
      "\n",
      "<Document href=\"https://www.superteams.ai/blog/how-to-implement-naive-rag-advanced-rag-and-modular-rag\"/>\n",
      "Naive RAG is a paradigm that combines information retrieval with natural language generation to produce responses to queries or prompts. In Naive RAG, retrieval is typically performed using retrieval models that rank the indexed data based on its relevance to the input query. These models generate text based on the input query and the retrieved context, aiming to produce coherent and contextually relevant responses. Advanced RAG models may fine-tune embeddings to capture task-specific semantics or domain knowledge, thereby improving the quality of retrieved information and generated responses. Dynamic embedding techniques enable RAG models to adaptively adjust embeddings during inference based on the context of the query or retrieved information.\n",
      "</Document>\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36msearch_arxiv\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "<Document source=\"http://arxiv.org/abs/2502.11228v2\" date=\"2025-05-22\" authors=\"Mohammad Reza Rezaei, Adji Bousso Dieng\"/>\n",
      "<Title>\n",
      "Vendi-RAG: Adaptively Trading-Off Diversity And Quality Significantly Improves Retrieval Augmented Generation With LLMs\n",
      "</Title>\n",
      "\n",
      "<Summary>\n",
      "Retrieval-augmented generation (RAG) enhances large language models (LLMs)\n",
      "for domain-specific question-answering (QA) tasks by leveraging external\n",
      "knowledge sources. However, traditional RAG systems primarily focus on\n",
      "relevance-based retrieval and often struggle with redundancy, especially when\n",
      "reasoning requires connecting information from multiple sources. This paper\n",
      "introduces Vendi-RAG, a framework based on an iterative process that jointly\n",
      "optimizes retrieval diversity and answer quality. This joint optimization leads\n",
      "to significantly higher accuracy for multi-hop QA tasks. Vendi-RAG leverages\n",
      "the Vendi Score (VS), a flexible similarity-based diversity metric, to promote\n",
      "semantic diversity in document retrieval. It then uses an LLM judge that\n",
      "evaluates candidate answers, generated after a reasoning step, and outputs a\n",
      "score that the retriever uses to balance relevance and diversity among the\n",
      "retrieved documents during each iteration. Experiments on three challenging\n",
      "datasets -- HotpotQA, MuSiQue, and 2WikiMultiHopQA -- demonstrate Vendi-RAG's\n",
      "effectiveness in multi-hop reasoning tasks. The framework achieves significant\n",
      "accuracy improvements over traditional single-step and multi-step RAG\n",
      "approaches, with accuracy increases reaching up to +4.2% on HotpotQA, +4.1% on\n",
      "2WikiMultiHopQA, and +1.3% on MuSiQue compared to Adaptive-RAG, the current\n",
      "best baseline. The benefits of Vendi-RAG are even more pronounced as the number\n",
      "of retrieved documents increases. Finally, we evaluated Vendi-RAG across\n",
      "different LLM backbones, including GPT-3.5, GPT-4, and GPT-4o-mini, and\n",
      "observed consistent improvements, demonstrating that the framework's advantages\n",
      "are model-agnostic.\n",
      "</Summary>\n",
      "\n",
      "<Content>\n",
      "Vendi-RAG: Adaptively Trading-Off Diversity And\n",
      "Quality Significantly Improves Retrieval\n",
      "Augmented Generation With LLMs\n",
      "Mohammad R. Rezaei1, 3 and Adji Bousso Dieng2, 3\n",
      "1Institute of Biomedical Engineering, University of Toronto\n",
      "2Department of Computer Science, Princeton University\n",
      "3Vertaix\n",
      "May 26, 2025\n",
      "Abstract\n",
      "Retrieval-augmented generation (RAG) enhances large language models (LLMs)\n",
      "for domain-specific question-answering (QA) tasks by leveraging external\n",
      "knowledge sources. However, traditional RAG systems primarily focus on\n",
      "relevance-based retrieval and often struggle with redundancy, especially when\n",
      "reasoning requires connecting information from multiple sources. This paper\n",
      "introduces Vendi-RAG, a framework based on an iterative process that jointly\n",
      "optimizes retrieval diversity and answer quality. This joint optimization leads\n",
      "to significantly higher accuracy for multi-hop QA tasks. Vendi-RAG leverages\n",
      "the Vendi Score (VS), a flexible similarity-based diversity metric, to promote\n",
      "semantic diversity in document retrieval. It then uses an LLM judge that eval-\n",
      "uates candidate answers, generated after a reasoning step, and outputs a\n",
      "score that the retriever uses to balance relevance and diversity among the\n",
      "retrieved documents during each iteration. Experiments on three challenging\n",
      "datasets—HotpotQA, MuSiQue, and 2WikiMultiHopQA—demonstrate Vendi-\n",
      "RAG’s effectiveness in multi-hop reasoning tasks. The framework achieves sig-\n",
      "nificant accuracy improvements over traditional single-step or multi-step RAG\n",
      "approaches, with accuracy increases reaching +4.2% on HotpotQA, +4.1% on\n",
      "2WikiMultiHopQA, and +1.3% on MuSiQue compared to Adaptive-RAG, the\n",
      "current best baseline. The benefits of Vendi-RAG are even more pronounced as\n",
      "the number of retrieved documents increases. Finally, we evaluated Vendi-RAG\n",
      "across different LLM backbones, including GPT-3.5, GPT-4, and GPT-4o-mini,\n",
      "and observed consistent improvements, demonstrating that the framework’s\n",
      "advantages are model-agnostic.\n",
      "Keywords: RAG, LLMs, Question Answering, NLP, Diversity, Vendi Scoring\n",
      "1\n",
      "Introduction\n",
      "Retrieval-augmented generation (RAG) has emerged as a transformative framework\n",
      "for enhancing the performance of large language models (LLMs) in domain-specific\n",
      "tasks such as question-answering (QA). By retrieving relevant information from\n",
      "external sources beyond the training set, RAG enables LLMs to answer specialized\n",
      "1\n",
      "arXiv:2502.11228v2  [cs.CL]  22 May 2025\n",
      "Retrieval\n",
      "Query\n",
      "Answer\n",
      "LLM Judge \n",
      "Quality Check\n",
      "Rewritten  \n",
      "Query\n",
      "Final \n",
      "Answer\n",
      "Vendi Score\n",
      "Vendi Score\n",
      "Diversity  \n",
      "weight s\n",
      "Decision Block \n",
      "s > threshold Thr \n",
      "Iteration < N\n",
      "…\n",
      "Reasoning\n",
      "Figure 1: The process begins with an initial retrieval step, where a diverse set of\n",
      "documents is retrieved using the Vendi Score, ensuring broad semantic coverage.\n",
      "Next, leveraging a reasoning step to construct a coherent path to the final answer,\n",
      "the LLM generates an answer, which then undergoes quality assessment by an LLM\n",
      "judge. Based on the answer quality, the retriever is adjusted to balance diversity and\n",
      "relevance: high-quality answers limit the emphasis on diversity, while low-quality\n",
      "answers prompt the retriever to prioritize diversity more heavily. This adjustment\n",
      "is controlled by an adaptive parameter, s, which is updated over iterations. The\n",
      "process continues until the answer quality reaches an optimal threshold, denoted\n",
      "by Thr. Finally, the highest-quality responses and documents are selected, ensuring\n",
      "both diversity and accuracy.\n",
      "queries more effectively Achiam et al. (2023); Team et al. (2023); Jiang et al. (2024).\n",
      "This approach has been particularly successful in single-hop QA, where a question\n",
      "can be answered using information from a single document Raiaan et al. (2024);\n",
      "Kwiatkowski et al. (2019). For instance, answering a question such as \"Who wrote\n",
      "the novel Frankenstein?\" only requires retrieving relevant information from a single\n",
      "document containing this fact.\n",
      "However, multi-hop QA introduces significantly greater complexit\n",
      "</Content>\n",
      "</Document>\n",
      "\n",
      "---\n",
      "\n",
      "<Document source=\"http://arxiv.org/abs/2408.05933v1\" date=\"2024-08-12\" authors=\"Fei Liu, Zejun Kang, Xing Han\"/>\n",
      "<Title>\n",
      "Optimizing RAG Techniques for Automotive Industry PDF Chatbots: A Case Study with Locally Deployed Ollama Models\n",
      "</Title>\n",
      "\n",
      "<Summary>\n",
      "With the growing demand for offline PDF chatbots in automotive industrial\n",
      "production environments, optimizing the deployment of large language models\n",
      "(LLMs) in local, low-performance settings has become increasingly important.\n",
      "This study focuses on enhancing Retrieval-Augmented Generation (RAG) techniques\n",
      "for processing complex automotive industry documents using locally deployed\n",
      "Ollama models. Based on the Langchain framework, we propose a multi-dimensional\n",
      "optimization approach for Ollama's local RAG implementation. Our method\n",
      "addresses key challenges in automotive document processing, including\n",
      "multi-column layouts and technical specifications. We introduce improvements in\n",
      "PDF processing, retrieval mechanisms, and context compression, tailored to the\n",
      "unique characteristics of automotive industry documents. Additionally, we\n",
      "design custom classes supporting embedding pipelines and an agent supporting\n",
      "self-RAG based on LangGraph best practices. To evaluate our approach, we\n",
      "constructed a proprietary dataset comprising typical automotive industry\n",
      "documents, including technical reports and corporate regulations. We compared\n",
      "our optimized RAG model and self-RAG agent against a naive RAG baseline across\n",
      "three datasets: our automotive industry dataset, QReCC, and CoQA. Results\n",
      "demonstrate significant improvements in context precision, context recall,\n",
      "answer relevancy, and faithfulness, with particularly notable performance on\n",
      "the automotive industry dataset. Our optimization scheme provides an effective\n",
      "solution for deploying local RAG systems in the automotive sector, addressing\n",
      "the specific needs of PDF chatbots in industrial production environments. This\n",
      "research has important implications for advancing information processing and\n",
      "intelligent production in the automotive industry.\n",
      "</Summary>\n",
      "\n",
      "<Content>\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Optimizing RAG Techniques for Automotive Industry PDF Chatbots: A Case Study with \n",
      "Locally Deployed Ollama Models \n",
      "Optimizing RAG Techniques Based on Locally Deployed Ollama Models \n",
      "A Case Study with Locally Deployed Ollama Models \n",
      "Fei Liu * \n",
      "China Automotive Technology & Research Center, liufei@catarc.ac.cn \n",
      "Zejun Kang \n",
      "China Automotive Technology & Research Center, kangzejun@catarc.ac.cn \n",
      "Xing Han \n",
      "China Automotive Technology & Research Center, hanxing@catarc.ac.cn \n",
      "With the growing demand for offline PDF chatbots in automotive industrial production environments, optimizing the deployment \n",
      "of large language models (LLMs) in local, low-performance settings has become increasingly important. This study focuses on \n",
      "enhancing Retrieval-Augmented Generation (RAG) techniques for processing complex automotive industry documents using \n",
      "locally deployed Ollama models. \n",
      "Based on the Langchain framework, we propose a multi-dimensional optimization approach for Ollama's local RAG \n",
      "implementation. Our method addresses key challenges in automotive document processing, including multi-column layouts and \n",
      "technical specifications. We introduce improvements in PDF processing, retrieval mechanisms, and context compression, tailored \n",
      "to the unique characteristics of automotive industry documents. Additionally, we design custom classes supporting embedding \n",
      "pipelines and an agent supporting self-RAG based on LangGraph best practices. \n",
      "To evaluate our approach, we constructed a proprietary dataset comprising typical automotive industry documents, including \n",
      "technical reports and corporate regulations. We compared our optimized RAG model and self-RAG agent against a naive RAG \n",
      "baseline across three datasets: our automotive industry dataset, QReCC, and CoQA. Results demonstrate significant improvements \n",
      "in context precision, context recall, answer relevancy, and faithfulness, with particularly notable performance on the automotive \n",
      "industry dataset. \n",
      "Our optimization scheme provides an effective solution for deploying local RAG systems in the automotive sector, addressing the \n",
      "specific needs of PDF chatbots in industrial production environments. This research has important implications for advancing \n",
      "information processing and intelligent production in the automotive industry. \n",
      " \n",
      "* Place the footnote text for the author (if applicable) here.  \n",
      "CCS CONCEPTS • Computing methodologies • Artificial intelligence • Natural language processing • Natural language \n",
      "generation \n",
      " \n",
      "Additional Keywords and Phrases: Automotive Industry, Langchain, self-rag, PDF Processing, RAG, Ollama \n",
      "1 INTRODUCTION \n",
      "1.1 Research Background \n",
      "The automotive industry is undergoing a significant digital transformation, with an increasing reliance on complex \n",
      "technical documentation for various processes [1]. This shift encompasses design, manufacturing, and quality \n",
      "control, all of which now heavily depend on efficient information management systems [2]. The growing volume of \n",
      "technical documents, often in PDF format, has created a pressing need for advanced information retrieval and \n",
      "question-answering capabilities in industrial settings [3]. \n",
      "Large Language Models (LLMs) have emerged as powerful tools in natural language processing, demonstrating \n",
      "remarkable abilities in tasks such as document understanding and question answering [4]. These models have \n",
      "shown potential in handling the complex, domain-specific language often found in automotive documentation. \n",
      "However, the application of LLMs in industrial environments presents unique challenges, particularly in terms of \n",
      "computational resources and data privacy [5]. \n",
      "Among the various techniques developed to enhance LLM performance, Retrieval-Augmented Generation (RAG) \n",
      "has gained significant attention [6]. RAG combines the generative capabilities of LLMs with external knowledge \n",
      "retrieval, allowing for more accurate and contextually relevant responses. This approach, initially proposed by \n",
      "Lew\n",
      "</Content>\n",
      "</Document>\n",
      "\n",
      "---\n",
      "\n",
      "<Document source=\"http://arxiv.org/abs/2409.12682v1\" date=\"2024-09-19\" authors=\"Jiho Shin, Reem Aleithan, Hadi Hemmati, Song Wang\"/>\n",
      "<Title>\n",
      "Retrieval-Augmented Test Generation: How Far Are We?\n",
      "</Title>\n",
      "\n",
      "<Summary>\n",
      "Retrieval Augmented Generation (RAG) has shown notable advancements in\n",
      "software engineering tasks. Despite its potential, RAG's application in unit\n",
      "test generation remains under-explored. To bridge this gap, we take the\n",
      "initiative to investigate the efficacy of RAG-based LLMs in test generation. As\n",
      "RAGs can leverage various knowledge sources to enhance their performance, we\n",
      "also explore the impact of different sources of RAGs' knowledge bases on unit\n",
      "test generation to provide insights into their practical benefits and\n",
      "limitations. Specifically, we examine RAG built upon three types of domain\n",
      "knowledge: 1) API documentation, 2) GitHub issues, and 3) StackOverflow Q&As.\n",
      "Each source offers essential knowledge for creating tests from different\n",
      "perspectives, i.e., API documentations provide official API usage guidelines,\n",
      "GitHub issues offer resolutions of issues related to the APIs from the library\n",
      "developers, and StackOverflow Q&As present community-driven solutions and best\n",
      "practices. For our experiment, we focus on five widely used and typical\n",
      "Python-based machine learning (ML) projects, i.e., TensorFlow, PyTorch,\n",
      "Scikit-learn, Google JAX, and XGBoost to build, train, and deploy complex\n",
      "neural networks efficiently. We conducted experiments using the top 10% most\n",
      "widely used APIs across these projects, involving a total of 188 APIs. We\n",
      "investigate the effectiveness of four state-of-the-art LLMs (open and\n",
      "closed-sourced), i.e., GPT-3.5-Turbo, GPT-4o, Mistral MoE 8x22B, and Llamma 3.1\n",
      "405B. Additionally, we compare three prompting strategies in generating unit\n",
      "test cases for the experimental APIs, i.e., zero-shot, a Basic RAG, and an\n",
      "API-level RAG on the three external sources. Finally, we compare the cost of\n",
      "different sources of knowledge used for the RAG.\n",
      "</Summary>\n",
      "\n",
      "<Content>\n",
      "Retrieval-Augmented Test Generation: How Far Are We?\n",
      "JIHO SHIN, York University, Canada\n",
      "REEM ALEITHAN, York University, Canada\n",
      "HADI HEMMATI, York University, Canada\n",
      "SONG WANG, York University, Canada\n",
      "Retrieval Augmented Generation (RAG) has shown notable advancements in software engineering tasks.\n",
      "Despite its potential, RAG’s application in unit test generation remains under-explored. To bridge this gap, we\n",
      "take the initiative to investigate the efficacy of RAG-based LLMs in test generation. As RAGs can leverage\n",
      "various knowledge sources to enhance their performance, we also explore the impact of different sources of\n",
      "RAGs’ knowledge bases on unit test generation to provide insights into their practical benefits and limitations.\n",
      "Specifically, we examine RAG built upon three types of domain knowledge: 1) API documentation, 2) GitHub\n",
      "issues, and 3) StackOverflow Q&As. Each source offers essential knowledge for creating tests from different\n",
      "perspectives, i.e., API documentations provide official API usage guidelines, GitHub issues offer resolutions of\n",
      "issues related to the APIs from the library developers, and StackOverflow Q&As present community-driven\n",
      "solutions and best practices. For our experiment, we focus on five widely used and typical Python-based\n",
      "machine learning (ML) projects, i.e., TensorFlow, PyTorch, Scikit-learn, Google JAX, and XGBoost to build,\n",
      "train, and deploy complex neural networks efficiently. We conducted experiments using the top 10% most\n",
      "widely used APIs across these projects, involving a total of 188 APIs.\n",
      "We investigate the effectiveness of four state-of-the-art LLMs (open and closed-sourced), i.e., GPT-3.5-Turbo,\n",
      "GPT-4o, Mistral MoE 8x22B, and Llamma 3.1 405B. Additionally, we compare three prompting strategies in\n",
      "generating unit test cases for the experimental APIs, i.e., zero-shot, a Basic RAG, and an API-level RAG on the\n",
      "three external sources. Finally, we compare the cost of different sources of knowledge used for the RAG.\n",
      "We conduct both qualitative and quantitative evaluations to investigate the generated test cases. For the\n",
      "quantitative analysis, we assess the syntactical and dynamic correctness of the generated tests. We observe\n",
      "that RAG does not improve the syntactical or dynamic correctness of unit test cases. However, using Basic\n",
      "RAG could improve the line coverage by an average of 8.94% and API-level RAG by 9.68%. We investigate\n",
      "the token cost of different RAGs with different prompting strategies. We find that RAGs using GitHub issue\n",
      "documents have the highest token cost. We also find that using limiting the number of test cases for cost\n",
      "efficiency significantly reduces the cost. Finally, we perform a manual analysis over a subset of the generated\n",
      "tests to evaluate how different strategies impact the software under test in more depth. We find that RAG helps\n",
      "cover unique lines by providing unique states of programs that cannot be generated directly by LLMs. Our\n",
      "study suggests that RAG has great potential in improving unit tests’ line coverage when the right documents\n",
      "with unique examples of program states are given. Proposing new retrieval techniques that can search these\n",
      "documents will further improve RAG-based unit test generation for future work.\n",
      "CCS Concepts: • Software and its engineering →Software maintenance tools; Automatic programming; •\n",
      "Computing methodologies →Natural language generation; Machine translation; • General and reference\n",
      "→Surveys and overviews.\n",
      "Authors’ Contact Information: Jiho Shin, jihoshin@yorku.ca, York University, Toronto, ON, Canada; Reem Aleithan,\n",
      "reem1100@yorku.ca, York University, Toronto, ON, Canada; Hadi Hemmati, hemmati@yorku.ca, York University, Toronto,\n",
      "ON, Canada; Song Wang, wangsong@yorku.ca, York University, Toronto, ON, Canada.\n",
      "Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee\n",
      "provided that copies are not made or distributed for profit or commercial advantage and that copies bear \n",
      "</Content>\n",
      "</Document>\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36manswer_question\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: expert\n",
      "\n",
      "Certainly. Let's consider a case study in the automotive industry, as described in a research article on optimizing RAG techniques for PDF chatbots. The context involved processing complex automotive industry documents using locally deployed models. Here, Modular RAG was utilized to efficiently handle multi-column layouts and technical specifications typical of automotive documents [6].\n",
      "\n",
      "Challenges in this scenario included the need to process diverse document types and the requirement for high context precision and recall. Traditional methods, like Naive RAG, might struggle with these complexities due to their monolithic design, which limits the ability to tailor retrieval processes to meet specific document processing needs.\n",
      "\n",
      "By implementing a Modular RAG system, the retrieval mechanisms could be separately optimized to handle the specific challenges posed by automotive document structures. Custom modules for PDF processing and context compression were designed, allowing for tailored embedding pipelines and retrieval strategies. This modular approach resulted in significant improvements in context precision, recall, and answer relevancy, particularly when applied to the proprietary automotive industry dataset [6].\n",
      "\n",
      "The benefits of using Modular RAG in this production setting included enhanced flexibility to adapt retrieval processes to different document types and improved scalability and accuracy in generating contextually relevant responses. This case study demonstrates how Modular RAG can be effectively deployed to address complex data requirements in real-world scenarios, showcasing its advantage over more static frameworks like Naive RAG.\n",
      "\n",
      "[6] arxiv.org/abs/2408.05933v1\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mask_question\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "That's fascinating, Dr. Kim. It's clear that Modular RAG offers tangible benefits in complex environments like the automotive industry. One last question: are there any specific challenges or limitations associated with implementing Modular RAG that organizations should be aware of?\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36msearch_arxiv\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "<Document source=\"http://arxiv.org/abs/2508.05650v1\" date=\"2025-07-26\" authors=\"Jiaxuan Liang, Shide Zhou, Kailong Wang\"/>\n",
      "<Title>\n",
      "OmniBench-RAG: A Multi-Domain Evaluation Platform for Retrieval-Augmented Generation Tools\n",
      "</Title>\n",
      "\n",
      "<Summary>\n",
      "While Retrieval Augmented Generation (RAG) is now widely adopted to enhance\n",
      "LLMs, evaluating its true performance benefits in a reproducible and\n",
      "interpretable way remains a major hurdle. Existing methods often fall short:\n",
      "they lack domain coverage, employ coarse metrics that miss sub document\n",
      "precision, and fail to capture computational trade offs. Most critically, they\n",
      "provide no standardized framework for comparing RAG effectiveness across\n",
      "different models and domains.\n",
      "  We introduce OmniBench RAG, a novel automated platform for multi domain\n",
      "evaluation of RAG systems. The platform quantifies performance gains across\n",
      "accuracy and efficiency dimensions, spanning nine knowledge fields including\n",
      "culture, geography, and health. We introduce two standardized metrics:\n",
      "Improvements (accuracy gains) and Transformation (efficiency differences\n",
      "between pre RAG and post RAG models), enabling reproducible comparisons across\n",
      "models and tasks. The platform features dynamic test generation, modular\n",
      "evaluation pipelines, and automated knowledge base construction. Our evaluation\n",
      "reveals striking variability in RAG effectiveness, from significant gains in\n",
      "culture to declines in mathematics, highlighting the critical importance of\n",
      "systematic, domain aware assessment. A demonstration video is available at:\n",
      "https://www.youtube.com/watch?v=BZx83QFcTCI. Code and datasets:\n",
      "https://github.com/Garnett-Liang/Omnibench-RAG.\n",
      "</Summary>\n",
      "\n",
      "<Content>\n",
      "OmniBench-RAG: A Multi-Domain Evaluation\n",
      "Platform for Retrieval-Augmented Generation Tools\n",
      "Jiaxuan Liang*, Shide Zhou*, and Kailong Wang†\n",
      "Huazhong University of Science and Technology\n",
      "{liangjx, shidez, wangkl}@hust.edu.cn\n",
      "Abstract—While Retrieval Augmented Generation (RAG) is\n",
      "now widely adopted to enhance LLMs, evaluating its true\n",
      "performance benefits in a reproducible and interpretable way\n",
      "remains a major hurdle. Existing methods often fall short: they\n",
      "lack domain coverage, employ coarse metrics that miss sub\n",
      "document precision, and fail to capture computational trade\n",
      "offs. Most critically, they provide no standardized framework\n",
      "for comparing RAG effectiveness across different models and\n",
      "domains.\n",
      "We introduce OmniBench RAG, a novel automated platform\n",
      "for multi domain evaluation of RAG systems. The platform\n",
      "quantifies performance gains across accuracy and efficiency\n",
      "dimensions, spanning nine knowledge fields including culture,\n",
      "geography, and health. We introduce two standardized metrics:\n",
      "Improvements (accuracy gains) and Transformation (efficiency\n",
      "differences between pre RAG and post RAG models), enabling\n",
      "reproducible comparisons across models and tasks. The platform\n",
      "features dynamic test generation, modular evaluation pipelines,\n",
      "and automated knowledge base construction. Our evaluation\n",
      "reveals striking variability in RAG effectiveness, from significant\n",
      "gains in culture to declines in mathematics, highlighting the\n",
      "critical importance of systematic, domain aware assessment. A\n",
      "demonstration video is available at: https://www.youtube.com/\n",
      "watch?v=BZx83QFcTCI. Code and datasets: https://github.com/\n",
      "Garnett-Liang/Omnibench-RAG.\n",
      "I. INTRODUCTION\n",
      "Retrieval-Augmented Generation (RAG) is a key technique\n",
      "for enhancing Large Language Models (LLMs) [1], [2]. By\n",
      "grounding model responses in external, verifiable knowledge,\n",
      "RAG promises to mitigate hallucinations [3], improve factual\n",
      "accuracy [4], and provide up-to-date information [5]. However,\n",
      "the true effectiveness of RAG is far from uniform. Recent\n",
      "studies [6], [7] reveal a significant disparity: while RAG\n",
      "can boost the accuracy of smaller models like Llama-3.2-3B-\n",
      "Instruct by as much as 38.12%, its impact on state-of-the-art\n",
      "models such as GPT-4o, which excel with extended context\n",
      "windows, is often less pronounced. This variability, which\n",
      "depends not only on the model’s scale but also heavily on\n",
      "the knowledge domain, underscores a critical challenge: the\n",
      "lack of a systematic platform to quantify the value of RAG\n",
      "across these diverse contexts.\n",
      "Current RAG evaluation approaches suffer from fundamen-\n",
      "tal limitations that impede reproducible and comprehensive as-\n",
      "sessment: First, they lack automated multi-domain evalua-\n",
      "tion capabilities and rely on non-deterministic components.\n",
      "*Jiaxuan Liang and Shide Zhou are co-first authors.\n",
      "†Kailong Wang is the corresponding author.\n",
      "Existing benchmarks typically require manual configuration\n",
      "for each knowledge domain and fail to provide unified assess-\n",
      "ment across diverse fields like finance, healthcare, or culture,\n",
      "making cross-domain performance analysis labor-intensive and\n",
      "inconsistent. Moreover, key metrics in leading frameworks\n",
      "(e.g., LLM-based scoring in Ragas [8]) inadvertently introduce\n",
      "randomness due to reliance on large language models in the\n",
      "evaluation loop, undermining result reproducibility. Second,\n",
      "they employ static datasets and coarse-grained metrics.\n",
      "Most frameworks rely on fixed benchmarks and document-\n",
      "level retrieval metrics (e.g., MRR@k[9]), missing the critical\n",
      "sub-document precision needed to assess whether models\n",
      "extract specific facts accurately. They also lack the ability to\n",
      "dynamically generate test cases that probe complex reasoning\n",
      "patterns. Third, they fail to capture the computational\n",
      "trade-offs inherent in RAG systems. Without automated\n",
      "profiling of resource utilization and efficiency metrics, practi-\n",
      "tioners cannot make informed decisions about the cost-benefit\n",
      "trade-offs of deploying RAG in production environments.\n",
      "Our \n",
      "</Content>\n",
      "</Document>\n",
      "\n",
      "---\n",
      "\n",
      "<Document source=\"http://arxiv.org/abs/2407.21059v1\" date=\"2024-07-26\" authors=\"Yunfan Gao, Yun Xiong, Meng Wang, Haofen Wang\"/>\n",
      "<Title>\n",
      "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks\n",
      "</Title>\n",
      "\n",
      "<Summary>\n",
      "Retrieval-augmented Generation (RAG) has markedly enhanced the capabilities\n",
      "of Large Language Models (LLMs) in tackling knowledge-intensive tasks. The\n",
      "increasing demands of application scenarios have driven the evolution of RAG,\n",
      "leading to the integration of advanced retrievers, LLMs and other complementary\n",
      "technologies, which in turn has amplified the intricacy of RAG systems.\n",
      "However, the rapid advancements are outpacing the foundational RAG paradigm,\n",
      "with many methods struggling to be unified under the process of\n",
      "\"retrieve-then-generate\". In this context, this paper examines the limitations\n",
      "of the existing RAG paradigm and introduces the modular RAG framework. By\n",
      "decomposing complex RAG systems into independent modules and specialized\n",
      "operators, it facilitates a highly reconfigurable framework. Modular RAG\n",
      "transcends the traditional linear architecture, embracing a more advanced\n",
      "design that integrates routing, scheduling, and fusion mechanisms. Drawing on\n",
      "extensive research, this paper further identifies prevalent RAG\n",
      "patterns-linear, conditional, branching, and looping-and offers a comprehensive\n",
      "analysis of their respective implementation nuances. Modular RAG presents\n",
      "innovative opportunities for the conceptualization and deployment of RAG\n",
      "systems. Finally, the paper explores the potential emergence of new operators\n",
      "and paradigms, establishing a solid theoretical foundation and a practical\n",
      "roadmap for the continued evolution and practical deployment of RAG\n",
      "technologies.\n",
      "</Summary>\n",
      "\n",
      "<Content>\n",
      "1\n",
      "Modular RAG: Transforming RAG Systems into\n",
      "LEGO-like Reconfigurable Frameworks\n",
      "Yunfan Gao, Yun Xiong, Meng Wang, Haofen Wang\n",
      "Abstract—Retrieval-augmented\n",
      "Generation\n",
      "(RAG)\n",
      "has\n",
      "markedly enhanced the capabilities of Large Language Models\n",
      "(LLMs) in tackling knowledge-intensive tasks. The increasing\n",
      "demands of application scenarios have driven the evolution\n",
      "of RAG, leading to the integration of advanced retrievers,\n",
      "LLMs and other complementary technologies, which in turn\n",
      "has amplified the intricacy of RAG systems. However, the rapid\n",
      "advancements are outpacing the foundational RAG paradigm,\n",
      "with many methods struggling to be unified under the process\n",
      "of “retrieve-then-generate”. In this context, this paper examines\n",
      "the limitations of the existing RAG paradigm and introduces\n",
      "the modular RAG framework. By decomposing complex RAG\n",
      "systems into independent modules and specialized operators, it\n",
      "facilitates a highly reconfigurable framework. Modular RAG\n",
      "transcends the traditional linear architecture, embracing a\n",
      "more advanced design that integrates routing, scheduling, and\n",
      "fusion mechanisms. Drawing on extensive research, this paper\n",
      "further identifies prevalent RAG patterns—linear, conditional,\n",
      "branching, and looping—and offers a comprehensive analysis\n",
      "of their respective implementation nuances. Modular RAG\n",
      "presents\n",
      "innovative\n",
      "opportunities\n",
      "for\n",
      "the\n",
      "conceptualization\n",
      "and deployment of RAG systems. Finally, the paper explores\n",
      "the potential emergence of new operators and paradigms,\n",
      "establishing a solid theoretical foundation and a practical\n",
      "roadmap for the continued evolution and practical deployment\n",
      "of RAG technologies.\n",
      "Index Terms—Retrieval-augmented generation, large language\n",
      "model, modular system, information retrieval\n",
      "I. INTRODUCTION\n",
      "L\n",
      "ARGE Language Models (LLMs) have demonstrated\n",
      "remarkable capabilities, yet they still face numerous\n",
      "challenges, such as hallucination and the lag in information up-\n",
      "dates [1]. Retrieval-augmented Generation (RAG), by access-\n",
      "ing external knowledge bases, provides LLMs with important\n",
      "contextual information, significantly enhancing their perfor-\n",
      "mance on knowledge-intensive tasks [2]. Currently, RAG, as\n",
      "an enhancement method, has been widely applied in various\n",
      "practical application scenarios, including knowledge question\n",
      "answering, recommendation systems, customer service, and\n",
      "personal assistants. [3]–[6]\n",
      "During the nascent stages of RAG , its core framework is\n",
      "constituted by indexing, retrieval, and generation, a paradigm\n",
      "referred to as Naive RAG [7]. However, as the complexity\n",
      "of tasks and the demands of applications have escalated, the\n",
      "Yunfan Gao is with Shanghai Research Institute for Intelligent Autonomous\n",
      "Systems, Tongji University, Shanghai, 201210, China.\n",
      "Yun Xiong is with Shanghai Key Laboratory of Data Science, School of\n",
      "Computer Science, Fudan University, Shanghai, 200438, China.\n",
      "Meng Wang and Haofen Wang are with College of Design and Innovation,\n",
      "Tongji University, Shanghai, 20092, China. (Corresponding author: Haofen\n",
      "Wang. E-mail: carter.whfcarter@gmail.com)\n",
      "limitations of Naive RAG have become increasingly apparent.\n",
      "As depicted in Figure 1, it predominantly hinges on the\n",
      "straightforward similarity of chunks, result in poor perfor-\n",
      "mance when confronted with complex queries and chunks with\n",
      "substantial variability. The primary challenges of Naive RAG\n",
      "include: 1) Shallow Understanding of Queries. The semantic\n",
      "similarity between a query and document chunk is not always\n",
      "highly consistent. Relying solely on similarity calculations\n",
      "for retrieval lacks an in-depth exploration of the relationship\n",
      "between the query and the document [8]. 2) Retrieval Re-\n",
      "dundancy and Noise. Feeding all retrieved chunks directly\n",
      "into LLMs is not always beneficial. Research indicates that\n",
      "an excess of redundant and noisy information may interfere\n",
      "with the LLM’s identification of key information, thereby\n",
      "increasing the risk of generating erroneous and hallucinated\n",
      "responses. [9]\n",
      "To overcome the aforementioned limitations, \n",
      "</Content>\n",
      "</Document>\n",
      "\n",
      "---\n",
      "\n",
      "<Document source=\"http://arxiv.org/abs/2403.09040v3\" date=\"2025-07-16\" authors=\"Jennifer Hsia, Afreen Shaikh, Zhiruo Wang, Graham Neubig\"/>\n",
      "<Title>\n",
      "RAGGED: Towards Informed Design of Scalable and Stable RAG Systems\n",
      "</Title>\n",
      "\n",
      "<Summary>\n",
      "Retrieval-augmented generation (RAG) enhances language models by integrating\n",
      "external knowledge, but its effectiveness is highly dependent on system\n",
      "configuration. Improper retrieval settings can degrade performance, making RAG\n",
      "less reliable than closed-book generation. In this work, we introduce RAGGED, a\n",
      "framework for systematically evaluating RAG systems across diverse\n",
      "retriever-reader configurations, retrieval depths, and datasets. Our analysis\n",
      "reveals that reader robustness to noise is the key determinant of RAG stability\n",
      "and scalability. Some readers benefit from increased retrieval depth, while\n",
      "others degrade due to their sensitivity to distracting content. Through\n",
      "large-scale experiments on open-domain, multi-hop, and specialized-domain\n",
      "datasets, we show that retrievers, rerankers, and prompts influence performance\n",
      "but do not fundamentally alter these reader-driven trends. By providing a\n",
      "principled framework and new metrics to assess RAG stability and scalability,\n",
      "RAGGED enables systematic evaluation of retrieval-augmented generation systems,\n",
      "guiding future research on optimizing retrieval depth and model robustness.\n",
      "</Summary>\n",
      "\n",
      "<Content>\n",
      "RAGGED: Towards Informed Design of Scalable and Stable RAG Systems\n",
      "Jennifer Hsia * 1 Afreen Shaikh * 2 Zhiruo Wang 2 Graham Neubig 2\n",
      "Abstract\n",
      "Retrieval-augmented generation (RAG) enhances\n",
      "language models by integrating external knowl-\n",
      "edge, but its effectiveness is highly dependent on\n",
      "system configuration. Improper retrieval settings\n",
      "can degrade performance, making RAG less reli-\n",
      "able than closed-book generation. In this work,\n",
      "we introduce RAGGED, a framework for system-\n",
      "atically evaluating RAG systems across diverse\n",
      "retriever-reader configurations, retrieval depths,\n",
      "and datasets. Our analysis reveals that reader ro-\n",
      "bustness to noise is the key determinant of RAG\n",
      "stability and scalability. Some readers benefit\n",
      "from increased retrieval depth, while others de-\n",
      "grade due to their sensitivity to distracting content.\n",
      "Through large-scale experiments on open-domain,\n",
      "multi-hop, and specialized-domain datasets, we\n",
      "show that retrievers, rerankers, and prompts in-\n",
      "fluence performance but do not fundamentally\n",
      "alter these reader-driven trends. By providing a\n",
      "principled framework and new metrics to assess\n",
      "RAG stability and scalability, RAGGED enables\n",
      "systematic evaluation of retrieval-augmented gen-\n",
      "eration systems, guiding future research on opti-\n",
      "mizing retrieval depth and model robustness. 1\n",
      "1. Introduction\n",
      "Retrieval-augmented generation (RAG) (Chen et al., 2017;\n",
      "Lewis et al., 2020) enhances large language models (LLMs)\n",
      "by retrieving relevant external contexts, enabling more spe-\n",
      "cific and factually grounded responses. However, despite\n",
      "its promise, RAG’s effectiveness is not guaranteed. In fact,\n",
      "improper configurations can degrade model performance,\n",
      "leading to outputs that are worse than closed-book genera-\n",
      "*Equal contribution 1Machine Learning Department, Carnegie\n",
      "Mellon University, Pittsburgh, USA 2The Language Technologies\n",
      "Institute, Carnegie Mellon University, Pittsburgh, USA. Correspon-\n",
      "dence to: Jennifer Hsia <jhsia2@andrew.cmu.edu>.\n",
      "Proceedings of the 42 nd International Conference on Machine\n",
      "Learning, Vancouver, Canada. PMLR 267, 2025. Copyright 2025\n",
      "by the author(s).\n",
      "1Code and data for the RAGGED framework are available at\n",
      "https://github.com/neulab/ragged\n",
      "…\n",
      "Reader\n",
      "Retriever\n",
      "… \n",
      "Documents\n",
      "R1: When does RAG \n",
      "improve over closed-book \n",
      "generation? §4\n",
      "R2: Impact of \n",
      "increasing context \n",
      "documents. §5\n",
      "R4: Impact of \n",
      "retriever choice §7\n",
      "R3: Robustness to \n",
      "irrelevant information §6\n",
      "Figure 1: Roadmap of what our framework RAGGED anal-\n",
      "yses across the RAG pipeline.\n",
      "tion. Understanding when and why RAG helps or harms is\n",
      "critical for optimizing system design.\n",
      "Most prior work evaluates RAG under controlled conditions\n",
      "and curated contexts (Liu et al., 2023; Cuconasu et al., 2024),\n",
      "which fail to reflect real-world retrieval challenges. In prac-\n",
      "tice, retrieved contexts contain both relevant and irrelevant\n",
      "information, making the reader model’s ability to filter noise\n",
      "a critical factor in RAG success. Additionally, prior studies\n",
      "provide conflicting findings on retrieval depth (k)— while\n",
      "some suggest increasing k improves performance (Izacard\n",
      "& Grave, 2021), others observe diminishing returns (Liu\n",
      "et al., 2023) or even degradation at high k (Cuconasu et al.,\n",
      "2024; Jiang et al., 2024). This lack of consensus leaves\n",
      "practitioners without clear guidance on how to configure\n",
      "RAG systems for different tasks.\n",
      "To address these challenges, we introduce RAGGED\n",
      "(Retrieval-Augmented Generation Generalized Evalu-\n",
      "ation Device), a framework for systematically evaluating\n",
      "RAG performance across retrieval depths, model architec-\n",
      "tures, and retrieval conditions. Unlike prior work, which\n",
      "often relies on synthetic or manual retrieval modifications,\n",
      "RAGGED assesses models under realistic retrieval scenar-\n",
      "ios — analyzing performance on naturally retrieved top-k\n",
      "contexts rather than manually curated, oracle-aware con-\n",
      "texts.\n",
      "Our study reveals that reader robustness to noise is the pri-\n",
      "mary factor driving RAG stability and scalability, rather\n",
      "than retriever quality al\n",
      "</Content>\n",
      "</Document>\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36msearch_web\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "<Document href=\"https://adasci.org/how-does-modular-rag-improve-upon-naive-rag/\"/>\n",
      "Naive RAG, the initial implementation of Retrieval-Augmented Generation, operates on a straightforward principle: retrieve relevant documents from an external knowledge base and use these documents to inform the generative process. The retrieval process in Naive RAG is relatively static and lacks flexibility, often leading to inefficiencies and suboptimal integration with the generative model. By adopting a modular architecture, this approach addresses the limitations of Naive RAG, offering enhanced flexibility, scalability, and efficiency. Unlike Naive RAG, which operates as a monolithic entity, Modular RAG breaks down the retrieval and generation processes into distinct, interchangeable modules. * *Seamless Integration*: Generative models in Modular RAG are designed to seamlessly integrate with various retrieval modules, enhancing the coherence and relevance of generated responses.\n",
      "</Document>\n",
      "\n",
      "---\n",
      "\n",
      "<Document href=\"https://rabiloo.com/blog/the-3-types-of-rag-models-naive-rag-modular-rag-and-advanced-rag\"/>\n",
      "Building AI systems requires choosing the right tools for the job, and Retrieval-Augmented Generation (RAG) offers various models, each designed to serve different needs. Essentially, a RAG model retrieves information from external databases, knowledge bases, or the web and then uses that data to generate relevant, meaningful responses. Naive RAG begins by indexing the data source for quick retrieval of relevant information. Modular RAG builds on the basic principles of Naive RAG by breaking down the retrieval and generation processes into separate, specialized modules. Unlike Naive RAG or Modular RAG, this stage is more precise and contextually aware, ensuring that the system retrieves only the most relevant and contextually appropriate data. The generation module in Advanced RAG then processes the refined data using sophisticated models like T5 or GPT-3.\n",
      "</Document>\n",
      "\n",
      "---\n",
      "\n",
      "<Document href=\"https://medium.com/@drjulija/what-are-naive-rag-advanced-rag-modular-rag-paradigms-edff410c202e\"/>\n",
      "I share the differences between Naive RAG, Advanced RAG and Modular RAG frameworks. Retrieval-Augmented Generation (RAG) framework is designed to help mitigate these challenges. RAG enhances LLMs’ knowledge base with additional, domain-specific data. RAG systems can be split into 3 categories: See the comparison between all three paradigms of RAG — Naive RAG, Advanced RAG and Modular RAG below.\n",
      "</Document>\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36manswer_question\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: expert\n",
      "\n",
      "Yes, there are specific challenges and limitations associated with implementing Modular RAG that organizations should consider:\n",
      "\n",
      "1. **Integration Complexity**: While Modular RAG provides flexibility through its separate modules, integrating these components can sometimes be more complex than using a monolithic system like Naive RAG. Organizations must carefully design each module and ensure seamless interaction between them to maintain the system's overall efficiency and effectiveness [5].\n",
      "\n",
      "2. **Resource Intensive**: The modular approach might require more computational resources, especially if multiple or sophisticated modules are used. This can increase the cost of deployment, particularly in environments with limited infrastructure capabilities [5].\n",
      "\n",
      "3. **Maintenance Overhead**: With several independent modules, maintaining and updating the system can be more demanding. Each module might need individual attention to ensure it keeps up with evolving data requirements and technology advancements [4].\n",
      "\n",
      "4. **Configuration Challenges**: Determining the optimal configuration of different modules can be challenging. The effectiveness of Modular RAG systems can heavily depend on selecting the right modules and fine-tuning them to work in harmony, which may require skilled personnel [5].\n",
      "\n",
      "Despite these challenges, the benefits of adaptability, precision, and scalability often outweigh the downsides for many organizations with complex data processing needs.\n",
      "\n",
      "Sources:\n",
      "[4] medium.com/@drjulija/what-are-naive-rag-advanced-rag-modular-rag-paradigms-edff410c202e\n",
      "[5] adasci.org/how-does-modular-rag-improve-upon-naive-rag\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mask_question\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Thank you so much for your help, Dr. Kim!\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36msearch_arxiv\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "<Document source=\"http://arxiv.org/abs/2502.11228v2\" date=\"2025-05-22\" authors=\"Mohammad Reza Rezaei, Adji Bousso Dieng\"/>\n",
      "<Title>\n",
      "Vendi-RAG: Adaptively Trading-Off Diversity And Quality Significantly Improves Retrieval Augmented Generation With LLMs\n",
      "</Title>\n",
      "\n",
      "<Summary>\n",
      "Retrieval-augmented generation (RAG) enhances large language models (LLMs)\n",
      "for domain-specific question-answering (QA) tasks by leveraging external\n",
      "knowledge sources. However, traditional RAG systems primarily focus on\n",
      "relevance-based retrieval and often struggle with redundancy, especially when\n",
      "reasoning requires connecting information from multiple sources. This paper\n",
      "introduces Vendi-RAG, a framework based on an iterative process that jointly\n",
      "optimizes retrieval diversity and answer quality. This joint optimization leads\n",
      "to significantly higher accuracy for multi-hop QA tasks. Vendi-RAG leverages\n",
      "the Vendi Score (VS), a flexible similarity-based diversity metric, to promote\n",
      "semantic diversity in document retrieval. It then uses an LLM judge that\n",
      "evaluates candidate answers, generated after a reasoning step, and outputs a\n",
      "score that the retriever uses to balance relevance and diversity among the\n",
      "retrieved documents during each iteration. Experiments on three challenging\n",
      "datasets -- HotpotQA, MuSiQue, and 2WikiMultiHopQA -- demonstrate Vendi-RAG's\n",
      "effectiveness in multi-hop reasoning tasks. The framework achieves significant\n",
      "accuracy improvements over traditional single-step and multi-step RAG\n",
      "approaches, with accuracy increases reaching up to +4.2% on HotpotQA, +4.1% on\n",
      "2WikiMultiHopQA, and +1.3% on MuSiQue compared to Adaptive-RAG, the current\n",
      "best baseline. The benefits of Vendi-RAG are even more pronounced as the number\n",
      "of retrieved documents increases. Finally, we evaluated Vendi-RAG across\n",
      "different LLM backbones, including GPT-3.5, GPT-4, and GPT-4o-mini, and\n",
      "observed consistent improvements, demonstrating that the framework's advantages\n",
      "are model-agnostic.\n",
      "</Summary>\n",
      "\n",
      "<Content>\n",
      "Vendi-RAG: Adaptively Trading-Off Diversity And\n",
      "Quality Significantly Improves Retrieval\n",
      "Augmented Generation With LLMs\n",
      "Mohammad R. Rezaei1, 3 and Adji Bousso Dieng2, 3\n",
      "1Institute of Biomedical Engineering, University of Toronto\n",
      "2Department of Computer Science, Princeton University\n",
      "3Vertaix\n",
      "May 26, 2025\n",
      "Abstract\n",
      "Retrieval-augmented generation (RAG) enhances large language models (LLMs)\n",
      "for domain-specific question-answering (QA) tasks by leveraging external\n",
      "knowledge sources. However, traditional RAG systems primarily focus on\n",
      "relevance-based retrieval and often struggle with redundancy, especially when\n",
      "reasoning requires connecting information from multiple sources. This paper\n",
      "introduces Vendi-RAG, a framework based on an iterative process that jointly\n",
      "optimizes retrieval diversity and answer quality. This joint optimization leads\n",
      "to significantly higher accuracy for multi-hop QA tasks. Vendi-RAG leverages\n",
      "the Vendi Score (VS), a flexible similarity-based diversity metric, to promote\n",
      "semantic diversity in document retrieval. It then uses an LLM judge that eval-\n",
      "uates candidate answers, generated after a reasoning step, and outputs a\n",
      "score that the retriever uses to balance relevance and diversity among the\n",
      "retrieved documents during each iteration. Experiments on three challenging\n",
      "datasets—HotpotQA, MuSiQue, and 2WikiMultiHopQA—demonstrate Vendi-\n",
      "RAG’s effectiveness in multi-hop reasoning tasks. The framework achieves sig-\n",
      "nificant accuracy improvements over traditional single-step or multi-step RAG\n",
      "approaches, with accuracy increases reaching +4.2% on HotpotQA, +4.1% on\n",
      "2WikiMultiHopQA, and +1.3% on MuSiQue compared to Adaptive-RAG, the\n",
      "current best baseline. The benefits of Vendi-RAG are even more pronounced as\n",
      "the number of retrieved documents increases. Finally, we evaluated Vendi-RAG\n",
      "across different LLM backbones, including GPT-3.5, GPT-4, and GPT-4o-mini,\n",
      "and observed consistent improvements, demonstrating that the framework’s\n",
      "advantages are model-agnostic.\n",
      "Keywords: RAG, LLMs, Question Answering, NLP, Diversity, Vendi Scoring\n",
      "1\n",
      "Introduction\n",
      "Retrieval-augmented generation (RAG) has emerged as a transformative framework\n",
      "for enhancing the performance of large language models (LLMs) in domain-specific\n",
      "tasks such as question-answering (QA). By retrieving relevant information from\n",
      "external sources beyond the training set, RAG enables LLMs to answer specialized\n",
      "1\n",
      "arXiv:2502.11228v2  [cs.CL]  22 May 2025\n",
      "Retrieval\n",
      "Query\n",
      "Answer\n",
      "LLM Judge \n",
      "Quality Check\n",
      "Rewritten  \n",
      "Query\n",
      "Final \n",
      "Answer\n",
      "Vendi Score\n",
      "Vendi Score\n",
      "Diversity  \n",
      "weight s\n",
      "Decision Block \n",
      "s > threshold Thr \n",
      "Iteration < N\n",
      "…\n",
      "Reasoning\n",
      "Figure 1: The process begins with an initial retrieval step, where a diverse set of\n",
      "documents is retrieved using the Vendi Score, ensuring broad semantic coverage.\n",
      "Next, leveraging a reasoning step to construct a coherent path to the final answer,\n",
      "the LLM generates an answer, which then undergoes quality assessment by an LLM\n",
      "judge. Based on the answer quality, the retriever is adjusted to balance diversity and\n",
      "relevance: high-quality answers limit the emphasis on diversity, while low-quality\n",
      "answers prompt the retriever to prioritize diversity more heavily. This adjustment\n",
      "is controlled by an adaptive parameter, s, which is updated over iterations. The\n",
      "process continues until the answer quality reaches an optimal threshold, denoted\n",
      "by Thr. Finally, the highest-quality responses and documents are selected, ensuring\n",
      "both diversity and accuracy.\n",
      "queries more effectively Achiam et al. (2023); Team et al. (2023); Jiang et al. (2024).\n",
      "This approach has been particularly successful in single-hop QA, where a question\n",
      "can be answered using information from a single document Raiaan et al. (2024);\n",
      "Kwiatkowski et al. (2019). For instance, answering a question such as \"Who wrote\n",
      "the novel Frankenstein?\" only requires retrieving relevant information from a single\n",
      "document containing this fact.\n",
      "However, multi-hop QA introduces significantly greater complexit\n",
      "</Content>\n",
      "</Document>\n",
      "\n",
      "---\n",
      "\n",
      "<Document source=\"http://arxiv.org/abs/2508.13828v1\" date=\"2025-08-19\" authors=\"Yifei Chen, Guanting Dong, Yutao Zhu, Zhicheng Dou\"/>\n",
      "<Title>\n",
      "Revisiting RAG Ensemble: A Theoretical and Mechanistic Analysis of Multi-RAG System Collaboration\n",
      "</Title>\n",
      "\n",
      "<Summary>\n",
      "Retrieval-Augmented Generation (RAG) technology has been widely applied in\n",
      "recent years. However, despite the emergence of various RAG frameworks, a\n",
      "single RAG framework still cannot adapt well to a broad range of downstream\n",
      "tasks. Therefore, how to leverage the advantages of multiple RAG systems has\n",
      "become an area worth exploring. To address this issue, we have conducted a\n",
      "comprehensive and systematic investigation into ensemble methods based on RAG\n",
      "systems. Specifically, we have analyzed the RAG ensemble framework from both\n",
      "theoretical and mechanistic analysis perspectives. From the theoretical\n",
      "analysis, we provide the first explanation of the RAG ensemble framework from\n",
      "the perspective of information entropy. In terms of mechanism analysis, we have\n",
      "explored the RAG ensemble framework from both the pipeline and module levels.\n",
      "We carefully select four different pipelines (Branching, Iterative, Loop, and\n",
      "Agentic) and three different modules (Generator, Retriever, and Reranker) to\n",
      "solve seven different research questions. The experiments show that aggregating\n",
      "multiple RAG systems is both generalizable and robust, whether at the pipeline\n",
      "level or the module level. Our work lays the foundation for similar research on\n",
      "the multi-RAG system ensemble.\n",
      "</Summary>\n",
      "\n",
      "<Content>\n",
      "Revisiting RAG Ensemble: A Theoretical and Mechanistic\n",
      "Analysis of Multi-RAG System Collaboration\n",
      "Yifei Chen\n",
      "zhangboguodong@ruc.edu.cn\n",
      "Gaoling School of Artificial Intelligence, Renmin\n",
      "University of China\n",
      "Haidian Qu, Beijing Shi, China\n",
      "Guanting Dong\n",
      "Gaoling School of Artificial Intelligence, Renmin\n",
      "University of China\n",
      "Haidian Qu, Beijing Shi, China\n",
      "Yutao Zhu\n",
      "Gaoling School of Artificial Intelligence, Renmin\n",
      "University of China\n",
      "Haidian Qu, Beijing Shi, China\n",
      "Zhicheng Dou\n",
      "Gaoling School of Artificial Intelligence, Renmin\n",
      "University of China\n",
      "Haidian Qu, Beijing Shi, China\n",
      "Abstract\n",
      "Retrieval-Augmented Generation (RAG) technology has been widely\n",
      "applied in recent years. However, despite the emergence of various\n",
      "RAG frameworks, a single RAG framework still cannot adapt well\n",
      "to a broad range of downstream tasks. Therefore, how to leverage\n",
      "the advantages of multiple RAG systems has become an area worth\n",
      "exploring. To address this issue, we have conducted a comprehen-\n",
      "sive and systematic investigation into ensemble methods based on\n",
      "RAG systems. Specifically, we have analyzed the RAG ensemble\n",
      "framework from both theoretical and mechanistic analysis perspec-\n",
      "tives. From the theoretical analysis, we provide the first explanation\n",
      "of the RAG ensemble framework from the perspective of informa-\n",
      "tion entropy. In terms of mechanism analysis, we have explored\n",
      "the RAG ensemble framework from both the pipeline and module\n",
      "levels. We carefully select four different pipelines (Branching, Iter-\n",
      "ative, Loop, and Agentic) and three different modules (Generator,\n",
      "Retriever, and Reranker) to solve seven different research questions.\n",
      "The experiments show that aggregating multiple RAG systems is\n",
      "both generalizable and robust, whether at the pipeline level or the\n",
      "module level. Our work lays the foundation for similar research on\n",
      "the multi-RAG system ensemble.\n",
      "CCS Concepts\n",
      "• Information systems →Information integration.\n",
      "Keywords\n",
      "Retrieval-Augmented Generation, Pipeline Ensemble, Module En-\n",
      "semble, Model Preference\n",
      "ACM Reference Format:\n",
      "Yifei Chen, Guanting Dong, Yutao Zhu, and Zhicheng Dou. 2025. Revisiting\n",
      "RAG Ensemble: A Theoretical and Mechanistic Analysis of Multi-RAG\n",
      "Permission to make digital or hard copies of all or part of this work for personal or\n",
      "classroom use is granted without fee provided that copies are not made or distributed\n",
      "for profit or commercial advantage and that copies bear this notice and the full citation\n",
      "on the first page. Copyrights for components of this work owned by others than the\n",
      "author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\n",
      "republish, to post on servers or to redistribute to lists, requires prior specific permission\n",
      "and/or a fee. Request permissions from permissions@acm.org.\n",
      "CIKM’25, Seoul, Korea\n",
      "© 2025 Copyright held by the owner/author(s). Publication rights licensed to ACM.\n",
      "ACM ISBN 978-1-4503-XXXX-X/2018/06\n",
      "https://doi.org/XXXXXXX.XXXXXXX\n",
      "System Collaboration. In Proceedings of CIKM’25. ACM, New York, NY, USA,\n",
      "13 pages. https://doi.org/XXXXXXX.XXXXXXX\n",
      "1\n",
      "Introduction\n",
      "The emergence of Large Language Models (LLMs) has profoundly\n",
      "revolutionized many real-world tasks that rely on natural lan-\n",
      "guage [4, 45, 70]. However, when dealing with knowledge-intensive\n",
      "tasks, LLMs relying solely on their parametric knowledge often suf-\n",
      "fer from factual inconsistencies or hallucinations. To address these\n",
      "limitations, Retrieval-Augmented Generation (RAG) methods have\n",
      "been proposed, augmenting LLMs with dynamically retrieved exter-\n",
      "nal knowledge. This integration enhances response accuracy and\n",
      "reliability by grounding outputs in verifiable information sources.\n",
      "As research in this field advances, more and more RAG methods\n",
      "have been proposed. Component Module RAG inserts various mod-\n",
      "ules into the standard pipeline to better complete the retrieval task.\n",
      "For instance, the LongLLMLingua and RECOMP methods refine\n",
      "the retrieved knowledge with a refiner, and the SKR and Adaptive\n",
      "RAG methods distinguish the diffi\n",
      "</Content>\n",
      "</Document>\n",
      "\n",
      "---\n",
      "\n",
      "<Document source=\"http://arxiv.org/abs/2410.12812v1\" date=\"2024-10-01\" authors=\"Sarah Packowski, Inge Halilovic, Jenifer Schlotfeldt, Trish Smith\"/>\n",
      "<Title>\n",
      "Optimizing and Evaluating Enterprise Retrieval-Augmented Generation (RAG): A Content Design Perspective\n",
      "</Title>\n",
      "\n",
      "<Summary>\n",
      "Retrieval-augmented generation (RAG) is a popular technique for using large\n",
      "language models (LLMs) to build customer-support, question-answering solutions.\n",
      "In this paper, we share our team's practical experience building and\n",
      "maintaining enterprise-scale RAG solutions that answer users' questions about\n",
      "our software based on product documentation. Our experience has not always\n",
      "matched the most common patterns in the RAG literature. This paper focuses on\n",
      "solution strategies that are modular and model-agnostic. For example, our\n",
      "experience over the past few years - using different search methods and LLMs,\n",
      "and many knowledge base collections - has been that simple changes to the way\n",
      "we create knowledge base content can have a huge impact on our RAG solutions'\n",
      "success. In this paper, we also discuss how we monitor and evaluate results.\n",
      "Common RAG benchmark evaluation techniques have not been useful for evaluating\n",
      "responses to novel user questions, so we have found a flexible, \"human in the\n",
      "lead\" approach is required.\n",
      "</Summary>\n",
      "\n",
      "<Content>\n",
      "Optimizing and Evaluating Enterprise Retrieval-Augmented\n",
      "Generation (RAG): A Content Design Perspective\n",
      "Sarah Packowski\n",
      "spackows@ca.ibm.com\n",
      "IBM\n",
      "Canada\n",
      "Inge Halilovic\n",
      "ingeh@us.ibm.com\n",
      "IBM\n",
      "United States\n",
      "Jenifer Schlotfeldt\n",
      "jschlot@us.ibm.com\n",
      "IBM\n",
      "United States\n",
      "Trish Smith\n",
      "smith@ca.ibm.com\n",
      "IBM\n",
      "Canada\n",
      "ABSTRACT\n",
      "Retrieval-augmented generation (RAG) is a popular technique for\n",
      "using large language models (LLMs) to build customer-support,\n",
      "question-answering solutions. In this paper, we share our team’s\n",
      "practical experience building and maintaining enterprise-scale RAG\n",
      "solutions that answer users’ questions about our software based on\n",
      "product documentation. Our experience has not always matched\n",
      "the most common patterns in the RAG literature. This paper focuses\n",
      "on solution strategies that are modular and model-agnostic. For\n",
      "example, our experience over the past few years - using different\n",
      "search methods and LLMs, and many knowledge base collections -\n",
      "has been that simple changes to the way we create knowledge base\n",
      "content can have a huge impact on our RAG solutions’ success. In\n",
      "this paper, we also discuss how we monitor and evaluate results.\n",
      "Common RAG benchmark evaluation techniques have not been\n",
      "useful for evaluating responses to novel user questions, so we have\n",
      "found a flexible, \"human in the lead\" approach is required.\n",
      "CCS CONCEPTS\n",
      "• Computing methodologies →Artificial intelligence; Natu-\n",
      "ral language generation; • Applied computing →Document\n",
      "management and text processing.\n",
      "KEYWORDS\n",
      "Retrieval-augmented generation, RAG, Large language models\n",
      "ACM Reference Format:\n",
      "Sarah Packowski, Inge Halilovic, Jenifer Schlotfeldt, and Trish Smith. 2024.\n",
      "Optimizing and Evaluating Enterprise Retrieval-Augmented Generation\n",
      "(RAG): A Content Design Perspective. In Proceedings of 8th International\n",
      "Conference on Advances in Artificial Intelligence (ICAAI ’24). ACM, New York,\n",
      "NY, USA, 6 pages.\n",
      "Permission to make digital or hard copies of part or all of this work for personal or\n",
      "classroom use is granted without fee provided that copies are not made or distributed\n",
      "for profit or commercial advantage and that copies bear this notice and the full citation\n",
      "on the first page. Copyrights for third-party components of this work must be honored.\n",
      "For all other uses, contact the owner/author(s).\n",
      "ICAAI ’24, October 2024, London, UK\n",
      "© 2024 Copyright held by the owner/author(s).\n",
      "ACM ISBN 979-8-4007-1801-4/24/10.\n",
      "1\n",
      "INTRODUCTION\n",
      "Retrieval-augmented generation (RAG) is an effective way to use\n",
      "large language models (LLMs) to answer questions while avoiding\n",
      "hallucinations and factual inaccuracy[12, 20, 46]. Basic RAG is sim-\n",
      "ple: 1) search a knowledge base for relevant content; 2) compose a\n",
      "prompt grounded in the retrieved content; and 3) prompt an LLM to\n",
      "generate output. For the retrieval step, one approach dominates the\n",
      "literature: 1) segment content text into chunks; 2) index vectorized\n",
      "chunks for search in a vector database; and 3) when generating\n",
      "answers, ground prompts in a subset of retrieved chunks[13]. Our\n",
      "RAG solutions don’t always use vector databases for search.\n",
      "Wikipedia has long been influenced by and had an influence on\n",
      "scientific research [21, 41]. With respect to RAG, Wikipedia is a dom-\n",
      "inant source of knowledge base content for training data and bench-\n",
      "marks, including: 2WikiMultiHopQA, AmbigQA, ASQA, DART,\n",
      "FEVER, HotpotQA, KILT, MuSiQue, Natural Questions, NoMIRACL,\n",
      "PopQA, SQuAD, StrategyQA, SuperGLUE, TriviaQA, WikiAsp, Wik-\n",
      "iBio, WikiEval, and Wizard of Wikipedia[8, 9, 14–16, 18, 22, 23, 25,\n",
      "28, 29, 31, 34, 39, 40, 42–44, 48]. The knowledge base for our team’s\n",
      "RAG solutions is our own product documentation, which is struc-\n",
      "tured differently from Wikipedia articles.\n",
      "Using common benchmarks to test your RAG implementation\n",
      "involves these steps: 1) index the given knowledge base content in\n",
      "your retriever component; 2) prompt your solution to answer the\n",
      "given questions; and 3) compare generated answers to expected\n",
      "answers, using methods such as exact match, cos\n",
      "</Content>\n",
      "</Document>\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36msearch_web\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "<Document href=\"https://www.linkedin.com/pulse/rag-architectural-review-strategic-outlook-2025-bal%C3%A1zs-feh%C3%A9r-bwzpf\"/>\n",
      "Jul 24, 2025·In a naive RAG pipeline, the flow is fixed: the user's query is used to retrieve documents, which are then passed to the LLM for generation. An\n",
      "</Document>\n",
      "\n",
      "---\n",
      "\n",
      "<Document href=\"https://medium.com/aingineer/a-comprehensive-guide-to-implementing-modular-rag-for-scalable-ai-systems-3fb47c46dc8e\"/>\n",
      "In the rapidly evolving landscape of AI, Modular RAG (Retrieval-Augmented Generation) has emerged as a transformative approach to building robust, scalable, and adaptable AI systems. By decoupling retrieval, reasoning, and generation into independent modules, Modular RAG empowers engineering leaders, architects, and senior engineers to design systems that are not only efficient but also flexible enough to meet the dynamic demands of modern enterprises. Modular RAG is an architectural paradigm that separates the core components of Retrieval-Augmented Generation into three independent modules: For instance, retrieval mechanisms can be powered by vector search engines like Elasticsearch, while the reasoning module may use transformer-based models for contextual understanding. Implement mechanisms to fine-tune modules for domain-specific requirements, such as using custom embeddings for retrieval or training bespoke generation models.\n",
      "</Document>\n",
      "\n",
      "---\n",
      "\n",
      "<Document href=\"https://dextralabs.com/blog/rag-pipeline-explained-diagram-implementation/\"/>\n",
      "**Retrieval-Augmented Generation (RAG)** is changing how we build and use large language models. _**RAG in Action:**A question-answering process with 3 steps — (1) Indexing: split and store document chunks as vectors; (2) Retrieval: fetch top-k relevant chunks; (3) Generation: use retrieved chunks with the question to generate the final answer via LLM._ _A system-level RAG pipeline showing how queries pass through indexing, retrieval, and generation stages — ideal for illustrating how external knowledge integrates with LLMs to produce accurate responses._ A **RAG stack** refers to the full set of tools used to build a RAG system — including the retriever (like FAISS or Pinecone), vector store, LLM (like GPT), prompt layer, and sometimes rerankers or query rewriters.\n",
      "</Document>\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36manswer_question\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: expert\n",
      "\n",
      "You're welcome! If you have any more questions in the future or need further assistance, feel free to reach out. Have a great day and good luck with your research on RAG systems!\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36msave_interview\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "\u001b[1;32minterview\u001b[0m:\n",
      "Human: So you said you were writing an article on Modular RAG 가 기존의 Naive RAG 와 어떤 차이가 있는지와 production level 에서 사용하는 이점?\n",
      "AI: Hello, Dr. Kim. My name is Alex Thompson, and I'm an analyst deeply interested in the practical applications of machine learning frameworks, particularly Modular RAG and Naive RAG. I'm eager to understand the differences between these two frameworks, especially in terms of their flexibility and efficiency in handling complex data requirements in production settings. Could you please elaborate on these aspects for us?\n",
      "AI: Certainly, Dr. Kim. The primary difference between Modular RAG and Naive RAG lies in their structure and approach to information retrieval and generation.\n",
      "\n",
      "Naive RAG starts by indexing data sources to enable quick retrieval of relevant information. It maintains a straightforward approach where the retrieval and generation processes occur in a single, undivided pipeline. This simplicity can sometimes limit its flexibility and precision, particularly in handling complex or specialized data requirements in production environments [4].\n",
      "\n",
      "Modular RAG, on the other hand, builds upon Naive RAG by compartmentalizing the retrieval and generation processes into distinct, specialized modules. This modularity allows for greater flexibility as each module can be independently optimized or replaced to better suit specific data characteristics or processing needs. For example, different retrieval techniques can be employed within the same system, depending on the data type and retrieval demands. This separation allows for more precise and context-aware data retrieval, which enhances the overall responsiveness and relevance of the generated outputs [4][5].\n",
      "\n",
      "In production settings, using Modular RAG offers several benefits:\n",
      "\n",
      "1. **Customization**: By separating the retrieval and generation processes, organizations can tailor each component to better align with their data and application requirements, ensuring higher performance and efficiency.\n",
      "\n",
      "2. **Scalability**: Modular systems can be easily scaled by adapting or swapping individual modules without overhauling the entire architecture, which is particularly useful as data complexity grows.\n",
      "\n",
      "3. **Improved Accuracy**: With specialized modules, Modular RAG systems can achieve better accuracy in retrieving relevant data and generating coherent outputs, as each module is tuned to handle specific tasks optimally.\n",
      "\n",
      "These features make Modular RAG a more robust choice for handling complex and dynamic data requirements in production environments compared to Naive RAG [4].\n",
      "\n",
      "Sources:\n",
      "[4] medium.com/@drjulija/what-are-naive-rag-advanced-rag-modular-rag-paradigms-edff410c202e\n",
      "[5] blog.searce.com/unlocking-the-power-of-llm-rag-discover-naive-advanced-and-modular-retrieval-augmented-6922b353d8d3\n",
      "AI: Thank you for that detailed explanation, Dr. Kim. Could you please provide a specific example or case study where Modular RAG was effectively implemented in a production setting, highlighting the challenges faced and how the modular approach was beneficial?\n",
      "AI: Certainly. Let's consider a case study in the automotive industry, as described in a research article on optimizing RAG techniques for PDF chatbots. The context involved processing complex automotive industry documents using locally deployed models. Here, Modular RAG was utilized to efficiently handle multi-column layouts and technical specifications typical of automotive documents [6].\n",
      "\n",
      "Challenges in this scenario included the need to process diverse document types and the requirement for high context precision and recall. Traditional methods, like Naive RAG, might struggle with these complexities due to their monolithic design, which limits the ability to tailor retrieval processes to meet specific document processing needs.\n",
      "\n",
      "By implementing a Modular RAG system, the retrieval mechanisms could be separately optimized to handle the specific challenges posed by automotive document structures. Custom modules for PDF processing and context compression were designed, allowing for tailored embedding pipelines and retrieval strategies. This modular approach resulted in significant improvements in context precision, recall, and answer relevancy, particularly when applied to the proprietary automotive industry dataset [6].\n",
      "\n",
      "The benefits of using Modular RAG in this production setting included enhanced flexibility to adapt retrieval processes to different document types and improved scalability and accuracy in generating contextually relevant responses. This case study demonstrates how Modular RAG can be effectively deployed to address complex data requirements in real-world scenarios, showcasing its advantage over more static frameworks like Naive RAG.\n",
      "\n",
      "[6] arxiv.org/abs/2408.05933v1\n",
      "AI: That's fascinating, Dr. Kim. It's clear that Modular RAG offers tangible benefits in complex environments like the automotive industry. One last question: are there any specific challenges or limitations associated with implementing Modular RAG that organizations should be aware of?\n",
      "AI: Yes, there are specific challenges and limitations associated with implementing Modular RAG that organizations should consider:\n",
      "\n",
      "1. **Integration Complexity**: While Modular RAG provides flexibility through its separate modules, integrating these components can sometimes be more complex than using a monolithic system like Naive RAG. Organizations must carefully design each module and ensure seamless interaction between them to maintain the system's overall efficiency and effectiveness [5].\n",
      "\n",
      "2. **Resource Intensive**: The modular approach might require more computational resources, especially if multiple or sophisticated modules are used. This can increase the cost of deployment, particularly in environments with limited infrastructure capabilities [5].\n",
      "\n",
      "3. **Maintenance Overhead**: With several independent modules, maintaining and updating the system can be more demanding. Each module might need individual attention to ensure it keeps up with evolving data requirements and technology advancements [4].\n",
      "\n",
      "4. **Configuration Challenges**: Determining the optimal configuration of different modules can be challenging. The effectiveness of Modular RAG systems can heavily depend on selecting the right modules and fine-tuning them to work in harmony, which may require skilled personnel [5].\n",
      "\n",
      "Despite these challenges, the benefits of adaptability, precision, and scalability often outweigh the downsides for many organizations with complex data processing needs.\n",
      "\n",
      "Sources:\n",
      "[4] medium.com/@drjulija/what-are-naive-rag-advanced-rag-modular-rag-paradigms-edff410c202e\n",
      "[5] adasci.org/how-does-modular-rag-improve-upon-naive-rag\n",
      "AI: Thank you so much for your help, Dr. Kim!\n",
      "AI: You're welcome! If you have any more questions in the future or need further assistance, feel free to reach out. Have a great day and good luck with your research on RAG systems!\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mwrite_section\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "## Evaluating the Flexibility and Efficiency of Modular RAG vs. Naive RAG\n",
      "\n",
      "### Summary\n",
      "\n",
      "In the rapidly advancing field of machine learning, Retrieval-Augmented Generation (RAG) frameworks have become pivotal in enhancing the capabilities of Large Language Models (LLMs), especially in complex data scenarios. Dr. Kim, an academic researcher with a special focus on machine learning frameworks, delves into a critical comparison of Modular RAG and Naive RAG, analyzing their flexibility and efficiency in production settings. The insights gathered offer novel perspectives on the adaptability and performance of these frameworks across different domains.\n",
      "\n",
      "RAG is designed to mitigate the typical issues faced by LLMs, such as hallucinations and factual inaccuracies, by grounding responses in retrievable external knowledge [1]. The conventional Naive RAG functions through a simple mechanism of retrieving and leveraging relevant documents for generation tasks. However, as highlighted in recent evaluations, Naive RAG's lack of flexibility often leads to inefficiencies, especially in adapting to diverse domain requirements [2]. This has paved the way for Modular RAG, which breaks down the RAG process into separable modules, offering greater adaptability and precision [3].\n",
      "\n",
      "In their comprehensive evaluation, Liang and colleagues introduce OmniBench RAG, a platform that further exemplifies how Modular RAG can be leveraged to assess RAG systems across multiple domains with precision [4]. On the other hand, frameworks such as Vendi-RAG demonstrate how dynamic integration and iterative processes can enhance RAG’s performance by balancing retrieval diversity and answer quality [5]. \n",
      "\n",
      "Emerging research emphasizes the need for a modular approach, which is agile not just in technical performance but also in handling various complexities inherent in real-world scenarios. Such findings underscore the critical role of modular architectures in optimizing RAG's effectiveness when applied to intricate, domain-specific applications.\n",
      "\n",
      "Sources:\n",
      "1. [2] https://medium.com/@drjulija/what-are-naive-rag-advanced-rag-modular-rag-paradigms-edff410c202e\n",
      "2. [2] https://adasci.org/how-does-modular-rag-improve-upon-naive-rag/\n",
      "3. [2] https://blog.searce.com/unlocking-the-power-of-llm-rag-discover-naive-advanced-and-modular-retrieval-augmented-6922b353d8d3\n",
      "4. [1] http://arxiv.org/abs/2508.05650v1\n",
      "5. [4] http://arxiv.org/abs/2502.11228v2\n",
      "\n",
      "### Comprehensive Analysis\n",
      "\n",
      "#### 1. Introduction to RAG Frameworks\n",
      "\n",
      "Retrieval-Augmented Generation (RAG) frameworks represent a fusion of information retrieval and natural language generation, enhancing LLMs' competence by accessing external information sources. This aspect is crucial for Dr. Kim’s interest since RAG systems address foundational challenges, such as large context window limitations and the need for real-time factual accuracy [4]. While Naive RAG follows a monolithic structure, Modular RAG emerges as a flexible paradigm by decomposing the processes into specialized, interchangeable modules [2].\n",
      "\n",
      "#### 2. Naive RAG: Strengths and Limitations\n",
      "\n",
      "The Naive RAG framework begins with straightforward indexing and retrieval, relying heavily on the relevance of documents to input queries [2]. It functions efficiently in environments requiring a basic level of retrieval and generation without complex modular demands. However, its static nature becomes a liability in dynamic production settings where rapid adaptability is required:\n",
      "\n",
      "- **Efficiency Concerns**: Naive RAG lacks agility, making it less efficient in handling diverse data requirements. As tasks become complex, the framework's inefficiencies in integrating retrieval and generation processes lead to increased latency and possible incorrect outputs [3].\n",
      "\n",
      "- **Integration Challenges**: The monolithic framework restricts seamless integration with advanced retrieval systems, hindering its effectiveness in scaling across different domains [3].\n",
      "\n",
      "#### 3. Modular RAG: Advancements and Benefits\n",
      "\n",
      "The Modular RAG framework represents a significant shift towards configurable and scalable AI systems. By segregating retrieval, reasoning, and generation into distinct components, it addresses many of the efficiency challenges associated with Naive RAG [2]:\n",
      "\n",
      "- **Enhanced Flexibility**: Modular RAG's ability to interchange modules allows it to tailor components specifically to domain requirements, ensuring a higher degree of accuracy and relevance in generated responses [2].\n",
      "\n",
      "- **Scalable Architecture**: The framework’s architecture supports a variety of retrieval mechanisms, such as Elasticsearch, enhancing its efficacy in diverse production environments [6].\n",
      "\n",
      "- **Superior Performance Metrics**: Tools like OmniBench RAG illustrate how Modular RAG's architecture can be utilized for systematic performance evaluations across domains, highlighting significant gains in areas like cultural data and efficiency drops in domains like mathematics [4].\n",
      "\n",
      "#### 4. Comparative Evaluations and Domain Applications\n",
      "\n",
      "By leveraging advanced platform evaluations like OmniBench RAG, the nuanced differences in RAG systems can be quantified. The introduction of standardized metrics – Improvements and Transformation – allows for reproducible comparisons, providing deep insights into the respective capabilities of Modular versus Naive RAG:\n",
      "\n",
      "- **Domain-Specific Evaluation**: Modular RAG demonstrates significant improvements in knowledge-intensive and domain-specific tasks due to its modular nature, allowing for precise evaluations and scalability [4].\n",
      "\n",
      "- **Iterative Improvements**: Frameworks like Vendi-RAG that incorporate iterative methods further demonstrate the adaptability and improved performance of Modular RAG under strenuous conditions [5].\n",
      "\n",
      "In conclusion, the comprehensive analysis underscores Modular RAG's superiority in managing complex, evolving data requirements in production settings, ultimately advocating for its broader adoption in scenarios demanding high adaptability and efficiency. Dr. Kim would likely find Modular RAG to be the favorable choice for its robustness, agility, and precision across varying domains.\n",
      "\n",
      "### Sources\n",
      "[1] http://arxiv.org/abs/2508.05650v1  \n",
      "[2] https://medium.com/@drjulija/what-are-naive-rag-advanced-rag-modular-rag-paradigms-edff410c202e  \n",
      "[3] https://adasci.org/how-does-modular-rag-improve-upon-naive-rag/  \n",
      "[4] http://arxiv.org/abs/2502.11228v2  \n",
      "[5] https://blog.searce.com/unlocking-the-power-of-llm-rag-discover-naive-advanced-and-modular-retrieval-augmented-6922b353d8d3  \n",
      "[6] https://dextralabs.com/blog/rag-pipeline-explained-diagram-implementation/  \n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "# 주제 설정\n",
    "topic = \"Modular RAG 가 기존의 Naive RAG 와 어떤 차이가 있는지와 production level 에서 사용하는 이점\"\n",
    "\n",
    "# 인터뷰 시작 메시지 생성\n",
    "messages = [HumanMessage(f\"So you said you were writing an article on {topic}?\")]\n",
    "\n",
    "# 스레드 ID 설정\n",
    "config = RunnableConfig(\n",
    "    recursion_limit=100,\n",
    "    configurable={\"thread_id\": random_uuid()},\n",
    ")\n",
    "\n",
    "# 그래프 실행\n",
    "invoke_graph(\n",
    "    interview_graph,\n",
    "    {\"analyst\": analysts[0], \"messages\": messages, \"max_num_turns\": 5},\n",
    "    config,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Evaluating the Flexibility and Efficiency of Modular RAG vs. Naive RAG\n",
       "\n",
       "### Summary\n",
       "\n",
       "In the rapidly advancing field of machine learning, Retrieval-Augmented Generation (RAG) frameworks have become pivotal in enhancing the capabilities of Large Language Models (LLMs), especially in complex data scenarios. Dr. Kim, an academic researcher with a special focus on machine learning frameworks, delves into a critical comparison of Modular RAG and Naive RAG, analyzing their flexibility and efficiency in production settings. The insights gathered offer novel perspectives on the adaptability and performance of these frameworks across different domains.\n",
       "\n",
       "RAG is designed to mitigate the typical issues faced by LLMs, such as hallucinations and factual inaccuracies, by grounding responses in retrievable external knowledge [1]. The conventional Naive RAG functions through a simple mechanism of retrieving and leveraging relevant documents for generation tasks. However, as highlighted in recent evaluations, Naive RAG's lack of flexibility often leads to inefficiencies, especially in adapting to diverse domain requirements [2]. This has paved the way for Modular RAG, which breaks down the RAG process into separable modules, offering greater adaptability and precision [3].\n",
       "\n",
       "In their comprehensive evaluation, Liang and colleagues introduce OmniBench RAG, a platform that further exemplifies how Modular RAG can be leveraged to assess RAG systems across multiple domains with precision [4]. On the other hand, frameworks such as Vendi-RAG demonstrate how dynamic integration and iterative processes can enhance RAG’s performance by balancing retrieval diversity and answer quality [5]. \n",
       "\n",
       "Emerging research emphasizes the need for a modular approach, which is agile not just in technical performance but also in handling various complexities inherent in real-world scenarios. Such findings underscore the critical role of modular architectures in optimizing RAG's effectiveness when applied to intricate, domain-specific applications.\n",
       "\n",
       "Sources:\n",
       "1. [2] https://medium.com/@drjulija/what-are-naive-rag-advanced-rag-modular-rag-paradigms-edff410c202e\n",
       "2. [2] https://adasci.org/how-does-modular-rag-improve-upon-naive-rag/\n",
       "3. [2] https://blog.searce.com/unlocking-the-power-of-llm-rag-discover-naive-advanced-and-modular-retrieval-augmented-6922b353d8d3\n",
       "4. [1] http://arxiv.org/abs/2508.05650v1\n",
       "5. [4] http://arxiv.org/abs/2502.11228v2\n",
       "\n",
       "### Comprehensive Analysis\n",
       "\n",
       "#### 1. Introduction to RAG Frameworks\n",
       "\n",
       "Retrieval-Augmented Generation (RAG) frameworks represent a fusion of information retrieval and natural language generation, enhancing LLMs' competence by accessing external information sources. This aspect is crucial for Dr. Kim’s interest since RAG systems address foundational challenges, such as large context window limitations and the need for real-time factual accuracy [4]. While Naive RAG follows a monolithic structure, Modular RAG emerges as a flexible paradigm by decomposing the processes into specialized, interchangeable modules [2].\n",
       "\n",
       "#### 2. Naive RAG: Strengths and Limitations\n",
       "\n",
       "The Naive RAG framework begins with straightforward indexing and retrieval, relying heavily on the relevance of documents to input queries [2]. It functions efficiently in environments requiring a basic level of retrieval and generation without complex modular demands. However, its static nature becomes a liability in dynamic production settings where rapid adaptability is required:\n",
       "\n",
       "- **Efficiency Concerns**: Naive RAG lacks agility, making it less efficient in handling diverse data requirements. As tasks become complex, the framework's inefficiencies in integrating retrieval and generation processes lead to increased latency and possible incorrect outputs [3].\n",
       "\n",
       "- **Integration Challenges**: The monolithic framework restricts seamless integration with advanced retrieval systems, hindering its effectiveness in scaling across different domains [3].\n",
       "\n",
       "#### 3. Modular RAG: Advancements and Benefits\n",
       "\n",
       "The Modular RAG framework represents a significant shift towards configurable and scalable AI systems. By segregating retrieval, reasoning, and generation into distinct components, it addresses many of the efficiency challenges associated with Naive RAG [2]:\n",
       "\n",
       "- **Enhanced Flexibility**: Modular RAG's ability to interchange modules allows it to tailor components specifically to domain requirements, ensuring a higher degree of accuracy and relevance in generated responses [2].\n",
       "\n",
       "- **Scalable Architecture**: The framework’s architecture supports a variety of retrieval mechanisms, such as Elasticsearch, enhancing its efficacy in diverse production environments [6].\n",
       "\n",
       "- **Superior Performance Metrics**: Tools like OmniBench RAG illustrate how Modular RAG's architecture can be utilized for systematic performance evaluations across domains, highlighting significant gains in areas like cultural data and efficiency drops in domains like mathematics [4].\n",
       "\n",
       "#### 4. Comparative Evaluations and Domain Applications\n",
       "\n",
       "By leveraging advanced platform evaluations like OmniBench RAG, the nuanced differences in RAG systems can be quantified. The introduction of standardized metrics – Improvements and Transformation – allows for reproducible comparisons, providing deep insights into the respective capabilities of Modular versus Naive RAG:\n",
       "\n",
       "- **Domain-Specific Evaluation**: Modular RAG demonstrates significant improvements in knowledge-intensive and domain-specific tasks due to its modular nature, allowing for precise evaluations and scalability [4].\n",
       "\n",
       "- **Iterative Improvements**: Frameworks like Vendi-RAG that incorporate iterative methods further demonstrate the adaptability and improved performance of Modular RAG under strenuous conditions [5].\n",
       "\n",
       "In conclusion, the comprehensive analysis underscores Modular RAG's superiority in managing complex, evolving data requirements in production settings, ultimately advocating for its broader adoption in scenarios demanding high adaptability and efficiency. Dr. Kim would likely find Modular RAG to be the favorable choice for its robustness, agility, and precision across varying domains.\n",
       "\n",
       "### Sources\n",
       "[1] http://arxiv.org/abs/2508.05650v1  \n",
       "[2] https://medium.com/@drjulija/what-are-naive-rag-advanced-rag-modular-rag-paradigms-edff410c202e  \n",
       "[3] https://adasci.org/how-does-modular-rag-improve-upon-naive-rag/  \n",
       "[4] http://arxiv.org/abs/2502.11228v2  \n",
       "[5] https://blog.searce.com/unlocking-the-power-of-llm-rag-discover-naive-advanced-and-modular-retrieval-augmented-6922b353d8d3  \n",
       "[6] https://dextralabs.com/blog/rag-pipeline-explained-diagram-implementation/  "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 완성된 인터뷰 섹션 출력\n",
    "Markdown(interview_graph.get_state(config).values[\"sections\"][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import List, Annotated\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "\n",
    "# ResearchGraphState 상태 정의\n",
    "class ResearchGraphState(TypedDict):\n",
    "    # 연구 주제\n",
    "    topic: str\n",
    "    # 생성할 분석가의 최대 수\n",
    "    max_analysts: int\n",
    "    # 인간 분석가의 피드백\n",
    "    human_analyst_feedback: str\n",
    "    # 질문을 하는 분석가 목록\n",
    "    analysts: List[Analyst]\n",
    "    # Send() API 키를 포함하는 섹션 리스트\n",
    "    sections: Annotated[list, operator.add]\n",
    "    # 최종 보고서의 서론\n",
    "    introduction: str\n",
    "    # 최종 보고서의 본문 내용\n",
    "    content: str\n",
    "    # 최종 보고서의 결론\n",
    "    conclusion: str\n",
    "    # 최종 보고서\n",
    "    final_report: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sj/3r49bs6178v7nfhcxh7lpzd80000gn/T/ipykernel_43129/556175951.py:1: LangGraphDeprecatedSinceV10: Importing Send from langgraph.constants is deprecated. Please use 'from langgraph.types import Send' instead. Deprecated in LangGraph V1.0 to be removed in V2.0.\n",
      "  from langgraph.constants import Send\n"
     ]
    }
   ],
   "source": [
    "from langgraph.constants import Send\n",
    "\n",
    "\n",
    "# 모든 인터뷰를 시작\n",
    "def initiate_all_interviews(state: ResearchGraphState):\n",
    "    # 사람의 피드백 확인\n",
    "    human_analyst_feedback = state.get(\"human_analyst_feedback\")\n",
    "\n",
    "    # 만약, 사람의 피드백이 있으면 분석가 생성으로 돌아가기\n",
    "    if human_analyst_feedback:\n",
    "        return \"create_analysts\"\n",
    "\n",
    "    # 그렇지 않으면 Send() 함수를 통해 인터뷰 병렬로 시작\n",
    "    else:\n",
    "        topic = state[\"topic\"]\n",
    "        return [\n",
    "            Send(\n",
    "                \"conduct_interview\",\n",
    "                {\n",
    "                    \"analyst\": analyst,\n",
    "                    \"messages\": [\n",
    "                        HumanMessage(\n",
    "                            content=f\"So you said you were writing an article on {topic}?\"\n",
    "                        )\n",
    "                    ],\n",
    "                },\n",
    "            )\n",
    "            for analyst in state[\"analysts\"]\n",
    "        ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 보고서 작성 지시사항\n",
    "report_writer_instructions = \"\"\"You are a technical writer creating a report on this overall topic:\n",
    "\n",
    "{topic}\n",
    "\n",
    "You have a team of analysts. Each analyst has done two things:\n",
    "\n",
    "1. They conducted an interview with an expert on a specific sub-topic.\n",
    "2. They write up their finding into a memo.\n",
    "\n",
    "Your task:\n",
    "\n",
    "1. You will be given a collection of memos from your analysts.  \n",
    "2. Carefully review and analyze the insights from each memo.  \n",
    "3. Consolidate these insights into a detailed and comprehensive summary that integrates the central ideas from all the memos.  \n",
    "4. Organize the key points from each memo into the appropriate sections provided below, ensuring that each section is logical and well-structured.  \n",
    "5. Include all required sections in your report, using `### Section Name` as the header for each.  \n",
    "6. Aim for approximately 250 words per section, providing in-depth explanations, context, and supporting details.  \n",
    "\n",
    "**Sections to consider (including optional ones for greater depth):**\n",
    "\n",
    "- **Background**: Theoretical foundations, key concepts, and preliminary information necessary to understand the methodology and results.\n",
    "- **Related Work**: Overview of prior studies and how they compare or relate to the current research.\n",
    "- **Problem Definition**: A formal and precise definition of the research question or problem the paper aims to address.\n",
    "- **Methodology (or Methods)**: Detailed description of the methods, algorithms, models, data collection processes, or experimental setups used in the study.\n",
    "- **Implementation Details**: Practical details of how the methods or models were implemented, including software frameworks, computational resources, or parameter settings.\n",
    "- **Experiments**: Explanation of experimental protocols, datasets, evaluation metrics, procedures, and configurations employed to validate the methods.\n",
    "- **Results**: Presentation of experimental outcomes, often with statistical tables, graphs, figures, or qualitative analyses.\n",
    "\n",
    "To format your report:\n",
    "\n",
    "1. Use markdown formatting.\n",
    "2. Include no pre-amble for the report.\n",
    "3. Use no sub-heading.\n",
    "4. Start your report with a single title header: ## Insights\n",
    "5. Do not mention any analyst names in your report.\n",
    "6. Preserve any citations in the memos, which will be annotated in brackets, for example [1] or [2].\n",
    "7. Create a final, consolidated list of sources and add to a Sources section with the `## Sources` header.\n",
    "8. List your sources in order and do not repeat.\n",
    "\n",
    "[1] Source 1\n",
    "[2] Source 2\n",
    "\n",
    "Here are the memos from your analysts to build your report from:\n",
    "\n",
    "{context}\"\"\"\n",
    "\n",
    "\n",
    "# 보고서 작성 함수 정의\n",
    "def write_report(state: ResearchGraphState):\n",
    "    # 모든 섹션 가져오기\n",
    "    sections = state[\"sections\"]\n",
    "    topic = state[\"topic\"]\n",
    "\n",
    "    # 모든 섹션을 하나의 문자열로 연결\n",
    "    formatted_str_sections = \"\\n\\n\".join([f\"{section}\" for section in sections])\n",
    "\n",
    "    # 섹션을 요약하여 최종 보고서 작성\n",
    "    system_message = report_writer_instructions.format(\n",
    "        topic=topic, context=formatted_str_sections\n",
    "    )\n",
    "    report = llm.invoke(\n",
    "        [SystemMessage(content=system_message)]\n",
    "        + [HumanMessage(content=f\"Write a report based upon these memos.\")]\n",
    "    )\n",
    "    return {\"content\": report.content}\n",
    "\n",
    "\n",
    "# 서론과 결론 작성 지시사항\n",
    "intro_conclusion_instructions = \"\"\"You are a technical writer finishing a report on {topic}\n",
    "\n",
    "You will be given all of the sections of the report.\n",
    "\n",
    "You job is to write a crisp and compelling introduction or conclusion section.\n",
    "\n",
    "The user will instruct you whether to write the introduction or conclusion.\n",
    "\n",
    "Include no pre-amble for either section.\n",
    "\n",
    "Target around 200 words, crisply previewing (for introduction),  or recapping (for conclusion) all of the sections of the report.\n",
    "\n",
    "Use markdown formatting.\n",
    "\n",
    "For your introduction, create a compelling title and use the # header for the title.\n",
    "\n",
    "For your introduction, use ## Introduction as the section header.\n",
    "\n",
    "For your conclusion, use ## Conclusion as the section header.\n",
    "\n",
    "Here are the sections to reflect on for writing: {formatted_str_sections}\"\"\"\n",
    "\n",
    "\n",
    "# 서론 작성 함수 정의\n",
    "def write_introduction(state: ResearchGraphState):\n",
    "    # 모든 섹션 가져오기\n",
    "    sections = state[\"sections\"]\n",
    "    topic = state[\"topic\"]\n",
    "\n",
    "    # 모든 섹션을 하나의 문자열로 연결\n",
    "    formatted_str_sections = \"\\n\\n\".join([f\"{section}\" for section in sections])\n",
    "\n",
    "    # 섹션을 요약하여 서론 작성\n",
    "    instructions = intro_conclusion_instructions.format(\n",
    "        topic=topic, formatted_str_sections=formatted_str_sections\n",
    "    )\n",
    "    intro = llm.invoke(\n",
    "        [instructions] + [HumanMessage(content=f\"Write the report introduction\")]\n",
    "    )\n",
    "    return {\"introduction\": intro.content}\n",
    "\n",
    "\n",
    "# 결론 작성 함수 정의\n",
    "def write_conclusion(state: ResearchGraphState):\n",
    "    # 모든 섹션 가져오기\n",
    "    sections = state[\"sections\"]\n",
    "    topic = state[\"topic\"]\n",
    "\n",
    "    # 모든 섹션을 하나의 문자열로 연결\n",
    "    formatted_str_sections = \"\\n\\n\".join([f\"{section}\" for section in sections])\n",
    "\n",
    "    # 섹션을 요약하여 결론 작성\n",
    "    instructions = intro_conclusion_instructions.format(\n",
    "        topic=topic, formatted_str_sections=formatted_str_sections\n",
    "    )\n",
    "    conclusion = llm.invoke(\n",
    "        [instructions] + [HumanMessage(content=f\"Write the report conclusion\")]\n",
    "    )\n",
    "    return {\"conclusion\": conclusion.content}\n",
    "\n",
    "\n",
    "# 최종 보고서 작성 함수 정의\n",
    "def finalize_report(state: ResearchGraphState):\n",
    "    # 모든 섹션을 모아 최종 보고서 작성\n",
    "    content = state[\"content\"]\n",
    "    if content.startswith(\"## Insights\"):\n",
    "        content = content.strip(\"## Insights\")\n",
    "    if \"## Sources\" in content:\n",
    "        try:\n",
    "            content, sources = content.split(\"\\n## Sources\\n\")\n",
    "        except:\n",
    "            sources = None\n",
    "    else:\n",
    "        sources = None\n",
    "\n",
    "    final_report = (\n",
    "        state[\"introduction\"]\n",
    "        + \"\\n\\n---\\n\\n## Main Idea\\n\\n\"\n",
    "        + content\n",
    "        + \"\\n\\n---\\n\\n\"\n",
    "        + state[\"conclusion\"]\n",
    "    )\n",
    "    if sources is not None:\n",
    "        final_report += \"\\n\\n## Sources\\n\" + sources\n",
    "    return {\"final_report\": final_report}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjkAAAKgCAIAAAD24PuxAAAAAXNSR0IArs4c6QAAIABJREFUeJzs3XdgU9XfBvBv0ozuvekubdNFC7TQsvcQFJQho4IiCqIiThRFGaKg4sAJOEFkiwyRvUFGWzqge++9m6SZ7x+XX+yLpQyb3pv2+fyVJjcnT1ro03POTcLTarUEAADAYXy2AwAAANwFugoAALgOXQUAAFyHrgIAAK5DVwEAANehqwAAgOuMVqxYwXYGAINR3iK9VFMmU6tuNNScqy6xExlbCUWnKovavHyyovB8dam9yNhSP5ePVxRcqC51EJtYCu54+VRlUVJDtauJmZhvxPY3D+DBCdgOAMB1LRr1X+X5Co1mgpNnfF1lsazJ1MhIplYqNOo6ZYupkeBOl6UatUKjrlMqTIzk+rgsU6sVGnW9osWYb3Sny1K1qkYhL5fLyJjeSL7obWa1yCfUQiBk+5sKcH94eC0wwJ20aNQCHv9IeX6tqiXKxtlGKGY70X+V3FDtaWrhLDY9XlH4iIs323EA7hW6CqBtJyuL95VkrQzsz3YQvdhVnFmjkC/zj+DxeGxnAbg7dBVAG9Ra7Y/5qY+5+rAdRI9qlC3OYtN8aUOIpR3bWQDuAl0FcLtrtRWOYhPz7rGp80dpzlC7Hj3NrdgOAtAenLMO8P98lBmv0mq6SVER0WQXn0JZY3ZTPdtBANqDeRXAP6qVLS1qVTc8vVvA51sKRNi5As7CvArgljplS1WLtBsWFRGpNJqn4k6wnQLgjtBVALesSrtmJTRmOwVr5nsF7S3JYjsFQNuwBghARHS9vqpZpQi0sGU7CJtEfL6FQMR2CoA2oKsAiIhkapVUrWI7BcvypA2NSuUwhx5sBwG4HdYAASi9qe5QWV7nP+6695f+/P2GB7jjzMlDkxJiOzyPl6nlgbKcDh8W4L9DVwHQ6aqizn+LPJlMenj/Lr+AoPu9Y9zVi7k5GQ9wx3sxycWnVC7Vx8gA/wW6CoD8zazDrR30NHh87N8frXlz6sQB/UOdRw2U7Nr2PRGdP3tsWD8fuVz2yqKYd15fQETpackfrXlz2sMDh0R4PTv3kUvnTzF3T7lxvX+o8/kzRx8aEfbs3Ec2fLJy0dNTtFrtsH4++3Zt6fC0IZZ2zsamHT4swH+E91kHoP42TnKNWh8jK5SKpUueGjpi/Kq1X7t7eMdevfDmy/N9fAMGDx0z79klB//YeejkdSLSaDTvvrHI0tr6zXfWNUubzp0++vKiWQeOxTm59MjJSieiQ3/sWrv++54BQaamZjeT4x2dXVev+1YfgUvlzacqi6b16KmPwQEeGLoKurtSuXRfSfbjbn76GLymsqKhvm7wsDFBIb2JaPioiTv2n/Pw9CWirMw03Toen8///NvfjE1MbGzticjbJ+Dgvu2ZmSlOLj0y01OIaN7CJQGSUObg9NTkQUNH6yMtEQn5Rkn1Vegq4Bp0FXR3lQpZtUKup8GdXHr0ix7y0Zq3qirKowYN7+Hm6e3jz9yUlZkyetwk5rJMJt2/99fE61fzcrNqqiuZK+0dnIgoKys1MmqIrqhKiwtlMqlfQIieAtsIxUNxHiBwD/aroLvzNrN6TG/TCB6Pt/azH8aMm7z9103TJg74/KN3ZTIpEUmlzSVFBT39g5kFwIVzJ584sv/R6XMPHI+/klw2b8HLPB7Py9uPiDJSk/tEROsGzMy4SURBIeF6CmzE4w2yc9XT4AAPDF0F3Z2FkcDTxFx/45uZWbz0+ordBy8uXf7RsSP731/+EhFlpt0kIn9JMBHFX72Ylpq0bNVnY8ZPFgqFRJSZnuLtG2BsbFJRXtJQX+frF6AbLSszzc7e0dLKWk9pG1SKTzOv62lwgAeGroLurlGlXJl6RR8ja7Xa0ycOKRQtzARr0pTZI0ZNKMzPI6KcrDSBQMBsXJWVFRORk/Ot2Ux5afGVS2eYGsvMSCEiP/9/VvxyMlO9ewbc+TH/qwqFXEN4fwDgHHQVdHcWAqFUrWpSKzt8ZGlz44q3Xvx03fLUlMTamqqjh/f9eWBXZPQQIqqpqeLzjeKvXqysKPPw6klEJ44cIKK01KRV7y6xsLRycHQhouyMNBMTU1c3D92YtXU1submuCsXWlr0ssfmKDKe5xmsj5EB/gu8xxIAVSpkSo1GH++wHnft0tpVrxfkZZuZW/oHBI2dOOXhyTMFAkF+XvabS+YV5Gdv2LSrb+SAHb9u2vT1J81NDX37DXzv/Q1ff7Hm6J+/z5qzoKqqoryseNMvB3QDnj11+MOVb2g06j+OxpqamnV4YBHfqPNfFg1wV+gqAGL2aZQaDdsp2Lc2Pfb94Oju+cEowGU4Zx2ANKR9NenC2pABdzpAq9Wufuelf18vb5Ebi9v+GBGhWPzWux93aMx/5OVmbvn+yzZvqqwqc7B3bvOmngFBs+YsbGfYuPpKZ2MzFBVwEOZVAEREP+SnWAnEw7v3S4tkapWj2FTExzY2cA66CuAWuUbdrOr4MywMhUqrEfKMbEVitoMAtAF/QAHcUqOQl8ib2U7Bmk8y4zU8/OUKHIWuArjF1djsZGVhWlMt20FYEFtb8ZRnkL2w7b03ANZhDRDg/7lSW+ZpainkdaM/4/Kkjb2t7Pk8HttBAO6oG/2HBLgX/W2cpSpVvrSR7SCd5GhFgVStRFEBx6GrAG7nY2b5V3leoayJ7SB6Z8TjEdFgvFktcB7WAAHallBf6WdmndZU62VqyXaWjnehutTcSDDS0Z3tIAD3BPMqgLaFWzmYCYTHygs+y0rgdZUlsnqlwojHu1xTVq+Uj0BRgeHAvArgLlIbawMtbKoU8o8y4rxMLR9385OpVcn11SI+P9zaoVmlTGyoNjcS9LKy1/flJpUy6T4vpzTWmBgJelnaFcmbdxZlOIpMX+oZ1qJR480pwLBgXgVwF4EWNkRkLzKe5xXc09zKTmQs4htlN9dnSesthSI+n5dYX/nfL5eXlf5y5vidjsmVNVgKRTwe3e/lXGlDjUJuKzJ2MTZ93jfspZ5hRISiAoODeRUAJ2RnZy9btmznzp1sBwHgIsyrAACA69BVAADAdegqAADgOnQVAABwHboKAAC4Dl0FAABch64CAACuQ1cBAADXoasAAIDr0FUAAMB16CoAAOA6dBUAAHAdugoAALgOXQUAAFyHrgIAAK5DVwEAANehqwAAgOvQVQAAwHXoKgAA4Dp0FQAAcB26CgAAuA5dBQAAXIeuAgAArkNXAXACj8ezs7NjOwUAR6GrADhBq9VWV1eznQKAo9BVAADAdegqAADgOnQVAABwHboKAAC4Dl0FAABch64CAACuQ1cBAADXoasAAIDr0FUAAMB16CoAAOA6dBUAAHAdugoAALgOXQUAAFyHrgIAAK5DVwEAANfxtFot2xkAuq/Zs2c3Njby+fyWlpbq6mpnZ2c+ny+Xy48cOcJ2NAAOwbwKgE3Tp0+vqqoqKiqqrKzUaDQlJSVFRUUWFhZs5wLgFnQVAJsmTZrk6enZ+hoejzdo0CD2EgFwEboKgGXTpk0Ti8W6Lz08PKZNm8ZqIgDOQVcBsOyxxx5zc3NjLvN4vOHDh7u6urIdCoBb0FUA7Js1axYztfLw8JgyZQrbcQA4B10FwL5Jkya5ubkxkyoXFxe24wBwjoDtAAD3rUWryW9uKG2RqjRqtrN0mIh5s+pOnHAeM+RkZSHbWTqMiZHAy9TS1diM7SBg8PD6KjAwh8vyj1bkS1UqH3PLRqWC7TjQHjOBMLWh1tXEbLFvmJuJOdtxwIChq8CQ7CvJvlBdOq1HT7aDwH2oUyl2FWa+F9TP0wSvG4MHhP0qMBhnKovPVpWgqAyOtUD0rHfws/Gn2A4CBgxdBQZjX2n2BGfPezgQuGiis9fP+alspwBDha4CwyBVq3KbGywEIraDwAOyERmnNNawnQIMFboKDEOFQuZuit0OA2YjNJZ3ofM2oZOhq8BAaKlZpWQ7BDw4jVbTgPM24UGhqwAAgOvQVQAAwHXoKgAA4Dp0FQAAcB26CgAAuA5dBQAAXIeuAgAArkNXAQAA16GrAACA69BVAADAdegqAADgOnQVQBe05LGRMdGS2LPH2A4C0DHQVQD/yavTxny29Hm2U+hFF35qYHDQVQAPLjslubyogO0UetGFnxoYIgHbAQD06NT+nWcP7C3OyxKbmPqF9J40d4F3YAgRbXh7ydVTR2a88HptRfmZg7tf/3STJDwyN+3G3s1f5qbfNOIL+gwZPmX+YgtrG2acY7u3nv9rf3FulpWtfWj/gVOfecnSxnbbhrV/bf+ZiOLOnYyJlry87uu+Q0a2M0g72hxfl3PW4qXWtvbHf99elJXRMyTsmXfW2Do4E5G0uWnfD18lXDpbWVrSw9MnatRDE2Ke5vP/3x+gR3dt2frZBw6ubp/tPcFcc/bQ3s1r3raxd9pw4IxKqTi6+9crJw4X52XbODiF9hs4cNwjfiHhbT61yycOn9q/Ky89VSgSBvWN6j1w2IAxE/XzcwO4HeZV0GVlJMX/uPa97JSkiKGjbR2cYs8e/+CFJ2sqy4hIIBQS0en9u07u2+7pH2RqZlGan/v+oicSLp3tM3hEz9Dwk7/veG/+dGlzExGdObB7y6drqspKHprxpImZ+al9O39Y+y4RhUQODOrbn4hcPL0nz1vk4uHdziDtuNP4RCQUCYno2pnj+376xt3XT61RJV+9uO2LtcytW9av+mv7z8amphNjnq4oKdr57fpju7feNvig8ZOEQlFlSVFxXjZzTfKVC0Q05OHHeDzer198uOOrj1tkspGPzvALCTux97f1ry2UNjf9+6ldPnH4q+WvFGWlR40aN3DsI7FnT3zz3msZSfF6+LkBtAHzKuiy9mz+kogmxMyf+fxrRLR83pTc1Jsn9+2c9uxLzOSjurzsw60HXDy9iWj715+0yGR9h4x6eukqIvptw7rD2386/+fvY6fPyU5NlvSOHDLh0SETHguOHLDm+TnXL55WqVRh0YNzUpNT4q64evpMfWZx+4O0k/NO4wsEAh6PT0TlBbmf7Dluambu5R/047r3ki5fJKIWmbSmolzSO/KJl5d5+gXy+fzfv/8q/vypcY/PbT24mYVVxPAxfx87lHDhdA8vX61Wy9x9yITHiCgj6ToRzX97jX9obyLq1X+wWq3RtPXUTuz9jYjGznhy0twFRNRn8PCS/BxLa1v9/xgBCF0FXZZKpUq7fpWIQvsNYK5Z/ePe244JiYxmioqIMhJiicgnKIT50ieoFxGlXY8dO30OUzwMG0dnItKo1dLGBmaZrrV2Bmkn6l3HDx80wtTMnIi8JcFEJGtuVKlUYhPTZV/98s8d7R2JqKGm+t/jD39k2t/HDsVfPDMhZn52SpK0qcE/rK9TD3cicnbzLMxK3/T+W30Hj7R37jFw/CPMA/2bs4cnER3Z8VN1abGdi+uAMQ9LwiPbeVIAHQtdBV2TrKlRo1YTkYmZxZ2OsXd21V1uqKshot3ffb77u891V1aUFBHRpWOHdn33WVVp8V0ftJ1B2nHX8Y1NTJkLImMT5oJGrdIaGe3Z+PnR3VvlUmn74wf17W/n7JqeECtrbky+fIGIBo1/hLlp5ouvNzXUpcZf/XPbD0S069tPQ/sPfH71p0ZGRrcNMuyRaQVZGWcP7jm1fxcR7dn4hX9Y32ff+ZDpPAB9Q1dB1yQ2ufVrXdrUcKdj+Px/fiObWlgS0YhHH48aMV53pcjEpLq85NsVr2u12pGPzeg/fFx1RenG1W/dacA7DdJOzvsav7W/jx3a/8tGodg4ZslbHr4B8RdPH9nxy50OHv3YzB3frI89eyrp6kWBSBw96tY5EY6u7m9/vSU37UZBVnr8+dNx505cPX10wIVTEUNH3zaCSGw8/63Vj8x9Nj8j9ca1S+f+/CM9IfbY7q1PLFl2L2kB/iOcWwFdk0hszKzvMTMJIvps6Qsx0ZKd365v8/ieQWFEVFdVGRQRFRQRJRCJSgvyBEJhaUGeVqslotkvLg2KiFLIW5jjtRoNEfF4PCJSyOXtD9JOzvbHb0dJXjYRufv4jXt8blBEVF1VBRFp7nCvIROnGAkEl44dyEyKjxw22sTMjIgUctmRnb/8uPZdb0nI0IlTXl731cjHZugmgrc9tcsn/tq46k2BUBQ5bMxTr69Y8M4HRFR5tykjQEfBvAq6rCnzF3+1/OXD23+qr62qKCrMvJFgY+80fsZTbR48ZlrMyT92xJ8/9f6iJ/xCw0/9sau5oX7J2g0+QWF8Pl+j0Wxas8yph0fchdM9fPyKczL3bN7w8JxnrO0diCjt+rVd334a2n/QnQbx8g+6U0hXL992xm/n2bn5+hNRbtqNbRvWKhUtsuYmHo9XXlywZ/MG5myI1ixtbPsMGnHtzDEiGjRuEnOlUGx87s99BZlptdWVnv6BTfW1fx87TESBffoR0W1PLT0h9vxff+RnpfUZOEytUV85cYSIAnv3e6CfDMB9M1qxYgXbGQDurk6pOFNVHGHjeO93cfPx85IEZ99MSrt+rbayvFfUoEUrP3Fw6UFEsWdPFGSl+waHhUUPYQ62sLbxCexVWVqckRiXkRjv4RsQs+StfsPHmpiZ8wVGzfX1OSnJ5tbWTy9d5ebjl3kjITftZtSo8YHh/eIvnq6rqkhPjAuLHhzYp1+bg7QTsv3xs28mts7ZWFfLnI836cmFbr7+ZYX5cmlzcXaWm6/fU6+vFIrEOSnJpQW542fMPbJzi7SpMWrUeFcvX+aBlAp53LmTNvZOT77+LjNn4vF4of0H1lSUx58/lXb9WkFWelj0kHlvruwZHE5Ezu5erZ/a6CmzWuSylNi/b1z7OyMxvoeP39Rnl4x49HFmqHshVatSG2snu/rc+08QQIfHrD8AcFyetHFF6pWF3iFsBzFIWq32i7cWx549PmPRqxOfaG+6pj+VLbK9Jdk/9x3FyqODocMaIIDeHdvza9aNxDZvCosaPHDcI3p99E/fWFRVVlKQmebg6jZmWoxeHwtAT9BVAHo3ZmrMmKmslURGUrxc2hwSOSBmyVu6s94BDAu6CqCL++7IZbYjAPxXOGcdAAC4Dl0FAABch64CAACuQ1cBAADXoasAAIDr0FUAAMB16CoAAOA6dBUAAHAdugoAALgOXQUAAFyHrgLDIOLzbYVitlPAg9MQuZmYsZ0CDBW6CgyDq7FZVnN9i0bNdhB4QCWyJisB/tqAB4SuAoMxytE9R1rPdgp4QKUt0sEOPdhOAYYKXQUGY7Fv2NnKkmJ5M9tB4L4drShwNzGPsnFiOwgYKnwuMBgShUbzfMLpMGsHEY/vKDbVaDVsJ4L2aIhXKm+qVbY4iU0X4DOd4T9AV4HhOVCam9JQ06JVl8mlbGd5cCqVsrq6xsmp7alGS4u8rKzc09Oz03N1JDcTMyuhuL+Nc6SNI9tZwLChqwDYcfjw4StXrqxcubLNW7du3bphw4YRI0asW7eu06MBcA72qwDYkZKSEhgYeKdbr1y5olarz50799NPP3VuLgAuQlcBsCM1NfVOXdXU1FRaWsrn85VK5fbt269cudLp6QC4BV0FwI52uioxMbG2tpa5XFNTs2bNmqamps5NB8At6CoAFmRnZ3t4eIhEojZvvXbtWl1dne7LoqKiJUuWdGI6AM5BVwGwIDU1VSKR3OnW5ORkHo+n+5LP5ycmJr777rudlQ6AcwRsBwDojlJTU4OCgu50a01NDY936xxdIyMje3t7CwuLVatWdW5GAA5BVwGwIDU1dezYsXe6taamxsnJ6c8//2xoaDAyMjIzw1u+QneH11cBsCAqKurChQsCwV3+WDxy5MiFCxfef//9zsoFwFHYrwLobFlZWV5eXnctKiLq1atXQ0NDp4QC4DR0FUBnS0lJaWezqjVXV9cNGzboPxEA16GrADpbWlpaOycB3iY/P7+5GW8tD90dugqgs937vIqI/vjjj99//13PiQC4Dl0F0NnS0tLaeSfA2wwYMKClpUXPiQC4DuesA3SqjIwMHx8fIyOjezw+MjIyMjJSz6EAuA7zKoBOdV+TKsbVq1fVarXeEgEYAHQVQKdq/6NA2vTLL7/ExcXpLRGAAUBXAXSqB5hXTZw4USaT6S0RgAHA+1YAdKp+/fpdvnyZz8efiQD3Af9hADpPenq6n5/f/RaVVCo9e/as3kIBGAB0FUDnaefzFdthamq6YsWKxsZG/YQCMADoKoDO82BdRUTPPvtsTU2NHhIBGAZ0FUDneeCumjlzpqenpx4SARgGdBVA53ngriooKDh37pweEgEYBnQVQCcpKioaNGhQ6w+nv3dKpfLPP//UQygAw4CuAugkbm5uly5derB3oNBqtSNGjNBDKADDgPcDBOg87u7uRUVFD7Dz1LNnz549e+onFIABwLwKoPO4ubkVFhY+wB3Pnj2bnp6uh0QAhgFdBdB53N3dH6yrtm7dKpfL9ZAIwDCgqwA6zwN31YgRI/z9/fWQCMAwoKsAOs8Dd9WsWbNMTEz0kAjAMKCrADrPg3VVZWXlzp079ZMIwDCgqwA6j5ubW3Fx8f3eKzEx8fr16/pJBGAY0FUAneoBTgV0cXGZOXOm3hIBGAC8vgqgUzHLgO7u7vd+l+DgYH0mAjAAmFcBdKoH2LL67bffioqK9JYIwACgqwA61QN01ZYtW3ASIHRz6CqATnW/XaVWq5977jk7Ozt9hgLgOnQVQKdyc3O7rwU9IyOjSZMm6TMRgAFAVwF0qvudV6WkpOzevVufiQAMALoKoFPx+XwXF5d7f5XV1atXy8vL9RwKgOt4Wq2W7QwA3cugQYPMzMwUCkV9fb2bm9uBAwfaOTg5OdnGxsbNza0TAwJwDl5fBdBJevfuzfsf3ZumDxkypP17hYaGdko6AE7DGiBAJ5k7d65AIGj9Gfa2trZDhw5t/15r165taGjQfzoATkNXAXSSJUuWBAQE6FbdNRqNnZ1dZGRkO3dRKBQHDhywtLTsrIwAHIWuAug8y5cvd3FxYS7zeLz+/fu3f7xSqfziiy86JRoAp6GrADqPv7//lClTxGIxEdnY2Nx1s8rMzKz9iRdAN4GuAuhUTz31VK9evdRqtZ2dXURERPsHnzlzpv2zBAG6CZwHCAZAQ9rKFlmNooV3Dwdz34xXX8pe0xQQFZXWWNv+kUdvJHh4eNz1MIPB47mbmJkZCdnOAYYHr68CrttRlHGoNE+l1ViLxJpu9s9V3tIiFAiMjIzYDtIx7MUmN+urJRa2czwkwZa2bMcBQ4KuAk77OiepqkU+zKGHmN9Ffl9Do0qxoyjjJd/evazwhrxwr9BVwF0bc29UKeQjHfCWDV3QptwbSwMiAsyt2Q4ChgHnVgBH5csac6QNKKquaopbz18L0thOAQYDXQUcldfcoNFq2E4B+mInNL5WW44fMNwjdBVwVGWLzElsxnYK0KNgK7siWSPbKcAwoKuAoxQadYtGxXYK0KNKuYxHXeNlCKB36CoAAOA6dBUAAHAdugoAALgOXQUAAFyHrgIAAK5DVwEAANehqwAAgOvQVQAAwHXoKgAA4Dp0FQAAcB26CgAAuA5dBV3HS4+OiImWXDtzjO0g9+3U/p1vzHgoJlqy89tPO2RARYs8JloSEy0pzssmou9WLY2Jlmz5dE2HDE5EFSWFzPhNDfUdNSZAOwRsBwDo7krycn5c+x4RDX14qm9QKNtxALgIXQXAssb6GiISGRs/s+x9trMAcBS6Crqg33/46u/jfzY3NvQZNOKpN1YYGRmlxF7+4MUnTcwsNp+4xhzz3tPTs1OSnnztvVFTZh7bs23L+tV9Bo8Y9/jcP376pig3K2LoqNkvLr1+6dz3H7xNRH2HjHjm7Q8FAgERlRXk7fn+y8yk+KaGOq+A4PEz5kQMHUNERTmZb85+2MTU/JNdR3Zv+iLu/ElTM/Ox058YM+2JdqLu/2Xj7u8+IyKFXB4TLRkyccqzb69pqq/btmFdemJsY11dSGT0I3Of9ZaEMMfnpt3Yu/nL3PSbRnxBnyHDp8xfbGFtw9x06ejB33/8urww39rO8cnX3/33Y/F4vEtHDx7/fXtBZqpfSO+F766ztncgImlz074fvkq4dLaytKSHp0/UqIcmxDzN5/OJSKPRbPti7Y2rFytKi23sHUL6DXhs3gvMvVq7dubYF28ttrKzX/XjbjtHlw76MQL8A/tV0NUc37v9r99+ljdLG2qqzxzYfWrfzrveRSgUElF5Yf7PH6+0c3RpqKk+tW/nV+++uuvb9RFDR8qlzRePHDz3514iUioVH73yzOXjf/oG9Ro6cWp6Quznby7OTb9JRAKRmIha5NLP33xBrVQ6OPcoLyrY8umawuyMdh46IKzP4IceJSIjgWDyvEV9Bg1TqVSrF84+f3ifs7tn9JiHkq9eXLlgdk7KDSIqzc99f9ETCZfO9hk8omdo+Mnfd7w3f7q0uYmICrPSv135RllBXs/gsKCIft+tevPfj5WWELt70xeefgF8vuDGtUub1rzFXL9l/aq/tv9sbGo6MebpipKind+uP7Z7K3PToa3fH921pba6atD4R5QtilP7dn629HmN5v99nG9xXvY3K94Qio3f+Gwzigr0BPMq6GpkTQ1fHDgjEok/ePHJjMS4G9cujp46q/278Ph85nfuuu1/9vDy5QsEZw/uSbx09rN9J+0cXVRK1eUTh1Pjr42Y9Hheeoqto7NTD49FKz8RCIVFORkpcVeuXzjtHRDM5/OYiUifwSMfnvOMUql4/fHxVaXFN65ecvf1v9NDS8Ijiej84X0CoXDqM4uJKOHS2eK8bAsb25c/+kYoFIVHD/30jUUHtmxcsvbLM4f2tshkfYeMenrpKiL6bcO6w9t/Ov/n72Onzzl9YI9Wq/X0D1q+8Tc+n7//l+92f/f5bY9VVVa8ftdRcyvrsOih619bmHT5Qk1lmZm5ZU1FuaR35BMvL/P0C+Tz+b9//1X8+VPjHp8rl0oPbtlERItWfBQ+YGio7PJ0AAAgAElEQVTDszWLJw/PTklKjbvi2MP91ne7uWn9qwuVLfKX133t6RfYET9AgDZgXgVdzfDJ003NzAVCYeSw0UTUUFtzj3d09vDq4eVLRO6+fkTk6unDzBLce/oTUVNdLRH5hYS/883WpV/8IBAKicjGwZGIGmqrW48zYOwEIhIKRR49A4iooe5eAzDSE2OJyMsvUCgUEVHPkHAiSr1+lYgyEmKJyCfo1nqgT1AvIkq7HktEuanJRBQWNYhZuxv+yPR/j9xv+FhzK2siCu03kLkmJyVZbGK67Ktf3vlmK9M0NvaORNRQU01EOSlJMmkTEYVEDiAiSxvbn88m/vp3WnBktJa0zAjfrHi9oqRwwNiH+w4ZeV9PE+C+YF4FXY2F1a39G6FQTEQatfoe7ygSGzMXBAIREYlNTG9dLzImIrVGwzTflk/fv3LyL61We6dxxCZm/xtQzMy07it/Q20tESVfvRgTLdFd2dxQL2tuZGpv93eft54zVZQUMfMbIjIxs2CuNLWw/PfIJmbmt56gUGhiai6TNkmbm7Va7Z6Nnx/dvVUuld52fHVFORGJTUyYYm5TZlI8EcWePV5fU21la3dfzxTg3qGroHvg8ZjdJt0VzC/3+7Vn4+eXTxy2c3KZ+eJSSyvrP375LiX2cocGJTMLS2YCN23BktbXC0RipoFGPPp41IjxuutFJia6HmpuamCubK6v+/fIuqesVLQwEyZzS6u/jx3a/8tGodg4ZslbHr4B8RdPH9nxy/+SWBBRi0ymUirvVFdPvLws+eqlhItntn+1buG7H3XctwHg/8EaIHQLJmZmRKRStDBnOpTk5ZTk5zzAOMxLa6NGPRQ1cpx/eERpXu4DzJza5xvci4iqykq9A0ODIqKcPb0KstPVGrVQKOoZFEZEdVWVQRFRQRFRApGotCCPaRHmRMHkKxeZCd+l44f+PXLSlQvNjfVElPj32VuPFRhakpdNRO4+fuMenxsUEVVXVaF7Rl6SoFt3vHyeabgXHh4SEy1J/Ps8j3jMTYPGT4pZ/Cafz7/w14Gsm4kd+H0AaA3zKugW3HtK7JxcqstLP3lt4aCxD188dsjN17+o3TP02uTm65eeGHfhyH6hWJyXnuLhH1BbVZ4Sd+XMwT1Bfft3SNS+g0c6uXmUFxUsnzclctjo6+dPF+VmjZ0+J7TfwDHTYk7+sSP+/Kn3Fz3hFxp+6o9dzQ31S9Zu8PIPGjxh8rE9v+ZnpKx+LsbBxfXm7bM9LRHJpc3Ln5wa0n/ApaOHmO0rKzt7N19/5lT4bRvWKhUtsuYmHo9XXlywZ/OGqc8sHj1l9vG9275duXTA2AkJl87VVVWE9hsYFj24oqTw1rhacvbwGvHo4yf2bv9x3XsfbPmjQ74JALfBvAq6BYFA8NyKj5zcPOqrK3PTb76w+lNXTx8iUqkU93Dvf0ycPT84YoBSoUy4eDY4ImrJh19GDB1dUVyYdv1ah0UVCl9fvyli6Oiq0uKDWzbLZNIZL7w+44XXiMjF0/vVj7/z69Un7fq1g1s2O7t5vrjmc+bVXd6SkHlvrhSbmGQkxqUnxr/4/mciY2MiUiuVRKRSKolo5OTHA8L6ntq3Uy5tDu03cN6bq4gocvjYqNET7Jxdr548aiQQvvj+F4/Nf1EkMj5/+A8imvva8mkLl/B4vJO/72isqxkyccriDzb8O/PUZ14yMTUvyEw7dxhdBXrBa2eLGIBFvxWm50obRjq4sx0E9OWbnOQ1wdHuJuZsBwEDgDVAAL07tufXrBtt7+WERQ0eOO6RTk8EYGDQVQB6N2ZqzJipMWynADBg2K8CAACuQ1cBAADXoasAAIDr0FUAAMB16CoAAOA6dBUAAHAdugoAALgOXQUAAFyHrgIAAK7D+1ZAF5Fy8KixVRsfMAidycnBwSywJ9spoAtCV0EXYWNt3SeqH9spujUekZlQlElKtoNAF4Sugi7Ce0C/BoER2ym6u2atmu0I0DWhq6CLkBvxSNuRn88LANyBcysAAIDr0FUAAMB16CoAAOA6dBUAAHAdugoAALgOXQUAAFyHrgIAAK5DVwEAANehqwAAgOvQVQAAwHXoKgAA4Dp0FQAAcB26CgAAuA5dBQAAXIeugu5uz+YNMdGSyyePtHNMQ13Nns0b5FJpJ+aiyyePxJ8/9QB3/PiVZ2KiJTWV5ff6QCcOPzumf0y05NC2Hx7g4QA6AboKuruoUQ8t+/Ln8Ogh7Ryz57vPj+7YYmxq2mmpqsqKN658o6Kk8AHum5GU4OTuaevgdC8HK1rk361aamJquuzLnwePn/wADwfQCXharZbtDABt+K0wPVfaMNLBXa+PolIqnx7R28XTZ+2vB3Z+u/7gls2zFi89umNLXU1lr6jBs196y8bO/ukRfXTHf38yPutmwv6fv8vPSBMZG/cZOGzW4jeNTU13fLP+0NbN0xa+/OevP0h6R4ycPOPjV58dPWV2StzlmsqKz34/sXBs/+CIAW99+SMR7fz204NbNi1Z+2Wv/oPmDe/dMzjMP6zvkR0/2zq5jJoya+Lsp7dtWPvX9p+ZRxw3Y27MS28xl//89fvtX39y21N459utkvBI3ZdVZcVLHh0ZMXS0Vqu5GXvZ2d3r4TnP9B8xjoiO7d56/vAfRbnZDs6uwydNHz/zyaQrFz5aMp+5o8jY+PuT8Ud3brl0/GBRTraltU2v6MFT5y+2srNXyGXzhvcOioiysLK+cvLI6p/2NDc2/PubcL/f/G9yktcER7ubmD/Qjw66F3wuMHRrhVnpapXKyz+QiErycogo4eKZZ95Zc+bAnssnDrv5+E1+cuHUBUv2bPx80PhJwx6ZWl6c//Erz1pY2z77zvtlBfk7vllvaWc/9ZnFpfm5RHT11NGYl5dZ29mX5GUTUezZExNi5lla22bfTCIiT38J86D5GSlE5C0JLivMJ6Ki3MwhEx97bsXHP65bseOrjweMmTB04pT4C6fLC/Pf3PCji4eXLq1faO/J8xbd9hTsnFxbf5l1M5mIctNvPrHkrZDIgb+sX/XrZx/2HzHu5O87tny6JiA84uV1X106enDbhrWefhI3H78Rk6af2r/r4TnP9h0y8sTebds2rB00ftLMRa9n3kzc/d1nlcVFb274sbQwj4jy0lIGjJ0wb+lKLVGb34RO+YlBN4Wugm4tN+MmEXkFBBFRXkaqiZnFy+u+MjGzaGpouHzisEhsLDYx1Wo0RBQWPUQSHrn18w/UKtXEmKcjho4hoiO7tqTGXaFnqDArncfjLVr5cQ8vXyK6fPwwEU2et2jko48T0f5fNhKRp9+trspNTzExs7Bzck2Nv0ZED895dsTkx4no9IHdKbGX66urPP2DqkqLXT19QiIHtE7rH9bXP6xv+88oL/0mEc1/a3Vov4FEtGfzFw111UR0av8uIprzytuefoH2Tq4X/tqfEnc5KCKqpUVGRP1GjHV0dVvz/FwTM4t5b6wQGZsERUTt++Gr7JRkIirITCOiXlGDnnztPSK60zcBQH/QVdCt5aWnMF3V3FhfXVYSHDHAxMxCN/Vh2oX57e/lH0REmUnXiejXzz/89fMPmRFcvXyb6usqSgp9AkOZoiKivIwUPp8/YMxE5sv8jFQi8pIEE1F1RWljbU1o/4G6DgiOiGYOa6yrJSIbe6f8zFS1SuXhH/gAzygn9Qafz5eERxCRUtEibWywsXdqkUmZZ/T2nEd1RxoJhESUn55qJBB4+gUmXT6vbJH3HTxcZGxCRAq5TKVU2jm7ElFuWgoRDRz3CHPHNr8J/+GHAHB36Cro1goy0ojIOyA4LSHu38t0XpIgZu1LbGLi4ulNRGq1moheW79RJBIzR1ra2uWm3ySinsFhzDUqpbIwK93N19/EzOzWo2SliU1MnN29iOjG1UvMIzKVRkTmVjZMURXnZDq5e1rbO1y/eJqIvP/VVfeyX5WXdtPYzEIoEhNRStxVrVYb0DtCo1ETkYun91Ovvac70tnTS6VUFudle0mC+Xy+SqkgIhNzC+bW9KR4Igru059ZUSQi/169mZva/CZ03M8EoA3oKui+NBpNbvpNF09vsYlpXquJFLNMZ2puaevgTER1NZXmVtYpsZf9w/p6+knyM1KKcrPcffz/+OkbpULx3qbf4s+fJiLvwOD/3femVqtl2oghlzYrFYprp4+qlMrfvvyIiDxvrTqmCYTCE3u39Rk4/NC27zUazdhpMURUX1tDRI0N9aX5uUxHMvqPGu8tCbntWbj7+usul+TlSJsahELRyX07HXu4/fzJSiIaPWWWiZmFvUuP2sqKpsZ6tUq145v1vfoPevrNVRnJ17Varbd/EBH5BPXiGxnFnz/Vf8Q4jUbzy/rVjq7uk59epNFo8jNSHF3dzSysmEdp85ugzx8UALoKurHi3Cy1SuXpH9TOMh0RhQ8YFnfuxKYP3v5s74lZi5eKjMV7N36hVCrsXXosWL5WKBIzPecdGMocn5t2k4hal8rjz712cMvGr5a/EhQR1Stq8N/HDrn5+FWWFksb6weMfbimvOyDF580t7KetnDJ6KkxRBTab+Dvmzcc2rrZ2d2zdVfZO/ewd+7RzjNKjb9KRFOfXXxy3/aCzDTHHu7Pr1rv36sPEb3+6aYdX3+yYdlLRBTYp9/kpxb+swQqCSYiWwent778ec+mL9Yunsc3Muo3fOycV9+xtLYtyslUyOW6JiaiNr8Jevj5APwD56wDR3XOOessunr66IZlLz3+3KsPz+mmpyXgnHW4d5hXQddRXph/4eiBf1+v0aj5fKPbrjQ1txg/48nOitaGgoxUIurh05PFDACGAl0FXYeTu+eU+S+yneJe5WWm3bbbBAB3gq4CYMdrn3zHdgQAg4H3AwQAAK5DVwEAANehqwAAgOvQVQAAwHXoKgAA4Dp0FQAAcB26CgAAuA5dBQAAXIeuAgAArkNXAQAA16GrAACA69BVAADAdegqAADgOnQVcJSFQGTMw+cAdGXOxqZGPB7bKcAwoKuAo5yNzfJljWynAH1RaDSpjTWuxmZsBwHDgK4Cjgq1steQlu0UoC9Fsqah9j3YTgEGA10FHGXM589w899SkMZ2EOh4MrVqe1HGa3592A4CBoOn1eJPV+CuhPqq9Znxwxzc7EUmlgIB/rEaNB7xKlpk9SrFkbK83/qNM+YbsZ0IDAa6CriuUNa0qygzpbGmQaVQd91/rlqNVqFUisUitoPokYephUaj7WvjONdDwnYWMDDoKgBOyM7OXrZs2c6dO9kOAsBF2K8C4AR7e/unnnqK7RQAHIV5FQAAcB3mVQCcUFVV9eOPP7KdAoCj0FUAnFBfX3/06FG2UwBwFLoKgBOwXwXQDuxXAQAA12FeBcAJ2K8CaAe6CoATsF8F0A50FQAnYL8KoB3YrwIAAK7DvAqAE7BfBdAOdBUAJ2C/CqAd6CoATsB+FUA7sF8FAABch3kVACdgvwqgHegqAE7AfhVAO9BVAJyA/SqAdmC/CgAAuA7zKgBOwH4VQDvQVQCcgP0qgHagqwA4wcHBYf78+WynAOAo7FcBAADXYV4FwAlVVVXff/892ykAOApdBcAJjY2N2dnZbKcA4Ch0FQAn2NnZjRgxgu0UAByF/SoAAOA6zKsAOAH7VQDtQFcBcEJ9ff3x48fZTgHAUegqAE7A66sA2oH9KgAA4DrMqwA4AftVAO1AVwFwAvarANqBrgLgBOxXAbQD+1UAAMB1mFcBcAL2qwDaga4C4ATsVwG0A10FwAnYrwJoB/arAACA6zCvAuAE7FcBtANdBcAJ2K8CaAfWAAHYtGDBgqamJh6Pp1Qqm5qarK2teTxeS0vL7t272Y4GwCECtgMAdGu9e/duvfRXXl5ORM7OzqyGAuAcrAECsGnq1KkeHh6tr9Fqtb169WIvEQAXoasA2GRvbz9mzBgej6e7xsXFZebMmayGAuAcdBUAy6ZPn+7u7q77MjQ0NDQ0lNVEAJyDrgJgma2t7ejRo5mplaOj4+zZs9lOBMA56CoA9k2fPp3ZtQoKCgoJCWE7DgDn4DxA4K5qRQufdw/HGT6+pfmgcWNqDhx4bE5MrbKF7Tidx0YoZjsCGAa8vgo4J7G+amdRZkJ9pYuxeYOqG/3i7m68TS1zmxsG2rk879tLyMMaD7QHXQXccqaqeEdhxnhnT3uRiRGve8yqurFmtbJcLt1amL4tcqwt5lhwZ+gq4JDTlUV7S7Jj3APYDgKd7b3UK38NnIS5FdwJ/m0Ah+xDUXVXs90Dvs5OYjsFcBe6Crgiq7m+SaVkOwWww0FscqW2jO0UwF3oKuCKUlmzr7k12ymAHTZCsa3IpEWjYTsIcBS6CrhCoVU3KhVspwDWZDfV4VwauBN0FQAAcB26CgAAuA5dBQAAXIeuAgAArkNXAQAA16GrAACA69BVAADAdegqAADgOnQVAABwHboKAAC4Dl0FAABch64CaI+iRR4TLYmJlhTnZXfOI363amlMtGTLp2s65+H+raKkkHnKTQ31bGUAuA26CqAzVJUVx0RL/tr+812PdPH0kfSOdHb3uJdhN615+5lRkR0R8B9CoVjSO1LSO9LISNCxIwM8MPxbBOgMV078dY9HTpq7YNLcBfdypEqpjD174r/laoONg+M732zt8GEB/gvMq8Cwndq/872np88f2ef5iYM+f/PF3NQbrW96e86j80f2WTC2/7vzpiVcOstcX5STGRMteWZkRH111fcfLn/uoQGvThtzbPc/v50vHT342uPjnhgQ+OLDQ5OvXGz9cO8veqL19Cjh0tmYaMmiCQOZLzUazdbPPlg6c8JTw8JfmTr6x4/eq6uqJKLlT03Z/vUnRLRtw9qYaImsubmdZ9R6DbCdqHHnTj45JFTaWC9rboyJlmzbsJaImurrNq5+65Wpo58ZFfnFW4tz027c9pQzEuOWPzVl9aIn5g4OiYmWXL94Rve4L08ZFRMtOfn7jn+vAf59/M9VC2fPH9n37TmP7v/lO7VaTUQLxvaPiZZUlhYTUWNdLXOX3Zu+YO7y1fJXYqIlSnzIC3QQdBUYsIyk+B/XvpedkhQxdLStg1Ps2eMfvPBkTWUZER3a9sOPa98rzE6PGDrav1efnNTkT15dwNSVQCQmoha59PM3X1ArlQ7OPcqLCrZ8uqYwO4OICrPSv135RllBXs/gsKCIft+tevPe8xza+v3RXVtqq6sGjX9E2aI4tW/nZ0uf12g0wx6Z7uDqRkSh/QdOnrdIIBLe44DtRHXx8B780KNEJBSKJs9bFBI5UKVSrV44+/zhfc7untFjHkq+enHlgtk5KTeYY4iopUX2zco3pE2N3gFB4QOGEVHi3+eYByrMzqgsKeLz+f1Hjrstw6Vjh75+99WCzNQxU2cLRaLd333+9buvElFg735ExNRhWsI1IuLz+VnJicy98jJSiAiriNBR0FVgwPZs/pKIJsTMX/juutU/7fUODJZJm07u2ymXSv/48Wsimrd05cJ317368bejp8wmor3ff0lEfD6PmQP1GTxywbtrl2/cZu/Sg4huXL1ERKcP7NFqtZ7+Qcs3/vbcex9PfOLpewwjl0oPbtlERItWfPT00lVrtuwTiMTZKUmpcVdGPvq4g6s7EfXqP3jqM4uZ5rgX7UR19fIZ+vBjTJ9NfWZxWPTgG1cvFudlW9jYvvzRN/PeWLloxccqRcuBLRuJiGfEJyKNWh3Yu9/63cdiXnqr/4ixRBR39iTzQPEXTjNVam51+0czH925hYimPLN4+nOvrPh+p7ck+OqpI4VZ6QFhfYkoJ5Xpqlgejzdg7MT0pDiNRtPcWF9WkOfg6sbn4zcMdAz8SwJDpVIq065fJaLQfgOYa1b/uPfXv9OmPftSWsI1uVRKRFGjJjA3RQ4fQ0S5qTdan9s2YOwEZs7h0TOAiBrqaogoNzWZiMKiBjG/Z4c/Mv0e8+SkJMmkTUQUEjmAiCxtbH8+m/jr32nBkdH//cm2GfU26YmxROTlF8h0Yc+QcCJKvX619TFMZxNRxNBRQrFxbVU5c35jwsUzRNR/5PjbxlS0yHNSk4nIN6gXc41vcC9mWEl4hK6rMhLjPP2DQiIHqhQteWkpzJWSsIj//sQBGJihg6Fqqq/TqNVEZGJmcdtNDXW1RCQ2MTE2NWWusbS1u3WvhnpmskJEYhMz5oJILGamL0Qka25qPaapheU95qmuKGceVCC81yW+e9dm1Ns01NYSUfLVizHREt2VzQ31suZG3Zf2rq63xjE2CY8ecu3MsYRLZy2sbDKTrxsJBJHDxt42ZmN9rVarJaJVC2a1vr68uHDUlNnGpqa5qTdkzc156Sljpj0h6R1BRBnJcS1yOREF9EZXQYdBV4GhMjG79etb2tRw201W1rZEpJDLFS1ykdiYiJrqa5mbLG1sdZfvMKw5ETX/b8zm+rrWt/J4PCJSKm6dMsAUG8PMwoKIWmQylVKpj7q6KzMLSyLyCwmftmBJ6+uZTS8Gn2+ku9x/5PhrZ44l/X2BuWPvgcN139J/xjS3Yi489cZKF3dP3fXW9o58Pl8SHplw6ezZQ3u0Wm1AeF975x7W9o6ZydfVKjURBfXtr7fnCt0O1gDBUIlNTF08vYko+fIF5prPlr4QEy3Z+e36gPAIEzMLrVZ79dRR5qa/j/9JRIF9+pmambc/rLckhIiSr1xk5hOXjh9qfSvTZHnpN5kv486d1N3kJQliLiRdPk9ESkXLCw8PiYmWJP59noh4xCMihULegd+BW8WpVDBRmdW5qrJS78DQoIgoZ0+vgux0tUZ9p+2xPoOGCcXGqfGXr50+SkRRox769zHGpqZuvv5EpJDLgiKigiKiGupqKsuKxaYmRMQsAx7dtZWIJL0jme9wemJ8bvpNO0cXR1f3Dnyy0M1hXgUGbMr8xV8tf/nw9p/qa6sqigozbyTY2DuNn/GUsanpY/Of3/bF2u8/fCcl/mpdVXnS5Qt8Pv/xRa/edczBEyYf2/NrfkbK6udiHFxcb8Zebn1r+MCh8edPXT19dOPqt2TNTfXVlbqbbB2cR0+ZfXzvtm9XLh0wdkLCpXN1VRWh/QaGRQ8mIhsHByI6e3Bvi1Q6auosWwfn//70bRyciEilaNn8wTuS8L4Dxjzs5OZRXlSwfN6UyGGjr58/XZSbNXb6nNB+A9u8u8jYpM+gYVdOHkn8+7xAJO4zeHibhz0088lN7y/77ct1uek3jU1NT/+xSyASf7r7GBH5h/UlosqSoh5evpbWtkTkF9r772OHiGjguIf/+xME0MG8CgxY1Kjxr3z8raOr+8UjB7NuJvaKGvTmlz9a2tgS0fgZTy5Y/qGltd25Q3uTLl/wlgS/u2l7z+Cwu47pLQmZ9+ZKsYlJRmJcemL8i+9/JjI2JiK1UklEwx6eNnzSdL6RUXpCrJuv35NvvEdEKqWKue/c15ZPW7iEx+Od/H1HY13NkIlTFn+wgblp7ONzTM0tK4oLD237QdvWbtMDcHR1Z05bP3dob9bNRIFQ+Pr6TRFDR1eVFh/cslkmk8544fUZL7zWzgiRw2+doR45bDSzWPpvQyY8NvulNx3dPC4dPXjmwJ4+g0e+u3GbjYMjEfkE9WIWGP3D+zIHB4T1uXUhHJtV0JF4zOoBAOtOVhYeLy+c7OrDdhBgx+q0a/uiJohwmju0BWuAAJ3t509WSpua2rxp/MwnvQOCOz0RANehqwA625Ovvcd2BAADg+k2AABwHboKAAC4Dl0FAABch64CAACuQ1cBAADXoasAAIDr0FUAAMB16CoAAOA6dBUAAHAdugoAALgOXQUAAFyH9wMElrW0tCQmJiYkJFysKfUYN4LtOMAaP3NrtiMAd6GrgAXl5eWJ/5OXlxcWFhYWFvbYsFH71TK2owE7qhXyGoUcHwgCd4Kugk6Snp6u6yciYvrpkUceCQgI0B1zKumclojHak5gRaVCZlVR99JLL0VERERERAQGBrKdCLgFn7UI+iKXyxNb8fLyCvsfJyenNu9yqaZ0W0H6Ex6STg8LbFKT9v3Ua4eiJly+fDk2NjY2NjY/P58prb59+7b+awa6LXQVdKSysjJdOeXn5zPNFB4eHhYWJhaL72WEuPrKjTnJY5087UXGYr6R/iMDm+pVimqFfEt+6r7oCSb8f5Z5pFJpbGxsXFxcbGxscXFxxP/07NmT1bzAGnQV/FdpaWm6fuLxeLrJ0wP/OZzeVLuzKDO+tsJaZFynVHR0Xq7SalUqlUAoZDtH5+lpblkiax5s5/qcT2g7hzU1NcX+T2VlJTPZ6tu3r6+vbyeGBZahq+C+yWSy1ot73t7ed13cezCNKmX32bvKzc1dtWrVTz/9xHaQzsQzF9zflnl9fT0z2YqLi6upqdGtE3p5eektJHACugruSWlpqa6cCgsLw1q5x8U9aF92dvayZct27tzJdhCDUVtby0y24uLimpqa+vbty/SWh4cH29Gg4+E8QLij1NRUXT8ZGRkxzfToo4/6+/uzHQ2AbGxsRo8ePXr0aCKqrq5memvr1q1yuZwprYiIiB49erAdEzoG5lXwD6lU2npxz9fXVzd5cnR0ZDtdF4d5VUepqKhgJltxcXEqlUrXWy4uLmxHgweHruruSkpKdOVUXFzcenFPJBKxna4bQVfpQ1lZme58Qj6frzsvo2M3VqEToKu6o5SUFF0/CYVCXTn5+fmxHa37QlfpW3FxMTPZio2NFYlEuvmWvb0929Hg7tBV3UJzc3PrxT0/Pz9dPzk4OLCdDghd1ckKCwt18y1zc3NmshUREWFra8t2NGgbuqrLKi4u1pVTaWlp68U9YXd6EY+hQFexJT8/nymt2NhYa2trprQiIiKsrKzYjgb/QFd1KTdv3tT1k1gs1pUTXu3PfegqLsjJydG9fsve3l7XWxYWFmxH6+7QVYZNt7iXkJCQmJgYEBCg6yeswhsWdBXXZGVl6eZbLi4uutcdm2dPx6IAACAASURBVJmZsR2tO0JXGZ7Wi3tlZWWtF/cE9/kuAMAd6Couy8jI0O1vubu7696f0NjYmO1o3QW6yjC0XtwzNjbWlRPeEq3LQFcZirS0NN37E/r6+urOg8cbuOgVuoqjmpqaWp+5J5FIdP1kZ2fHdjroeOgqQ3Tz5k3d/pa/v79unRArHB0OXcUhRUVFunIqLy/XlVN4eLiRET4do4tDVxm65ORk3fsThoSE6N6fkI8PO+4I6CqW3bhxQ9dPJiYmWNzrttBVXUliYqKut8LDw3XrhGznMmDoqs7W2NjYenEvMDAQi3uArurC4uPjmdKKj4/XTbZ69+7Ndi4Dg0XVzlBYWKgrp8rKSqaZFi5cGBYWhsU9gK6tT58+ffr0YS4zk61vvvkmKSlJN9kKCwtjO6MBQFfpS3Jysq6fzMzMmH6aOXOmj48P29EAgB3MyRdEpFarmZMyNmzYkJKSojspIyQkhO2MHIU1wA7T0NDQenEvODhYt7iHNxmDu8IaYLelVCp1m1tZWVm6NycMCgpiOxqHoKv+k4KCAl05VVdXt35ZLk7+gfuCrgIiamlp0b3oOC8vT/dm8AEBAWxHYxnWAO9bUlKSrp8sLCyYZpo9e7a3tzfb0QDAsInF4oEDBw4cOJD57FOmt1avXl1UVKTrre752T2YV91dfX1968W90NBQ3eTJxsaG7XTQRWBeBe1oamrSzbcqKip052V0nxe3YF7Vtvz8fF051dbWMs30wgsvhIWF8Xg8ttMBQPdibm4+bNiwYcOGMX89Mx8auXfv3pqaGt2bwXt5ebEdU4/QVf9oPXmysrJi+umJJ57o2v8CAMCwWFlZjRgxYsSIEURUW1vLTLZ27NjR1NSkOy/Dw8OD7ZgdDGuARETV1dUvvvhi67eNsLa2ZjsUdC85OTmbN2/+8MMP2Q4Chqq6uprprcuXL0+YMGHBggVsJ+pImFcREVVUVBgZGf3www9sB4HuS6vV5uTksJ0CDJidnd2YMWPGjBmTkJDw5ZdfdrGuwnnVAABdTdfbVkdXAQAA16GrAACA69BVAADAdegqAADgOnQVAABwHboKAAC4Dl0FAABch64CAACuQ1cBAADXoasAAIDr0FUAAMB16CoAAOA6dBUAAHAdugoAALgOXQUAAFzXrT8XeM2aNXv27OHz+a2/CRqNJiEhgdVc0I2sW7du+/btRkZGun+EPB5PrVbjHyHcr7fffvuvv/7Sfcnj8bRaraOj45EjR1jN1TG69bxq5syZnp6ePB6P/z9ENGDAALZzQTcyY8aMf/8jjI6OZjsXGJ6YmBgXFxfdPyQej8fj8fr06cN2ro7RrbvKx8enX79+ra+xtrZ+6qmn2EsE3Y6np+fAgQNbX2NlZTVnzhz2EoGhCgwMvK2ZXF1dZ82axV6ijtStu4r5q9bd3V33ZXBwcGRkJKuJoNuZMWOGm5ub7suAgADMq+DBzJo1y8XFhbms0WhCQ0NDQkLYDtUxuntX+fj4REREMJft7e3nzZvHdiLodjw8PHTlZGVlNXfuXLYTgaEKDAwMDQ1lLvfo0SMmJobtRB2mu3cVs2vl4eFBRBKJpHfv3mzHge5o1qxZzNRKIpFERUWxHQcM2Jw5c5ipVUhISFBQENtxOgy66tbUytLSEpsEwBZ3d/fo6GhLS0tMquA/kkgkYWFhdnZ2M2fOZDtLR7rLOevpTbU7ijJzmutrFS2dmKqzaTQalUolEonYDqJH3maWCo26j7XjM17BbGe5u13FWZeqS/g8XlZTPdtZOolWq1Equ/g/wts4iE2cxaZTevTsY+3Adpa7KJA1bitMT22olalVLRo123HuwoB+ofU0s1JpNf1tnWe7B7R/ZHtddbG69Kf81EH2Lk5iU1O+QA85ofPw+bxyubRGIf+zLG9X//FivhHbie5oSdI5L1NLJ2NTF2MzXvd9+V/XJ9eqS2TN12rLJ7v4jHHyYDvOHSXUV63PjB/l6GEnMrYUiLrza1I7HI/HK5U3V7bI4uoqfuo7qr0j7/R9P1yW91d5/l27DgyOXKP+MD326MBJbAdp2/MJZ0Kt7MOt7NkOAp1nT3FWpI3T425+bAdpw4Xq0l8L0570CGQ7SBeX3lR3prKonbpqe7+qUaU8VlGIouqSjPlGMR6Sz7O4+LYIf5Tm9DS3RlF1N1N79IyrqyiWNbMd5HZqrXZnUQaKqhMEmFv3sXbcUZx5pwPa7qrkhiqePmMBu1yNTS9Ul7Cdog2Xa8rsRMZspwAWGPF4Nxqr2U5xu5sN1Sqthu0U3YWj2ORSVemdbm27q4plUi9TC32mAjaZGQm9TC2rFXK2g9xOq9W6GJuynQJY4G5iUS6Tsp3idsXyZl8zK7ZTdBeuJmZ83h1nSW2fMSFVK2WcP9cF/otCWZOGOLdFnCttIMKUvjtSatT1xLkZTLNKKVPjN2En4RFlNdXd6Va8vgoAALgOXQUAAFyHrgIAAK5DVwEAANehqwAAgOvQVQAAwHXoKgAA4Dp0FQAAcB26CgAAuA5dBQAAXIeuAgAArkNXAQAA17HcVRUlhTHRkphoSVODoX5U+dlDe2OiJW/PefS/DNIFvg//1959xzdV7n8A/yZpkibde+9C0xY6aAuUvYeoWJlqEUQFRBS5yOWnUsTBuF4VBEXExZDLRlBQZIMKAm1pC4WW7tK9053R5PfHqaGW0gEp57T9vF/8cZqcPPmSnJxPnuc8Oadr2fL+8shw2Y5PV+urQRbfwbiL5yPDZQsnDdZvs4sjRkWGy66eO6HfZqFF2AO0ieWsEgrFsuAwWXCYQGBARCUFuZHhsl93b2O3qkev2esAnc3BzVMWHGbv0njd9Au/HI4Ml2UkJz5wgx16B7m5nTd7EXr1CZQFh5mYW7BdV49w7/azdNq49ctfZbuuB6f3+lneM1rY2K7YvFP35+VTv7JaDmuavQ7Q2SbPnj959nzdn1dOP+yG16F3kJvbebMXYdEH69mrpcdptv2k3bxemJPt7NmL1aIeXGfUr5+smj9+QE2lfP2h0zYOTlUV5a9MDCeiyS+8Mm3eYiL6POpff536ZV7Uuvramh2ffNBv6Kh+Q0cd2LpxdMTMwROe+NeUsUS05bfL/1k8NyMpkYh2bVy3a+O6r0/FSIyMMpJuHPx6U0ZyooBv0G/YyCkvvd6eL3pnjuw9/9PB3MxUsUTaq0/w5NnzPXz76O46fXBPYW6WwEBo5+T69EuLggYNJ6Kc9JT/e+4JidT4433H92/9LOb301Ij4/HTZ42bNot5YHlx0c4NazKSbpSXFDu5efYfPf7xyJcFAkGzp44MlxHRB9sOevj4E9HuLz4+9sM3A8dMZD75GbduHPvftykJcTXVld7+gX0HDhk/bZaBUFiUd0f3Ohibmj1MnT3HnbTbb0U+OfSxiPlRa4lo28fvnzr4P75A8N25eAMDg7zM9H8/85itk8unB04unBheWVH+r482nz96KP7S+W0Xrm95f/kfvx4ZN23WtPlvvDwmhGkwas4UD5n/B98f1Gg0h7/fHH3+VGFOlpd/0KjJ0weOmdh6Mc3ewY3vvHHlzPFnX19ubml98tDunNTb3n0CX16x2tLGPuqFKc2286/XvHPlzPGZi5aVFxWe+3n/sk+3yoLCSovyd3yyOjMpUV5eYmlrLwsKm7XkLYlR4xVQd3/+34u/HS0vKfT2Dxz62D/Gn1vfAjUaza7P1t248mdRfq6FtU2f/oOenrtILJHe+yIsjhhVWpC3eO3GsBHjiKiVelr5z3bOO89dzIv2kHvCI9u+ZPrcMRdOR4bLlvzni5Bhox9gT8i8L822q/u1c+rg7m0fv+cfOqj/yHHH920vzs9zcvOcvWxl777BTGu3E2L3bP6kICu9vr7O1sE5fPzjk2cvYO5q9vkaO/W5pvXvvHiLd/8rKLaffsYAfYP7E1FG0g0iSoq7SkR8Pj/1ejxzb+btm0TkHzrAQGjA7G2/WbPCwsbG2t6haSMjnpxu4+hMRH0HDH5q7kIDkTA/K+PDhbPiLp7vN3SUd9+g04f2vPvS9Nqa6taLuZ0Q+926d9NuJoQOH2tpYxd9/uSaRXPKiguI6Oiub79b9+6dtOTQ4WN7B/RLv3X946Xz4y6eJyIDkZiIFPW1G/5vUYNKZWPvVJiTvePT1XfSbjPNfr3m7StnjgsMDAaNezz/Tsb+LRv2bv6kQ69SaVH+e/OfuXLmNy//gAnTZ+Xfydy96aNj//v23jUfps6ew9zKRrd1Me87n8/XNDSkJcbptkZZcH8iMhCKiWjXpv/cuna1d2BI00YMRMKn5i5klkdNnj7iyelEtO2jVYe++VylVIydGlmUm/151JJTB3d3qDahSEhEV8+d/PH7zS5evRo06utX/tz12boWt3MDoZCIzh7Zd/rH3W69/aRGJtWV8vdeeibmwimxRDJq8gyVQnnh2KGPlsxjGj91cPexXd+WlxQOHDPRQCTa88XH7S/s6M5vftu3o7y0ZMjEJ1UK5Zkf965f/irfQHDvi9BU6/W08p/taXwCQx5+T9gnbLBfyAAicnDzeGruQgdXjwfbE967XbXSDlNSdlrSmSP7QoePsbS2zUq59emyVxR1tcyH68NXIm/Hxzh7y8LHPl6Yl7N/y4a9X37y9xP94/PVrH69BJXesop5h9JvMe9QNI/HGzT+8eSEGI1GU1MlL8jOtHF0trJ14PMFRFSUe2de1LoPvjvY7Pvg6IgZNo4uRBQwYOjUl18XCkXnjh5U1NWFDBvz4vL3X1+9YeLMOUW5d34/dqj1Yg58vYmIJkW+tGDlfz74/qCHr39dbfXpH/fW19Ye/u4LIpq7/L0FK/+z9L9fjp3yHBEd/GYTEfH5POZbZ7+ho+evXBf11S5rByciunHlIrPZJfz1h1AoWrV1z7x3VjNfUX/bt0Op6MBl4NMSE9QqlX9Y+OtrPps6/40Vm3c+v3QFE/NNPUydPYqJuYWhVJqTdltZX1dXU5OdkjR4/JNElBwXq9saZUEhRMQT8IlIKBJvPHz27U3/OEokFIqmvvw6n88nopERM0ZHzKirqTp/7BARvfr+JzMXLn3/u/0GQuH+rRs0mg5ctZbH4xNRYXbGe9/un/vv9yIXv0VECX/92eJ2zjx7aWHB2p0/rdyyy7WX7MT+nWXFBXYubqt3Hn5+6YoPvj8gMDBIuX4t9o+zTJ+biMZNm7Xog/UrNu/07hPYzqrqa2t/3rGViBau+ujF5e+v3vGjgUicdjMhNeFasxeh2QNbr6eV/2xP490n6OH3hIHhQ337DSAiRzfPqS+/7uju+WB7wnu3q9ba4fGIqKq87PUP1894Zemqb/caCIXV8oro86eI6MDWzzQazaDxT7y96fuX3/5wwcp1RPTr7m01VfJ7P1/N6tfXa6ufrJIFhereodvxMW69/fqEDVYrFZlJNxt3GYGhupUlUuPB459oT7O346KJyNOvcfjO0y+AiJKuRbfyEJVKmXTtChH17T+IueWD7w7+cClp2rzFSXFX62triWjgmEnMXWEjxzHjck3n3gwaP4nZhbl6+xBRZUUZEd24eomIPPz6GpuZE1G/ISN/uJS0/fcbIrFh+18lBxd3Ikq8eunzqCUHvt4oLysZNzWyd0C/Zqs9TJ09jW+/AVqtNj3pxs2Yv4goZPgYOxe329djiSg96QYR9e0/RLfykImTDaXSNtu8nRDXoFYbCIXuvf2IyNTc0t7ZraZSnpue0tHygoaMkhoZE5GHzJ+I6mqq1Gr1/VbuExbu4ObBLMdfvEBEIUNGCYUiIjK3tmG2k8Srl5SK+uyUJCIKHjKCWXn4E1PbWU/6zYS62moi6hM2iIhMLSy3nY//4VKSf1h46w9spZ4H+892V70DgjmyJ9Rpul212Y69q7udixuz2Xv5BTCfo/ra2luxV4ho4OjGkfCQYWP4AoFapboVe1n32HZ+vh6Yfo5Xufn4GUqlGbdu1NXUZCbfHDdtliw4lIhuX49R1NcTkU/w3XfI0t7h3sM8LWL2v/u3bNi/ZYPuxqK8nNYeUl6qaWggIt3IfpPWyolILJHoXlBTSytmobpSzvRXiEgsMWIWRGIx04NhDlYREfNRfGAu3j6Rb7x1YOvGv079SkSHv9vs4u0z5aXXQoeP0VedPY1PUOi1P85m3Epkxnj9Q8N9AkKunP1No9FkJt+0dnCysLHVrWxt79SeNivLy4hIrVIxB350CnOzXbx9OlSeoaTxHRQZSpgFTYOaDFr+0FnbO+qWq+TlRGT29/vO5AoRVVeWM99jiEjy99ZoZGraznpKiwqZTYsZHWq/VurR3dKh/2x35eot48ieUKfpdtVmO5ImuzhjMwsiqqupqamq0Gq1RGRmZcPcZWBgYGRqVlVeVlVR0eSJ2vX5emD62ZL4fL4sKCzu4vnzRw9otVqfoBBreydza9uU69ca1A1ExAxfMtr59hCR1MSUiEZFzBg46u6RbZFE0tpDjBo/t7XVlc3uMjO3JCJlfb1SUc/0h6rljZ80UwtL3XKLjIxNiKimuqqdlatVKmahruYfD5kwY/aQiU+l3Yi/nRBz4djhO6nJP363uVlWPUydPQ0zgpp263phTpazV2+JkVHvgOALxw5dOXNcpaj3DQprurJA0K5RBGbXLxQbLvt4S9PbnTy89V3+PzDjQgxTc8ui3Ds1VXe34Sq5nIjMLKx132Bq5I2d7Gp5xT2NtbwFGpmYEJGirk6tUnUorlqppyP/xe6PO3vCJiXdfZY226lrcgyMGd8zNjUzNjXn8XharVa3U1Wr1fXV1URkZnl3A2jn5+uB6a11Zhjwt307iUgWHEZEvv36J8fHZiQnWtk62Dq6tKcRHvGISKlsPAjk7RdIRBUlxX6hA/1CBxqIRPnZma1/xiRGRkyH9/pffzC3rF++KDJctvfLT3yCQiVGJlqt9sqZ35i7Lp08xtTZZofJ3cePGUJhvphkpyRFhsvmDA9k3s5/FCA11h1cVdTV3rh89zBSyo24nRvWpN6ICxw0bNqCJR9uP0RERTnZzVp4mDp7GncfP0OpNCU+Niv5JpNMsqAwIvpt/w/NvsO2gccjIpVCQURevn2JSKWol5iY+IUO9A0ZkJWSVFFWIjEy0lfZzbbzewUOGs5Mo2Iip7QwLznuKhEFDR4hEhsy3buEy38QkVar/eufM+Bb2QLdZX7MQsJfvxORSqlY9MSwyHBZ/KXfm70I7a9HX69Jt6GfPSGPx3xhZf58gD1hi9pspyA7k/mBnbysNO1GPDNOKJZIfUMGEtHl08eZ1aLPnVCplIZSqW+/sBafqFn9eqG3Hjozvao4L8fJ3cvU3JKIevUNvnTiKBENntCuMVkisrCxIaLzPx9U1NaOmfrsuGmRpw/vif39zIcLZ/XqG3Tm8L6aSvkb6zYyBxLuZ8pLr38eteSX3d/Ly0uKcu6k3IizsLabOPMFQ6n06Zde3fXZum/WrrgZe6WipDDhrz/4fP6MhUvbLCxs5DiPPdsykhJXvTTDPzT84omfiShi7kIjE7NmawYNGXHpxNHdn39cVlR448pFMyvrorw7zF1lRQW/7d1x+fSv4WMniSXSmzGXicgvdECzFh6mzp5GIBD06hN8/cqfROQTFMIMuBubmackxOomAbaHhY1daUHega83yYJCnn5x0dDHIn7/5ce1r80dMv6JovycuD/PufXyDR87SV9lN9vO711h/Iznz/20PzczLWruVFlg6OWzxxvU6n5DRzHfyodNitj12brf9u2okpeXFuQV5GQ1fWwrW6Cljf3YKc+dPLjry/eWDxo/Ke7ihYqSor79BweGD733RWh/PdCUXvaE5tY2RJR07eq+Lz/tO2DIg+0J79VmO8Zm5msWzh72+NPxf11QqZTm1rYhQ0cS0fQFb3yw4Oq5n/aXlxSamlleOnWM2dPee6jl3vqnLVjCoTnrTPwy06l7BzXOCfYJbJw14BPU3q+342c8LzU2Lcq9c3TXt1qNxsHNY+l/t/QK6Jd07erPO762d3Z7bfWG0OHjWm9k4JiJ//rvl7aOLn8e/zk1MT5g4JD/2/QdM7w+ceac+VFrTc2tLhw9mPDXHx4y/5Vbd3v7tz2NSigSv/X59iETnywtzD97ZJ/UyHTmwqVNf0yq8+yif/uFDlQp6hOjL42f/nz4uMeJSKVUEdGAUROeXxrF5wl+3b3t8Heb5aXF0xf8a/7K/9zbyAPX2QPpti7/0MYJAj6BoURkZmVt7+zazkYi5i4kopvRl/46+QsRzV4aNWbKs0R04sAPydeix0x5dtmnW/U19fbe7fzeFaRGxqu+3hsybHRuRurJg7tU9YqJz8x5bXXjMYbx058fOXk6j8e7+NvPPD7/hWWriKjh74kMrWyBRDT7zahpC97g8XinD+2pqigb9viU19dsbPFFaH890JRe9oQDRz/m7NVbpVL+tGOrvKzkwfaE92qzHSs7x5mvvnny4K6C7ExbR5c3P9kiFImJyNs/MGrLLg+Zf/zFC7//etjMwvrld1ZPfGZOe+rvaJH3w2MOmjWzPTspv75mhHXnHisDFq1Pjfs8aLiNqF1D3o/M9Cu/vuzex8Sgw4Mb0NVdLivg83mveXLrC9mB3NRbVeVjbds1cNelnfv5wDdrVrj2kq3ZcZitGtRazbrkmKODWu59dslZOts+fq+2uuXfwU18Zg7zc30A/SorLmzlV7cLV/330ZYDQCcO/JB6I77FuwIHDh084clHXlEn6pJZNefNd9kuAXocSxs7BBJwyripkeOmRrJdxSPSJbMKAAD0aMQTU0e0+0flrMC1FgEAgOuQVQAAwHXIKgAA4DpkFQAAcB2yCgAAuA5ZBQAAXIesAgAArkNWAQAA1yGrAACA61o+b4UhT2DY7uuAQVfkKNHb1Zj0yElizNfb2cyhKxHyBYZ8zu1zRHzsCR8dHvHc7nORkfv2q6wNDQvqazuzKmCTSqtJrarg2knWmSsHFiv0eX026CoKFbVWYkO2q2jOTizJrathu4qeolhZp2xo4RI5jJazytPIHN9uu7FiRV1/Szu2q2hBoJl1haqF69JCt6cl8jBqfuVS1nkYmxro77pl0LpSZX2Ihc397m05qzykJi4Sk7PFOZ1ZGLDmUF7abDdftqtowQtufr8UZCpbuvwgdGOXygukfIMAUyu2C2nOViQNMLM+XpjVjnXhYe3LSZnv0fd+97Z8rUXG+tQ4uVox3MpJIsDp2LuJMpVi953kFT79fUzM2a6lZVVq1ZyYkzOce7tKjNmuBTpdvabhUllBg1azwieM7Vru6+vMm+k1FeNsXbEn7CR5itodWbe2hYw1F4rut05rWUVE+3JTjuRnaLQaqaA7X6pVo9EoVUpD7g2X65GdWJIgLw2xsJ3lKvPm3mBLU/Wahs9S484U5/QztylS1LFdziOi1WoVCoWhYXfeCJvRaDUVKuVTjp6zXbnYy2/q54KMn/IzKpQKa7FEqWlgu5w2dKEdmp1YGltRNMzaaaFnX3OhuJU128gqItIQlSnry5Td+Yh3RkbG999///7777NdSCfiEblKTcTcm2rVivSaSrW2p4wH5uTkfPHFF2vXrmW7kEfHVCiyF0vZrqK9NEQVyvoSVT21sctkX3Jy8r59+6KiotgupG18Hs/TyKw9v51qu0vLJ7IWGVqLukBEP7AGA0NhcXlvY44Oi/VYnkambJfw6AiEpQZFZdgIOYtPZCkytOwKe8JagVhcKu9m2xJ+CwwAAFyHrAIAAK5DVgEAANchqwAAgOuQVQAAwHXIKgAA4DpkFQAAcB2yCgAAuA5ZBQAAXIesAgAArkNWAQAA1yGrAACA65BVAADAdcgqAADgOmQVAABwHbIKAAC4DlkFAABch6wCAACuQ1YBAADXIasAAIDrkFUAAMB1yCoAAOA6A7YL4AQTExM+n79p06bAwMDAwEAzMzO2K4Ieh8fjubq6sl0FdG2FhYUxMTHnzp1zcnJiuxY942m1WrZr4ISMjIwLFy7Ex8fHx8dbWFgE/s3NzY3t0qBHSEtLe/vtt/fu3ct2IdDFlJSUREdHR0dHx8TEKJXK0NDQkJCQ8ePHi8VitkvTJ/SrGnl4eHh4eDDLmZmZ8fHxcXFx27dvl8vlgU2wXSYAAJWVlTHhFBMTU11dHRoaGhoaOnv2bBcXF7ZL6yzoV7WhoqIi/m8JCQlNcwtDhaBH6FdB6+RyOZNP0dHRFRUVTD6FhIT0kLEfZFXHxDdhbm6OoULQF2QV3KuqqkqXTyUlJSEhISEhIaGhoZ6enmyX9qhhDLBjmo4EZmVlMaG1Y8eOiooKDBUCwMOrqalhBveio6Pz8vKYztNTTz3l7e3NdmlsQr9KP+RyedMuV0BAgC63zM3N2a4OugD0q3qy+vp6Jpyio6Ozs7OZfAoJCfHx8WG7NK5AVnWKprllZmamyy13d3e2SwOOQlb1NEqlksmnmJiY1NRUZnAvNDTU19eX7dK4CGOAnaLFocKdO3eWl5c3HSrk8XhsVwoAj05DQ4Ou/5SUlMTk07Jly/z9/dkujevQr3qkmg0V9u3bV5dbFhYWbFcHbEK/qrvSarW6fLpx44ZufkRAQADbpXUlyCo2JSQk6HLLxMQkKCgIQ4U9FrKqm4mNjWXG92JjY3Xje0FBQWzX1VUhq7giOzs7Li6OyS0MFfZAyKpuID4+XncKiaCgIN0UCbbr6g6QVVyEocIeCFnVRV2/fl2XT3369GG6UCEhIXw+zgyuT8iqLqDZUKEut3QnhYJuAFnVhSQmJuqm8PXu3Vt3CgkDA8xW6yzIqi4mOztbl1ulpaVNhwrxPa5LQ1ZxXFJSkq7/5OnpqRvf62aniOUsZFUXVllZ2XSo0N/fX5dblpaWbFcHHYOs4qDbt2/rThHr7Oys6z9JJBK2S+txkFXdx/Xr13W5ZWRkpMutHnjqsK4IWcURaWlpulPw2dvbM+EUGhpqZGTE+L4rNQAAFhFJREFUdmk9GrKqe7pz544ut4qLi5sOFQoEArargxYgq1iUkZGhyydra2vdT6BMTU3ZLg0aIau6v6qqqqZDhb6+vrrcsrKyYru6nm7ZsmUnT57k8/k83j8+jLGxsazW1f1lZWXpThFramqq6z/hBJ7chKzqcW7cuKHLLYlEosstLy8vtkvriZKSkpYuXVpYWKi7paGhwcfHZ8+ePazW1T3l5OToTiEhlUp18yPwpY37kFU9Wk5Oji63ioqKMFTIirfffvvEiRO6P0UiUVRU1MSJE1ktqvvIy8vTzS83MDDQnULCxsaG7dKgA5BV0AhDhWxJSkp68803CwoKmD979eq1e/dutovq2goLC3X9JyLS5ZOdnR3bpcEDQlZByzBU+CitWLHi+PHjRCQWi6OioiZMmMB2RV1PSUlJ9N/UarVufoSjoyPbpYEeIKugbU2HCgsLC3W5FRQUhKFCvdB1rfz9/bdv3852OV1GWVmZbv5eTU1N6N+cnZ3ZLg30DFkFHVNdXd10qFAmk2GoUC/eeeedc+fOvfvuu+PGjWO7Fk6Ty+W6/pNcLtf9PtfNzY3t0qATIavgoSQmJupyy9DQkJWhwuiKoj9K8irVyry6mkf2pHqnUCqKiopdunSHgMczF4p8jC1mOveSCPR5Zryqqird+Y1KSkp0x59wSsyeA1kFepObm6vLrYKCgqazCjvvnJ7bs26l1FTYiaVOEiMe4eIpLKtWq0pU9WeLcj7tO8Tb+KF+qFRTU6ObH5Gfn68b38MR054JWQWdoqamhgkt5qJcPj4+utyytrbW17N8k5mYU1c9wQ6DP5yzPfvWq54BfUz/MSz86aef/vTTT+fOnbvfo+rr63X9p+zsbN3vc3v37v1IqgbuQlbBo9B0qFAsFutyy9vb+4HbjK4oOpKfPskO11DmIoWmYW9OyudBw/l/d3aXL19+4cIFpVIZExPTdE2lUqk7P2xqaqru+JOvry9LtQMXIavgUWs6VJifn990VmErQ4WjRo2aPn36ggULdLdsSI0T8HhhFvjFDEftyE6a79En0My6oaHhlVdeuXbtmlar1Wg0sbGxDQ0Nuv5TUlKSLp/8/f3Zrho4ClkFbNINFTKjhfcbKnzyySfz8vLEYvGIESNWr17N3PjerSt9zSwdDY3ZKx9a80dpvo+xeYCSv3z58tu3b+uuryYUCrVare76uQEBAWxXCl0Asgo45H5DhXPnzq2trSUiHo/n5+e3fv16S0vL+dfOTrBzdTDElRo46nRxjrC8Mmb9V2lpaU1vFwgEly9fZq8u6JKQVcBRTYcKU1JSeLzGwx4ajcbV1XXlypVf8eTIKi47XZxzdu+BssMnmC8ZTd9BnEUeOgpZBV1ASEiIbk/HsLOzM1n28ozefZFVnHW6OEdSVeuaWZSQkJCcnFxVVVVfXy+Xy7VaLbIKOqqzfvUCoC8RERFMUGk0Gq1WKxQKLS0tBQIBMyoIXGZnZzc1dNjUqVOJKD8/Pz09PTk5+dq1azNnzsRFT6BDkFXAdaWlpRKJxNLSUiqVenl5MbOZZTLZ/Gtn2S4NOsDBwcHBwWHw4MFsFwJdErIKuO7ChQu//PKLTCbz9PRkuxYAYAeyCrqAxx57jO0SAIBNfLYLAAAAaAOyCgAAuA5ZBQAAXIesAgAArkNWAQAA1yGrAACA65BV0IOUFResX/7qi6OCXxrdT15WujhiVGS47Oq5Ew/T5htPj44Ml0WfP0FE534+EBkue/v5p/RXMgAQsgp6lv9t/CjmwmlzK5sJM+eIxKJefQJlwWEm5hb6at/cykYWHObu090uwnThl8OR4bKM5ES2C4GeC78Fhh6kqqKCiEY+Oe3xWS8T0aIP1uu3/aBBw4MGDddvm1xw5fSvbJcAPR2yCnqKF4YFqFRKItqz+ZM9mz/57PCZ9xdElhbkLV67MWzEuBMHdu345IN+Q0dNnvPKkW1f3oq9au/i+uxry3379WcefmL/zt9/PZKbkWpmad13wOCpLy82tbBs9hTnfj7wzZoVrr1ka3YcLsjOfHPGhGYrrN15xMXbR6PRHP5+c/T5U4U5WV7+QaMmTx84ZmLrxeekp/zfc09IpMbLPv1q54Y1EmPTtzd930o7CycNriwrfW31hgvHDqUlJmg02oGjxs9e9q5AICAilUq5d/MncRfPl+bnGZmZu3j1mvXGO47unkSkex36DR11YOvGYZOe/mn7FqbNqDlTJr/wyrR5i/X0hgB0AMYAoad4cs4CG0dnIuoTNuipuQulxiZN7xUKhURUlJO9acUbFja2UiPjjKTEze8uU6tURHTup/07Pl1dUpD32Mw5EiPjMz/u/XbdytafTmgolgWHMf+c3L2YGw2EIiLa9tGqQ998rlIqxk6NLMrN/jxqyamDu9toTSgiIoWibvN7/66trnL17t16O8z6O9evsXFw7jtgSG2V/MyRfcd++Ja597O3Xj++Z3tNZcXwJ6eYW9lcv/znqpdnlhcXEZGB0ICJxm/WrLCwsbFxcnpq7kLmUaMmT/fvN+BBX36Ah4J+FfQUEXMX3rp2tTgvp2//QZMiX2p2L4/PJ6KcjNRV3+z19g/Mn56xbObE8pLCO+m3PXz8025dlwWHDZsUMWzS0/5hg1a/+vy1P8+q1WoDg/t+gqxsHVZs3klEarX6/XnPENHIydMd3DzqaqrOHztERK++/4l7b7/Hnn3h9SeH79+6YVTEDN1V3u/FE/CJSNPQ4Bvcf37UWiJqox0ej4j8QwfMeXMlEVlY2fyy+/sLvxx6cva8W7FX4v48R0TvfrXH3tW9oaEh6oUp2SlJv+7Z9uxr/+bzBURUlHtnXtS6YY81ThL5adsWjUYzMmKGR7c7FAddBfpVAHfZOrl4+wcSkYObh0RqTERVFeVE9OLy91ds3jls0tNEZGFrz8RGbVVle9rc/9WG9FvXXXvJZi15m4huJ8Q1qNUGQqF7bz8iMjW3tHd2q6mU56antKe1sVOeYxba007AgKGNC+FDiaggO7Oupiru0gUi8vD1t3d1Z64oHzJsNBElXr2ke6BEajx4/BMdfPEAOhH6VQB3GUrvXmVYaCiuq63WNDQQ0cUTR/dtWV+Sn9vRBm9cvXjsh28MpUZvrNskEhsSUWV5GRGpVarIcFnTNQtzs128fdps0NrRkVloTzuSv8c5jc3MmYXa6iomfc0srHUPMTW3IqJqeYXuFkt7B+bIFgBHIKsA2lBamPflqmVarXb00zMHjJxQWpT/1QdvteeBleVlX6z8FxEtWLnW1tGFudHI1JSIhGLDZR9vabqyk4d3e9pkxuja2U5ddRWzUFvZ2AU0MjFnpoTU/H0XEVVXVhCRqdXdqSIIKuAajAECtCE/O1Or1RLRc68t9wsdqKxXMLdrNZpWHqXVaj9fubSqomL8jOdDh4/T3e7l25eIVIp6iYmJX+hA35ABWSlJFWUlEiOjVlq7V3vauXL2BFN57J9nicjJs5ehVBo0aBgRpSXGF+flMIfTrpz9jYiCBo2475PxeESkUig6VCGAHqFfBdAGR3cvPp+v0Wi2rn7bzsk15o+zTp69ctNTDny98YnnX77fo84fPXgz+hIRpd+68eHCWcyN/UeOGzdt1tDHIn7/5ce1r80dMv6JovycuD/PufXyDR87qUNVmVlZt9lOcnz0h69EOrl7nTmyj4hGR8wgIllQWMiw0TEXTq+aN3PAyAmJsZdz01OsbB0mPvPC/Z7LwsautCDvwNebxk19pmnuAjwy6FcBtMHSxm7KvNfdevnGX/y9MDd7ydpN0+YttrJ3/OvUrzX3n14hLytlFlISYpOuXWX+FdzJJqLZS6PGTHmWiE4c+CH5WvSYKc8u+3Qrj8fraGFttjNt/hKVUnnmyD6+QDB+xvO6eRmLPlw/8Zk5irq6Ewd+yM9KDxk2etU3e6VGxvd7ooi5C4noZvSl1MTrHS0SQC94zBABQJcz/9rZCXauDoYdGzrrIRZHjNL9zJmtGk4X57hLTZ5zaXvCCECbMAYIwAllxYV7vvj4fvcuXPXfR1sOALcgqwA4wdLGDoEEcD/IKoBu6LMfz7BdAoA+YW4FAABwHbIKAAC4DlkFAABch6wCAACuQ1YBAADXIasAAIDrkFUAAMB1yCoAAOA6ZBUAAHAdsgq6KlOhSMDDBsxdQj5PiDcI9ARbEnRVhnxBmbKe7SrgvkoV9dZiCdtVQDeBrIKuyt/UqlKtZLsKuC+1VutuZMp2FdBNIKugq5rp3OtU0R1lqxeSB7ZcLS+yF0s9pcgq0A9kFXRh3/Yb/V3WzWKMBHLMX2UFhYqaZb37sV0IdB+4LjB0baXK+o9TYjNrKv1MrWq68pCgVqtVKpVisZjtQh4cj8erVCnlKmWYpd1ir0C2y4FuBVkF3UGhojaztqpWrWK7kAdXWFi4Z8+exYsXs13Ig+PxeJZCsbuRqamBiO1aoLvBtRahO7ATS+3EUrareChplYofU7JH2jizXQgAF+F4FQAAcB2yCgAAuA5ZBQAAXIesAgAArkNWAQAA1yGrAACA65BVAADAdcgqAADgOmQVAABwHbIKAAC4DlkFAABch6wCAACuQ1YBAADXIasAAIDrkFUAAMB1yCoAAOA6ZBUAAHAdsgoAALgOWQUAAFyHrAIAAK5DVgEAANchqwAAgOuQVQCcwOPxLC0t2a4CgKOQVQCcoNVqy8rK2K4CgKOQVQAAwHXIKgAA4DpkFQAAcB2yCgAAuA5ZBQAAXIesAgAArkNWAQAA1yGrAACA65BVAADAdcgqAADgOmQVAABwHbIKAAC4DlkFAABch6wCAACuQ1YBAADXIasAAIDreFqtlu0aAHquF198MSYmRiAQEJFGo+HxeDweT6vVxsbGsl0aAIegXwXApkWLFtnb2zMRJRAI+Hw+j8fz8vJiuy4AbkFWAbApODhYJpM1vcXQ0HDmzJnsVQTARcgqAJbNmjXL2tpa96ezs/OUKVNYrQiAc5BVACwLCQnx8/NjlsVi8YwZM9iuCIBzkFUA7IuMjGS6Vu7u7hEREWyXA8A5yCoA9vXr18/f318ikUyfPp3tWgC4yIDtAgC6mGJFnY1YcrGs4FJpfr1GXa5S9DGxCja3uV5ZEicveeDluunjLRzN3UcM3pF9K6GyNMDUKtDMJl5e/MDL1ytLS5X1Q6wcQ8xtqhtUxgIh268cwIPD76sA2iuhsvTL9ITahob8+hrdjVotEY94elkm4vFI94l8yGUdQ77AwdDIUiRe4NHXTWqi95cF4BFAVgG0oVyl2JGVJBIIThRm1TSo2S7nwTkYSp9y8CKiCEdPtmsB6BhkFUBrqhtU86+dqVKp6zVdOKWashSJTQxEW4NH8dqxMgBHIKsA7ut08Z29OSmZtVVsF6J/Y21dJjt49jY2Z7sQgHZBVgG07NPUa+dL8+rU3aQ7dS97sXRJr+BgM+t2rAvAMsxZB2jBiaLsU8U53TioiKhAUfvR7Zgb8lK2CwFoG7IKoLnsuurvs26pNRq2C+l0pcr6IwXpbFcB0DaMAQL8Q2JV2drk6CJFHduFPDpuUpOvg0exXQVAa9CvAviHE4XZPSqoiKhcqdifm8p2FQCtQVYB/ENPCyoiqlQrJQKcwgY4DVkFcNfHKbExFUVsV9GaO4d+OT1qulbfx9K2Zd1Kr5Hrt00APUJWATTKqK28XlnGdhVtqErJMOntwePr+ZNbqVZ+np6g3zYB9AhZBdBIq9XWqlVsV9GGqtvpJl7undGyokFTqqzvjJYBHh4GqQEaaYnkamXntV9fUpb5w6GSv2I0SqW5v6z3ohcM7ayJqDIp9fJLb4ZtWZex40DJxWhjbw/nyeNcIiYSkbahIe27PUUXLqvkldaDQr1efKYqLcvxibGdUV6dRm0lMuyMlgEeHvpVAI2+yrjReY1XJqVefmGJori038crg/+zojavIGHVx8xhp+rMHCLK2n3E/ZmnRhzfZdU/KHnDN+raOiJK2bLzzoFjHrOmhO/caOTqFLtklVat7qR+VU5d9a2q8s5oGeDhIasAGuXW1bRjrQd0679fGnu69V31L6mzg0kvD79lr8ivJ1WnZxFR7Z1cvkjU+9U5FsF9hMZGliEB2oYGZVmFqrI6+8BR9+eedhg3XGRm6v5shKGdDREZe7l1UpHHCjI7qWWAh4SsAiAiUmu1/qaWndR4VWpmZXKac8QEvrDxgocia0siUpSUE1F1WpZ1eIjE0Y65S1FUQkRia8uSy7Faldp+zFBdOyJLcyN3FwOppJPqNDbA9RiBo3C8CoCIyIDHc5YYd1LjVbfTiShhxUfNbheaGDNJ5vT4mLsrp2VJXRwFhuLKWymG9ra6DCOiurwCk14enVQkc/L1zmsc4GEgqwAanSq600ktq+vqBBLDoI9WNLvd2NtNVVVTX1Bk7Hl3WK8mI5sJJEVxmaGt1d1GauvkibetB/brpCKJKLVG7mVk1nntAzwwZBVAI0OBoJNaFluYa5QqM7/eArGIuaX2Tp6hgy3fwKAiIYmITLzvTpeoTE5zfzaCiIjP0yjvzqHPP35Oq1abeHdWv4pHPMxZB87C8SqARkt6BXdSy1YD+xnaWad/t4dJqazdh6+++railDlYlWlgJNUN9NUVFKmra5jZExbBfSqTUvNPXii7duPOj7/mHTtFRMbenTIJkIhsDSWDLO07qXGAh4R+FUAjX2MLZ4lxTl213ls2kEoC17yVvPHbk0Oe4otE5gG+AR/+W2JnwxydMpV569asTssiImYM0GnS6JrMO4mrN2rVapuhA+xGDa7NKxRbd9YEkOFWTu5S005qHOAh4ZogAI1KlPWL488X99RxsKmO3vM8/NmuAqBl6FcBNLIWGdqIpa1nVfr2/fUFzU9uq6qu4fH5LU4l54uEsiXz9Fhk0R9XSv682uJdynK5yKLlmRG2IwZZD2hthNOAx3eXmuipRgD9Q78K4C4Nad9KvHStopjtQh61x+zc3vAOYrsKgPvC3AqAu/jEi3DwZLuKR81OLHnNK5DtKgBag6wC+AcvYzMHQyO2q3h0+DyerVgq4PHYLgSgNcgqgH+wEUlWysJ6zsEbd6nJGv9wtqsAaAOOVwG0oEKpmBd3tkKlYLuQzjXW1mVZr048EQaAvqBfBdACc5F4jquvpUjMdiGdyEZs+IYX5lNA14A56wAte8zeTalpuCYvvlRWwHYtesYn6mduu8Czj5CPb6vQNWAMEKA1WqIv0hNOF9+pUavZrkU/7MSSD3wHuhmZYjYFdCHIKoC27ctNTa4qT6wqK+vKZ7XoZWyu1mgWeQX0NbVqx+oAHIKsAuiAT1KuXa0oNBeKFQ0Npar6uoYGImI6KFqtlni8di7//bnj8Xidskyk1RJZiiRmBqISRR2fx/sscJijoRH6UtBFIasAOkZNVN+gMhYIL5TmlSjqR9g4WQrFR/MzytXKSfbu7Vz+KT9DrlY+Ye9u3jnLR/LTecQba+siERjk1FV33mUkAR4NZBUAAHAdZgEBAADXIasAAIDrkFUAAMB1yCoAAOA6ZBUAAHAdsgoAALju/wExHb1TPRV+EwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.constants import START, END\n",
    "from langchain_teddynote.graphs import visualize_graph\n",
    "\n",
    "# 그래프 생성\n",
    "builder = StateGraph(ResearchGraphState)\n",
    "\n",
    "# 노드 정의\n",
    "builder.add_node(\"create_analysts\", create_analysts)\n",
    "builder.add_node(\"human_feedback\", human_feedback)\n",
    "builder.add_node(\"conduct_interview\", interview_builder.compile())\n",
    "builder.add_node(\"write_report\", write_report)\n",
    "builder.add_node(\"write_introduction\", write_introduction)\n",
    "builder.add_node(\"write_conclusion\", write_conclusion)\n",
    "builder.add_node(\"finalize_report\", finalize_report)\n",
    "\n",
    "# 엣지 정의\n",
    "builder.add_edge(START, \"create_analysts\")\n",
    "builder.add_edge(\"create_analysts\", \"human_feedback\")\n",
    "builder.add_conditional_edges(\n",
    "    \"human_feedback\", initiate_all_interviews, [\"create_analysts\", \"conduct_interview\"]\n",
    ")\n",
    "\n",
    "# 인터뷰 결과 보고서 작성\n",
    "builder.add_edge(\"conduct_interview\", \"write_report\")\n",
    "builder.add_edge(\"conduct_interview\", \"write_introduction\")\n",
    "builder.add_edge(\"conduct_interview\", \"write_conclusion\")\n",
    "\n",
    "# 보고서 최종 정리\n",
    "builder.add_edge(\n",
    "    [\"write_conclusion\", \"write_report\", \"write_introduction\"], \"finalize_report\"\n",
    ")\n",
    "builder.add_edge(\"finalize_report\", END)\n",
    "\n",
    "# 컴파일\n",
    "memory = MemorySaver()\n",
    "graph = builder.compile(interrupt_before=[\"human_feedback\"], checkpointer=memory)\n",
    "\n",
    "visualize_graph(graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mcreate_analysts\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "affiliation='Tech Innovations Journal' name='Alice Turing' role='Data Science Researcher' description='Alice is focused on understanding the technological advancements and practical implementations in data science. Her primary concern is how new methodologies, like Modular RAG, provide scalable and efficient solutions compared to traditional methods such as Naive RAG. Alice is motivated by the potential of modular systems to improve adaptability in rapidly changing environments.'\n",
      "affiliation='Production Efficiency Solutions Inc.' name='Mohammed Sinclair' role='Operations Analyst' description='Mohammed examines the benefits of Modular RAG over Naive RAG at the production level, primarily looking at operational efficiency and cost-effectiveness. His focus is on how modularity can lead to reduced downtime and better resource allocation in production environments. Mohammed is driven by finding solutions that reduce operational costs while maintaining high production standards.'\n",
      "affiliation='Cognitive Computing Lab' name='Rita Nakamura' role='Artificial Intelligence Specialist' description=\"Rita's focus is on the cognitive and learning aspects of AI systems, such as Modular RAG. She is concerned with how modular frameworks can enhance the learning capabilities and flexibility of AI, compared to more rigid traditional systems like Naive RAG. Rita is motivated by creating AI systems that are not only efficient but also capable of evolving with new data.\"\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36m__interrupt__\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# 입력 데이터 설정\n",
    "max_analysts = 3\n",
    "topic = \"Explain how Modular RAG differs from traditional Naive RAG and the benefits of using it at the production level.\"\n",
    "\n",
    "# config 설정\n",
    "config = RunnableConfig(\n",
    "    recursion_limit=30,\n",
    "    configurable={\"thread_id\": random_uuid()},\n",
    ")\n",
    "\n",
    "# 입력 데이터 설정\n",
    "inputs = {\"topic\": topic, \"max_analysts\": max_analysts}\n",
    "\n",
    "# 그래프 실행: 첫 번째 중단 지점까지\n",
    "invoke_graph(graph, inputs, config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'configurable': {'thread_id': '56d6c5d3-51f2-4c1b-b859-3eefcb4d2eb3',\n",
       "  'checkpoint_ns': '',\n",
       "  'checkpoint_id': '1f083d3b-644b-6600-8002-8769af338823'}}"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 새로운 분석가 추가\n",
    "graph.update_state(\n",
    "    config,\n",
    "    {\"human_analyst_feedback\": \"Add Prof. Jeffrey Hinton as a head of AI analyst\"},\n",
    "    as_node=\"human_feedback\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mcreate_analysts\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "affiliation='Vector Institute' name='Prof. Jeffrey Hinton' role='Head of AI Analyst' description='As the head of AI analysts, Prof. Hinton brings a pioneering perspective on deep learning and neural networks. He is particularly interested in how Modular RAG can improve efficiency and scalability over Naive RAG, and what implications this has for advancing AI in production environments.'\n",
      "affiliation='OpenAI' name='Dr. Lisa Torres' role='Scalability Specialist' description='Dr. Torres focuses on the scalability benefits of Modular RAG versus Naive RAG. Her analysis centers around how Modular RAG can handle larger datasets more efficiently and its adaptability to various production-level scenarios.'\n",
      "affiliation='DeepMind' name='Dr. Amir Kahn' role='Performance Analyst' description=\"Dr. Kahn's expertise lies in evaluating the performance metrics of AI models. He compares Modular and Naive RAG in terms of speed and accuracy, providing an in-depth look at how these improvements can minimize latency and enhance reliability in real-world applications.\"\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36m__interrupt__\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# 그래프 실행\n",
    "invoke_graph(graph, None, config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'configurable': {'thread_id': '56d6c5d3-51f2-4c1b-b859-3eefcb4d2eb3',\n",
       "  'checkpoint_ns': '',\n",
       "  'checkpoint_id': '1f083d3b-7e43-6c88-8004-c644448fdcc5'}}"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 그래프 재개\n",
    "graph.update_state(config, {\"human_analyst_feedback\": None}, as_node=\"human_feedback\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mask_question\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello, I'm John Harper, a technology writer exploring cutting-edge advancements in AI scalability. Dr. Torres, could you please explain how Modular RAG differs from traditional Naive RAG, specifically in terms of scalability and handling large datasets? How does modularity enhance its performance in production environments?\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mask_question\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello, Dr. Kahn, my name is Alex Ramirez, and I'm a technology journalist interested in exploring advancements in AI performance metrics. I'm particularly intrigued by the differences between Modular RAG and Naive RAG. Could you provide an explanation of how these two approaches differ specifically, and what benefits modular RAG offers when used in production environments?\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mask_question\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello, my name is Dr. Emily Carter and I’m thrilled to have the opportunity to explore the nuances of Modular RAG and its advantages over traditional Naive RAG with you, Prof. Hinton. As an aspiring AI researcher, I’m especially intrigued by the practical implications of these advancements in real-world applications. To start, could you explain how Modular RAG differentiates from traditional Naive RAG in its structure and functionality?\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36msearch_web\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "<Document href=\"https://rabiloo.com/blog/the-3-types-of-rag-models-naive-rag-modular-rag-and-advanced-rag\"/>\n",
      "Building AI systems requires choosing the right tools for the job, and Retrieval-Augmented Generation (RAG) offers various models, each designed to serve different needs. Essentially, a RAG model retrieves information from external databases, knowledge bases, or the web and then uses that data to generate relevant, meaningful responses. Naive RAG begins by indexing the data source for quick retrieval of relevant information. Modular RAG builds on the basic principles of Naive RAG by breaking down the retrieval and generation processes into separate, specialized modules. Unlike Naive RAG or Modular RAG, this stage is more precise and contextually aware, ensuring that the system retrieves only the most relevant and contextually appropriate data. The generation module in Advanced RAG then processes the refined data using sophisticated models like T5 or GPT-3.\n",
      "</Document>\n",
      "\n",
      "---\n",
      "\n",
      "<Document href=\"https://medium.com/@drjulija/what-are-naive-rag-advanced-rag-modular-rag-paradigms-edff410c202e\"/>\n",
      "I share the differences between Naive RAG, Advanced RAG and Modular RAG frameworks. Retrieval-Augmented Generation (RAG) framework is designed to help mitigate these challenges. RAG enhances LLMs’ knowledge base with additional, domain-specific data. RAG systems can be split into 3 categories: See the comparison between all three paradigms of RAG — Naive RAG, Advanced RAG and Modular RAG below.\n",
      "</Document>\n",
      "\n",
      "---\n",
      "\n",
      "<Document href=\"https://www.digitalocean.com/community/conceptual-articles/rag-ai-agents-agentic-rag-comparative-analysis\"/>\n",
      "Jan 14, 2025·The Modular RAG architecture transcends the Naive and Advanced RAG models, offering improved adaptability and versatility. It uses multiple\n",
      "</Document>\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36msearch_arxiv\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "<Document source=\"http://arxiv.org/abs/2508.05650v1\" date=\"2025-07-26\" authors=\"Jiaxuan Liang, Shide Zhou, Kailong Wang\"/>\n",
      "<Title>\n",
      "OmniBench-RAG: A Multi-Domain Evaluation Platform for Retrieval-Augmented Generation Tools\n",
      "</Title>\n",
      "\n",
      "<Summary>\n",
      "While Retrieval Augmented Generation (RAG) is now widely adopted to enhance\n",
      "LLMs, evaluating its true performance benefits in a reproducible and\n",
      "interpretable way remains a major hurdle. Existing methods often fall short:\n",
      "they lack domain coverage, employ coarse metrics that miss sub document\n",
      "precision, and fail to capture computational trade offs. Most critically, they\n",
      "provide no standardized framework for comparing RAG effectiveness across\n",
      "different models and domains.\n",
      "  We introduce OmniBench RAG, a novel automated platform for multi domain\n",
      "evaluation of RAG systems. The platform quantifies performance gains across\n",
      "accuracy and efficiency dimensions, spanning nine knowledge fields including\n",
      "culture, geography, and health. We introduce two standardized metrics:\n",
      "Improvements (accuracy gains) and Transformation (efficiency differences\n",
      "between pre RAG and post RAG models), enabling reproducible comparisons across\n",
      "models and tasks. The platform features dynamic test generation, modular\n",
      "evaluation pipelines, and automated knowledge base construction. Our evaluation\n",
      "reveals striking variability in RAG effectiveness, from significant gains in\n",
      "culture to declines in mathematics, highlighting the critical importance of\n",
      "systematic, domain aware assessment. A demonstration video is available at:\n",
      "https://www.youtube.com/watch?v=BZx83QFcTCI. Code and datasets:\n",
      "https://github.com/Garnett-Liang/Omnibench-RAG.\n",
      "</Summary>\n",
      "\n",
      "<Content>\n",
      "OmniBench-RAG: A Multi-Domain Evaluation\n",
      "Platform for Retrieval-Augmented Generation Tools\n",
      "Jiaxuan Liang*, Shide Zhou*, and Kailong Wang†\n",
      "Huazhong University of Science and Technology\n",
      "{liangjx, shidez, wangkl}@hust.edu.cn\n",
      "Abstract—While Retrieval Augmented Generation (RAG) is\n",
      "now widely adopted to enhance LLMs, evaluating its true\n",
      "performance benefits in a reproducible and interpretable way\n",
      "remains a major hurdle. Existing methods often fall short: they\n",
      "lack domain coverage, employ coarse metrics that miss sub\n",
      "document precision, and fail to capture computational trade\n",
      "offs. Most critically, they provide no standardized framework\n",
      "for comparing RAG effectiveness across different models and\n",
      "domains.\n",
      "We introduce OmniBench RAG, a novel automated platform\n",
      "for multi domain evaluation of RAG systems. The platform\n",
      "quantifies performance gains across accuracy and efficiency\n",
      "dimensions, spanning nine knowledge fields including culture,\n",
      "geography, and health. We introduce two standardized metrics:\n",
      "Improvements (accuracy gains) and Transformation (efficiency\n",
      "differences between pre RAG and post RAG models), enabling\n",
      "reproducible comparisons across models and tasks. The platform\n",
      "features dynamic test generation, modular evaluation pipelines,\n",
      "and automated knowledge base construction. Our evaluation\n",
      "reveals striking variability in RAG effectiveness, from significant\n",
      "gains in culture to declines in mathematics, highlighting the\n",
      "critical importance of systematic, domain aware assessment. A\n",
      "demonstration video is available at: https://www.youtube.com/\n",
      "watch?v=BZx83QFcTCI. Code and datasets: https://github.com/\n",
      "Garnett-Liang/Omnibench-RAG.\n",
      "I. INTRODUCTION\n",
      "Retrieval-Augmented Generation (RAG) is a key technique\n",
      "for enhancing Large Language Models (LLMs) [1], [2]. By\n",
      "grounding model responses in external, verifiable knowledge,\n",
      "RAG promises to mitigate hallucinations [3], improve factual\n",
      "accuracy [4], and provide up-to-date information [5]. However,\n",
      "the true effectiveness of RAG is far from uniform. Recent\n",
      "studies [6], [7] reveal a significant disparity: while RAG\n",
      "can boost the accuracy of smaller models like Llama-3.2-3B-\n",
      "Instruct by as much as 38.12%, its impact on state-of-the-art\n",
      "models such as GPT-4o, which excel with extended context\n",
      "windows, is often less pronounced. This variability, which\n",
      "depends not only on the model’s scale but also heavily on\n",
      "the knowledge domain, underscores a critical challenge: the\n",
      "lack of a systematic platform to quantify the value of RAG\n",
      "across these diverse contexts.\n",
      "Current RAG evaluation approaches suffer from fundamen-\n",
      "tal limitations that impede reproducible and comprehensive as-\n",
      "sessment: First, they lack automated multi-domain evalua-\n",
      "tion capabilities and rely on non-deterministic components.\n",
      "*Jiaxuan Liang and Shide Zhou are co-first authors.\n",
      "†Kailong Wang is the corresponding author.\n",
      "Existing benchmarks typically require manual configuration\n",
      "for each knowledge domain and fail to provide unified assess-\n",
      "ment across diverse fields like finance, healthcare, or culture,\n",
      "making cross-domain performance analysis labor-intensive and\n",
      "inconsistent. Moreover, key metrics in leading frameworks\n",
      "(e.g., LLM-based scoring in Ragas [8]) inadvertently introduce\n",
      "randomness due to reliance on large language models in the\n",
      "evaluation loop, undermining result reproducibility. Second,\n",
      "they employ static datasets and coarse-grained metrics.\n",
      "Most frameworks rely on fixed benchmarks and document-\n",
      "level retrieval metrics (e.g., MRR@k[9]), missing the critical\n",
      "sub-document precision needed to assess whether models\n",
      "extract specific facts accurately. They also lack the ability to\n",
      "dynamically generate test cases that probe complex reasoning\n",
      "patterns. Third, they fail to capture the computational\n",
      "trade-offs inherent in RAG systems. Without automated\n",
      "profiling of resource utilization and efficiency metrics, practi-\n",
      "tioners cannot make informed decisions about the cost-benefit\n",
      "trade-offs of deploying RAG in production environments.\n",
      "Our \n",
      "</Content>\n",
      "</Document>\n",
      "\n",
      "---\n",
      "\n",
      "<Document source=\"http://arxiv.org/abs/2406.00944v3\" date=\"2025-02-28\" authors=\"Shicheng Xu, Liang Pang, Huawei Shen, Xueqi Cheng\"/>\n",
      "<Title>\n",
      "A Theory for Token-Level Harmonization in Retrieval-Augmented Generation\n",
      "</Title>\n",
      "\n",
      "<Summary>\n",
      "Retrieval-augmented generation (RAG) utilizes retrieved texts to enhance\n",
      "large language models (LLMs). Studies show that while RAG provides valuable\n",
      "external information (benefit), it may also mislead LLMs (detriment) with noisy\n",
      "or incorrect retrieved texts. Although many existing methods attempt to\n",
      "preserve benefit and avoid detriment, they lack a theoretical explanation for\n",
      "RAG. The benefit and detriment in the next token prediction of RAG remain a\n",
      "black box that cannot be quantified or compared in an explainable manner, so\n",
      "existing methods are data-driven, need additional utility evaluators or\n",
      "post-hoc. This paper takes the first step towards providing a theory to explain\n",
      "and trade off the benefit and detriment in RAG. First, we model RAG as the\n",
      "fusion between distribution of LLMs knowledge and distribution of retrieved\n",
      "texts. Then, we formalize the trade-off between the value of external knowledge\n",
      "(benefit) and its potential risk of misleading LLMs (detriment) in next token\n",
      "prediction of RAG by distribution difference in this fusion. Finally, we prove\n",
      "that the actual effect of RAG on the token, which is the comparison between\n",
      "benefit and detriment, can be predicted without any training or accessing the\n",
      "utility of retrieval. Based on our theory, we propose a practical novel method,\n",
      "Tok-RAG, which achieves collaborative generation between the pure LLM and RAG\n",
      "at token level to preserve benefit and avoid detriment. Experiments in\n",
      "real-world tasks using LLMs such as OPT, LLaMA-2, and Mistral show the\n",
      "effectiveness of our method and support our theoretical findings.\n",
      "</Summary>\n",
      "\n",
      "<Content>\n",
      "Published as a conference paper at ICLR 2025\n",
      "A THEORY FOR TOKEN-LEVEL HARMONIZATION IN\n",
      "RETRIEVAL-AUGMENTED GENERATION\n",
      "Shicheng Xu1,2, Liang Pang1∗, Huawei Shen1, Xueqi Cheng1\n",
      "1CAS Key Laboratory of AI Safety, Institute of Computing Technology, Chinese Academy of Sciences\n",
      "2University of Chinese Academy of Sciences\n",
      "{xushicheng21s,pangliang,shenhuawei,cxq}@ict.ac.cn\n",
      "ABSTRACT\n",
      "Retrieval-augmented generation (RAG) utilizes retrieved texts to enhance large\n",
      "language models (LLMs). Studies show that while RAG provides valuable external\n",
      "information (benefit), it may also mislead LLMs (detriment) with noisy or incorrect\n",
      "retrieved texts. Although many existing methods attempt to preserve benefit and\n",
      "avoid detriment, they lack a theoretical explanation for RAG. The benefit and\n",
      "detriment in the next token prediction of RAG remain a ’black box’ that cannot\n",
      "be quantified or compared in an explainable manner, so existing methods are data-\n",
      "driven, need additional utility evaluators or post-hoc. This paper takes the first step\n",
      "towards providing a theory to explain and trade off the benefit and detriment in\n",
      "RAG. First, we model RAG as the fusion between distribution of LLM’s knowledge\n",
      "and distribution of retrieved texts. Then, we formalize the trade-off between the\n",
      "value of external knowledge (benefit) and its potential risk of misleading LLMs\n",
      "(detriment) in next token prediction of RAG by distribution difference in this\n",
      "fusion. Finally, we prove that the actual effect of RAG on the token, which is the\n",
      "comparison between benefit and detriment, can be predicted without any training or\n",
      "accessing the utility of retrieval. Based on our theory, we propose a practical novel\n",
      "method, Tok-RAG, which achieves collaborative generation between the pure\n",
      "LLM and RAG at token level to preserve benefit and avoid detriment. Experiments\n",
      "in real-world tasks using LLMs such as OPT, LLaMA-2, and Mistral show the\n",
      "effectiveness of our method and support our theoretical findings. Code is available1.\n",
      "1\n",
      "INTRODUCTION\n",
      "Retrieval-augmented generation (RAG) has shown promising performance in enhancing Large\n",
      "Language Models (LLMs) by integrating retrieved texts (Xu et al., 2023; Shi et al., 2023; Asai et al.,\n",
      "2023; Ram et al., 2023). Studies indicate that while RAG provides LLMs with valuable additional\n",
      "knowledge (benefit), it also poses a risk of misleading them (detriment) due to noisy or incorrect\n",
      "retrieved texts (Ram et al., 2023; Xu et al., 2024b;a; Jin et al., 2024; Xie et al., 2023). Existing\n",
      "methods attempt to preserve benefit and avoid detriment by adding utility evaluators for retrieval,\n",
      "prompt engineering, or fine-tuning LLMs (Asai et al., 2023; Ding et al., 2024; Xu et al., 2024b; Yoran\n",
      "et al., 2024; Ren et al., 2023; Mallen et al., 2022; Jiang et al., 2023). However, existing methods are\n",
      "data-driven, need evaluator for utility of retrieved texts or post-hoc. A theory-based method, focusing\n",
      "on core principles of RAG is urgently needed, which is crucial for reliable improvements without\n",
      "relying on additional training or utility evaluators and improving our understanding for RAG.\n",
      "This paper takes the first step in providing a theoretical framework to explain and trade off the benefit\n",
      "and detriment at token level in RAG and proposes a novel method to preserve benefit and avoid\n",
      "detriment based on our theoretical findings. Specifically, this paper pioneers in modeling next token\n",
      "prediction in RAG as the fusion between the distribution of LLM’s knowledge and the distribution\n",
      "of retrieved texts as shown in Figure 1. Our theoretical derivation based on this formalizes the core\n",
      "of this fusion as the subtraction between two terms measured by the distribution difference: one is\n",
      "distribution completion and the other is distribution contradiction. Further analysis indicates that\n",
      "∗Corresponding Author\n",
      "1https://github.com/xsc1234/Tok-RAG\n",
      "1\n",
      "arXiv:2406.00944v3  [cs.CL]  28 Feb 2025\n",
      "Published as a conference paper at ICLR 2025\n",
      "Query\n",
      "Wole\n",
      "Query\n",
      "Ernst\n",
      "Soyinka\n",
      "…\n",
      "LLM’s \n",
      "Distribution\n",
      "Retr\n",
      "</Content>\n",
      "</Document>\n",
      "\n",
      "---\n",
      "\n",
      "<Document source=\"http://arxiv.org/abs/2507.18515v1\" date=\"2025-07-24\" authors=\"Zezhou Yang, Ting Peng, Cuiyun Gao, Chaozheng Wang, Hailiang Huang, Yuetang Deng\"/>\n",
      "<Title>\n",
      "A Deep Dive into Retrieval-Augmented Generation for Code Completion: Experience on WeChat\n",
      "</Title>\n",
      "\n",
      "<Summary>\n",
      "Code completion, a crucial task in software engineering that enhances\n",
      "developer productivity, has seen substantial improvements with the rapid\n",
      "advancement of large language models (LLMs). In recent years,\n",
      "retrieval-augmented generation (RAG) has emerged as a promising method to\n",
      "enhance the code completion capabilities of LLMs, which leverages relevant\n",
      "context from codebases without requiring model retraining. While existing\n",
      "studies have demonstrated the effectiveness of RAG on public repositories and\n",
      "benchmarks, the potential distribution shift between open-source and\n",
      "closed-source codebases presents unique challenges that remain unexplored. To\n",
      "mitigate the gap, we conduct an empirical study to investigate the performance\n",
      "of widely-used RAG methods for code completion in the industrial-scale codebase\n",
      "of WeChat, one of the largest proprietary software systems. Specifically, we\n",
      "extensively explore two main types of RAG methods, namely identifier-based RAG\n",
      "and similarity-based RAG, across 26 open-source LLMs ranging from 0.5B to 671B\n",
      "parameters. For a more comprehensive analysis, we employ different retrieval\n",
      "techniques for similarity-based RAG, including lexical and semantic retrieval.\n",
      "Based on 1,669 internal repositories, we achieve several key findings: (1) both\n",
      "RAG methods demonstrate effectiveness in closed-source repositories, with\n",
      "similarity-based RAG showing superior performance, (2) the effectiveness of\n",
      "similarity-based RAG improves with more advanced retrieval techniques, where\n",
      "BM25 (lexical retrieval) and GTE-Qwen (semantic retrieval) achieve superior\n",
      "performance, and (3) the combination of lexical and semantic retrieval\n",
      "techniques yields optimal results, demonstrating complementary strengths.\n",
      "Furthermore, we conduct a developer survey to validate the practical utility of\n",
      "RAG methods in real-world development environments.\n",
      "</Summary>\n",
      "\n",
      "<Content>\n",
      "A Deep Dive into Retrieval-Augmented Generation\n",
      "for Code Completion: Experience on WeChat\n",
      "Zezhou Yang\n",
      "Tencent\n",
      "Guangzhou, China\n",
      "zezhouyang@tencent.com\n",
      "Ting Peng\n",
      "Tencent\n",
      "Guangzhou, China\n",
      "sakurapeng@tencent.com\n",
      "Cuiyun Gao∗\n",
      "The Chinese University of Hong Kong\n",
      "Hong Kong, China\n",
      "cuiyungao@outlook.com\n",
      "Chaozheng Wang\n",
      "The Chinese University of Hong Kong\n",
      "Hong Kong, China\n",
      "adf111178@gmail.com\n",
      "Hailiang Huang\n",
      "Tencent\n",
      "Guangzhou, China\n",
      "eraserhuang@tencent.com\n",
      "Yuetang Deng\n",
      "Tencent\n",
      "Guangzhou, China\n",
      "yuetangdeng@tencent.com\n",
      "Abstract—Code completion, a crucial task in software engi-\n",
      "neering that enhances developer productivity, has seen substantial\n",
      "improvements with the rapid advancement of large language\n",
      "models (LLMs). In recent years, retrieval-augmented generation\n",
      "(RAG) has emerged as a promising method to enhance the code\n",
      "completion capabilities of LLMs, which leverages relevant con-\n",
      "text from codebases without requiring model retraining. While\n",
      "existing studies have demonstrated the effectiveness of RAG on\n",
      "public repositories and benchmarks, the potential distribution\n",
      "shift between open-source and closed-source codebases presents\n",
      "unique challenges that remain unexplored. To mitigate the gap,\n",
      "we conduct an empirical study to investigate the performance of\n",
      "widely-used RAG methods for code completion in the industrial-\n",
      "scale codebase of WeChat, one of the largest proprietary software\n",
      "systems. Specifically, we extensively explore two main types\n",
      "of RAG methods, namely identifier-based RAG and similarity-\n",
      "based RAG, across 26 open-source LLMs ranging from 0.5B\n",
      "to 671B parameters. For a more comprehensive analysis, we\n",
      "employ different retrieval techniques for similarity-based RAG,\n",
      "including lexical and semantic retrieval. Based on 1,669 internal\n",
      "repositories, we achieve several key findings: (1) both RAG\n",
      "methods demonstrate effectiveness in closed-source repositories,\n",
      "with similarity-based RAG showing superior performance, (2)\n",
      "the effectiveness of similarity-based RAG improves with more\n",
      "advanced retrieval techniques, where BM25 (lexical retrieval)\n",
      "and GTE-Qwen (semantic retrieval) achieve superior perfor-\n",
      "mance, and (3) the combination of lexical and semantic retrieval\n",
      "techniques yields optimal results, demonstrating complemen-\n",
      "tary strengths. Furthermore, we conduct a developer survey\n",
      "to validate the practical utility of RAG methods in real-world\n",
      "development environments.\n",
      "Index Terms—large language model, retrieval-augmented gen-\n",
      "eration, code completion\n",
      "I. INTRODUCTION\n",
      "Code completion, which automatically predicts and sug-\n",
      "gests code fragments based on the surrounding programming\n",
      "context, has evolved from simple token-level suggestions to\n",
      "∗Cuiyun Gao is the corresponding author.\n",
      "This research is supported by National Key R&D Program of China\n",
      "(No. 2022YFB3103900), National Natural Science Foundation of China\n",
      "under project (No. 62472126), Natural Science Foundation of Guangdong\n",
      "Province (Project No. 2023A1515011959), Shenzhen-Hong Kong Jointly\n",
      "Funded Project (Category A, No. SGDX20230116091246007), and Shenzhen\n",
      "Basic Research (General Project No. JCYJ20220531095214031).\n",
      "generating entire code blocks [1], [2]. Studies have demon-\n",
      "strated that code completion tools substantially enhance de-\n",
      "veloper productivity in real-world software development [3],\n",
      "[4]. Notably, 87% of professional developers report significant\n",
      "improvements in their coding efficiency when utilizing code\n",
      "completion tools in industrial settings [5]. Recent advances\n",
      "in large language models (LLMs) have further transformed\n",
      "various software engineering tasks [6]–[10], demonstrating\n",
      "unprecedented capabilities in code understanding and gen-\n",
      "eration. These models have achieved particularly impressive\n",
      "performance in code completion tasks [11]–[13].\n",
      "To enhance LLMs’ performance on domain-specific tasks,\n",
      "researchers have explored Retrieval-Augmented Generation\n",
      "(RAG), which augments model inference by retrieving and in-\n",
      "corporating relevant context from the target codebase without\n",
      "requiring param\n",
      "</Content>\n",
      "</Document>\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36msearch_web\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "<Document href=\"https://rabiloo.com/blog/the-3-types-of-rag-models-naive-rag-modular-rag-and-advanced-rag\"/>\n",
      "Building AI systems requires choosing the right tools for the job, and Retrieval-Augmented Generation (RAG) offers various models, each designed to serve different needs. Essentially, a RAG model retrieves information from external databases, knowledge bases, or the web and then uses that data to generate relevant, meaningful responses. Naive RAG begins by indexing the data source for quick retrieval of relevant information. Modular RAG builds on the basic principles of Naive RAG by breaking down the retrieval and generation processes into separate, specialized modules. Unlike Naive RAG or Modular RAG, this stage is more precise and contextually aware, ensuring that the system retrieves only the most relevant and contextually appropriate data. The generation module in Advanced RAG then processes the refined data using sophisticated models like T5 or GPT-3.\n",
      "</Document>\n",
      "\n",
      "---\n",
      "\n",
      "<Document href=\"https://www.superteams.ai/blog/how-to-implement-naive-rag-advanced-rag-and-modular-rag\"/>\n",
      "Naive RAG is a paradigm that combines information retrieval with natural language generation to produce responses to queries or prompts. In Naive RAG, retrieval is typically performed using retrieval models that rank the indexed data based on its relevance to the input query. These models generate text based on the input query and the retrieved context, aiming to produce coherent and contextually relevant responses. Advanced RAG models may fine-tune embeddings to capture task-specific semantics or domain knowledge, thereby improving the quality of retrieved information and generated responses. Dynamic embedding techniques enable RAG models to adaptively adjust embeddings during inference based on the context of the query or retrieved information.\n",
      "</Document>\n",
      "\n",
      "---\n",
      "\n",
      "<Document href=\"https://medium.com/@drjulija/what-are-naive-rag-advanced-rag-modular-rag-paradigms-edff410c202e\"/>\n",
      "I share the differences between Naive RAG, Advanced RAG and Modular RAG frameworks. Retrieval-Augmented Generation (RAG) framework is designed to help mitigate these challenges. RAG enhances LLMs’ knowledge base with additional, domain-specific data. RAG systems can be split into 3 categories: See the comparison between all three paradigms of RAG — Naive RAG, Advanced RAG and Modular RAG below.\n",
      "</Document>\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36msearch_arxiv\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "<Document source=\"http://arxiv.org/abs/2505.13006v1\" date=\"2025-05-19\" authors=\"Yuyang Li, Philip J. M. Kerbusch, Raimon H. R. Pruim, Tobias Käfer\"/>\n",
      "<Title>\n",
      "Evaluating the Performance of RAG Methods for Conversational AI in the Airport Domain\n",
      "</Title>\n",
      "\n",
      "<Summary>\n",
      "Airports from the top 20 in terms of annual passengers are highly dynamic\n",
      "environments with thousands of flights daily, and they aim to increase the\n",
      "degree of automation. To contribute to this, we implemented a Conversational AI\n",
      "system that enables staff in an airport to communicate with flight information\n",
      "systems. This system not only answers standard airport queries but also\n",
      "resolves airport terminology, jargon, abbreviations, and dynamic questions\n",
      "involving reasoning. In this paper, we built three different\n",
      "Retrieval-Augmented Generation (RAG) methods, including traditional RAG, SQL\n",
      "RAG, and Knowledge Graph-based RAG (Graph RAG). Experiments showed that\n",
      "traditional RAG achieved 84.84% accuracy using BM25 + GPT-4 but occasionally\n",
      "produced hallucinations, which is risky to airport safety. In contrast, SQL RAG\n",
      "and Graph RAG achieved 80.85% and 91.49% accuracy respectively, with\n",
      "significantly fewer hallucinations. Moreover, Graph RAG was especially\n",
      "effective for questions that involved reasoning. Based on our observations, we\n",
      "thus recommend SQL RAG and Graph RAG are better for airport environments, due\n",
      "to fewer hallucinations and the ability to handle dynamic questions.\n",
      "</Summary>\n",
      "\n",
      "<Content>\n",
      "arXiv:2505.13006v1  [cs.CL]  19 May 2025\n",
      "Evaluating the Performance of RAG Methods for Conversational AI in the\n",
      "Airport Domain\n",
      "Yuyang Li1, Philip J.M. Kerbusch2, Raimon H.R. Pruim2, Tobias Käfer1\n",
      "1Karlsruhe Institute of Technology, 2Royal Schiphol Group\n",
      "2Royal Schiphol Group, 1Karlsruhe Institute of Technology\n",
      "yuyang.li@kit.edu,\n",
      "tobias.kaefer@kit.edu\n",
      "Abstract\n",
      "Airports from the top 20 in terms of annual\n",
      "passengers are highly dynamic environments\n",
      "with thousands of flights daily, and they aim\n",
      "to increase the degree of automation. To con-\n",
      "tribute to this, we implemented a Conversa-\n",
      "tional AI system that enables staff in an air-\n",
      "port to communicate with flight information\n",
      "systems. This system not only answers stan-\n",
      "dard airport queries but also resolves airport ter-\n",
      "minology, jargon, abbreviations, and dynamic\n",
      "questions involving reasoning. In this paper, we\n",
      "built three different Retrieval-Augmented Gen-\n",
      "eration (RAG) methods, including traditional\n",
      "RAG, SQL RAG, and Knowledge Graph-based\n",
      "RAG (Graph RAG). Experiments showed that\n",
      "traditional RAG achieved 84.84% accuracy\n",
      "using BM25 + GPT-4 but occasionally pro-\n",
      "duced hallucinations, which is risky to airport\n",
      "safety. In contrast, SQL RAG and Graph RAG\n",
      "achieved 80.85% and 91.49% accuracy respec-\n",
      "tively, with significantly fewer hallucinations.\n",
      "Moreover, Graph RAG was especially effective\n",
      "for questions that involved reasoning. Based\n",
      "on our observations, we thus recommend SQL\n",
      "RAG and Graph RAG are better for airport en-\n",
      "vironments, due to fewer hallucinations and the\n",
      "ability to handle dynamic questions.\n",
      "1\n",
      "Introduction\n",
      "Amsterdam Airport Schiphol, one of the top 20\n",
      "airports in the world, ranked by annual passenger\n",
      "numbers, handles thousands of flights each day.\n",
      "These airports rely on staff like gate planners and\n",
      "apron controllers to access and update data across\n",
      "systems. For these employees, traditional database\n",
      "queries can be complex and time-consuming for\n",
      "some employees who are not query experts when\n",
      "they need flight information. A conversational AI\n",
      "system with a natural language query (NLQ) inter-\n",
      "face allows all employees to interact with systems\n",
      "naturally, asking questions like, “Which fights are\n",
      "at ramp D07?” and receiving instant answers. This\n",
      "improves productivity, and streamlines workflows,\n",
      "especially in high-pressure areas like at the gate,\n",
      "where less educated workers require access to up-\n",
      "to-date information. By replacing strict query for-\n",
      "mats with intuitive, real-time responses, conversa-\n",
      "tional AI enhances decision-making and efficiency,\n",
      "making it a suitable solution for dynamic environ-\n",
      "ments such as airports.\n",
      "Building such a system is challenging because\n",
      "flight data is stored by experts in tables using avi-\n",
      "ation abbreviations. We need our system to un-\n",
      "derstand these datasets to answer questions from\n",
      "the airport domain. Additionally, ensuring avia-\n",
      "tion safety is a major concern; the system must\n",
      "be safe and enable employees to perform accurate\n",
      "operations. We address those challenges using two\n",
      "research questions.\n",
      "The first question is how to handle flight data so\n",
      "that our system can answer different questions. We\n",
      "divided the questions into three types:\n",
      "• Straightforward questions: Questions that\n",
      "can be directly answered from the flight data.\n",
      "• Questions involving specialized airport jar-\n",
      "gon, abbreviations, and incomplete queries:\n",
      "Operators often use shorthand or omit con-\n",
      "text. Flight “KL0123” might be referred to as\n",
      "“0123” or “123,” while gate “C05” might be\n",
      "shortened to “C5.” Abbreviations like “KLM”\n",
      "for “KLM Royal Dutch Airlines” or “Delta”\n",
      "for “Delta Air Lines” are also common. Op-\n",
      "erators frequently ask short, incomplete ques-\n",
      "tions, e. g., “Which flights are at D04?” or\n",
      "“What is the gate for that Delta airline?” With-\n",
      "out resolving missing details such, these ques-\n",
      "tions cannot be answered.\n",
      "• Dynamic questions: Questions that involve\n",
      "additional calculations and reasoning, espe-\n",
      "cially related to time.\n",
      "Examples include\n",
      "“What is the connecting flight’s onramp time\n",
      "for D\n",
      "</Content>\n",
      "</Document>\n",
      "\n",
      "---\n",
      "\n",
      "<Document source=\"http://arxiv.org/abs/2503.02922v1\" date=\"2025-03-04\" authors=\"Joyce Cahoon, Prerna Singh, Nick Litombe, Jonathan Larson, Ha Trinh, Yiwen Zhu, Andreas Mueller, Fotis Psallidas, Carlo Curino\"/>\n",
      "<Title>\n",
      "Optimizing open-domain question answering with graph-based retrieval augmented generation\n",
      "</Title>\n",
      "\n",
      "<Summary>\n",
      "In this work, we benchmark various graph-based retrieval-augmented generation\n",
      "(RAG) systems across a broad spectrum of query types, including OLTP-style\n",
      "(fact-based) and OLAP-style (thematic) queries, to address the complex demands\n",
      "of open-domain question answering (QA). Traditional RAG methods often fall\n",
      "short in handling nuanced, multi-document synthesis tasks. By structuring\n",
      "knowledge as graphs, we can facilitate the retrieval of context that captures\n",
      "greater semantic depth and enhances language model operations. We explore\n",
      "graph-based RAG methodologies and introduce TREX, a novel, cost-effective\n",
      "alternative that combines graph-based and vector-based retrieval techniques.\n",
      "Our benchmarking across four diverse datasets highlights the strengths of\n",
      "different RAG methodologies, demonstrates TREX's ability to handle multiple\n",
      "open-domain QA types, and reveals the limitations of current evaluation\n",
      "methods.\n",
      "  In a real-world technical support case study, we demonstrate how TREX\n",
      "solutions can surpass conventional vector-based RAG in efficiently synthesizing\n",
      "data from heterogeneous sources. Our findings underscore the potential of\n",
      "augmenting large language models with advanced retrieval and orchestration\n",
      "capabilities, advancing scalable, graph-based AI solutions.\n",
      "</Summary>\n",
      "\n",
      "<Content>\n",
      "Optimizing open-domain question answering with graph-based\n",
      "retrieval augmented generation\n",
      "Joyce Cahoon\n",
      "jcahoon@microsoft.com\n",
      "Microsoft\n",
      "Redmond, WA, USA\n",
      "Prerna Singh\n",
      "prernasingh@microsoft.com\n",
      "Microsoft\n",
      "Redmond, WA, USA\n",
      "Nick Litombe\n",
      "nicklitombe@microsoft.com\n",
      "Microsoft\n",
      "Austin, TX, USA\n",
      "Jonathan Larson\n",
      "jolarso@microsoft.com\n",
      "Microsoft\n",
      "Redmond, WA, USA\n",
      "Ha Trinh\n",
      "trinhha@microsoft.com\n",
      "Microsoft\n",
      "Redmond, WA, USA\n",
      "Yiwen Zhu\n",
      "yiwzh@microsoft.com\n",
      "Microsoft\n",
      "Mountain View, CA, USA\n",
      "Andreas Mueller\n",
      "amueller@microsoft.com\n",
      "Microsoft\n",
      "Mountain View, CA, USA\n",
      "Fotis Psallidas\n",
      "fotis.psallidas@microsoft.com\n",
      "Microsoft\n",
      "New York, NY, USA\n",
      "Carlo Curino\n",
      "carlo.curino@microsoft.com\n",
      "Microsoft\n",
      "Redmond, WA, USA\n",
      "ABSTRACT\n",
      "In this work, we benchmark various graph-based retrieval-augmented\n",
      "generation (RAG) systems across a broad spectrum of query types,\n",
      "including OLTP-style (fact-based) and OLAP-style (thematic) queries,\n",
      "to address the complex demands of open-domain question answer-\n",
      "ing (QA). Traditional RAG methods often fall short in handling nu-\n",
      "anced, multi-document synthesis tasks. By structuring knowledge\n",
      "as graphs, we can facilitate the retrieval of context that captures\n",
      "greater semantic depth and enhances language model operations.\n",
      "We explore graph-based RAG methodologies and introduce TREX,\n",
      "a novel, cost-effective alternative that combines graph-based and\n",
      "vector-based retrieval techniques. Our benchmarking across four di-\n",
      "verse datasets highlights the strengths of different RAG methodolo-\n",
      "gies, demonstrates TREX’s ability to handle multiple open-domain\n",
      "QA types, and reveals the limitations of current evaluation methods.\n",
      "In a real-world technical support case study, we demonstrate\n",
      "how TREX solutions can surpass conventional vector-based RAG in\n",
      "efficiently synthesizing data from heterogeneous sources. Our find-\n",
      "ings underscore the potential of augmenting large language models\n",
      "with advanced retrieval and orchestration capabilities, advancing\n",
      "scalable, graph-based AI solutions.\n",
      "1\n",
      "INTRODUCTION\n",
      "Traditionally, knowledge workers—such as executives, managers,\n",
      "and analysts—relied on data warehousing and operational databases\n",
      "to make faster, more informed decisions [4]. Today, the range of\n",
      "decision support tools has expanded to include foundational mod-\n",
      "els, with AI software spending projected to reach $297.9 billion by\n",
      "2027 [15]. Although the modern knowledge worker operates in a\n",
      "vastly different environment, one not limited to carefully curated\n",
      "data warehouses with complex multi-dimensional models for on-\n",
      "line analytical processing (OLAP) or highly structured operational\n",
      "databases supporting online transaction processing (OLTP) [4], the\n",
      "core challenges and types of questions that these decision support\n",
      "systems address remain highly relevant.\n",
      "The need for decision support tools capable of processing vast\n",
      "amounts of unstructured data at scale is particularly evident in\n",
      "open-domain question answering (QA). This field, which spans Nat-\n",
      "ural Language Processing (NLP), Information Extraction (IE), and\n",
      "Information Retrieval (IR), focuses on answering questions without\n",
      "relying on predefined context [50]. Large language models (LLMs)\n",
      "such as GPT [42], Claude [2], and Llama [41] have emerged as\n",
      "powerful tools for this purpose, generating human-like responses\n",
      "to complex queries while processing extensive text inputs. In vari-\n",
      "ous enterprise applications that leverage LLMs, user queries can\n",
      "often be classified as either OLTP or OLAP. OLTP-style queries are\n",
      "simple, fact-based questions that can be answered through direct\n",
      "key-value lookups, retrieval from single text snippets, or by locally\n",
      "traversing multiple related pieces of text, while OLAP-style queries\n",
      "are open-ended, thematic, and require aggregating, synthesizing\n",
      "and abstracting information across multiple documents [47]. Just\n",
      "as operational databases are optimized for OLTP tasks and data\n",
      "warehouses for OLAP workloads, specialized LLM applications are\n",
      "now emerging to address these distinct query types, with tailored\n",
      "approach\n",
      "</Content>\n",
      "</Document>\n",
      "\n",
      "---\n",
      "\n",
      "<Document source=\"http://arxiv.org/abs/2412.05838v1\" date=\"2024-12-08\" authors=\"Aniruddha Salve, Saba Attar, Mahesh Deshmukh, Sayali Shivpuje, Arnab Mitra Utsab\"/>\n",
      "<Title>\n",
      "A Collaborative Multi-Agent Approach to Retrieval-Augmented Generation Across Diverse Data\n",
      "</Title>\n",
      "\n",
      "<Summary>\n",
      "Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by\n",
      "incorporating external, domain-specific data into the generative process. While\n",
      "LLMs are highly capable, they often rely on static, pre-trained datasets,\n",
      "limiting their ability to integrate dynamic or private data. Traditional RAG\n",
      "systems typically use a single-agent architecture to handle query generation,\n",
      "data retrieval, and response synthesis. However, this approach becomes\n",
      "inefficient when dealing with diverse data sources, such as relational\n",
      "databases, document stores, and graph databases, often leading to performance\n",
      "bottlenecks and reduced accuracy. This paper proposes a multi-agent RAG system\n",
      "to address these limitations. Specialized agents, each optimized for a specific\n",
      "data source, handle query generation for relational, NoSQL, and document-based\n",
      "systems. These agents collaborate within a modular framework, with query\n",
      "execution delegated to an environment designed for compatibility across various\n",
      "database types. This distributed approach enhances query efficiency, reduces\n",
      "token overhead, and improves response accuracy by ensuring that each agent\n",
      "focuses on its specialized task. The proposed system is scalable and adaptable,\n",
      "making it ideal for generative AI workflows that require integration with\n",
      "diverse, dynamic, or private data sources. By leveraging specialized agents and\n",
      "a modular execution environment, the system provides an efficient and robust\n",
      "solution for handling complex, heterogeneous data environments in generative AI\n",
      "applications.\n",
      "</Summary>\n",
      "\n",
      "<Content>\n",
      "A COLLABORATIVE MULTI-AGENT APPROACH TO\n",
      "RETRIEVAL-AUGMENTED GENERATION ACROSS DIVERSE DATA\n",
      "SOURCES\n",
      "Aniruddha Salve\n",
      "iASYS Technology Solutions Pvt. Ltd.\n",
      "Pune, Maharashtra, India\n",
      "aniruddha.salve@iasys.co.in\n",
      "Mahesh Deshmukh\n",
      "iASYS Technology Solutions Pvt. Ltd.\n",
      "Pune, Maharashtra, India\n",
      "mahesh.deshmukh@iasys.co.in\n",
      "Saba Attar\n",
      "SVPM’s College of Engineering\n",
      "Baramati, Pune, Maharashtra, India\n",
      "sabaattar1702@gmail.com\n",
      "Sayali Shivpuje\n",
      "SVPM’s College of Engineering\n",
      "Baramati, Pune, Maharashtra, India\n",
      "shivpujesayali.2243@gmail.com\n",
      "Arnab Mitra Utsab\n",
      "School of Data and Sciences\n",
      "Brac University\n",
      "Dhaka, Bangladesh\n",
      "arnab.mitra.utsab@g.bracu.ac.bd\n",
      "ABSTRACT\n",
      "Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by incorporating\n",
      "external, domain-specific data into the generative process. While LLMs are highly capable, they\n",
      "often rely on static, pre-trained datasets, limiting their ability to integrate dynamic or private data.\n",
      "Traditional RAG systems typically use a single-agent architecture to handle query generation, data\n",
      "retrieval, and response synthesis. However, this approach becomes inefficient when dealing with\n",
      "diverse data sources, such as relational databases, document stores, and graph databases, often leading\n",
      "to performance bottlenecks and reduced accuracy.\n",
      "This paper proposes a Multi-Agent RAG system to address these limitations. Specialized agents, each\n",
      "optimized for a specific data source, handle query generation for relational, NoSQL, and document-\n",
      "based systems. These agents collaborate within a modular framework, with query execution delegated\n",
      "to an environment designed for compatibility across various database types. This distributed approach\n",
      "enhances query efficiency, reduces token overhead, and improves response accuracy by ensuring that\n",
      "each agent focuses on its specialized task.\n",
      "The proposed system is scalable and adaptable, making it ideal for generative AI workflows that\n",
      "require integration with diverse, dynamic, or private data sources. By leveraging specialized agents\n",
      "and a modular execution environment, the system provides an efficient and robust solution for\n",
      "handling complex, heterogeneous data environments in generative AI applications.\n",
      "Keywords Multi-Agent RAG Systems · Retrieval-Augmented Generation · Large Language Models · Database\n",
      "Integration · Generative AI\n",
      "arXiv:2412.05838v1  [cs.AI]  8 Dec 2024\n",
      "1\n",
      "Introduction\n",
      "Large Language Models (LLMs) have significantly advanced natural language processing by enabling sophisticated\n",
      "query interpretation and text generation. [1, 8] Despite their capabilities, LLMs are limited by their reliance on static\n",
      "pre-trained datasets, which restricts their ability to incorporate dynamic, domain-specific, or private data into their\n",
      "responses. Retrieval-Augmented Generation (RAG) systems address this challenge by integrating external data retrieval\n",
      "with generative processes, providing more context-aware and accurate outputs.\n",
      "Traditional RAG systems typically employ single-agent architectures where a single system is responsible for query\n",
      "generation, data retrieval, and response synthesis. While effective for basic use cases, these monolithic designs often\n",
      "face limitations when dealing with diverse data sources, such as relational databases, document stores, and graph-based\n",
      "data [19]. These systems also require elaborate prompts containing schemas, examples, and user queries, leading to\n",
      "inefficiencies in token usage, increased processing latency, and potential inaccuracies in query handling.\n",
      "To overcome these challenges, this paper proposes a Multi-Agent RAG system [22, 23]. Unlike traditional approaches,\n",
      "this system delegates the task of query generation to specialized agents, each tailored to a specific type of database.\n",
      "These agents generate optimized, database-specific queries without directly executing or retrieving data. Queries are\n",
      "executed in a separate execution environment, ensuring compatibility with diverse data storage systems. The retrieved\n",
      "context is then com\n",
      "</Content>\n",
      "</Document>\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36msearch_arxiv\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "<Document source=\"http://arxiv.org/abs/2407.21059v1\" date=\"2024-07-26\" authors=\"Yunfan Gao, Yun Xiong, Meng Wang, Haofen Wang\"/>\n",
      "<Title>\n",
      "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks\n",
      "</Title>\n",
      "\n",
      "<Summary>\n",
      "Retrieval-augmented Generation (RAG) has markedly enhanced the capabilities\n",
      "of Large Language Models (LLMs) in tackling knowledge-intensive tasks. The\n",
      "increasing demands of application scenarios have driven the evolution of RAG,\n",
      "leading to the integration of advanced retrievers, LLMs and other complementary\n",
      "technologies, which in turn has amplified the intricacy of RAG systems.\n",
      "However, the rapid advancements are outpacing the foundational RAG paradigm,\n",
      "with many methods struggling to be unified under the process of\n",
      "\"retrieve-then-generate\". In this context, this paper examines the limitations\n",
      "of the existing RAG paradigm and introduces the modular RAG framework. By\n",
      "decomposing complex RAG systems into independent modules and specialized\n",
      "operators, it facilitates a highly reconfigurable framework. Modular RAG\n",
      "transcends the traditional linear architecture, embracing a more advanced\n",
      "design that integrates routing, scheduling, and fusion mechanisms. Drawing on\n",
      "extensive research, this paper further identifies prevalent RAG\n",
      "patterns-linear, conditional, branching, and looping-and offers a comprehensive\n",
      "analysis of their respective implementation nuances. Modular RAG presents\n",
      "innovative opportunities for the conceptualization and deployment of RAG\n",
      "systems. Finally, the paper explores the potential emergence of new operators\n",
      "and paradigms, establishing a solid theoretical foundation and a practical\n",
      "roadmap for the continued evolution and practical deployment of RAG\n",
      "technologies.\n",
      "</Summary>\n",
      "\n",
      "<Content>\n",
      "1\n",
      "Modular RAG: Transforming RAG Systems into\n",
      "LEGO-like Reconfigurable Frameworks\n",
      "Yunfan Gao, Yun Xiong, Meng Wang, Haofen Wang\n",
      "Abstract—Retrieval-augmented\n",
      "Generation\n",
      "(RAG)\n",
      "has\n",
      "markedly enhanced the capabilities of Large Language Models\n",
      "(LLMs) in tackling knowledge-intensive tasks. The increasing\n",
      "demands of application scenarios have driven the evolution\n",
      "of RAG, leading to the integration of advanced retrievers,\n",
      "LLMs and other complementary technologies, which in turn\n",
      "has amplified the intricacy of RAG systems. However, the rapid\n",
      "advancements are outpacing the foundational RAG paradigm,\n",
      "with many methods struggling to be unified under the process\n",
      "of “retrieve-then-generate”. In this context, this paper examines\n",
      "the limitations of the existing RAG paradigm and introduces\n",
      "the modular RAG framework. By decomposing complex RAG\n",
      "systems into independent modules and specialized operators, it\n",
      "facilitates a highly reconfigurable framework. Modular RAG\n",
      "transcends the traditional linear architecture, embracing a\n",
      "more advanced design that integrates routing, scheduling, and\n",
      "fusion mechanisms. Drawing on extensive research, this paper\n",
      "further identifies prevalent RAG patterns—linear, conditional,\n",
      "branching, and looping—and offers a comprehensive analysis\n",
      "of their respective implementation nuances. Modular RAG\n",
      "presents\n",
      "innovative\n",
      "opportunities\n",
      "for\n",
      "the\n",
      "conceptualization\n",
      "and deployment of RAG systems. Finally, the paper explores\n",
      "the potential emergence of new operators and paradigms,\n",
      "establishing a solid theoretical foundation and a practical\n",
      "roadmap for the continued evolution and practical deployment\n",
      "of RAG technologies.\n",
      "Index Terms—Retrieval-augmented generation, large language\n",
      "model, modular system, information retrieval\n",
      "I. INTRODUCTION\n",
      "L\n",
      "ARGE Language Models (LLMs) have demonstrated\n",
      "remarkable capabilities, yet they still face numerous\n",
      "challenges, such as hallucination and the lag in information up-\n",
      "dates [1]. Retrieval-augmented Generation (RAG), by access-\n",
      "ing external knowledge bases, provides LLMs with important\n",
      "contextual information, significantly enhancing their perfor-\n",
      "mance on knowledge-intensive tasks [2]. Currently, RAG, as\n",
      "an enhancement method, has been widely applied in various\n",
      "practical application scenarios, including knowledge question\n",
      "answering, recommendation systems, customer service, and\n",
      "personal assistants. [3]–[6]\n",
      "During the nascent stages of RAG , its core framework is\n",
      "constituted by indexing, retrieval, and generation, a paradigm\n",
      "referred to as Naive RAG [7]. However, as the complexity\n",
      "of tasks and the demands of applications have escalated, the\n",
      "Yunfan Gao is with Shanghai Research Institute for Intelligent Autonomous\n",
      "Systems, Tongji University, Shanghai, 201210, China.\n",
      "Yun Xiong is with Shanghai Key Laboratory of Data Science, School of\n",
      "Computer Science, Fudan University, Shanghai, 200438, China.\n",
      "Meng Wang and Haofen Wang are with College of Design and Innovation,\n",
      "Tongji University, Shanghai, 20092, China. (Corresponding author: Haofen\n",
      "Wang. E-mail: carter.whfcarter@gmail.com)\n",
      "limitations of Naive RAG have become increasingly apparent.\n",
      "As depicted in Figure 1, it predominantly hinges on the\n",
      "straightforward similarity of chunks, result in poor perfor-\n",
      "mance when confronted with complex queries and chunks with\n",
      "substantial variability. The primary challenges of Naive RAG\n",
      "include: 1) Shallow Understanding of Queries. The semantic\n",
      "similarity between a query and document chunk is not always\n",
      "highly consistent. Relying solely on similarity calculations\n",
      "for retrieval lacks an in-depth exploration of the relationship\n",
      "between the query and the document [8]. 2) Retrieval Re-\n",
      "dundancy and Noise. Feeding all retrieved chunks directly\n",
      "into LLMs is not always beneficial. Research indicates that\n",
      "an excess of redundant and noisy information may interfere\n",
      "with the LLM’s identification of key information, thereby\n",
      "increasing the risk of generating erroneous and hallucinated\n",
      "responses. [9]\n",
      "To overcome the aforementioned limitations, \n",
      "</Content>\n",
      "</Document>\n",
      "\n",
      "---\n",
      "\n",
      "<Document source=\"http://arxiv.org/abs/2505.13006v1\" date=\"2025-05-19\" authors=\"Yuyang Li, Philip J. M. Kerbusch, Raimon H. R. Pruim, Tobias Käfer\"/>\n",
      "<Title>\n",
      "Evaluating the Performance of RAG Methods for Conversational AI in the Airport Domain\n",
      "</Title>\n",
      "\n",
      "<Summary>\n",
      "Airports from the top 20 in terms of annual passengers are highly dynamic\n",
      "environments with thousands of flights daily, and they aim to increase the\n",
      "degree of automation. To contribute to this, we implemented a Conversational AI\n",
      "system that enables staff in an airport to communicate with flight information\n",
      "systems. This system not only answers standard airport queries but also\n",
      "resolves airport terminology, jargon, abbreviations, and dynamic questions\n",
      "involving reasoning. In this paper, we built three different\n",
      "Retrieval-Augmented Generation (RAG) methods, including traditional RAG, SQL\n",
      "RAG, and Knowledge Graph-based RAG (Graph RAG). Experiments showed that\n",
      "traditional RAG achieved 84.84% accuracy using BM25 + GPT-4 but occasionally\n",
      "produced hallucinations, which is risky to airport safety. In contrast, SQL RAG\n",
      "and Graph RAG achieved 80.85% and 91.49% accuracy respectively, with\n",
      "significantly fewer hallucinations. Moreover, Graph RAG was especially\n",
      "effective for questions that involved reasoning. Based on our observations, we\n",
      "thus recommend SQL RAG and Graph RAG are better for airport environments, due\n",
      "to fewer hallucinations and the ability to handle dynamic questions.\n",
      "</Summary>\n",
      "\n",
      "<Content>\n",
      "arXiv:2505.13006v1  [cs.CL]  19 May 2025\n",
      "Evaluating the Performance of RAG Methods for Conversational AI in the\n",
      "Airport Domain\n",
      "Yuyang Li1, Philip J.M. Kerbusch2, Raimon H.R. Pruim2, Tobias Käfer1\n",
      "1Karlsruhe Institute of Technology, 2Royal Schiphol Group\n",
      "2Royal Schiphol Group, 1Karlsruhe Institute of Technology\n",
      "yuyang.li@kit.edu,\n",
      "tobias.kaefer@kit.edu\n",
      "Abstract\n",
      "Airports from the top 20 in terms of annual\n",
      "passengers are highly dynamic environments\n",
      "with thousands of flights daily, and they aim\n",
      "to increase the degree of automation. To con-\n",
      "tribute to this, we implemented a Conversa-\n",
      "tional AI system that enables staff in an air-\n",
      "port to communicate with flight information\n",
      "systems. This system not only answers stan-\n",
      "dard airport queries but also resolves airport ter-\n",
      "minology, jargon, abbreviations, and dynamic\n",
      "questions involving reasoning. In this paper, we\n",
      "built three different Retrieval-Augmented Gen-\n",
      "eration (RAG) methods, including traditional\n",
      "RAG, SQL RAG, and Knowledge Graph-based\n",
      "RAG (Graph RAG). Experiments showed that\n",
      "traditional RAG achieved 84.84% accuracy\n",
      "using BM25 + GPT-4 but occasionally pro-\n",
      "duced hallucinations, which is risky to airport\n",
      "safety. In contrast, SQL RAG and Graph RAG\n",
      "achieved 80.85% and 91.49% accuracy respec-\n",
      "tively, with significantly fewer hallucinations.\n",
      "Moreover, Graph RAG was especially effective\n",
      "for questions that involved reasoning. Based\n",
      "on our observations, we thus recommend SQL\n",
      "RAG and Graph RAG are better for airport en-\n",
      "vironments, due to fewer hallucinations and the\n",
      "ability to handle dynamic questions.\n",
      "1\n",
      "Introduction\n",
      "Amsterdam Airport Schiphol, one of the top 20\n",
      "airports in the world, ranked by annual passenger\n",
      "numbers, handles thousands of flights each day.\n",
      "These airports rely on staff like gate planners and\n",
      "apron controllers to access and update data across\n",
      "systems. For these employees, traditional database\n",
      "queries can be complex and time-consuming for\n",
      "some employees who are not query experts when\n",
      "they need flight information. A conversational AI\n",
      "system with a natural language query (NLQ) inter-\n",
      "face allows all employees to interact with systems\n",
      "naturally, asking questions like, “Which fights are\n",
      "at ramp D07?” and receiving instant answers. This\n",
      "improves productivity, and streamlines workflows,\n",
      "especially in high-pressure areas like at the gate,\n",
      "where less educated workers require access to up-\n",
      "to-date information. By replacing strict query for-\n",
      "mats with intuitive, real-time responses, conversa-\n",
      "tional AI enhances decision-making and efficiency,\n",
      "making it a suitable solution for dynamic environ-\n",
      "ments such as airports.\n",
      "Building such a system is challenging because\n",
      "flight data is stored by experts in tables using avi-\n",
      "ation abbreviations. We need our system to un-\n",
      "derstand these datasets to answer questions from\n",
      "the airport domain. Additionally, ensuring avia-\n",
      "tion safety is a major concern; the system must\n",
      "be safe and enable employees to perform accurate\n",
      "operations. We address those challenges using two\n",
      "research questions.\n",
      "The first question is how to handle flight data so\n",
      "that our system can answer different questions. We\n",
      "divided the questions into three types:\n",
      "• Straightforward questions: Questions that\n",
      "can be directly answered from the flight data.\n",
      "• Questions involving specialized airport jar-\n",
      "gon, abbreviations, and incomplete queries:\n",
      "Operators often use shorthand or omit con-\n",
      "text. Flight “KL0123” might be referred to as\n",
      "“0123” or “123,” while gate “C05” might be\n",
      "shortened to “C5.” Abbreviations like “KLM”\n",
      "for “KLM Royal Dutch Airlines” or “Delta”\n",
      "for “Delta Air Lines” are also common. Op-\n",
      "erators frequently ask short, incomplete ques-\n",
      "tions, e. g., “Which flights are at D04?” or\n",
      "“What is the gate for that Delta airline?” With-\n",
      "out resolving missing details such, these ques-\n",
      "tions cannot be answered.\n",
      "• Dynamic questions: Questions that involve\n",
      "additional calculations and reasoning, espe-\n",
      "cially related to time.\n",
      "Examples include\n",
      "“What is the connecting flight’s onramp time\n",
      "for D\n",
      "</Content>\n",
      "</Document>\n",
      "\n",
      "---\n",
      "\n",
      "<Document source=\"http://arxiv.org/abs/2410.14167v1\" date=\"2024-10-18\" authors=\"Jiajing Chen, Runyuan Bao, Hongye Zheng, Zhen Qi, Jianjun Wei, Jiacheng Hu\"/>\n",
      "<Title>\n",
      "Optimizing Retrieval-Augmented Generation with Elasticsearch for Enhanced Question-Answering Systems\n",
      "</Title>\n",
      "\n",
      "<Summary>\n",
      "This study aims to improve the accuracy and quality of large-scale language\n",
      "models (LLMs) in answering questions by integrating Elasticsearch into the\n",
      "Retrieval Augmented Generation (RAG) framework. The experiment uses the\n",
      "Stanford Question Answering Dataset (SQuAD) version 2.0 as the test dataset and\n",
      "compares the performance of different retrieval methods, including traditional\n",
      "methods based on keyword matching or semantic similarity calculation, BM25-RAG\n",
      "and TF-IDF- RAG, and the newly proposed ES-RAG scheme. The results show that\n",
      "ES-RAG not only has obvious advantages in retrieval efficiency but also\n",
      "performs well in key indicators such as accuracy, which is 0.51 percentage\n",
      "points higher than TF-IDF-RAG. In addition, Elasticsearch's powerful search\n",
      "capabilities and rich configuration options enable the entire\n",
      "question-answering system to better handle complex queries and provide more\n",
      "flexible and efficient responses based on the diverse needs of users. Future\n",
      "research directions can further explore how to optimize the interaction\n",
      "mechanism between Elasticsearch and LLM, such as introducing higher-level\n",
      "semantic understanding and context-awareness capabilities, to achieve a more\n",
      "intelligent and humanized question-answering experience.\n",
      "</Summary>\n",
      "\n",
      "<Content>\n",
      "Optimizing Retrieval-Augmented Generation with\n",
      "Elasticsearch for Enhanced Question-Answering\n",
      "Systems\n",
      "Jiajing Chen\n",
      "New York University\n",
      "New York, USA\n",
      "Zhen Qi\n",
      "Northeastern University\n",
      "Boston, USA\n",
      "Runyuan Bao\n",
      "Johns Hopkins University\n",
      "Baltimore, USA\n",
      "Jianjun Wei\n",
      "Washington University in St. Louis\n",
      "St Louis, USA\n",
      "The Chinese University of Hong\n",
      "Kong\n",
      "Hong Kong, China\n",
      "Abstract—This study aims to improve the accuracy and\n",
      "quality\n",
      "of\n",
      "large-scale\n",
      "language\n",
      "models\n",
      "(LLMs)\n",
      "in\n",
      "answering questions by integrating Elasticsearch into the\n",
      "Retrieval Augmented Generation (RAG) framework. The\n",
      "experiment uses the Stanford Question Answering Dataset\n",
      "(SQuAD) version 2.0 as the test dataset and compares the\n",
      "performance of different retrieval methods, including\n",
      "traditional\n",
      "methods\n",
      "based\n",
      "on\n",
      "keyword\n",
      "matching\n",
      "or\n",
      "semantic similarity calculation, BM25-RAG and TF-IDF-\n",
      "RAG, and the newly proposed ES-RAG scheme. The\n",
      "results show that ES-RAG not only has obvious advantages\n",
      "in retrieval efficiency but also performs well in key\n",
      "indicators such as accuracy, which is 0.51 percentage\n",
      "points\n",
      "higher\n",
      "than\n",
      "TF-IDF-RAG.\n",
      "In\n",
      "addition,\n",
      "Elasticsearch's\n",
      "powerful\n",
      "search\n",
      "capabilities\n",
      "and\n",
      "rich\n",
      "configuration options enable the entire question-answering\n",
      "system to better handle complex queries and provide more\n",
      "flexible and efficient responses based on the diverse needs\n",
      "of users. Future research directions can further explore\n",
      "how to optimize the interaction mechanism between\n",
      "Elasticsearch and LLM, such as introducing higher-level\n",
      "semantic\n",
      "understanding\n",
      "and\n",
      "context-awareness\n",
      "capabilities, to achieve a more intelligent and humanized\n",
      "question-answering experience.\n",
      "Keywords-Retrieval-enhanced generation, Elasticsearch,\n",
      "Large language models, Information retrieval\n",
      "I.\n",
      "INTRODUCTION\n",
      "In today's era of information explosion, efficiently extracting\n",
      "valuable information from massive data has become a crucial\n",
      "issue. With the continuous advancement of natural language\n",
      "processing technology, Large Language Models (LLMs) have\n",
      "shown unprecedented application potential in many fields with\n",
      "their powerful text understanding and generation capabilities\n",
      "[1]. However, although LLMs are able to answer questions or\n",
      "complete tasks based on their internally learned knowledge\n",
      "base, they often have limitations in mastering the latest or\n",
      "specific domain knowledge [2]. To solve this problem,\n",
      "researchers\n",
      "proposed\n",
      "a\n",
      "method\n",
      "that\n",
      "combines\n",
      "retrieval-\n",
      "augmented\n",
      "generation\n",
      "(RAG),\n",
      "which\n",
      "combines\n",
      "external\n",
      "document retrieval with LLM so that the model can take\n",
      "advantage of more information when generating responses. An\n",
      "extensive and up-to-date information source significantly\n",
      "improves the quality and accuracy of answers [3]. The core\n",
      "idea of RAG is that when faced with a query, the retrieval\n",
      "system is first used to find the most relevant documents from a\n",
      "large-scale document collection, and then these documents are\n",
      "provided to LLM as context to help it better understand the\n",
      "background of the problem. And generate more accurate\n",
      "answers accordingly [4].\n",
      "The traditional RAG framework usually uses a method based\n",
      "on keyword matching or semantic similarity calculation to\n",
      "implement the document retrieval process. Although this\n",
      "method has achieved certain success, it has difficulty in\n",
      "processing complex queries, improving the recall rate, and\n",
      "ensuring the diversity of retrieval results. There are still certain\n",
      "limitations. To this end, this article proposes an innovative\n",
      "improvement plan—that is, using Elasticsearch (ES) as the core\n",
      "search engine in RAG. Elasticsearch is an open-source\n",
      "distributed search and analysis engine. It not only supports full-\n",
      "text search but also provides rich aggregation functions to\n",
      "quickly and accurately index and retrieve unstructured data.\n",
      "Compared with traditional methods, using Elasticsearch can\n",
      "define query conditions more flexibly, support multiple types\n",
      "of field types such as text, numbers, etc., and have powerful\n",
      "sorting\n",
      "capabilities\n",
      "and\n",
      "efficient\n",
      "performance,\n",
      "which\n",
      "is\n",
      "important for improving the overall\n",
      "</Content>\n",
      "</Document>\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36msearch_web\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "<Document href=\"https://rabiloo.com/blog/the-3-types-of-rag-models-naive-rag-modular-rag-and-advanced-rag\"/>\n",
      "Building AI systems requires choosing the right tools for the job, and Retrieval-Augmented Generation (RAG) offers various models, each designed to serve different needs. Essentially, a RAG model retrieves information from external databases, knowledge bases, or the web and then uses that data to generate relevant, meaningful responses. Naive RAG begins by indexing the data source for quick retrieval of relevant information. Modular RAG builds on the basic principles of Naive RAG by breaking down the retrieval and generation processes into separate, specialized modules. Unlike Naive RAG or Modular RAG, this stage is more precise and contextually aware, ensuring that the system retrieves only the most relevant and contextually appropriate data. The generation module in Advanced RAG then processes the refined data using sophisticated models like T5 or GPT-3.\n",
      "</Document>\n",
      "\n",
      "---\n",
      "\n",
      "<Document href=\"https://adasci.org/how-does-modular-rag-improve-upon-naive-rag/\"/>\n",
      "Naive RAG, the initial implementation of Retrieval-Augmented Generation, operates on a straightforward principle: retrieve relevant documents from an external knowledge base and use these documents to inform the generative process. The retrieval process in Naive RAG is relatively static and lacks flexibility, often leading to inefficiencies and suboptimal integration with the generative model. By adopting a modular architecture, this approach addresses the limitations of Naive RAG, offering enhanced flexibility, scalability, and efficiency. Unlike Naive RAG, which operates as a monolithic entity, Modular RAG breaks down the retrieval and generation processes into distinct, interchangeable modules. * *Seamless Integration*: Generative models in Modular RAG are designed to seamlessly integrate with various retrieval modules, enhancing the coherence and relevance of generated responses.\n",
      "</Document>\n",
      "\n",
      "---\n",
      "\n",
      "<Document href=\"https://medium.com/@drjulija/what-are-naive-rag-advanced-rag-modular-rag-paradigms-edff410c202e\"/>\n",
      "I share the differences between Naive RAG, Advanced RAG and Modular RAG frameworks. Retrieval-Augmented Generation (RAG) framework is designed to help mitigate these challenges. RAG enhances LLMs’ knowledge base with additional, domain-specific data. RAG systems can be split into 3 categories: See the comparison between all three paradigms of RAG — Naive RAG, Advanced RAG and Modular RAG below.\n",
      "</Document>\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36manswer_question\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: expert\n",
      "\n",
      "Certainly, let's delve into the distinctions between Modular RAG and Naive RAG, particularly focusing on their applications and benefits in production environments.\n",
      "\n",
      "Naive RAG combines information retrieval with natural language generation to answer queries or prompts. It typically uses retrieval models that rank indexed data based on relevance to the input query. This approach generates responses by combining the input query with the retrieved context, aiming for coherence and relevance [3].\n",
      "\n",
      "In contrast, Modular RAG builds upon the basic principles of Naive RAG. It decomposes the retrieval and generation processes into separate, specialized modules. This separation allows each module to be optimized independently, offering greater control over each component of the RAG process. This modular structure enhances the precision and contextual awareness of the system, ensuring that the retrieved data is not only relevant but also contextually appropriate. The specialized design of each module allows for integration with sophisticated models, such as T5 or GPT-3, in the generation phase [4][5].\n",
      "\n",
      "**Benefits of Modular RAG in Production:**\n",
      "\n",
      "1. **Improved Precision and Contextual Relevance:**\n",
      "   - Modular RAG's separate modules allow for more precise retrieval, ensuring that only the most relevant and contextually suitable data is drawn for generation. This reduces the noise and potential inaccuracies that can arise from a single, monolithic approach [4].\n",
      "\n",
      "2. **Scalability and Flexibility:**\n",
      "   - The modular nature means that components can be scaled or modified independently without affecting the entire system. This flexibility is particularly beneficial in dynamic production environments where specific tasks may require different handling or specialized models [4][5].\n",
      "\n",
      "3. **Enhanced Efficiency:**\n",
      "   - By optimizing each module independently, Modular RAG can achieve efficiencies that aren't possible with Naive RAG. This results in reduced latency and improved response times, which are critical in real-world applications where speed and reliability are paramount [4].\n",
      "\n",
      "In summary, while Naive RAG offers a simpler, integrated approach to combining retrieval and generation, Modular RAG provides a more refined, flexible framework that improves accuracy, efficiency, and contextual relevance—key factors that enhance performance in production settings.\n",
      "\n",
      "Sources:\n",
      "[3] https://www.superteams.ai/blog/how-to-implement-naive-rag-advanced-rag-and-modular-rag\n",
      "[4] https://rabiloo.com/blog/the-3-types-of-rag-models-naive-rag-modular-rag-and-advanced-rag\n",
      "[5] https://medium.com/@drjulija/what-are-naive-rag-advanced-rag-modular-rag-paradigms-edff410c202e\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36manswer_question\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: expert\n",
      "\n",
      "Certainly, Prof. Hinton. Modular RAG differentiates from the traditional Naive RAG mainly in its architecture and approach to the retrieval and generation processes.\n",
      "\n",
      "1. **Architecture**: Naive RAG operates on a straightforward \"retrieve-then-generate\" model, where the retrieval process is relatively static and lacks flexibility. It functions as a single, monolithic entity [5]. In contrast, Modular RAG decomposes the retrieval and generation into independent modules and specialized operators, promoting a highly reconfigurable and scalable framework [1][4]. This modular approach allows for seamless integration of various retrieval techniques with generative models, enhancing the coherence and relevance of generated responses [5].\n",
      "\n",
      "2. **Flexibility and Scalability**: The modular nature of Modular RAG allows for interchangeable modules that can be independently updated or replaced, enabling easier adaptation to specific task requirements and technological advancements. This is in contrast with Naive RAG which is less adaptable to evolving needs [5].\n",
      "\n",
      "3. **Efficiency**: Modular RAG introduces advanced design elements like routing, scheduling, and fusion mechanisms. These elements optimize the flow of information and improve system efficiency, especially when handling complex queries, compared to the linear architecture of Naive RAG [1].\n",
      "\n",
      "The benefits of using Modular RAG in production environments include:\n",
      "\n",
      "- **Increased Efficiency**: By supporting routing, scheduling, and fusion mechanisms, Modular RAG can handle complex and diverse queries efficiently, thus improving the performance on knowledge-intensive tasks [1].\n",
      "\n",
      "- **Scalability**: The ability to reconfigure modules allows enterprises to scale the system up or down based on their computational and task-specific demands without overhauling the entire system [1].\n",
      "\n",
      "- **Enhanced Integration**: Modular RAG's design facilitates better integration with different retrieval mechanisms and LLMs, leading to more accurate and contextually aware responses [4][5].\n",
      "\n",
      "In sum, the Modular RAG framework offers a comprehensive roadmap for advancing RAG systems in a flexible, efficient, and scalable manner, making it well-suited for deployment in dynamic production environments.\n",
      "\n",
      "**Sources:**\n",
      "\n",
      "[1] http://arxiv.org/abs/2407.21059v1\n",
      "[4] https://rabiloo.com/blog/the-3-types-of-rag-models-naive-rag-modular-rag-and-advanced-rag\n",
      "[5] https://adasci.org/how-does-modular-rag-improve-upon-naive-rag/\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mask_question\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "That's a comprehensive overview! I’m curious, could you provide a specific example or case where implementing Modular RAG in a production environment significantly reduced latency compared to Naive RAG? How was the real-world reliability enhanced in this case?\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mask_question\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Thank you for the detailed explanation, Prof. Hinton. It’s fascinating how Modular RAG’s flexibility and efficiency potentially transform production applications. Could you provide a specific example or a case study of a production environment where Modular RAG has notably improved performance over Naive RAG?\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36manswer_question\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: expert\n",
      "\n",
      "Certainly, Dr. Torres. Modular RAG (Retrieval-Augmented Generation) diverges from the traditional Naive RAG approach by breaking down the retrieval and generation processes into separate, specialized modules. This design promotes scalability and efficiency, especially when dealing with large datasets.\n",
      "\n",
      "**Differences between Modular RAG and Naive RAG:**\n",
      "\n",
      "1. **Architecture**: \n",
      "   - **Naive RAG** operates as a single monolithic system where retrieval and generation processes are intertwined, potentially leading to inefficiencies when scaling [2].\n",
      "   - **Modular RAG**, on the other hand, divides these tasks into dedicated modules. This separation allows each module to be optimized individually, improving system performance and accuracy [3].\n",
      "\n",
      "2. **Scalability**:\n",
      "   - The modular approach enhances scalability because it can efficiently manage larger datasets by leveraging specialized units that focus on specific tasks. This is particularly beneficial in environments requiring the integration of diverse, extensive, or dynamic data sources [3].\n",
      "   \n",
      "3. **Adaptability**:\n",
      "   - Modular RAG is more adaptable to production-level scenarios due to its flexibility in module management. It can easily incorporate or update modules without overhauling the entire system, unlike Naive RAG, which might require significant restructuring when upgrading components or adding new functionalities [4].\n",
      "\n",
      "**Benefits of Using Modular RAG at the Production Level:**\n",
      "\n",
      "- **Enhanced Query Efficiency**: By delegating specific queries to modules optimized for different data sources (relational, NoSQL, etc.), Modular RAG reduces token overhead and processing latency, thereby speeding up response times and improving accuracy [3].\n",
      "- **Robustness and Flexibility**: Its architecture allows for easy integration of new data sources or adjustment to changing data types, making it highly suitable for dynamic production environments such as those involving real-time data updates [3].\n",
      "- **Improved Contextual Awareness**: Modular RAG can achieve better semantic depth by refining the retrieval process to ensure only the most pertinent and contextually appropriate data is processed [2].\n",
      "\n",
      "In summary, by structuring retrieval and generation into distinct, specialized tasks, Modular RAG offers significant scalability benefits, making it well-suited for complex, large-scale production environments. This modularity not only enhances performance but also facilitates adaptability to ever-changing organizational data needs [3].\n",
      "\n",
      "**Sources**:\n",
      "- [2] https://rabiloo.com/blog/the-3-types-of-rag-models-naive-rag-modular-rag-and-advanced-rag\n",
      "- [3] http://arxiv.org/abs/2412.05838v1\n",
      "- [4] https://www.digitalocean.com/community/conceptual-articles/rag-ai-agents-agentic-rag-comparative-analysis\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36msearch_arxiv\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "<Document source=\"http://arxiv.org/abs/2408.05933v1\" date=\"2024-08-12\" authors=\"Fei Liu, Zejun Kang, Xing Han\"/>\n",
      "<Title>\n",
      "Optimizing RAG Techniques for Automotive Industry PDF Chatbots: A Case Study with Locally Deployed Ollama Models\n",
      "</Title>\n",
      "\n",
      "<Summary>\n",
      "With the growing demand for offline PDF chatbots in automotive industrial\n",
      "production environments, optimizing the deployment of large language models\n",
      "(LLMs) in local, low-performance settings has become increasingly important.\n",
      "This study focuses on enhancing Retrieval-Augmented Generation (RAG) techniques\n",
      "for processing complex automotive industry documents using locally deployed\n",
      "Ollama models. Based on the Langchain framework, we propose a multi-dimensional\n",
      "optimization approach for Ollama's local RAG implementation. Our method\n",
      "addresses key challenges in automotive document processing, including\n",
      "multi-column layouts and technical specifications. We introduce improvements in\n",
      "PDF processing, retrieval mechanisms, and context compression, tailored to the\n",
      "unique characteristics of automotive industry documents. Additionally, we\n",
      "design custom classes supporting embedding pipelines and an agent supporting\n",
      "self-RAG based on LangGraph best practices. To evaluate our approach, we\n",
      "constructed a proprietary dataset comprising typical automotive industry\n",
      "documents, including technical reports and corporate regulations. We compared\n",
      "our optimized RAG model and self-RAG agent against a naive RAG baseline across\n",
      "three datasets: our automotive industry dataset, QReCC, and CoQA. Results\n",
      "demonstrate significant improvements in context precision, context recall,\n",
      "answer relevancy, and faithfulness, with particularly notable performance on\n",
      "the automotive industry dataset. Our optimization scheme provides an effective\n",
      "solution for deploying local RAG systems in the automotive sector, addressing\n",
      "the specific needs of PDF chatbots in industrial production environments. This\n",
      "research has important implications for advancing information processing and\n",
      "intelligent production in the automotive industry.\n",
      "</Summary>\n",
      "\n",
      "<Content>\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Optimizing RAG Techniques for Automotive Industry PDF Chatbots: A Case Study with \n",
      "Locally Deployed Ollama Models \n",
      "Optimizing RAG Techniques Based on Locally Deployed Ollama Models \n",
      "A Case Study with Locally Deployed Ollama Models \n",
      "Fei Liu * \n",
      "China Automotive Technology & Research Center, liufei@catarc.ac.cn \n",
      "Zejun Kang \n",
      "China Automotive Technology & Research Center, kangzejun@catarc.ac.cn \n",
      "Xing Han \n",
      "China Automotive Technology & Research Center, hanxing@catarc.ac.cn \n",
      "With the growing demand for offline PDF chatbots in automotive industrial production environments, optimizing the deployment \n",
      "of large language models (LLMs) in local, low-performance settings has become increasingly important. This study focuses on \n",
      "enhancing Retrieval-Augmented Generation (RAG) techniques for processing complex automotive industry documents using \n",
      "locally deployed Ollama models. \n",
      "Based on the Langchain framework, we propose a multi-dimensional optimization approach for Ollama's local RAG \n",
      "implementation. Our method addresses key challenges in automotive document processing, including multi-column layouts and \n",
      "technical specifications. We introduce improvements in PDF processing, retrieval mechanisms, and context compression, tailored \n",
      "to the unique characteristics of automotive industry documents. Additionally, we design custom classes supporting embedding \n",
      "pipelines and an agent supporting self-RAG based on LangGraph best practices. \n",
      "To evaluate our approach, we constructed a proprietary dataset comprising typical automotive industry documents, including \n",
      "technical reports and corporate regulations. We compared our optimized RAG model and self-RAG agent against a naive RAG \n",
      "baseline across three datasets: our automotive industry dataset, QReCC, and CoQA. Results demonstrate significant improvements \n",
      "in context precision, context recall, answer relevancy, and faithfulness, with particularly notable performance on the automotive \n",
      "industry dataset. \n",
      "Our optimization scheme provides an effective solution for deploying local RAG systems in the automotive sector, addressing the \n",
      "specific needs of PDF chatbots in industrial production environments. This research has important implications for advancing \n",
      "information processing and intelligent production in the automotive industry. \n",
      " \n",
      "* Place the footnote text for the author (if applicable) here.  \n",
      "CCS CONCEPTS • Computing methodologies • Artificial intelligence • Natural language processing • Natural language \n",
      "generation \n",
      " \n",
      "Additional Keywords and Phrases: Automotive Industry, Langchain, self-rag, PDF Processing, RAG, Ollama \n",
      "1 INTRODUCTION \n",
      "1.1 Research Background \n",
      "The automotive industry is undergoing a significant digital transformation, with an increasing reliance on complex \n",
      "technical documentation for various processes [1]. This shift encompasses design, manufacturing, and quality \n",
      "control, all of which now heavily depend on efficient information management systems [2]. The growing volume of \n",
      "technical documents, often in PDF format, has created a pressing need for advanced information retrieval and \n",
      "question-answering capabilities in industrial settings [3]. \n",
      "Large Language Models (LLMs) have emerged as powerful tools in natural language processing, demonstrating \n",
      "remarkable abilities in tasks such as document understanding and question answering [4]. These models have \n",
      "shown potential in handling the complex, domain-specific language often found in automotive documentation. \n",
      "However, the application of LLMs in industrial environments presents unique challenges, particularly in terms of \n",
      "computational resources and data privacy [5]. \n",
      "Among the various techniques developed to enhance LLM performance, Retrieval-Augmented Generation (RAG) \n",
      "has gained significant attention [6]. RAG combines the generative capabilities of LLMs with external knowledge \n",
      "retrieval, allowing for more accurate and contextually relevant responses. This approach, initially proposed by \n",
      "Lew\n",
      "</Content>\n",
      "</Document>\n",
      "\n",
      "---\n",
      "\n",
      "<Document source=\"http://arxiv.org/abs/2507.18515v1\" date=\"2025-07-24\" authors=\"Zezhou Yang, Ting Peng, Cuiyun Gao, Chaozheng Wang, Hailiang Huang, Yuetang Deng\"/>\n",
      "<Title>\n",
      "A Deep Dive into Retrieval-Augmented Generation for Code Completion: Experience on WeChat\n",
      "</Title>\n",
      "\n",
      "<Summary>\n",
      "Code completion, a crucial task in software engineering that enhances\n",
      "developer productivity, has seen substantial improvements with the rapid\n",
      "advancement of large language models (LLMs). In recent years,\n",
      "retrieval-augmented generation (RAG) has emerged as a promising method to\n",
      "enhance the code completion capabilities of LLMs, which leverages relevant\n",
      "context from codebases without requiring model retraining. While existing\n",
      "studies have demonstrated the effectiveness of RAG on public repositories and\n",
      "benchmarks, the potential distribution shift between open-source and\n",
      "closed-source codebases presents unique challenges that remain unexplored. To\n",
      "mitigate the gap, we conduct an empirical study to investigate the performance\n",
      "of widely-used RAG methods for code completion in the industrial-scale codebase\n",
      "of WeChat, one of the largest proprietary software systems. Specifically, we\n",
      "extensively explore two main types of RAG methods, namely identifier-based RAG\n",
      "and similarity-based RAG, across 26 open-source LLMs ranging from 0.5B to 671B\n",
      "parameters. For a more comprehensive analysis, we employ different retrieval\n",
      "techniques for similarity-based RAG, including lexical and semantic retrieval.\n",
      "Based on 1,669 internal repositories, we achieve several key findings: (1) both\n",
      "RAG methods demonstrate effectiveness in closed-source repositories, with\n",
      "similarity-based RAG showing superior performance, (2) the effectiveness of\n",
      "similarity-based RAG improves with more advanced retrieval techniques, where\n",
      "BM25 (lexical retrieval) and GTE-Qwen (semantic retrieval) achieve superior\n",
      "performance, and (3) the combination of lexical and semantic retrieval\n",
      "techniques yields optimal results, demonstrating complementary strengths.\n",
      "Furthermore, we conduct a developer survey to validate the practical utility of\n",
      "RAG methods in real-world development environments.\n",
      "</Summary>\n",
      "\n",
      "<Content>\n",
      "A Deep Dive into Retrieval-Augmented Generation\n",
      "for Code Completion: Experience on WeChat\n",
      "Zezhou Yang\n",
      "Tencent\n",
      "Guangzhou, China\n",
      "zezhouyang@tencent.com\n",
      "Ting Peng\n",
      "Tencent\n",
      "Guangzhou, China\n",
      "sakurapeng@tencent.com\n",
      "Cuiyun Gao∗\n",
      "The Chinese University of Hong Kong\n",
      "Hong Kong, China\n",
      "cuiyungao@outlook.com\n",
      "Chaozheng Wang\n",
      "The Chinese University of Hong Kong\n",
      "Hong Kong, China\n",
      "adf111178@gmail.com\n",
      "Hailiang Huang\n",
      "Tencent\n",
      "Guangzhou, China\n",
      "eraserhuang@tencent.com\n",
      "Yuetang Deng\n",
      "Tencent\n",
      "Guangzhou, China\n",
      "yuetangdeng@tencent.com\n",
      "Abstract—Code completion, a crucial task in software engi-\n",
      "neering that enhances developer productivity, has seen substantial\n",
      "improvements with the rapid advancement of large language\n",
      "models (LLMs). In recent years, retrieval-augmented generation\n",
      "(RAG) has emerged as a promising method to enhance the code\n",
      "completion capabilities of LLMs, which leverages relevant con-\n",
      "text from codebases without requiring model retraining. While\n",
      "existing studies have demonstrated the effectiveness of RAG on\n",
      "public repositories and benchmarks, the potential distribution\n",
      "shift between open-source and closed-source codebases presents\n",
      "unique challenges that remain unexplored. To mitigate the gap,\n",
      "we conduct an empirical study to investigate the performance of\n",
      "widely-used RAG methods for code completion in the industrial-\n",
      "scale codebase of WeChat, one of the largest proprietary software\n",
      "systems. Specifically, we extensively explore two main types\n",
      "of RAG methods, namely identifier-based RAG and similarity-\n",
      "based RAG, across 26 open-source LLMs ranging from 0.5B\n",
      "to 671B parameters. For a more comprehensive analysis, we\n",
      "employ different retrieval techniques for similarity-based RAG,\n",
      "including lexical and semantic retrieval. Based on 1,669 internal\n",
      "repositories, we achieve several key findings: (1) both RAG\n",
      "methods demonstrate effectiveness in closed-source repositories,\n",
      "with similarity-based RAG showing superior performance, (2)\n",
      "the effectiveness of similarity-based RAG improves with more\n",
      "advanced retrieval techniques, where BM25 (lexical retrieval)\n",
      "and GTE-Qwen (semantic retrieval) achieve superior perfor-\n",
      "mance, and (3) the combination of lexical and semantic retrieval\n",
      "techniques yields optimal results, demonstrating complemen-\n",
      "tary strengths. Furthermore, we conduct a developer survey\n",
      "to validate the practical utility of RAG methods in real-world\n",
      "development environments.\n",
      "Index Terms—large language model, retrieval-augmented gen-\n",
      "eration, code completion\n",
      "I. INTRODUCTION\n",
      "Code completion, which automatically predicts and sug-\n",
      "gests code fragments based on the surrounding programming\n",
      "context, has evolved from simple token-level suggestions to\n",
      "∗Cuiyun Gao is the corresponding author.\n",
      "This research is supported by National Key R&D Program of China\n",
      "(No. 2022YFB3103900), National Natural Science Foundation of China\n",
      "under project (No. 62472126), Natural Science Foundation of Guangdong\n",
      "Province (Project No. 2023A1515011959), Shenzhen-Hong Kong Jointly\n",
      "Funded Project (Category A, No. SGDX20230116091246007), and Shenzhen\n",
      "Basic Research (General Project No. JCYJ20220531095214031).\n",
      "generating entire code blocks [1], [2]. Studies have demon-\n",
      "strated that code completion tools substantially enhance de-\n",
      "veloper productivity in real-world software development [3],\n",
      "[4]. Notably, 87% of professional developers report significant\n",
      "improvements in their coding efficiency when utilizing code\n",
      "completion tools in industrial settings [5]. Recent advances\n",
      "in large language models (LLMs) have further transformed\n",
      "various software engineering tasks [6]–[10], demonstrating\n",
      "unprecedented capabilities in code understanding and gen-\n",
      "eration. These models have achieved particularly impressive\n",
      "performance in code completion tasks [11]–[13].\n",
      "To enhance LLMs’ performance on domain-specific tasks,\n",
      "researchers have explored Retrieval-Augmented Generation\n",
      "(RAG), which augments model inference by retrieving and in-\n",
      "corporating relevant context from the target codebase without\n",
      "requiring param\n",
      "</Content>\n",
      "</Document>\n",
      "\n",
      "---\n",
      "\n",
      "<Document source=\"http://arxiv.org/abs/2506.03401v1\" date=\"2025-06-03\" authors=\"Xiwei Xu, Hans Weytjens, Dawen Zhang, Qinghua Lu, Ingo Weber, Liming Zhu\"/>\n",
      "<Title>\n",
      "RAGOps: Operating and Managing Retrieval-Augmented Generation Pipelines\n",
      "</Title>\n",
      "\n",
      "<Summary>\n",
      "Recent studies show that 60% of LLM-based compound systems in enterprise\n",
      "environments leverage some form of retrieval-augmented generation (RAG), which\n",
      "enhances the relevance and accuracy of LLM (or other genAI) outputs by\n",
      "retrieving relevant information from external data sources. LLMOps involves the\n",
      "practices and techniques for managing the lifecycle and operations of LLM\n",
      "compound systems in production environments. It supports enhancing LLM systems\n",
      "through continuous operations and feedback evaluation. RAGOps extends LLMOps by\n",
      "incorporating a strong focus on data management to address the continuous\n",
      "changes in external data sources. This necessitates automated methods for\n",
      "evaluating and testing data operations, enhancing retrieval relevance and\n",
      "generation quality. In this paper, we (1) characterize the generic architecture\n",
      "of RAG applications based on the 4+1 model view for describing software\n",
      "architectures, (2) outline the lifecycle of RAG systems, which integrates the\n",
      "management lifecycles of both the LLM and the data, (3) define the key design\n",
      "considerations of RAGOps across different stages of the RAG lifecycle and\n",
      "quality trade-off analyses, (4) highlight the overarching research challenges\n",
      "around RAGOps, and (5) present two use cases of RAG applications and the\n",
      "corresponding RAGOps considerations.\n",
      "</Summary>\n",
      "\n",
      "<Content>\n",
      "arXiv:2506.03401v1  [cs.SE]  3 Jun 2025\n",
      "RAGOps: Operating and Managing Retrieval-Augmented Generation Pipelines\n",
      "Xiwei Xua,c, Hans Weytjensb, Dawen Zhanga, Qinghua Lua,c, Ingo Weberb,d, Liming Zhua,c\n",
      "aCSIRO’s Data61, Australia\n",
      "bTechnical University of Munich, School of CIT, Germany\n",
      "cUniversity of New South Wales, School of Computer Science and Engineering, Australia\n",
      "dFraunhofer Society, Munich, Germany\n",
      "Abstract\n",
      "Recent studies show that 60% of LLM-based compound systems in enterprise environments leverage some form\n",
      "of retrieval-augmented generation (RAG), which enhances the relevance and accuracy of LLM (or other genAI) out-\n",
      "puts by retrieving relevant information from external data sources. LLMOps involves the practices and techniques for\n",
      "managing the lifecycle and operations of LLM compound systems in production environments. It supports enhancing\n",
      "LLM systems through continuous operations and feedback evaluation. RAGOps extends LLMOps by incorporating\n",
      "a strong focus on data management to address the continuous changes in external data sources. This necessitates\n",
      "automated methods for evaluating and testing data operations, enhancing retrieval relevance and generation quality.\n",
      "In this paper, we (1) characterize the generic architecture of RAG applications based on the 4+1 model view for de-\n",
      "scribing software architectures, (2) outline the lifecycle of RAG systems, which integrates the management lifecycles\n",
      "of both the LLM and the data, (3) define the key design considerations of RAGOps across different stages of the RAG\n",
      "lifecycle and quality trade-off analyses, (4) highlight the overarching research challenges around RAGOps, and (5)\n",
      "present two use cases of RAG applications and the corresponding RAGOps considerations.\n",
      "Keywords: LLM, LLMOps, RAG, RAGOps\n",
      "1. Introduction\n",
      "Large Language Models (LLMs) can be instructed through prompting to perform a wide range of tasks, such as\n",
      "programming and translation. A notable trend in their application is the integration of LLMs into compound software\n",
      "systems, which consist of multiple components beyond the core language model [1]. Compound LLM systems can\n",
      "perform dynamic behaviors, whereas LLMs by themselves are inherently constrained by their reliance on static train-\n",
      "ing on datasets from some point in time, resulting in fixed parametric knowledge and limited grounding in specific\n",
      "contexts in which the systems are used, such as a given organization. In enterprise settings, 60% of the LLM com-\n",
      "pound systems incorporate some form of retrieval-augmented generation [2] (RAG), which improves the relevance,\n",
      "accuracy, and dynamism of LLM outputs by retrieving information from external data. RAG offers a solution to\n",
      "common challenges faced by LLMs, such as hallucinations, outdated data, and the difficulty of removing parametric\n",
      "knowledge, and open up the possibility to access proprietary, internal data from organizations using existing LLMs.\n",
      "By integrating real-time information retrieval, RAG enables continuous updates, possibly incorporating very recent\n",
      "information. RAG systems are compound systems that consist of multiple components blending the LLM’s parametric\n",
      "knowledge with external data retrieval, including, but not limited to retrieval sources, retriever, and generator [3].\n",
      "LLMOps refers to the practices and techniques used to manage the lifecycle and operation of LLM and LLM com-\n",
      "pound systems in production environments. The current state of LLMOps includes a variety of automated tools123\n",
      "designed to observe, monitor, optimize, and scale LLM applications. The functionality of these tools includes but\n",
      "is not limited to, model versioning, performance monitoring, continuous retraining, inference optimization, and in-\n",
      "frastructure scaling to accommodate fluctuating demand. Current LLMOps tools predominantly focus on model and\n",
      "1“Managed MLflow,” Databricks, accessed 5 April 2025, https://www.databricks.com/product/managed-mlflow\n",
      "2“Intelligent Observability,” New Relic, accessed 5 April 2025, http\n",
      "</Content>\n",
      "</Document>\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36msearch_arxiv\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "<Document source=\"http://arxiv.org/abs/2408.05933v1\" date=\"2024-08-12\" authors=\"Fei Liu, Zejun Kang, Xing Han\"/>\n",
      "<Title>\n",
      "Optimizing RAG Techniques for Automotive Industry PDF Chatbots: A Case Study with Locally Deployed Ollama Models\n",
      "</Title>\n",
      "\n",
      "<Summary>\n",
      "With the growing demand for offline PDF chatbots in automotive industrial\n",
      "production environments, optimizing the deployment of large language models\n",
      "(LLMs) in local, low-performance settings has become increasingly important.\n",
      "This study focuses on enhancing Retrieval-Augmented Generation (RAG) techniques\n",
      "for processing complex automotive industry documents using locally deployed\n",
      "Ollama models. Based on the Langchain framework, we propose a multi-dimensional\n",
      "optimization approach for Ollama's local RAG implementation. Our method\n",
      "addresses key challenges in automotive document processing, including\n",
      "multi-column layouts and technical specifications. We introduce improvements in\n",
      "PDF processing, retrieval mechanisms, and context compression, tailored to the\n",
      "unique characteristics of automotive industry documents. Additionally, we\n",
      "design custom classes supporting embedding pipelines and an agent supporting\n",
      "self-RAG based on LangGraph best practices. To evaluate our approach, we\n",
      "constructed a proprietary dataset comprising typical automotive industry\n",
      "documents, including technical reports and corporate regulations. We compared\n",
      "our optimized RAG model and self-RAG agent against a naive RAG baseline across\n",
      "three datasets: our automotive industry dataset, QReCC, and CoQA. Results\n",
      "demonstrate significant improvements in context precision, context recall,\n",
      "answer relevancy, and faithfulness, with particularly notable performance on\n",
      "the automotive industry dataset. Our optimization scheme provides an effective\n",
      "solution for deploying local RAG systems in the automotive sector, addressing\n",
      "the specific needs of PDF chatbots in industrial production environments. This\n",
      "research has important implications for advancing information processing and\n",
      "intelligent production in the automotive industry.\n",
      "</Summary>\n",
      "\n",
      "<Content>\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Optimizing RAG Techniques for Automotive Industry PDF Chatbots: A Case Study with \n",
      "Locally Deployed Ollama Models \n",
      "Optimizing RAG Techniques Based on Locally Deployed Ollama Models \n",
      "A Case Study with Locally Deployed Ollama Models \n",
      "Fei Liu * \n",
      "China Automotive Technology & Research Center, liufei@catarc.ac.cn \n",
      "Zejun Kang \n",
      "China Automotive Technology & Research Center, kangzejun@catarc.ac.cn \n",
      "Xing Han \n",
      "China Automotive Technology & Research Center, hanxing@catarc.ac.cn \n",
      "With the growing demand for offline PDF chatbots in automotive industrial production environments, optimizing the deployment \n",
      "of large language models (LLMs) in local, low-performance settings has become increasingly important. This study focuses on \n",
      "enhancing Retrieval-Augmented Generation (RAG) techniques for processing complex automotive industry documents using \n",
      "locally deployed Ollama models. \n",
      "Based on the Langchain framework, we propose a multi-dimensional optimization approach for Ollama's local RAG \n",
      "implementation. Our method addresses key challenges in automotive document processing, including multi-column layouts and \n",
      "technical specifications. We introduce improvements in PDF processing, retrieval mechanisms, and context compression, tailored \n",
      "to the unique characteristics of automotive industry documents. Additionally, we design custom classes supporting embedding \n",
      "pipelines and an agent supporting self-RAG based on LangGraph best practices. \n",
      "To evaluate our approach, we constructed a proprietary dataset comprising typical automotive industry documents, including \n",
      "technical reports and corporate regulations. We compared our optimized RAG model and self-RAG agent against a naive RAG \n",
      "baseline across three datasets: our automotive industry dataset, QReCC, and CoQA. Results demonstrate significant improvements \n",
      "in context precision, context recall, answer relevancy, and faithfulness, with particularly notable performance on the automotive \n",
      "industry dataset. \n",
      "Our optimization scheme provides an effective solution for deploying local RAG systems in the automotive sector, addressing the \n",
      "specific needs of PDF chatbots in industrial production environments. This research has important implications for advancing \n",
      "information processing and intelligent production in the automotive industry. \n",
      " \n",
      "* Place the footnote text for the author (if applicable) here.  \n",
      "CCS CONCEPTS • Computing methodologies • Artificial intelligence • Natural language processing • Natural language \n",
      "generation \n",
      " \n",
      "Additional Keywords and Phrases: Automotive Industry, Langchain, self-rag, PDF Processing, RAG, Ollama \n",
      "1 INTRODUCTION \n",
      "1.1 Research Background \n",
      "The automotive industry is undergoing a significant digital transformation, with an increasing reliance on complex \n",
      "technical documentation for various processes [1]. This shift encompasses design, manufacturing, and quality \n",
      "control, all of which now heavily depend on efficient information management systems [2]. The growing volume of \n",
      "technical documents, often in PDF format, has created a pressing need for advanced information retrieval and \n",
      "question-answering capabilities in industrial settings [3]. \n",
      "Large Language Models (LLMs) have emerged as powerful tools in natural language processing, demonstrating \n",
      "remarkable abilities in tasks such as document understanding and question answering [4]. These models have \n",
      "shown potential in handling the complex, domain-specific language often found in automotive documentation. \n",
      "However, the application of LLMs in industrial environments presents unique challenges, particularly in terms of \n",
      "computational resources and data privacy [5]. \n",
      "Among the various techniques developed to enhance LLM performance, Retrieval-Augmented Generation (RAG) \n",
      "has gained significant attention [6]. RAG combines the generative capabilities of LLMs with external knowledge \n",
      "retrieval, allowing for more accurate and contextually relevant responses. This approach, initially proposed by \n",
      "Lew\n",
      "</Content>\n",
      "</Document>\n",
      "\n",
      "---\n",
      "\n",
      "<Document source=\"http://arxiv.org/abs/2507.18515v1\" date=\"2025-07-24\" authors=\"Zezhou Yang, Ting Peng, Cuiyun Gao, Chaozheng Wang, Hailiang Huang, Yuetang Deng\"/>\n",
      "<Title>\n",
      "A Deep Dive into Retrieval-Augmented Generation for Code Completion: Experience on WeChat\n",
      "</Title>\n",
      "\n",
      "<Summary>\n",
      "Code completion, a crucial task in software engineering that enhances\n",
      "developer productivity, has seen substantial improvements with the rapid\n",
      "advancement of large language models (LLMs). In recent years,\n",
      "retrieval-augmented generation (RAG) has emerged as a promising method to\n",
      "enhance the code completion capabilities of LLMs, which leverages relevant\n",
      "context from codebases without requiring model retraining. While existing\n",
      "studies have demonstrated the effectiveness of RAG on public repositories and\n",
      "benchmarks, the potential distribution shift between open-source and\n",
      "closed-source codebases presents unique challenges that remain unexplored. To\n",
      "mitigate the gap, we conduct an empirical study to investigate the performance\n",
      "of widely-used RAG methods for code completion in the industrial-scale codebase\n",
      "of WeChat, one of the largest proprietary software systems. Specifically, we\n",
      "extensively explore two main types of RAG methods, namely identifier-based RAG\n",
      "and similarity-based RAG, across 26 open-source LLMs ranging from 0.5B to 671B\n",
      "parameters. For a more comprehensive analysis, we employ different retrieval\n",
      "techniques for similarity-based RAG, including lexical and semantic retrieval.\n",
      "Based on 1,669 internal repositories, we achieve several key findings: (1) both\n",
      "RAG methods demonstrate effectiveness in closed-source repositories, with\n",
      "similarity-based RAG showing superior performance, (2) the effectiveness of\n",
      "similarity-based RAG improves with more advanced retrieval techniques, where\n",
      "BM25 (lexical retrieval) and GTE-Qwen (semantic retrieval) achieve superior\n",
      "performance, and (3) the combination of lexical and semantic retrieval\n",
      "techniques yields optimal results, demonstrating complementary strengths.\n",
      "Furthermore, we conduct a developer survey to validate the practical utility of\n",
      "RAG methods in real-world development environments.\n",
      "</Summary>\n",
      "\n",
      "<Content>\n",
      "A Deep Dive into Retrieval-Augmented Generation\n",
      "for Code Completion: Experience on WeChat\n",
      "Zezhou Yang\n",
      "Tencent\n",
      "Guangzhou, China\n",
      "zezhouyang@tencent.com\n",
      "Ting Peng\n",
      "Tencent\n",
      "Guangzhou, China\n",
      "sakurapeng@tencent.com\n",
      "Cuiyun Gao∗\n",
      "The Chinese University of Hong Kong\n",
      "Hong Kong, China\n",
      "cuiyungao@outlook.com\n",
      "Chaozheng Wang\n",
      "The Chinese University of Hong Kong\n",
      "Hong Kong, China\n",
      "adf111178@gmail.com\n",
      "Hailiang Huang\n",
      "Tencent\n",
      "Guangzhou, China\n",
      "eraserhuang@tencent.com\n",
      "Yuetang Deng\n",
      "Tencent\n",
      "Guangzhou, China\n",
      "yuetangdeng@tencent.com\n",
      "Abstract—Code completion, a crucial task in software engi-\n",
      "neering that enhances developer productivity, has seen substantial\n",
      "improvements with the rapid advancement of large language\n",
      "models (LLMs). In recent years, retrieval-augmented generation\n",
      "(RAG) has emerged as a promising method to enhance the code\n",
      "completion capabilities of LLMs, which leverages relevant con-\n",
      "text from codebases without requiring model retraining. While\n",
      "existing studies have demonstrated the effectiveness of RAG on\n",
      "public repositories and benchmarks, the potential distribution\n",
      "shift between open-source and closed-source codebases presents\n",
      "unique challenges that remain unexplored. To mitigate the gap,\n",
      "we conduct an empirical study to investigate the performance of\n",
      "widely-used RAG methods for code completion in the industrial-\n",
      "scale codebase of WeChat, one of the largest proprietary software\n",
      "systems. Specifically, we extensively explore two main types\n",
      "of RAG methods, namely identifier-based RAG and similarity-\n",
      "based RAG, across 26 open-source LLMs ranging from 0.5B\n",
      "to 671B parameters. For a more comprehensive analysis, we\n",
      "employ different retrieval techniques for similarity-based RAG,\n",
      "including lexical and semantic retrieval. Based on 1,669 internal\n",
      "repositories, we achieve several key findings: (1) both RAG\n",
      "methods demonstrate effectiveness in closed-source repositories,\n",
      "with similarity-based RAG showing superior performance, (2)\n",
      "the effectiveness of similarity-based RAG improves with more\n",
      "advanced retrieval techniques, where BM25 (lexical retrieval)\n",
      "and GTE-Qwen (semantic retrieval) achieve superior perfor-\n",
      "mance, and (3) the combination of lexical and semantic retrieval\n",
      "techniques yields optimal results, demonstrating complemen-\n",
      "tary strengths. Furthermore, we conduct a developer survey\n",
      "to validate the practical utility of RAG methods in real-world\n",
      "development environments.\n",
      "Index Terms—large language model, retrieval-augmented gen-\n",
      "eration, code completion\n",
      "I. INTRODUCTION\n",
      "Code completion, which automatically predicts and sug-\n",
      "gests code fragments based on the surrounding programming\n",
      "context, has evolved from simple token-level suggestions to\n",
      "∗Cuiyun Gao is the corresponding author.\n",
      "This research is supported by National Key R&D Program of China\n",
      "(No. 2022YFB3103900), National Natural Science Foundation of China\n",
      "under project (No. 62472126), Natural Science Foundation of Guangdong\n",
      "Province (Project No. 2023A1515011959), Shenzhen-Hong Kong Jointly\n",
      "Funded Project (Category A, No. SGDX20230116091246007), and Shenzhen\n",
      "Basic Research (General Project No. JCYJ20220531095214031).\n",
      "generating entire code blocks [1], [2]. Studies have demon-\n",
      "strated that code completion tools substantially enhance de-\n",
      "veloper productivity in real-world software development [3],\n",
      "[4]. Notably, 87% of professional developers report significant\n",
      "improvements in their coding efficiency when utilizing code\n",
      "completion tools in industrial settings [5]. Recent advances\n",
      "in large language models (LLMs) have further transformed\n",
      "various software engineering tasks [6]–[10], demonstrating\n",
      "unprecedented capabilities in code understanding and gen-\n",
      "eration. These models have achieved particularly impressive\n",
      "performance in code completion tasks [11]–[13].\n",
      "To enhance LLMs’ performance on domain-specific tasks,\n",
      "researchers have explored Retrieval-Augmented Generation\n",
      "(RAG), which augments model inference by retrieving and in-\n",
      "corporating relevant context from the target codebase without\n",
      "requiring param\n",
      "</Content>\n",
      "</Document>\n",
      "\n",
      "---\n",
      "\n",
      "<Document source=\"http://arxiv.org/abs/2506.03401v1\" date=\"2025-06-03\" authors=\"Xiwei Xu, Hans Weytjens, Dawen Zhang, Qinghua Lu, Ingo Weber, Liming Zhu\"/>\n",
      "<Title>\n",
      "RAGOps: Operating and Managing Retrieval-Augmented Generation Pipelines\n",
      "</Title>\n",
      "\n",
      "<Summary>\n",
      "Recent studies show that 60% of LLM-based compound systems in enterprise\n",
      "environments leverage some form of retrieval-augmented generation (RAG), which\n",
      "enhances the relevance and accuracy of LLM (or other genAI) outputs by\n",
      "retrieving relevant information from external data sources. LLMOps involves the\n",
      "practices and techniques for managing the lifecycle and operations of LLM\n",
      "compound systems in production environments. It supports enhancing LLM systems\n",
      "through continuous operations and feedback evaluation. RAGOps extends LLMOps by\n",
      "incorporating a strong focus on data management to address the continuous\n",
      "changes in external data sources. This necessitates automated methods for\n",
      "evaluating and testing data operations, enhancing retrieval relevance and\n",
      "generation quality. In this paper, we (1) characterize the generic architecture\n",
      "of RAG applications based on the 4+1 model view for describing software\n",
      "architectures, (2) outline the lifecycle of RAG systems, which integrates the\n",
      "management lifecycles of both the LLM and the data, (3) define the key design\n",
      "considerations of RAGOps across different stages of the RAG lifecycle and\n",
      "quality trade-off analyses, (4) highlight the overarching research challenges\n",
      "around RAGOps, and (5) present two use cases of RAG applications and the\n",
      "corresponding RAGOps considerations.\n",
      "</Summary>\n",
      "\n",
      "<Content>\n",
      "arXiv:2506.03401v1  [cs.SE]  3 Jun 2025\n",
      "RAGOps: Operating and Managing Retrieval-Augmented Generation Pipelines\n",
      "Xiwei Xua,c, Hans Weytjensb, Dawen Zhanga, Qinghua Lua,c, Ingo Weberb,d, Liming Zhua,c\n",
      "aCSIRO’s Data61, Australia\n",
      "bTechnical University of Munich, School of CIT, Germany\n",
      "cUniversity of New South Wales, School of Computer Science and Engineering, Australia\n",
      "dFraunhofer Society, Munich, Germany\n",
      "Abstract\n",
      "Recent studies show that 60% of LLM-based compound systems in enterprise environments leverage some form\n",
      "of retrieval-augmented generation (RAG), which enhances the relevance and accuracy of LLM (or other genAI) out-\n",
      "puts by retrieving relevant information from external data sources. LLMOps involves the practices and techniques for\n",
      "managing the lifecycle and operations of LLM compound systems in production environments. It supports enhancing\n",
      "LLM systems through continuous operations and feedback evaluation. RAGOps extends LLMOps by incorporating\n",
      "a strong focus on data management to address the continuous changes in external data sources. This necessitates\n",
      "automated methods for evaluating and testing data operations, enhancing retrieval relevance and generation quality.\n",
      "In this paper, we (1) characterize the generic architecture of RAG applications based on the 4+1 model view for de-\n",
      "scribing software architectures, (2) outline the lifecycle of RAG systems, which integrates the management lifecycles\n",
      "of both the LLM and the data, (3) define the key design considerations of RAGOps across different stages of the RAG\n",
      "lifecycle and quality trade-off analyses, (4) highlight the overarching research challenges around RAGOps, and (5)\n",
      "present two use cases of RAG applications and the corresponding RAGOps considerations.\n",
      "Keywords: LLM, LLMOps, RAG, RAGOps\n",
      "1. Introduction\n",
      "Large Language Models (LLMs) can be instructed through prompting to perform a wide range of tasks, such as\n",
      "programming and translation. A notable trend in their application is the integration of LLMs into compound software\n",
      "systems, which consist of multiple components beyond the core language model [1]. Compound LLM systems can\n",
      "perform dynamic behaviors, whereas LLMs by themselves are inherently constrained by their reliance on static train-\n",
      "ing on datasets from some point in time, resulting in fixed parametric knowledge and limited grounding in specific\n",
      "contexts in which the systems are used, such as a given organization. In enterprise settings, 60% of the LLM com-\n",
      "pound systems incorporate some form of retrieval-augmented generation [2] (RAG), which improves the relevance,\n",
      "accuracy, and dynamism of LLM outputs by retrieving information from external data. RAG offers a solution to\n",
      "common challenges faced by LLMs, such as hallucinations, outdated data, and the difficulty of removing parametric\n",
      "knowledge, and open up the possibility to access proprietary, internal data from organizations using existing LLMs.\n",
      "By integrating real-time information retrieval, RAG enables continuous updates, possibly incorporating very recent\n",
      "information. RAG systems are compound systems that consist of multiple components blending the LLM’s parametric\n",
      "knowledge with external data retrieval, including, but not limited to retrieval sources, retriever, and generator [3].\n",
      "LLMOps refers to the practices and techniques used to manage the lifecycle and operation of LLM and LLM com-\n",
      "pound systems in production environments. The current state of LLMOps includes a variety of automated tools123\n",
      "designed to observe, monitor, optimize, and scale LLM applications. The functionality of these tools includes but\n",
      "is not limited to, model versioning, performance monitoring, continuous retraining, inference optimization, and in-\n",
      "frastructure scaling to accommodate fluctuating demand. Current LLMOps tools predominantly focus on model and\n",
      "1“Managed MLflow,” Databricks, accessed 5 April 2025, https://www.databricks.com/product/managed-mlflow\n",
      "2“Intelligent Observability,” New Relic, accessed 5 April 2025, http\n",
      "</Content>\n",
      "</Document>\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mask_question\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Fascinating insights, Dr. Torres. Could you share a specific example or case study where Modular RAG has been successfully implemented, highlighting its scalability and adaptability benefits in a production scenario?\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36msearch_web\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "<Document href=\"https://adasci.org/how-does-modular-rag-improve-upon-naive-rag/\"/>\n",
      "Naive RAG, the initial implementation of Retrieval-Augmented Generation, operates on a straightforward principle: retrieve relevant documents from an external knowledge base and use these documents to inform the generative process. The retrieval process in Naive RAG is relatively static and lacks flexibility, often leading to inefficiencies and suboptimal integration with the generative model. By adopting a modular architecture, this approach addresses the limitations of Naive RAG, offering enhanced flexibility, scalability, and efficiency. Unlike Naive RAG, which operates as a monolithic entity, Modular RAG breaks down the retrieval and generation processes into distinct, interchangeable modules. * *Seamless Integration*: Generative models in Modular RAG are designed to seamlessly integrate with various retrieval modules, enhancing the coherence and relevance of generated responses.\n",
      "</Document>\n",
      "\n",
      "---\n",
      "\n",
      "<Document href=\"https://medium.com/@drjulija/what-are-naive-rag-advanced-rag-modular-rag-paradigms-edff410c202e\"/>\n",
      "I share the differences between Naive RAG, Advanced RAG and Modular RAG frameworks. Retrieval-Augmented Generation (RAG) framework is designed to help mitigate these challenges. RAG enhances LLMs’ knowledge base with additional, domain-specific data. RAG systems can be split into 3 categories: See the comparison between all three paradigms of RAG — Naive RAG, Advanced RAG and Modular RAG below.\n",
      "</Document>\n",
      "\n",
      "---\n",
      "\n",
      "<Document href=\"https://ai.plainenglish.io/naive-rag-advanced-rag-modular-rag-b18b8669193e\"/>\n",
      "Generate response with LLMllm = ChatOpenAI(model=\"gpt-4o\")query = \"What is LangChain?\"docs = retriever.get_relevant_documents(query)context = \"\\n\\n\".join([doc.page_content for doc in docs])response = llm.invoke(f\"Context: {context}\\nQuestion: {query}\")print(response) from import from import from import from import Generate Response with Optimized Inputllm = ChatOpenAI(model=\"gpt-4o\")prompt = PromptTemplate.from_template(\"Answer based on context:\\n{context}\\nQuestion: {query}\")response = llm.invoke(prompt.format(context=context, query=\"What is LangChain?\"))print(response) from import from import from import from import from import from import from langchain.vectorstores import Qdrant  # Vector store for embeddingsfrom langchain.embeddings import OpenAIEmbeddings  # Embeddings modelfrom langchain.text_splitter import RecursiveCharacterTextSplitter  # Splits documents into chunksfrom langchain.chains import RetrievalQA  # Retrieval-Augmented Generation (RAG)from langchain.prompts import ChatPromptTemplate  # Prompt templatesfrom langchain.chat_models import ChatOpenAI  # GPT-based LLM from import\n",
      "</Document>\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36msearch_web\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "<Document href=\"https://adasci.org/how-does-modular-rag-improve-upon-naive-rag/\"/>\n",
      "Naive RAG, the initial implementation of Retrieval-Augmented Generation, operates on a straightforward principle: retrieve relevant documents from an external knowledge base and use these documents to inform the generative process. The retrieval process in Naive RAG is relatively static and lacks flexibility, often leading to inefficiencies and suboptimal integration with the generative model. By adopting a modular architecture, this approach addresses the limitations of Naive RAG, offering enhanced flexibility, scalability, and efficiency. Unlike Naive RAG, which operates as a monolithic entity, Modular RAG breaks down the retrieval and generation processes into distinct, interchangeable modules. * *Seamless Integration*: Generative models in Modular RAG are designed to seamlessly integrate with various retrieval modules, enhancing the coherence and relevance of generated responses.\n",
      "</Document>\n",
      "\n",
      "---\n",
      "\n",
      "<Document href=\"https://zilliz.com/blog/advancing-llms-native-advanced-modular-rag-approaches\"/>\n",
      "Jan 8, 2025·This post explores the key components of RAG, its evolution, technical implementation, evaluation methods, and potential for real-world\n",
      "</Document>\n",
      "\n",
      "---\n",
      "\n",
      "<Document href=\"https://kth.diva-portal.org/smash/get/diva2:1971472/FULLTEXT01.pdf\"/>\n",
      "65 xiv | List of Tables List of acronyms and abbreviations | xv List of acronyms and abbreviations Advanced RAG Advanced Retrieval-Augmented Generation AI Artificial Intelligence AQ Ambiguous Questions BERT Bidirectional Encoder Representations from Trans-formers CoRAG Chain-of-Retrieval Augmented Generation CoT Chain-of-Thought GPT Generative Pretrained Transformer Graph RAG Graph Retrieval-Augmented Generation HyDE Hypothetical Document Embeddings IRCoT Interleaving Retrieval with Chain-of-Thought LLM Large Language Model LoRA Low-Rank Adaptation LSTM Long Short-Term Memory MHQA Multi-hop Question Answering ML Machine Learning MMR Maximal Marginal Relevance Modular RAG Modular Retrieval-Augmented Generation MRR Mean Reciprocal Rank Naive RAG Naive Retrieval-Augmented Generation NER Named Entity Extraction NLG Natural Language Generation NLP Natural Language Processing NLU Natural Language Understanding xvi | List of acronyms and abbreviations PEFT Efficient-Fine-Tuning QA Question Answering QFS Query-Focused Summarization RAG Retrieval-Augmented Generation RAPTOR Recursive Abstractive Processing for Tree-Organized Retrieval Recursive RAG Recursive Retrieval-Augmented Generation RNN Recurrent Neural Network ToC Tree of Clarifications Introduction | 1 Chapter 1 Introduction This chapter introduces the research area of this study and the problem domain.\n",
      "</Document>\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36msearch_arxiv\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "<Document source=\"http://arxiv.org/abs/2508.05650v1\" date=\"2025-07-26\" authors=\"Jiaxuan Liang, Shide Zhou, Kailong Wang\"/>\n",
      "<Title>\n",
      "OmniBench-RAG: A Multi-Domain Evaluation Platform for Retrieval-Augmented Generation Tools\n",
      "</Title>\n",
      "\n",
      "<Summary>\n",
      "While Retrieval Augmented Generation (RAG) is now widely adopted to enhance\n",
      "LLMs, evaluating its true performance benefits in a reproducible and\n",
      "interpretable way remains a major hurdle. Existing methods often fall short:\n",
      "they lack domain coverage, employ coarse metrics that miss sub document\n",
      "precision, and fail to capture computational trade offs. Most critically, they\n",
      "provide no standardized framework for comparing RAG effectiveness across\n",
      "different models and domains.\n",
      "  We introduce OmniBench RAG, a novel automated platform for multi domain\n",
      "evaluation of RAG systems. The platform quantifies performance gains across\n",
      "accuracy and efficiency dimensions, spanning nine knowledge fields including\n",
      "culture, geography, and health. We introduce two standardized metrics:\n",
      "Improvements (accuracy gains) and Transformation (efficiency differences\n",
      "between pre RAG and post RAG models), enabling reproducible comparisons across\n",
      "models and tasks. The platform features dynamic test generation, modular\n",
      "evaluation pipelines, and automated knowledge base construction. Our evaluation\n",
      "reveals striking variability in RAG effectiveness, from significant gains in\n",
      "culture to declines in mathematics, highlighting the critical importance of\n",
      "systematic, domain aware assessment. A demonstration video is available at:\n",
      "https://www.youtube.com/watch?v=BZx83QFcTCI. Code and datasets:\n",
      "https://github.com/Garnett-Liang/Omnibench-RAG.\n",
      "</Summary>\n",
      "\n",
      "<Content>\n",
      "OmniBench-RAG: A Multi-Domain Evaluation\n",
      "Platform for Retrieval-Augmented Generation Tools\n",
      "Jiaxuan Liang*, Shide Zhou*, and Kailong Wang†\n",
      "Huazhong University of Science and Technology\n",
      "{liangjx, shidez, wangkl}@hust.edu.cn\n",
      "Abstract—While Retrieval Augmented Generation (RAG) is\n",
      "now widely adopted to enhance LLMs, evaluating its true\n",
      "performance benefits in a reproducible and interpretable way\n",
      "remains a major hurdle. Existing methods often fall short: they\n",
      "lack domain coverage, employ coarse metrics that miss sub\n",
      "document precision, and fail to capture computational trade\n",
      "offs. Most critically, they provide no standardized framework\n",
      "for comparing RAG effectiveness across different models and\n",
      "domains.\n",
      "We introduce OmniBench RAG, a novel automated platform\n",
      "for multi domain evaluation of RAG systems. The platform\n",
      "quantifies performance gains across accuracy and efficiency\n",
      "dimensions, spanning nine knowledge fields including culture,\n",
      "geography, and health. We introduce two standardized metrics:\n",
      "Improvements (accuracy gains) and Transformation (efficiency\n",
      "differences between pre RAG and post RAG models), enabling\n",
      "reproducible comparisons across models and tasks. The platform\n",
      "features dynamic test generation, modular evaluation pipelines,\n",
      "and automated knowledge base construction. Our evaluation\n",
      "reveals striking variability in RAG effectiveness, from significant\n",
      "gains in culture to declines in mathematics, highlighting the\n",
      "critical importance of systematic, domain aware assessment. A\n",
      "demonstration video is available at: https://www.youtube.com/\n",
      "watch?v=BZx83QFcTCI. Code and datasets: https://github.com/\n",
      "Garnett-Liang/Omnibench-RAG.\n",
      "I. INTRODUCTION\n",
      "Retrieval-Augmented Generation (RAG) is a key technique\n",
      "for enhancing Large Language Models (LLMs) [1], [2]. By\n",
      "grounding model responses in external, verifiable knowledge,\n",
      "RAG promises to mitigate hallucinations [3], improve factual\n",
      "accuracy [4], and provide up-to-date information [5]. However,\n",
      "the true effectiveness of RAG is far from uniform. Recent\n",
      "studies [6], [7] reveal a significant disparity: while RAG\n",
      "can boost the accuracy of smaller models like Llama-3.2-3B-\n",
      "Instruct by as much as 38.12%, its impact on state-of-the-art\n",
      "models such as GPT-4o, which excel with extended context\n",
      "windows, is often less pronounced. This variability, which\n",
      "depends not only on the model’s scale but also heavily on\n",
      "the knowledge domain, underscores a critical challenge: the\n",
      "lack of a systematic platform to quantify the value of RAG\n",
      "across these diverse contexts.\n",
      "Current RAG evaluation approaches suffer from fundamen-\n",
      "tal limitations that impede reproducible and comprehensive as-\n",
      "sessment: First, they lack automated multi-domain evalua-\n",
      "tion capabilities and rely on non-deterministic components.\n",
      "*Jiaxuan Liang and Shide Zhou are co-first authors.\n",
      "†Kailong Wang is the corresponding author.\n",
      "Existing benchmarks typically require manual configuration\n",
      "for each knowledge domain and fail to provide unified assess-\n",
      "ment across diverse fields like finance, healthcare, or culture,\n",
      "making cross-domain performance analysis labor-intensive and\n",
      "inconsistent. Moreover, key metrics in leading frameworks\n",
      "(e.g., LLM-based scoring in Ragas [8]) inadvertently introduce\n",
      "randomness due to reliance on large language models in the\n",
      "evaluation loop, undermining result reproducibility. Second,\n",
      "they employ static datasets and coarse-grained metrics.\n",
      "Most frameworks rely on fixed benchmarks and document-\n",
      "level retrieval metrics (e.g., MRR@k[9]), missing the critical\n",
      "sub-document precision needed to assess whether models\n",
      "extract specific facts accurately. They also lack the ability to\n",
      "dynamically generate test cases that probe complex reasoning\n",
      "patterns. Third, they fail to capture the computational\n",
      "trade-offs inherent in RAG systems. Without automated\n",
      "profiling of resource utilization and efficiency metrics, practi-\n",
      "tioners cannot make informed decisions about the cost-benefit\n",
      "trade-offs of deploying RAG in production environments.\n",
      "Our \n",
      "</Content>\n",
      "</Document>\n",
      "\n",
      "---\n",
      "\n",
      "<Document source=\"http://arxiv.org/abs/2504.07803v1\" date=\"2025-04-10\" authors=\"Mattia Rengo, Senad Beadini, Domenico Alfano, Roberto Abbruzzese\"/>\n",
      "<Title>\n",
      "A System for Comprehensive Assessment of RAG Frameworks\n",
      "</Title>\n",
      "\n",
      "<Summary>\n",
      "Retrieval Augmented Generation (RAG) has emerged as a standard paradigm for\n",
      "enhancing the factual accuracy and contextual relevance of Large Language\n",
      "Models (LLMs) by integrating retrieval mechanisms. However, existing evaluation\n",
      "frameworks fail to provide a holistic black-box approach to assessing RAG\n",
      "systems, especially in real-world deployment scenarios. To address this gap, we\n",
      "introduce SCARF (System for Comprehensive Assessment of RAG Frameworks), a\n",
      "modular and flexible evaluation framework designed to benchmark deployed RAG\n",
      "applications systematically. SCARF provides an end-to-end, black-box evaluation\n",
      "methodology, enabling a limited-effort comparison across diverse RAG\n",
      "frameworks. Our framework supports multiple deployment configurations and\n",
      "facilitates automated testing across vector databases and LLM serving\n",
      "strategies, producing a detailed performance report. Moreover, SCARF integrates\n",
      "practical considerations such as response coherence, providing a scalable and\n",
      "adaptable solution for researchers and industry professionals evaluating RAG\n",
      "applications. Using the REST APIs interface, we demonstrate how SCARF can be\n",
      "applied to real-world scenarios, showcasing its flexibility in assessing\n",
      "different RAG frameworks and configurations. SCARF is available at GitHub\n",
      "repository.\n",
      "</Summary>\n",
      "\n",
      "<Content>\n",
      "arXiv:2504.07803v1  [cs.CL]  10 Apr 2025\n",
      "A System for Comprehensive Assessment of RAG\n",
      "Frameworks\n",
      "Mattia Rengo , Senad Beadini , Domenico Alfano\n",
      ", Roberto Abbruzzese\n",
      "R&D Department, Eustema S.p.A.\n",
      "Napoli, Italy\n",
      "{m.rengo, s.beadini, d.alfano, r.abbruzzese}@eustema.it\n",
      "Code: https://github.com/Eustema-S-p-A/SCARF\n",
      "Abstract—Retrieval\n",
      "Augmented\n",
      "Generation\n",
      "(RAG)\n",
      "has\n",
      "emerged as a standard paradigm for enhancing the factual accu-\n",
      "racy and contextual relevance of Large Language Models (LLMs)\n",
      "by integrating retrieval mechanisms. However, existing evaluation\n",
      "frameworks fail to provide a holistic black-box approach to\n",
      "assessing RAG systems, especially in real-world deployment\n",
      "scenarios. To address this gap, we introduce SCARF (System for\n",
      "Comprehensive Assessment of RAG Frameworks), a modular and\n",
      "flexible evaluation framework designed to benchmark deployed\n",
      "RAG applications systematically. SCARF provides an end-to-\n",
      "end, black-box evaluation methodology, enabling a limited-effort\n",
      "comparison across diverse RAG frameworks. Our framework\n",
      "supports multiple deployment configurations and facilitates au-\n",
      "tomated testing across vector databases and LLM serving strate-\n",
      "gies, producing a detailed performance report. Moreover, SCARF\n",
      "integrates practical considerations such as response coherence,\n",
      "providing a scalable and adaptable solution for researchers\n",
      "and industry professionals evaluating RAG applications. Using\n",
      "the REST APIs interface, we demonstrate how SCARF can\n",
      "be applied to real-world scenarios, showcasing its flexibility in\n",
      "assessing different RAG frameworks and configurations. SCARF\n",
      "is available at GitHub repository.\n",
      "Index Terms—Retrieval-Augmented Generation, RAG Evalu-\n",
      "ation, LLM\n",
      "I. INTRODUCTION\n",
      "Retrieval-Augmented Generation (RAG) represents a re-\n",
      "markable\n",
      "advancement\n",
      "in\n",
      "Natural\n",
      "Language\n",
      "Processing\n",
      "(NLP), significantly enhancing the performance of generative\n",
      "Language Models (LMs). By combining the capabilities of\n",
      "LMs with external knowledge bases, RAG allows responses\n",
      "that are not only more accurate but also contextually relevant.\n",
      "Merging information retrieval with language generation, RAG\n",
      "systems address a major limitation of standalone LMs: their\n",
      "tendency to generate responses that, while coherent, may\n",
      "lack factual accuracy or grounding. Since traditional Large\n",
      "Language Models (LLMs) rely solely on pre-trained data, they\n",
      "may generate factually incorrect information and unreliable\n",
      "outputs. RAG systems address this limitation by retrieving\n",
      "relevant information in real-time, making them well-suited for\n",
      "tasks that require up-to-date, accurate, and context-aware re-\n",
      "sponses, such as answering questions, generating content, and\n",
      "supporting various real-world applications. RAG approaches\n",
      "[1] have evolved rapidly, resulting in a wide variety of system\n",
      "variants [2]–[5] and benchmarking frameworks [6]. Many\n",
      "existing evaluation methods [7], [8] focus on assessing specific\n",
      "components, such as the relevance of retrieved documents or\n",
      "the quality of generated responses. However, these methods\n",
      "often lack a comprehensive perspective, failing to provide\n",
      "a holistic, end-to-end evaluation that considers not only the\n",
      "interplay between the retrieval and generation components\n",
      "but also the flexibility to experiment with and optimize the\n",
      "underlying technical tools used in these processes. Another\n",
      "critical yet often overlooked aspect is the variability introduced\n",
      "by different deployment frameworks used for LLMs. Tools\n",
      "such as vLLM [9], OpenLLM [10], and Ollama [11] im-\n",
      "plement diverse optimization strategies, including techniques\n",
      "like quantization, batching, and caching. These strategies can\n",
      "have a significant impact on key performance metrics, such as\n",
      "latency, system efficiency, and overall responsiveness. These\n",
      "variations are particularly important in real-world scenarios,\n",
      "where deployment choices can directly affect the user experi-\n",
      "ence and system scalability. Despite their importance, existing\n",
      "evaluation methods rarely integrate the ability to manage and\n",
      "measur\n",
      "</Content>\n",
      "</Document>\n",
      "\n",
      "---\n",
      "\n",
      "<Document source=\"http://arxiv.org/abs/2502.11228v2\" date=\"2025-05-22\" authors=\"Mohammad Reza Rezaei, Adji Bousso Dieng\"/>\n",
      "<Title>\n",
      "Vendi-RAG: Adaptively Trading-Off Diversity And Quality Significantly Improves Retrieval Augmented Generation With LLMs\n",
      "</Title>\n",
      "\n",
      "<Summary>\n",
      "Retrieval-augmented generation (RAG) enhances large language models (LLMs)\n",
      "for domain-specific question-answering (QA) tasks by leveraging external\n",
      "knowledge sources. However, traditional RAG systems primarily focus on\n",
      "relevance-based retrieval and often struggle with redundancy, especially when\n",
      "reasoning requires connecting information from multiple sources. This paper\n",
      "introduces Vendi-RAG, a framework based on an iterative process that jointly\n",
      "optimizes retrieval diversity and answer quality. This joint optimization leads\n",
      "to significantly higher accuracy for multi-hop QA tasks. Vendi-RAG leverages\n",
      "the Vendi Score (VS), a flexible similarity-based diversity metric, to promote\n",
      "semantic diversity in document retrieval. It then uses an LLM judge that\n",
      "evaluates candidate answers, generated after a reasoning step, and outputs a\n",
      "score that the retriever uses to balance relevance and diversity among the\n",
      "retrieved documents during each iteration. Experiments on three challenging\n",
      "datasets -- HotpotQA, MuSiQue, and 2WikiMultiHopQA -- demonstrate Vendi-RAG's\n",
      "effectiveness in multi-hop reasoning tasks. The framework achieves significant\n",
      "accuracy improvements over traditional single-step and multi-step RAG\n",
      "approaches, with accuracy increases reaching up to +4.2% on HotpotQA, +4.1% on\n",
      "2WikiMultiHopQA, and +1.3% on MuSiQue compared to Adaptive-RAG, the current\n",
      "best baseline. The benefits of Vendi-RAG are even more pronounced as the number\n",
      "of retrieved documents increases. Finally, we evaluated Vendi-RAG across\n",
      "different LLM backbones, including GPT-3.5, GPT-4, and GPT-4o-mini, and\n",
      "observed consistent improvements, demonstrating that the framework's advantages\n",
      "are model-agnostic.\n",
      "</Summary>\n",
      "\n",
      "<Content>\n",
      "Vendi-RAG: Adaptively Trading-Off Diversity And\n",
      "Quality Significantly Improves Retrieval\n",
      "Augmented Generation With LLMs\n",
      "Mohammad R. Rezaei1, 3 and Adji Bousso Dieng2, 3\n",
      "1Institute of Biomedical Engineering, University of Toronto\n",
      "2Department of Computer Science, Princeton University\n",
      "3Vertaix\n",
      "May 26, 2025\n",
      "Abstract\n",
      "Retrieval-augmented generation (RAG) enhances large language models (LLMs)\n",
      "for domain-specific question-answering (QA) tasks by leveraging external\n",
      "knowledge sources. However, traditional RAG systems primarily focus on\n",
      "relevance-based retrieval and often struggle with redundancy, especially when\n",
      "reasoning requires connecting information from multiple sources. This paper\n",
      "introduces Vendi-RAG, a framework based on an iterative process that jointly\n",
      "optimizes retrieval diversity and answer quality. This joint optimization leads\n",
      "to significantly higher accuracy for multi-hop QA tasks. Vendi-RAG leverages\n",
      "the Vendi Score (VS), a flexible similarity-based diversity metric, to promote\n",
      "semantic diversity in document retrieval. It then uses an LLM judge that eval-\n",
      "uates candidate answers, generated after a reasoning step, and outputs a\n",
      "score that the retriever uses to balance relevance and diversity among the\n",
      "retrieved documents during each iteration. Experiments on three challenging\n",
      "datasets—HotpotQA, MuSiQue, and 2WikiMultiHopQA—demonstrate Vendi-\n",
      "RAG’s effectiveness in multi-hop reasoning tasks. The framework achieves sig-\n",
      "nificant accuracy improvements over traditional single-step or multi-step RAG\n",
      "approaches, with accuracy increases reaching +4.2% on HotpotQA, +4.1% on\n",
      "2WikiMultiHopQA, and +1.3% on MuSiQue compared to Adaptive-RAG, the\n",
      "current best baseline. The benefits of Vendi-RAG are even more pronounced as\n",
      "the number of retrieved documents increases. Finally, we evaluated Vendi-RAG\n",
      "across different LLM backbones, including GPT-3.5, GPT-4, and GPT-4o-mini,\n",
      "and observed consistent improvements, demonstrating that the framework’s\n",
      "advantages are model-agnostic.\n",
      "Keywords: RAG, LLMs, Question Answering, NLP, Diversity, Vendi Scoring\n",
      "1\n",
      "Introduction\n",
      "Retrieval-augmented generation (RAG) has emerged as a transformative framework\n",
      "for enhancing the performance of large language models (LLMs) in domain-specific\n",
      "tasks such as question-answering (QA). By retrieving relevant information from\n",
      "external sources beyond the training set, RAG enables LLMs to answer specialized\n",
      "1\n",
      "arXiv:2502.11228v2  [cs.CL]  22 May 2025\n",
      "Retrieval\n",
      "Query\n",
      "Answer\n",
      "LLM Judge \n",
      "Quality Check\n",
      "Rewritten  \n",
      "Query\n",
      "Final \n",
      "Answer\n",
      "Vendi Score\n",
      "Vendi Score\n",
      "Diversity  \n",
      "weight s\n",
      "Decision Block \n",
      "s > threshold Thr \n",
      "Iteration < N\n",
      "…\n",
      "Reasoning\n",
      "Figure 1: The process begins with an initial retrieval step, where a diverse set of\n",
      "documents is retrieved using the Vendi Score, ensuring broad semantic coverage.\n",
      "Next, leveraging a reasoning step to construct a coherent path to the final answer,\n",
      "the LLM generates an answer, which then undergoes quality assessment by an LLM\n",
      "judge. Based on the answer quality, the retriever is adjusted to balance diversity and\n",
      "relevance: high-quality answers limit the emphasis on diversity, while low-quality\n",
      "answers prompt the retriever to prioritize diversity more heavily. This adjustment\n",
      "is controlled by an adaptive parameter, s, which is updated over iterations. The\n",
      "process continues until the answer quality reaches an optimal threshold, denoted\n",
      "by Thr. Finally, the highest-quality responses and documents are selected, ensuring\n",
      "both diversity and accuracy.\n",
      "queries more effectively Achiam et al. (2023); Team et al. (2023); Jiang et al. (2024).\n",
      "This approach has been particularly successful in single-hop QA, where a question\n",
      "can be answered using information from a single document Raiaan et al. (2024);\n",
      "Kwiatkowski et al. (2019). For instance, answering a question such as \"Who wrote\n",
      "the novel Frankenstein?\" only requires retrieving relevant information from a single\n",
      "document containing this fact.\n",
      "However, multi-hop QA introduces significantly greater complexit\n",
      "</Content>\n",
      "</Document>\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36msearch_web\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "<Document href=\"https://adasci.org/how-does-modular-rag-improve-upon-naive-rag/\"/>\n",
      "Naive RAG, the initial implementation of Retrieval-Augmented Generation, operates on a straightforward principle: retrieve relevant documents from an external knowledge base and use these documents to inform the generative process. The retrieval process in Naive RAG is relatively static and lacks flexibility, often leading to inefficiencies and suboptimal integration with the generative model. By adopting a modular architecture, this approach addresses the limitations of Naive RAG, offering enhanced flexibility, scalability, and efficiency. Unlike Naive RAG, which operates as a monolithic entity, Modular RAG breaks down the retrieval and generation processes into distinct, interchangeable modules. * *Seamless Integration*: Generative models in Modular RAG are designed to seamlessly integrate with various retrieval modules, enhancing the coherence and relevance of generated responses.\n",
      "</Document>\n",
      "\n",
      "---\n",
      "\n",
      "<Document href=\"https://rabiloo.com/blog/the-3-types-of-rag-models-naive-rag-modular-rag-and-advanced-rag\"/>\n",
      "Building AI systems requires choosing the right tools for the job, and Retrieval-Augmented Generation (RAG) offers various models, each designed to serve different needs. Essentially, a RAG model retrieves information from external databases, knowledge bases, or the web and then uses that data to generate relevant, meaningful responses. Naive RAG begins by indexing the data source for quick retrieval of relevant information. Modular RAG builds on the basic principles of Naive RAG by breaking down the retrieval and generation processes into separate, specialized modules. Unlike Naive RAG or Modular RAG, this stage is more precise and contextually aware, ensuring that the system retrieves only the most relevant and contextually appropriate data. The generation module in Advanced RAG then processes the refined data using sophisticated models like T5 or GPT-3.\n",
      "</Document>\n",
      "\n",
      "---\n",
      "\n",
      "<Document href=\"https://medium.com/@drjulija/what-are-naive-rag-advanced-rag-modular-rag-paradigms-edff410c202e\"/>\n",
      "I share the differences between Naive RAG, Advanced RAG and Modular RAG frameworks. Retrieval-Augmented Generation (RAG) framework is designed to help mitigate these challenges. RAG enhances LLMs’ knowledge base with additional, domain-specific data. RAG systems can be split into 3 categories: See the comparison between all three paradigms of RAG — Naive RAG, Advanced RAG and Modular RAG below.\n",
      "</Document>\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36manswer_question\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: expert\n",
      "\n",
      "One compelling example of how Modular RAG can enhance performance in a production environment is illustrated in the automotive industry. Here, the demand for efficient document processing in local, low-performance settings, such as offline PDF chatbots, underscores the need for a more optimized approach [1].\n",
      "\n",
      "In this context, Modular RAG was utilized to process complex automotive industry documents, which often required handling multi-column layouts and technical specifications. The modular architecture allowed for tailored retrieval mechanisms and context compression specifically designed to cater to the unique characteristics of automotive documents [1].\n",
      "\n",
      "The study showed significant improvements over a Naive RAG baseline in metrics such as context precision, context recall, answer relevancy, and faithfulness, particularly on specific automotive datasets. The Modular RAG framework's ability to optimize PDF processing and retrieval specifically for the automotive industry's needs resulted in a more efficient and reliable system that could support the industry's rigorous information processing demands [1].\n",
      "\n",
      "This example highlights how Modular RAG can be strategically implemented to optimize performance for document-intensive tasks in a production setting, overcoming the inefficiencies typically associated with a Naive RAG approach.\n",
      "\n",
      "**Source:**\n",
      "\n",
      "[1] http://arxiv.org/abs/2408.05933v1\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36msave_interview\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "\u001b[1;32minterview\u001b[0m:\n",
      "Human: So you said you were writing an article on Explain how Modular RAG differs from traditional Naive RAG and the benefits of using it at the production level.?\n",
      "AI: Hello, my name is Dr. Emily Carter and I’m thrilled to have the opportunity to explore the nuances of Modular RAG and its advantages over traditional Naive RAG with you, Prof. Hinton. As an aspiring AI researcher, I’m especially intrigued by the practical implications of these advancements in real-world applications. To start, could you explain how Modular RAG differentiates from traditional Naive RAG in its structure and functionality?\n",
      "AI: Certainly, Prof. Hinton. Modular RAG differentiates from the traditional Naive RAG mainly in its architecture and approach to the retrieval and generation processes.\n",
      "\n",
      "1. **Architecture**: Naive RAG operates on a straightforward \"retrieve-then-generate\" model, where the retrieval process is relatively static and lacks flexibility. It functions as a single, monolithic entity [5]. In contrast, Modular RAG decomposes the retrieval and generation into independent modules and specialized operators, promoting a highly reconfigurable and scalable framework [1][4]. This modular approach allows for seamless integration of various retrieval techniques with generative models, enhancing the coherence and relevance of generated responses [5].\n",
      "\n",
      "2. **Flexibility and Scalability**: The modular nature of Modular RAG allows for interchangeable modules that can be independently updated or replaced, enabling easier adaptation to specific task requirements and technological advancements. This is in contrast with Naive RAG which is less adaptable to evolving needs [5].\n",
      "\n",
      "3. **Efficiency**: Modular RAG introduces advanced design elements like routing, scheduling, and fusion mechanisms. These elements optimize the flow of information and improve system efficiency, especially when handling complex queries, compared to the linear architecture of Naive RAG [1].\n",
      "\n",
      "The benefits of using Modular RAG in production environments include:\n",
      "\n",
      "- **Increased Efficiency**: By supporting routing, scheduling, and fusion mechanisms, Modular RAG can handle complex and diverse queries efficiently, thus improving the performance on knowledge-intensive tasks [1].\n",
      "\n",
      "- **Scalability**: The ability to reconfigure modules allows enterprises to scale the system up or down based on their computational and task-specific demands without overhauling the entire system [1].\n",
      "\n",
      "- **Enhanced Integration**: Modular RAG's design facilitates better integration with different retrieval mechanisms and LLMs, leading to more accurate and contextually aware responses [4][5].\n",
      "\n",
      "In sum, the Modular RAG framework offers a comprehensive roadmap for advancing RAG systems in a flexible, efficient, and scalable manner, making it well-suited for deployment in dynamic production environments.\n",
      "\n",
      "**Sources:**\n",
      "\n",
      "[1] http://arxiv.org/abs/2407.21059v1\n",
      "[4] https://rabiloo.com/blog/the-3-types-of-rag-models-naive-rag-modular-rag-and-advanced-rag\n",
      "[5] https://adasci.org/how-does-modular-rag-improve-upon-naive-rag/\n",
      "AI: Thank you for the detailed explanation, Prof. Hinton. It’s fascinating how Modular RAG’s flexibility and efficiency potentially transform production applications. Could you provide a specific example or a case study of a production environment where Modular RAG has notably improved performance over Naive RAG?\n",
      "AI: One compelling example of how Modular RAG can enhance performance in a production environment is illustrated in the automotive industry. Here, the demand for efficient document processing in local, low-performance settings, such as offline PDF chatbots, underscores the need for a more optimized approach [1].\n",
      "\n",
      "In this context, Modular RAG was utilized to process complex automotive industry documents, which often required handling multi-column layouts and technical specifications. The modular architecture allowed for tailored retrieval mechanisms and context compression specifically designed to cater to the unique characteristics of automotive documents [1].\n",
      "\n",
      "The study showed significant improvements over a Naive RAG baseline in metrics such as context precision, context recall, answer relevancy, and faithfulness, particularly on specific automotive datasets. The Modular RAG framework's ability to optimize PDF processing and retrieval specifically for the automotive industry's needs resulted in a more efficient and reliable system that could support the industry's rigorous information processing demands [1].\n",
      "\n",
      "This example highlights how Modular RAG can be strategically implemented to optimize performance for document-intensive tasks in a production setting, overcoming the inefficiencies typically associated with a Naive RAG approach.\n",
      "\n",
      "**Source:**\n",
      "\n",
      "[1] http://arxiv.org/abs/2408.05933v1\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36manswer_question\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: expert\n",
      "\n",
      "Yes, certainly! One compelling case study showcasing the successful implementation of Modular RAG is found in the development of a Conversational AI system tailored for airport environments. Airports are highly dynamic with thousands of daily flights, requiring systems that can handle complex, domain-specific data efficiently.\n",
      "\n",
      "**Case Study: Conversational AI in Airports**\n",
      "\n",
      "- **Implementation**: In this specific scenario, a Modular RAG system was implemented to enable airport staff to communicate with flight information systems seamlessly. The system was designed to interpret and respond to diverse queries involving airport jargon, abbreviations, and dynamic questions that necessitated reasoning based on real-time data [1].\n",
      "\n",
      "- **Scalability and Adaptability**: The Modular RAG's architecture allowed the system to manage extensive datasets efficiently, handling queries from multiple data sources like relational databases and knowledge graphs. This modularity ensured that each part of the system could be optimized for accuracy and speed, significantly reducing the incidence of errors and inefficiencies prevalent in Naive RAG implementations [1].\n",
      "\n",
      "- **Benefits Observed**: \n",
      "  - **Reduced Errors**: Modular RAG achieved higher accuracy in providing answers with fewer hallucinations compared to traditional methods, increasing safety and reliability in critical scenarios requiring precise information [1].\n",
      "  - **Enhanced Efficiency**: By breaking down the retrieval and generation components into distinct modules, the system improved query handling speed and accuracy, crucial for real-time, high-pressure environments like airports [1].\n",
      "  - **Real-Time Adaptability**: The flexibility inherent in a modular setup allowed for dynamic updating and integrating of new datasets without disrupting overall system functionality, a critical requirement given the ever-evolving nature of flight data [1].\n",
      "\n",
      "This example clearly demonstrates Modular RAG's ability to scale and adapt within a complex production environment, ensuring both efficiency and reliability in handling large and varied datasets.\n",
      "\n",
      "**Sources**:\n",
      "- [1] http://arxiv.org/abs/2505.13006v1\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36msave_interview\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "\u001b[1;32minterview\u001b[0m:\n",
      "Human: So you said you were writing an article on Explain how Modular RAG differs from traditional Naive RAG and the benefits of using it at the production level.?\n",
      "AI: Hello, I'm John Harper, a technology writer exploring cutting-edge advancements in AI scalability. Dr. Torres, could you please explain how Modular RAG differs from traditional Naive RAG, specifically in terms of scalability and handling large datasets? How does modularity enhance its performance in production environments?\n",
      "AI: Certainly, Dr. Torres. Modular RAG (Retrieval-Augmented Generation) diverges from the traditional Naive RAG approach by breaking down the retrieval and generation processes into separate, specialized modules. This design promotes scalability and efficiency, especially when dealing with large datasets.\n",
      "\n",
      "**Differences between Modular RAG and Naive RAG:**\n",
      "\n",
      "1. **Architecture**: \n",
      "   - **Naive RAG** operates as a single monolithic system where retrieval and generation processes are intertwined, potentially leading to inefficiencies when scaling [2].\n",
      "   - **Modular RAG**, on the other hand, divides these tasks into dedicated modules. This separation allows each module to be optimized individually, improving system performance and accuracy [3].\n",
      "\n",
      "2. **Scalability**:\n",
      "   - The modular approach enhances scalability because it can efficiently manage larger datasets by leveraging specialized units that focus on specific tasks. This is particularly beneficial in environments requiring the integration of diverse, extensive, or dynamic data sources [3].\n",
      "   \n",
      "3. **Adaptability**:\n",
      "   - Modular RAG is more adaptable to production-level scenarios due to its flexibility in module management. It can easily incorporate or update modules without overhauling the entire system, unlike Naive RAG, which might require significant restructuring when upgrading components or adding new functionalities [4].\n",
      "\n",
      "**Benefits of Using Modular RAG at the Production Level:**\n",
      "\n",
      "- **Enhanced Query Efficiency**: By delegating specific queries to modules optimized for different data sources (relational, NoSQL, etc.), Modular RAG reduces token overhead and processing latency, thereby speeding up response times and improving accuracy [3].\n",
      "- **Robustness and Flexibility**: Its architecture allows for easy integration of new data sources or adjustment to changing data types, making it highly suitable for dynamic production environments such as those involving real-time data updates [3].\n",
      "- **Improved Contextual Awareness**: Modular RAG can achieve better semantic depth by refining the retrieval process to ensure only the most pertinent and contextually appropriate data is processed [2].\n",
      "\n",
      "In summary, by structuring retrieval and generation into distinct, specialized tasks, Modular RAG offers significant scalability benefits, making it well-suited for complex, large-scale production environments. This modularity not only enhances performance but also facilitates adaptability to ever-changing organizational data needs [3].\n",
      "\n",
      "**Sources**:\n",
      "- [2] https://rabiloo.com/blog/the-3-types-of-rag-models-naive-rag-modular-rag-and-advanced-rag\n",
      "- [3] http://arxiv.org/abs/2412.05838v1\n",
      "- [4] https://www.digitalocean.com/community/conceptual-articles/rag-ai-agents-agentic-rag-comparative-analysis\n",
      "AI: Fascinating insights, Dr. Torres. Could you share a specific example or case study where Modular RAG has been successfully implemented, highlighting its scalability and adaptability benefits in a production scenario?\n",
      "AI: Yes, certainly! One compelling case study showcasing the successful implementation of Modular RAG is found in the development of a Conversational AI system tailored for airport environments. Airports are highly dynamic with thousands of daily flights, requiring systems that can handle complex, domain-specific data efficiently.\n",
      "\n",
      "**Case Study: Conversational AI in Airports**\n",
      "\n",
      "- **Implementation**: In this specific scenario, a Modular RAG system was implemented to enable airport staff to communicate with flight information systems seamlessly. The system was designed to interpret and respond to diverse queries involving airport jargon, abbreviations, and dynamic questions that necessitated reasoning based on real-time data [1].\n",
      "\n",
      "- **Scalability and Adaptability**: The Modular RAG's architecture allowed the system to manage extensive datasets efficiently, handling queries from multiple data sources like relational databases and knowledge graphs. This modularity ensured that each part of the system could be optimized for accuracy and speed, significantly reducing the incidence of errors and inefficiencies prevalent in Naive RAG implementations [1].\n",
      "\n",
      "- **Benefits Observed**: \n",
      "  - **Reduced Errors**: Modular RAG achieved higher accuracy in providing answers with fewer hallucinations compared to traditional methods, increasing safety and reliability in critical scenarios requiring precise information [1].\n",
      "  - **Enhanced Efficiency**: By breaking down the retrieval and generation components into distinct modules, the system improved query handling speed and accuracy, crucial for real-time, high-pressure environments like airports [1].\n",
      "  - **Real-Time Adaptability**: The flexibility inherent in a modular setup allowed for dynamic updating and integrating of new datasets without disrupting overall system functionality, a critical requirement given the ever-evolving nature of flight data [1].\n",
      "\n",
      "This example clearly demonstrates Modular RAG's ability to scale and adapt within a complex production environment, ensuring both efficiency and reliability in handling large and varied datasets.\n",
      "\n",
      "**Sources**:\n",
      "- [1] http://arxiv.org/abs/2505.13006v1\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36manswer_question\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: expert\n",
      "\n",
      "In a specific implementation of Retrieval-Augmented Generation (RAG) within the automotive industry, Modular RAG demonstrated significant improvements in real-world reliability and latency reduction compared to Naive RAG. This was particularly evident in processing complex industry documents, such as those with multi-column layouts and detailed technical specifications.\n",
      "\n",
      "**Case Study: Automotive Industry Document Processing**\n",
      "\n",
      "1. **Improved PDF Processing and Contextual Relevance:**\n",
      "   - The study highlighted that Modular RAG's approach allowed for targeted enhancements in PDF processing, retrieval mechanisms, and context compression tailored to automotive documents. This specificity was not feasible with Naive RAG, which lacks the distinct, interchangeable modules for specialized tasks [7].\n",
      "\n",
      "2. **Enhanced Efficiency with Modular Architecture:**\n",
      "   - By deploying a modular architecture, the system was able to break down the retrieval and generation processes. Each module was optimized independently. For example, improvements were made in retrieval accuracy using advanced indexing techniques and sophisticated linguistic models in the generation phase. This modular optimization led to reduced latency and improved overall system efficiency [7].\n",
      "\n",
      "3. **Real-World Reliability:**\n",
      "   - The separation of retrieval and generation into dedicated modules enhanced the reliability of responses, as each module could be updated and tuned without disrupting the overall system. This was particularly beneficial in ensuring the responses were both relevant and precise in real-time, a key requirement for industrial applications like PDF chatbots in production environments where efficiency and accuracy are crucial [7].\n",
      "\n",
      "By adopting a modular structure, the automotive case study achieved a remarkable boost in both operational speed and reliability, illustrating the practical benefits of Modular RAG over the more simplified Naive RAG framework.\n",
      "\n",
      "[7] http://arxiv.org/abs/2408.05933v1\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36msave_interview\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "\u001b[1;32minterview\u001b[0m:\n",
      "Human: So you said you were writing an article on Explain how Modular RAG differs from traditional Naive RAG and the benefits of using it at the production level.?\n",
      "AI: Hello, Dr. Kahn, my name is Alex Ramirez, and I'm a technology journalist interested in exploring advancements in AI performance metrics. I'm particularly intrigued by the differences between Modular RAG and Naive RAG. Could you provide an explanation of how these two approaches differ specifically, and what benefits modular RAG offers when used in production environments?\n",
      "AI: Certainly, let's delve into the distinctions between Modular RAG and Naive RAG, particularly focusing on their applications and benefits in production environments.\n",
      "\n",
      "Naive RAG combines information retrieval with natural language generation to answer queries or prompts. It typically uses retrieval models that rank indexed data based on relevance to the input query. This approach generates responses by combining the input query with the retrieved context, aiming for coherence and relevance [3].\n",
      "\n",
      "In contrast, Modular RAG builds upon the basic principles of Naive RAG. It decomposes the retrieval and generation processes into separate, specialized modules. This separation allows each module to be optimized independently, offering greater control over each component of the RAG process. This modular structure enhances the precision and contextual awareness of the system, ensuring that the retrieved data is not only relevant but also contextually appropriate. The specialized design of each module allows for integration with sophisticated models, such as T5 or GPT-3, in the generation phase [4][5].\n",
      "\n",
      "**Benefits of Modular RAG in Production:**\n",
      "\n",
      "1. **Improved Precision and Contextual Relevance:**\n",
      "   - Modular RAG's separate modules allow for more precise retrieval, ensuring that only the most relevant and contextually suitable data is drawn for generation. This reduces the noise and potential inaccuracies that can arise from a single, monolithic approach [4].\n",
      "\n",
      "2. **Scalability and Flexibility:**\n",
      "   - The modular nature means that components can be scaled or modified independently without affecting the entire system. This flexibility is particularly beneficial in dynamic production environments where specific tasks may require different handling or specialized models [4][5].\n",
      "\n",
      "3. **Enhanced Efficiency:**\n",
      "   - By optimizing each module independently, Modular RAG can achieve efficiencies that aren't possible with Naive RAG. This results in reduced latency and improved response times, which are critical in real-world applications where speed and reliability are paramount [4].\n",
      "\n",
      "In summary, while Naive RAG offers a simpler, integrated approach to combining retrieval and generation, Modular RAG provides a more refined, flexible framework that improves accuracy, efficiency, and contextual relevance—key factors that enhance performance in production settings.\n",
      "\n",
      "Sources:\n",
      "[3] https://www.superteams.ai/blog/how-to-implement-naive-rag-advanced-rag-and-modular-rag\n",
      "[4] https://rabiloo.com/blog/the-3-types-of-rag-models-naive-rag-modular-rag-and-advanced-rag\n",
      "[5] https://medium.com/@drjulija/what-are-naive-rag-advanced-rag-modular-rag-paradigms-edff410c202e\n",
      "AI: That's a comprehensive overview! I’m curious, could you provide a specific example or case where implementing Modular RAG in a production environment significantly reduced latency compared to Naive RAG? How was the real-world reliability enhanced in this case?\n",
      "AI: In a specific implementation of Retrieval-Augmented Generation (RAG) within the automotive industry, Modular RAG demonstrated significant improvements in real-world reliability and latency reduction compared to Naive RAG. This was particularly evident in processing complex industry documents, such as those with multi-column layouts and detailed technical specifications.\n",
      "\n",
      "**Case Study: Automotive Industry Document Processing**\n",
      "\n",
      "1. **Improved PDF Processing and Contextual Relevance:**\n",
      "   - The study highlighted that Modular RAG's approach allowed for targeted enhancements in PDF processing, retrieval mechanisms, and context compression tailored to automotive documents. This specificity was not feasible with Naive RAG, which lacks the distinct, interchangeable modules for specialized tasks [7].\n",
      "\n",
      "2. **Enhanced Efficiency with Modular Architecture:**\n",
      "   - By deploying a modular architecture, the system was able to break down the retrieval and generation processes. Each module was optimized independently. For example, improvements were made in retrieval accuracy using advanced indexing techniques and sophisticated linguistic models in the generation phase. This modular optimization led to reduced latency and improved overall system efficiency [7].\n",
      "\n",
      "3. **Real-World Reliability:**\n",
      "   - The separation of retrieval and generation into dedicated modules enhanced the reliability of responses, as each module could be updated and tuned without disrupting the overall system. This was particularly beneficial in ensuring the responses were both relevant and precise in real-time, a key requirement for industrial applications like PDF chatbots in production environments where efficiency and accuracy are crucial [7].\n",
      "\n",
      "By adopting a modular structure, the automotive case study achieved a remarkable boost in both operational speed and reliability, illustrating the practical benefits of Modular RAG over the more simplified Naive RAG framework.\n",
      "\n",
      "[7] http://arxiv.org/abs/2408.05933v1\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mwrite_section\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "## Advancing AI: The Promise and Potential of Modular Retrieval-Augmented Generation\n",
      "\n",
      "### Summary\n",
      "\n",
      "Retrieval-augmented Generation (RAG) has become a transformative approach in the realm of artificial intelligence, particularly for enhancing the capabilities of Large Language Models (LLMs) in knowledge-intensive tasks. Prof. Hinton, a leading authority on neural networks and deep learning, is particularly focused on the potential improvements that Modular RAG can bring over traditional Naive RAG models. The most compelling development in this area is the conceptualization of RAG systems akin to LEGO-like frameworks through modular decomposition, which ensures high reconfigurability and adaptability in various production environments [1].\n",
      "\n",
      "The source documents contain insights on Modular RAG’s potential to redefine the efficiency and scalability of AI systems through a non-linear architecture that incorporates advanced design elements such as routing, scheduling, and fusion mechanisms [1][2]. These modular structures allow for the integration of advanced retrieval methods and LLMs, enhancing the system’s ability to manage complex queries with greater precision and efficacy. This transformative evolution of RAG is evident in various domains, ranging from conversational AIs in airports to PDF chatbots in the automotive industry [3][4].\n",
      "\n",
      "Surprisingly, recent studies highlight that while traditional RAG systems experience limitations due to redundant and noise-laden retrievals, Modular RAG offers a pathway to mitigate such challenges by embracing a more segmented and precise retrieval methodology. The Modularity enhances the flexibility, scalability, and efficiency of these systems, aligning them more closely with the dynamism of real-world applications [5][6].\n",
      "\n",
      "Sources referenced:\n",
      "1. Yunfan Gao, Yun Xiong, Meng Wang, Haofen Wang. \"Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks.\" (2024).\n",
      "2. Yuyang Li et al. \"Evaluating the Performance of RAG Methods for Conversational AI in the Airport Domain.\" (2025).\n",
      "3. Fei Liu et al. \"Optimizing RAG Techniques for Automotive Industry PDF Chatbots: A Case Study with Locally Deployed Ollama Models.\" (2024).\n",
      "4. Jiajing Chen et al. \"Optimizing Retrieval-Augmented Generation with Elasticsearch for Enhanced Question-Answering Systems.\" (2024).\n",
      "5. Zezhou Yang et al. \"A Deep Dive into Retrieval-Augmented Generation for Code Completion: Experience on WeChat.\" (2025).\n",
      "6. Xiwei Xu et al. \"RAGOps: Operating and Managing Retrieval-Augmented Generation Pipelines.\" (2025).\n",
      "\n",
      "### Comprehensive Analysis\n",
      "\n",
      "The advancement of Retrieval-Augmented Generation (RAG) through a modular approach represents a significant leap forward in AI technology, especially in terms of efficiency and scalability in production environments. This section delves into the key insights derived from the selected documents, focusing on the unique attributes of Modular RAG and its implications for AI development.\n",
      "\n",
      "#### Modular RAG vs. Naive RAG\n",
      "\n",
      "- **Decomposition into Modules**: Modular RAG deconstructs complex systems into independent modules, each with specialized operators that allow flexible and reconfigurable use. This approach helps in transcending the linear \"retrieve-then-generate\" architecture, leading to more nuanced and dynamic interpolations [1].\n",
      "- **Scalability and Flexibility**: Unlike Naive RAG that predominantly operates as a monolithic entity, the modular framework inherently supports scalability by enabling seamless integration and replacement of individual modules. This characteristic is vital for rapidly evolving application scenarios where adaptability is crucial [2][3].\n",
      "- **Enhanced Functionality and Precision**: By incorporating routing, scheduling, and fusion mechanisms, Modular RAG achieves better operational precision, handling complex queries effectively. In the context of conversational AI systems at airports, for instance, this modularity significantly reduces hallucinations and improves the system's performance on dynamic questions [4].\n",
      "\n",
      "#### Implementation in Diverse Sectors\n",
      "\n",
      "- **Conversational AI in Dynamic Environments**: In highly dynamic settings like airports, Modular RAG outperforms traditional models by effectively handling jargon, abbreviations, and dynamic queries. Knowledge Graph-based RAG, a variant of Modular RAG, has shown superior accuracy in these applications, providing a safer and more reliable AI component in critical operations [2].\n",
      "- **Automotive Industry Adoption**: Automating PDF-based chatbots within the automotive sector exemplifies another application area. By optimizing RAG techniques through modular frameworks, systems can better handle automotive-specific documentation complexities, like multi-column layouts and technical jargon, thus elevating the precision and relevance of responses [3].\n",
      "- **Code Completion in Software Engineering**: The adoption of RAG for code completion, especially in industrial-scale systems like WeChat, highlights the advantages of combining lexical and semantic retrieval techniques within a modular framework. These techniques enable the system to effectively navigate closed-source codebases, thus enhancing developer productivity and accuracy in software development environments [5].\n",
      "\n",
      "#### Operational and Management Insights\n",
      "\n",
      "- **RAGOps**: Operational frameworks like RAGOps extend these benefits by focusing on data management and operational life cycle automation, addressing the evolving requirements in enterprise environments. This approach underscores the importance of integrating real-time retrieval with LLM systems, ensuring that the AI components remain updated and relevant [6].\n",
      "- **Enterprise Applications**: By leveraging modular RAG structures, enterprises can significantly enhance the performance of compound AI systems. This enhancement is crucial for applications demanding high adaptability and precision, ensuring that AI solutions remain robust and efficient despite the constant influx of new data and evolving application needs [6].\n",
      "\n",
      "#### Future Prospects and Challenges\n",
      "\n",
      "- **Research Directions**: Future research could explore the integration of higher-level semantic understanding within Modular RAG systems to further refine their capability in handling multi-faceted queries and dynamic scenarios [4].\n",
      "- **Technical Challenges**: A prominent challenge in deploying Modular RAG involves ensuring seamless interoperability between diverse modules and maintaining the quality of data retrieval processes. Addressing these challenges is essential to fully leverage the potential of RAG in advancing AI technology [5].\n",
      "\n",
      "In summary, Modular RAG offers transformative solutions for enhancing AI systems, making them more adaptable, efficient, and precise. The integration of modular architectures into RAG systems aligns well with real-world demands, offering a sustainable pathway for the evolution of AI in commercial and industrial applications.\n",
      "\n",
      "### Sources\n",
      "[1] http://arxiv.org/abs/2407.21059v1  \n",
      "[2] http://arxiv.org/abs/2505.13006v1  \n",
      "[3] http://arxiv.org/abs/2408.05933v1  \n",
      "[4] http://arxiv.org/abs/2410.14167v1  \n",
      "[5] http://arxiv.org/abs/2507.18515v1  \n",
      "[6] http://arxiv.org/abs/2506.03401v1  \n",
      "[7] https://rabiloo.com/blog/the-3-types-of-rag-models-naive-rag-modular-rag-and-advanced-rag  \n",
      "[8] https://adasci.org/how-does-modular-rag-improve-upon-naive-rag/\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mconduct_interview\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "## Advancing AI: The Promise and Potential of Modular Retrieval-Augmented Generation\n",
      "\n",
      "### Summary\n",
      "\n",
      "Retrieval-augmented Generation (RAG) has become a transformative approach in the realm of artificial intelligence, particularly for enhancing the capabilities of Large Language Models (LLMs) in knowledge-intensive tasks. Prof. Hinton, a leading authority on neural networks and deep learning, is particularly focused on the potential improvements that Modular RAG can bring over traditional Naive RAG models. The most compelling development in this area is the conceptualization of RAG systems akin to LEGO-like frameworks through modular decomposition, which ensures high reconfigurability and adaptability in various production environments [1].\n",
      "\n",
      "The source documents contain insights on Modular RAG’s potential to redefine the efficiency and scalability of AI systems through a non-linear architecture that incorporates advanced design elements such as routing, scheduling, and fusion mechanisms [1][2]. These modular structures allow for the integration of advanced retrieval methods and LLMs, enhancing the system’s ability to manage complex queries with greater precision and efficacy. This transformative evolution of RAG is evident in various domains, ranging from conversational AIs in airports to PDF chatbots in the automotive industry [3][4].\n",
      "\n",
      "Surprisingly, recent studies highlight that while traditional RAG systems experience limitations due to redundant and noise-laden retrievals, Modular RAG offers a pathway to mitigate such challenges by embracing a more segmented and precise retrieval methodology. The Modularity enhances the flexibility, scalability, and efficiency of these systems, aligning them more closely with the dynamism of real-world applications [5][6].\n",
      "\n",
      "Sources referenced:\n",
      "1. Yunfan Gao, Yun Xiong, Meng Wang, Haofen Wang. \"Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks.\" (2024).\n",
      "2. Yuyang Li et al. \"Evaluating the Performance of RAG Methods for Conversational AI in the Airport Domain.\" (2025).\n",
      "3. Fei Liu et al. \"Optimizing RAG Techniques for Automotive Industry PDF Chatbots: A Case Study with Locally Deployed Ollama Models.\" (2024).\n",
      "4. Jiajing Chen et al. \"Optimizing Retrieval-Augmented Generation with Elasticsearch for Enhanced Question-Answering Systems.\" (2024).\n",
      "5. Zezhou Yang et al. \"A Deep Dive into Retrieval-Augmented Generation for Code Completion: Experience on WeChat.\" (2025).\n",
      "6. Xiwei Xu et al. \"RAGOps: Operating and Managing Retrieval-Augmented Generation Pipelines.\" (2025).\n",
      "\n",
      "### Comprehensive Analysis\n",
      "\n",
      "The advancement of Retrieval-Augmented Generation (RAG) through a modular approach represents a significant leap forward in AI technology, especially in terms of efficiency and scalability in production environments. This section delves into the key insights derived from the selected documents, focusing on the unique attributes of Modular RAG and its implications for AI development.\n",
      "\n",
      "#### Modular RAG vs. Naive RAG\n",
      "\n",
      "- **Decomposition into Modules**: Modular RAG deconstructs complex systems into independent modules, each with specialized operators that allow flexible and reconfigurable use. This approach helps in transcending the linear \"retrieve-then-generate\" architecture, leading to more nuanced and dynamic interpolations [1].\n",
      "- **Scalability and Flexibility**: Unlike Naive RAG that predominantly operates as a monolithic entity, the modular framework inherently supports scalability by enabling seamless integration and replacement of individual modules. This characteristic is vital for rapidly evolving application scenarios where adaptability is crucial [2][3].\n",
      "- **Enhanced Functionality and Precision**: By incorporating routing, scheduling, and fusion mechanisms, Modular RAG achieves better operational precision, handling complex queries effectively. In the context of conversational AI systems at airports, for instance, this modularity significantly reduces hallucinations and improves the system's performance on dynamic questions [4].\n",
      "\n",
      "#### Implementation in Diverse Sectors\n",
      "\n",
      "- **Conversational AI in Dynamic Environments**: In highly dynamic settings like airports, Modular RAG outperforms traditional models by effectively handling jargon, abbreviations, and dynamic queries. Knowledge Graph-based RAG, a variant of Modular RAG, has shown superior accuracy in these applications, providing a safer and more reliable AI component in critical operations [2].\n",
      "- **Automotive Industry Adoption**: Automating PDF-based chatbots within the automotive sector exemplifies another application area. By optimizing RAG techniques through modular frameworks, systems can better handle automotive-specific documentation complexities, like multi-column layouts and technical jargon, thus elevating the precision and relevance of responses [3].\n",
      "- **Code Completion in Software Engineering**: The adoption of RAG for code completion, especially in industrial-scale systems like WeChat, highlights the advantages of combining lexical and semantic retrieval techniques within a modular framework. These techniques enable the system to effectively navigate closed-source codebases, thus enhancing developer productivity and accuracy in software development environments [5].\n",
      "\n",
      "#### Operational and Management Insights\n",
      "\n",
      "- **RAGOps**: Operational frameworks like RAGOps extend these benefits by focusing on data management and operational life cycle automation, addressing the evolving requirements in enterprise environments. This approach underscores the importance of integrating real-time retrieval with LLM systems, ensuring that the AI components remain updated and relevant [6].\n",
      "- **Enterprise Applications**: By leveraging modular RAG structures, enterprises can significantly enhance the performance of compound AI systems. This enhancement is crucial for applications demanding high adaptability and precision, ensuring that AI solutions remain robust and efficient despite the constant influx of new data and evolving application needs [6].\n",
      "\n",
      "#### Future Prospects and Challenges\n",
      "\n",
      "- **Research Directions**: Future research could explore the integration of higher-level semantic understanding within Modular RAG systems to further refine their capability in handling multi-faceted queries and dynamic scenarios [4].\n",
      "- **Technical Challenges**: A prominent challenge in deploying Modular RAG involves ensuring seamless interoperability between diverse modules and maintaining the quality of data retrieval processes. Addressing these challenges is essential to fully leverage the potential of RAG in advancing AI technology [5].\n",
      "\n",
      "In summary, Modular RAG offers transformative solutions for enhancing AI systems, making them more adaptable, efficient, and precise. The integration of modular architectures into RAG systems aligns well with real-world demands, offering a sustainable pathway for the evolution of AI in commercial and industrial applications.\n",
      "\n",
      "### Sources\n",
      "[1] http://arxiv.org/abs/2407.21059v1  \n",
      "[2] http://arxiv.org/abs/2505.13006v1  \n",
      "[3] http://arxiv.org/abs/2408.05933v1  \n",
      "[4] http://arxiv.org/abs/2410.14167v1  \n",
      "[5] http://arxiv.org/abs/2507.18515v1  \n",
      "[6] http://arxiv.org/abs/2506.03401v1  \n",
      "[7] https://rabiloo.com/blog/the-3-types-of-rag-models-naive-rag-modular-rag-and-advanced-rag  \n",
      "[8] https://adasci.org/how-does-modular-rag-improve-upon-naive-rag/\n",
      "==================================================\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-Soffyuxth7vsUnobnzk0MGeR on tokens per min (TPM): Limit 30000, Used 29334, Requested 11245. Please try again in 21.158s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[231], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 그래프 실행\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43minvoke_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_teddynote/messages.py:409\u001b[0m, in \u001b[0;36minvoke_graph\u001b[0;34m(graph, inputs, config, node_names, callback)\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m namespace[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(namespace) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroot graph\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;66;03m# subgraphs=True 를 통해 서브그래프의 출력도 포함\u001b[39;00m\n\u001b[0;32m--> 409\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m namespace, chunk \u001b[38;5;129;01min\u001b[39;00m graph\u001b[38;5;241m.\u001b[39mstream(\n\u001b[1;32m    410\u001b[0m     inputs, config, stream_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupdates\u001b[39m\u001b[38;5;124m\"\u001b[39m, subgraphs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    411\u001b[0m ):\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m node_name, node_chunk \u001b[38;5;129;01min\u001b[39;00m chunk\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    413\u001b[0m         \u001b[38;5;66;03m# node_names가 비어있지 않은 경우에만 필터링\u001b[39;00m\n\u001b[1;32m    414\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(node_names) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m node_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m node_names:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/main.py:2647\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[0m\n\u001b[1;32m   2645\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mmatch_cached_writes():\n\u001b[1;32m   2646\u001b[0m     loop\u001b[38;5;241m.\u001b[39moutput_writes(task\u001b[38;5;241m.\u001b[39mid, task\u001b[38;5;241m.\u001b[39mwrites, cached\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m-> 2647\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mtick(\n\u001b[1;32m   2648\u001b[0m     [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mvalues() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t\u001b[38;5;241m.\u001b[39mwrites],\n\u001b[1;32m   2649\u001b[0m     timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[1;32m   2650\u001b[0m     get_waiter\u001b[38;5;241m=\u001b[39mget_waiter,\n\u001b[1;32m   2651\u001b[0m     schedule_task\u001b[38;5;241m=\u001b[39mloop\u001b[38;5;241m.\u001b[39maccept_push,\n\u001b[1;32m   2652\u001b[0m ):\n\u001b[1;32m   2653\u001b[0m     \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[1;32m   2654\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m _output(\n\u001b[1;32m   2655\u001b[0m         stream_mode, print_mode, subgraphs, stream\u001b[38;5;241m.\u001b[39mget, queue\u001b[38;5;241m.\u001b[39mEmpty\n\u001b[1;32m   2656\u001b[0m     )\n\u001b[1;32m   2657\u001b[0m loop\u001b[38;5;241m.\u001b[39mafter_tick()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/_runner.py:253\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;66;03m# panic on failure or timeout\u001b[39;00m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 253\u001b[0m     \u001b[43m_panic_or_proceed\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfutures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdone\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfutures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpanic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tb \u001b[38;5;241m:=\u001b[39m exc\u001b[38;5;241m.\u001b[39m__traceback__:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/_runner.py:511\u001b[0m, in \u001b[0;36m_panic_or_proceed\u001b[0;34m(futs, timeout_exc_cls, panic)\u001b[0m\n\u001b[1;32m    509\u001b[0m                 interrupts\u001b[38;5;241m.\u001b[39mappend(exc)\n\u001b[1;32m    510\u001b[0m             \u001b[38;5;28;01melif\u001b[39;00m fut \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m SKIP_RERAISE_SET:\n\u001b[0;32m--> 511\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m    512\u001b[0m \u001b[38;5;66;03m# raise combined interrupts\u001b[39;00m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m interrupts:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/_executor.py:81\u001b[0m, in \u001b[0;36mBackgroundExecutor.done\u001b[0;34m(self, task)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Remove the task from the tasks dict when it's done.\"\"\"\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 81\u001b[0m     \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GraphBubbleUp:\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;66;03m# This exception is an interruption signal, not an error\u001b[39;00m\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;66;03m# so we don't want to re-raise it on exit\u001b[39;00m\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mpop(task)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/concurrent/futures/_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/concurrent/futures/_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/_retry.py:42\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[1;32m     40\u001b[0m     task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     44\u001b[0m     ns: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/_internal/_runnable.py:657\u001b[0m, in \u001b[0;36mRunnableSeq.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    655\u001b[0m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[1;32m    656\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[0;32m--> 657\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    659\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/main.py:3026\u001b[0m, in \u001b[0;36mPregel.invoke\u001b[0;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[0m\n\u001b[1;32m   3023\u001b[0m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m|\u001b[39m Any] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   3024\u001b[0m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m-> 3026\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream(\n\u001b[1;32m   3027\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   3028\u001b[0m     config,\n\u001b[1;32m   3029\u001b[0m     context\u001b[38;5;241m=\u001b[39mcontext,\n\u001b[1;32m   3030\u001b[0m     stream_mode\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupdates\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   3031\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3032\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m stream_mode,\n\u001b[1;32m   3033\u001b[0m     print_mode\u001b[38;5;241m=\u001b[39mprint_mode,\n\u001b[1;32m   3034\u001b[0m     output_keys\u001b[38;5;241m=\u001b[39moutput_keys,\n\u001b[1;32m   3035\u001b[0m     interrupt_before\u001b[38;5;241m=\u001b[39minterrupt_before,\n\u001b[1;32m   3036\u001b[0m     interrupt_after\u001b[38;5;241m=\u001b[39minterrupt_after,\n\u001b[1;32m   3037\u001b[0m     durability\u001b[38;5;241m=\u001b[39mdurability,\n\u001b[1;32m   3038\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3039\u001b[0m ):\n\u001b[1;32m   3040\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   3041\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(chunk) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/main.py:2647\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[0m\n\u001b[1;32m   2645\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mmatch_cached_writes():\n\u001b[1;32m   2646\u001b[0m     loop\u001b[38;5;241m.\u001b[39moutput_writes(task\u001b[38;5;241m.\u001b[39mid, task\u001b[38;5;241m.\u001b[39mwrites, cached\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m-> 2647\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mtick(\n\u001b[1;32m   2648\u001b[0m     [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mvalues() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t\u001b[38;5;241m.\u001b[39mwrites],\n\u001b[1;32m   2649\u001b[0m     timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[1;32m   2650\u001b[0m     get_waiter\u001b[38;5;241m=\u001b[39mget_waiter,\n\u001b[1;32m   2651\u001b[0m     schedule_task\u001b[38;5;241m=\u001b[39mloop\u001b[38;5;241m.\u001b[39maccept_push,\n\u001b[1;32m   2652\u001b[0m ):\n\u001b[1;32m   2653\u001b[0m     \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[1;32m   2654\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m _output(\n\u001b[1;32m   2655\u001b[0m         stream_mode, print_mode, subgraphs, stream\u001b[38;5;241m.\u001b[39mget, queue\u001b[38;5;241m.\u001b[39mEmpty\n\u001b[1;32m   2656\u001b[0m     )\n\u001b[1;32m   2657\u001b[0m loop\u001b[38;5;241m.\u001b[39mafter_tick()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/_runner.py:162\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[0m\n\u001b[1;32m    160\u001b[0m t \u001b[38;5;241m=\u001b[39m tasks[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 162\u001b[0m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m                \u001b[49m\u001b[43m_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m                \u001b[49m\u001b[43mweakref\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m                \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m                \u001b[49m\u001b[43mfutures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweakref\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m                \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschedule_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m                \u001b[49m\u001b[43msubmit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/pregel/_retry.py:42\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[1;32m     40\u001b[0m     task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     44\u001b[0m     ns: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/_internal/_runnable.py:657\u001b[0m, in \u001b[0;36mRunnableSeq.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    655\u001b[0m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[1;32m    656\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[0;32m--> 657\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    659\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langgraph/_internal/_runnable.py:401\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    399\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(ret)\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 401\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[1;32m    403\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "Cell \u001b[0;32mIn[218], line 238\u001b[0m, in \u001b[0;36mwrite_section\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;66;03m# 섹션 작성을 위한 시스템 프롬프트 정의\u001b[39;00m\n\u001b[1;32m    237\u001b[0m system_message \u001b[38;5;241m=\u001b[39m section_writer_instructions\u001b[38;5;241m.\u001b[39mformat(focus\u001b[38;5;241m=\u001b[39manalyst\u001b[38;5;241m.\u001b[39mdescription)\n\u001b[0;32m--> 238\u001b[0m section \u001b[38;5;241m=\u001b[39m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mSystemMessage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msystem_message\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mHumanMessage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mUse this source to write your section: \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mcontext\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;66;03m# 상태에 섹션 추가\u001b[39;00m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msections\u001b[39m\u001b[38;5;124m\"\u001b[39m: [section\u001b[38;5;241m.\u001b[39mcontent]}\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:393\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    389\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    390\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    391\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    392\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatGeneration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m--> 393\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    403\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:1019\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m   1011\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m   1012\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1016\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m   1017\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m   1018\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m-> 1019\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:837\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    834\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[1;32m    835\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    836\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 837\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    838\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    840\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    843\u001b[0m         )\n\u001b[1;32m    844\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    845\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:1085\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1083\u001b[0m     result \u001b[38;5;241m=\u001b[39m generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[1;32m   1084\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1085\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1086\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m   1087\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1088\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1089\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_openai/chat_models/base.py:1183\u001b[0m, in \u001b[0;36mBaseChatOpenAI._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1181\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m raw_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(raw_response, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp_response\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1182\u001b[0m         e\u001b[38;5;241m.\u001b[39mresponse \u001b[38;5;241m=\u001b[39m raw_response\u001b[38;5;241m.\u001b[39mhttp_response  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m-> 1183\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m   1184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1185\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minclude_response_headers\n\u001b[1;32m   1186\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m raw_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(raw_response, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1188\u001b[0m ):\n\u001b[1;32m   1189\u001b[0m     generation_info \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(raw_response\u001b[38;5;241m.\u001b[39mheaders)}\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/langchain_openai/chat_models/base.py:1178\u001b[0m, in \u001b[0;36mBaseChatOpenAI._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1171\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _construct_lc_result_from_responses_api(\n\u001b[1;32m   1172\u001b[0m             response,\n\u001b[1;32m   1173\u001b[0m             schema\u001b[38;5;241m=\u001b[39moriginal_schema_obj,\n\u001b[1;32m   1174\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mgeneration_info,\n\u001b[1;32m   1175\u001b[0m             output_version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_version,\n\u001b[1;32m   1176\u001b[0m         )\n\u001b[1;32m   1177\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1178\u001b[0m         raw_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwith_raw_response\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1179\u001b[0m         response \u001b[38;5;241m=\u001b[39m raw_response\u001b[38;5;241m.\u001b[39mparse()\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_legacy_response.py:364\u001b[0m, in \u001b[0;36mto_raw_response_wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    360\u001b[0m extra_headers[RAW_RESPONSE_HEADER] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    362\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextra_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m extra_headers\n\u001b[0;32m--> 364\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cast(LegacyAPIResponse[R], \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_utils/_utils.py:287\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 287\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py:1147\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   1101\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m   1102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m   1103\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m   1145\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m   1146\u001b[0m     validate_response_format(response_format)\n\u001b[0;32m-> 1147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1150\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m   1151\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1152\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1153\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maudio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1154\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1155\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1156\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1157\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1158\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1159\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_completion_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1160\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1162\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodalities\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1163\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1165\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1166\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1167\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprompt_cache_key\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1168\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreasoning_effort\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1169\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1170\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msafety_identifier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1171\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1172\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1173\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1174\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1175\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1176\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1177\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1178\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1179\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1181\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mverbosity\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1184\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweb_search_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1185\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1186\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[1;32m   1187\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[1;32m   1188\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1189\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1190\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m   1192\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1196\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_base_client.py:1259\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1245\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1246\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1247\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1254\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1255\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1256\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1257\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1258\u001b[0m     )\n\u001b[0;32m-> 1259\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LangGraph/lib/python3.10/site-packages/openai/_base_client.py:1047\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1044\u001b[0m             err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1046\u001b[0m         log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1047\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1051\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould not resolve response (should never happen)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-Soffyuxth7vsUnobnzk0MGeR on tokens per min (TPM): Limit 30000, Used 29334, Requested 11245. Please try again in 21.158s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}"
     ]
    }
   ],
   "source": [
    "# 그래프 실행\n",
    "invoke_graph(graph, None, config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "# 그래프의 최종 상태 가져오기\n",
    "final_state = graph.get_state(config)\n",
    "\n",
    "# 최종 보고서 가져오기\n",
    "report = final_state.values.get(\"final_report\")\n",
    "\n",
    "# 마크다운 형식으로 최종 보고서 출력\n",
    "display(Markdown(report))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMs2OSEsEhI+eP+P38efuYH",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
