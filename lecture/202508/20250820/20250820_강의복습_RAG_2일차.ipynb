{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install PyDrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아래 링크를 복사하여 웹 브라우저에 붙여넣으세요.\n",
      "https://accounts.google.com/o/oauth2/auth?client_id=35726703810-4v13dfqmilhgv6shlc3cv9i3ktuh73j1.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&access_type=offline&response_type=code\n",
      "Authentication successful.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import mykeys\n",
    "\n",
    "project_name = 'CH12_RAG'\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = project_name\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = mykeys.get_key('LANG')\n",
    "os.environ[\"LANGCHAIN_HUB_API_KEY\"] = mykeys.get_key('LANG')\n",
    "os.environ[\"OPENAI_API_KEY\"] = mykeys.get_key('GPT')\n",
    "os.environ[\"GOOGLE_API_KEY\"] = mykeys.get_key('GOO')\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = mykeys.get_key('HF')\n",
    "os.environ[\"UPSTAGE_API_KEY\"] = mykeys.get_key('UP')\n",
    "os.environ[\"COHERE_API_KEY\"] = mykeys.get_key('COH')\n",
    "os.environ[\"JINA_API_KEY\"] = mykeys.get_key('JINA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith 추적을 시작합니다.\n",
      "[프로젝트명]\n",
      "CH12_RAG\n"
     ]
    }
   ],
   "source": [
    "from langchain_teddynote import logging\n",
    "\n",
    "# 프로젝트 이름을 입력합니다.\n",
    "logging.langsmith(project_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith 추적을 하지 않습니다.\n"
     ]
    }
   ],
   "source": [
    "from langchain_teddynote import logging\n",
    "\n",
    "# set_enable=False 로 지정하면 추적을 하지 않습니다.\n",
    "logging.langsmith(project_name, set_enable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CH12 Retrieval Augmented Generation(RAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "llama-index-core 0.11.23 requires numpy<2.0.0, but you have numpy 2.2.6 which is incompatible.\n",
      "langchain-chroma 0.2.2 requires chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0, but you have chromadb 1.0.20 which is incompatible.\n",
      "langchain-chroma 0.2.2 requires numpy<2.0.0,>=1.22.4; python_version < \"3.12\", but you have numpy 2.2.6 which is incompatible.\n",
      "langchain-azure-ai 0.1.2 requires numpy<2.0,>=1.24, but you have numpy 2.2.6 which is incompatible.\n",
      "onnx 1.18.0 requires protobuf>=4.25.1, but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -qU langchain umap-learn scikit-learn langchain_community tiktoken langchain-openai langchainhub chromadb langchain-anthropic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_community.document_loaders.recursive_url_loader import RecursiveUrlLoader\n",
    "from bs4 import BeautifulSoup as Soup\n",
    "import tiktoken\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    # 주어진 문자열에서 토큰의 개수를 반환합니다.\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "\n",
    "# LCEL 문서 로드\n",
    "url = \"https://python.langchain.com/docs/concepts/lcel/\"\n",
    "loader = RecursiveUrlLoader(\n",
    "    url=url, max_depth=20, extractor=lambda x: Soup(x, \"html.parser\").text\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "# PydanticOutputParser를 사용한 LCEL 문서 로드 (기본 LCEL 문서 외부)\n",
    "url = \"https://python.langchain.com/docs/how_to/output_parser_structured/\"\n",
    "loader = RecursiveUrlLoader(\n",
    "    url=url, max_depth=1, extractor=lambda x: Soup(x, \"html.parser\").text\n",
    ")\n",
    "docs_pydantic = loader.load()\n",
    "\n",
    "# Self Query를 사용한 LCEL 문서 로드 (기본 LCEL 문서 외부)\n",
    "url = \"https://python.langchain.com/docs/how_to/self_query/\"\n",
    "loader = RecursiveUrlLoader(\n",
    "    url=url, max_depth=1, extractor=lambda x: Soup(x, \"html.parser\").text\n",
    ")\n",
    "docs_sq = loader.load()\n",
    "\n",
    "# 문서 텍스트\n",
    "docs.extend([*docs_pydantic, *docs_sq])\n",
    "docs_texts = [d.page_content for d in docs]\n",
    "\n",
    "# 각 문서에 대한 토큰 수 계산\n",
    "counts = [num_tokens_from_string(d, \"cl100k_base\") for d in docs_texts]\n",
    "\n",
    "# 토큰 수의 히스토그램을 그립니다.\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(counts, bins=30, color=\"blue\", edgecolor=\"black\", alpha=0.7)\n",
    "plt.title(\"Token Counts in LCEL Documents\")\n",
    "plt.xlabel(\"Token Count\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(axis=\"y\", alpha=0.75)\n",
    "\n",
    "# 히스토그램을 표시합니다.\n",
    "plt.show\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num tokens in all context: 10707\n"
     ]
    }
   ],
   "source": [
    "# 문서 텍스트를 연결합니다.\n",
    "# 문서를 출처 메타데이터 기준으로 정렬합니다.\n",
    "d_sorted = sorted(docs, key=lambda x: x.metadata[\"source\"])\n",
    "d_reversed = list(reversed(d_sorted))  # 정렬된 문서를 역순으로 배열합니다.\n",
    "concatenated_content = \"\\n\\n\\n --- \\n\\n\\n\".join(\n",
    "    [\n",
    "        # 역순으로 배열된 문서의 내용을 연결합니다.\n",
    "        doc.page_content\n",
    "        for doc in d_reversed\n",
    "    ]\n",
    ")\n",
    "print(\n",
    "    \"Num tokens in all context: %s\"  # 모든 문맥에서의 토큰 수를 출력합니다.\n",
    "    % num_tokens_from_string(concatenated_content, \"cl100k_base\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 분할을 위한 코드\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "chunk_size_tok = 2000  # 토큰의 청크 크기를 설정합니다.\n",
    "# 재귀적 문자 텍스트 분할기를 초기화합니다. 토큰 인코더를 사용하여 청크 크기와 중복을 설정합니다.\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=chunk_size_tok, chunk_overlap=0\n",
    ")\n",
    "texts_split = text_splitter.split_text(\n",
    "    concatenated_content\n",
    ")  # 주어진 텍스트를 분할합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/LangChain/lib/python3.11/site-packages/langchain/embeddings/cache.py:58: UserWarning: Using default key encoder: SHA-1 is *not* collision-resistant. While acceptable for most cache scenarios, a motivated attacker can craft two different payloads that map to the same cache key. If that risk matters in your environment, supply a stronger encoder (e.g. SHA-256 or BLAKE2) via the `key_encoder` argument. If you change the key encoder, consider also creating a new cache, to avoid (the potential for) collisions with existing keys.\n",
      "  _warn_about_sha1_encoder()\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.embeddings import CacheBackedEmbeddings\n",
    "from langchain.storage import LocalFileStore\n",
    "\n",
    "store = LocalFileStore(\"./20250820_cache/\")\n",
    "\n",
    "# embeddings 인스턴스를 생성합니다.\n",
    "embd = OpenAIEmbeddings(model=\"text-embedding-3-small\", disallowed_special=())\n",
    "\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(\n",
    "    embd, store, namespace=embd.model\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "\n",
    "\n",
    "class StreamCallback(BaseCallbackHandler):\n",
    "    def on_llm_new_token(self, token: str, **kwargs):\n",
    "        print(token, end=\"\", flush=True)\n",
    "\n",
    "\n",
    "# ChatOpenAI 모델을 초기화합니다. 모델은 \"gpt-4-turbo-preview\"를 사용합니다.\n",
    "model = ChatOpenAI(\n",
    "    model=\"gpt-4.1\",\n",
    "    temperature=0,\n",
    "    streaming=True,\n",
    "    callbacks=[StreamCallback()],\n",
    ")\n",
    "\n",
    "# ChatAnthropic 모델을 초기화합니다. 온도는 0으로 설정하고, 모델은 \"claude-3-opus-20240229\"를 사용합니다.\n",
    "# model = ChatAnthropic(temperature=0, model=\"claude-3-opus-20240229\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import umap\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "RANDOM_SEED = 42  # 재현성을 위한 고정된 시드 값\n",
    "\n",
    "### --- 위의 인용된 코드에서 주석과 문서화를 추가함 --- ###\n",
    "\n",
    "\n",
    "def global_cluster_embeddings(\n",
    "    embeddings: np.ndarray,\n",
    "    dim: int,\n",
    "    n_neighbors: Optional[int] = None,\n",
    "    metric: str = \"cosine\",\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    UMAP을 사용하여 임베딩의 전역 차원 축소를 수행합니다.\n",
    "\n",
    "    매개변수:\n",
    "    - embeddings: numpy 배열로 된 입력 임베딩.\n",
    "    - dim: 축소된 공간의 목표 차원.\n",
    "    - n_neighbors: 선택 사항; 각 점을 고려할 이웃의 수.\n",
    "                   제공되지 않으면 임베딩 수의 제곱근으로 기본 설정됩니다.\n",
    "    - metric: UMAP에 사용할 거리 측정 기준.\n",
    "\n",
    "    반환값:\n",
    "    - 지정된 차원으로 축소된 임베딩의 numpy 배열.\n",
    "    \"\"\"\n",
    "    if n_neighbors is None:\n",
    "        n_neighbors = int((len(embeddings) - 1) ** 0.5)\n",
    "    return umap.UMAP(\n",
    "        n_neighbors=n_neighbors, n_components=dim, metric=metric\n",
    "    ).fit_transform(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_cluster_embeddings(\n",
    "    embeddings: np.ndarray, dim: int, num_neighbors: int = 10, metric: str = \"cosine\"\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    임베딩에 대해 지역 차원 축소를 수행합니다. 이는 일반적으로 전역 클러스터링 이후에 사용됩니다.\n",
    "\n",
    "    매개변수:\n",
    "    - embeddings: numpy 배열로서의 입력 임베딩.\n",
    "    - dim: 축소된 공간의 목표 차원 수.\n",
    "    - num_neighbors: 각 점에 대해 고려할 이웃의 수.\n",
    "    - metric: UMAP에 사용할 거리 측정 기준.\n",
    "\n",
    "    반환값:\n",
    "    - 지정된 차원으로 축소된 임베딩의 numpy 배열.\n",
    "    \"\"\"\n",
    "    return umap.UMAP(\n",
    "        n_neighbors=num_neighbors, n_components=dim, metric=metric\n",
    "    ).fit_transform(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimal_clusters(\n",
    "    embeddings: np.ndarray, max_clusters: int = 50, random_state: int = RANDOM_SEED\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    가우시안 혼합 모델(Gaussian Mixture Model)을 사용하여 베이지안 정보 기준(BIC)을 통해 최적의 클러스터 수를 결정합니다.\n",
    "\n",
    "    매개변수:\n",
    "    - embeddings: numpy 배열로서의 입력 임베딩.\n",
    "    - max_clusters: 고려할 최대 클러스터 수.\n",
    "    - random_state: 재현성을 위한 시드.\n",
    "\n",
    "    반환값:\n",
    "    - 발견된 최적의 클러스터 수를 나타내는 정수.\n",
    "    \"\"\"\n",
    "    max_clusters = min(\n",
    "        max_clusters, len(embeddings)\n",
    "    )  # 최대 클러스터 수와 임베딩의 길이 중 작은 값을 최대 클러스터 수로 설정\n",
    "    n_clusters = np.arange(1, max_clusters)  # 1부터 최대 클러스터 수까지의 범위를 생성\n",
    "    bics = []  # BIC 점수를 저장할 리스트\n",
    "    for n in n_clusters:  # 각 클러스터 수에 대해 반복\n",
    "        gm = GaussianMixture(\n",
    "            n_components=n, random_state=random_state\n",
    "        )  # 가우시안 혼합 모델 초기화\n",
    "        gm.fit(embeddings)  # 임베딩에 대해 모델 학습\n",
    "        bics.append(gm.bic(embeddings))  # 학습된 모델의 BIC 점수를 리스트에 추가\n",
    "    return n_clusters[np.argmin(bics)]  # BIC 점수가 가장 낮은 클러스터 수를 반환\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GMM_cluster(embeddings: np.ndarray, threshold: float, random_state: int = 0):\n",
    "    \"\"\"\n",
    "    확률 임계값을 기반으로 가우시안 혼합 모델(GMM)을 사용하여 임베딩을 클러스터링합니다.\n",
    "\n",
    "    매개변수:\n",
    "    - embeddings: numpy 배열로서의 입력 임베딩.\n",
    "    - threshold: 임베딩을 클러스터에 할당하기 위한 확률 임계값.\n",
    "    - random_state: 재현성을 위한 시드.\n",
    "\n",
    "    반환값:\n",
    "    - 클러스터 레이블과 결정된 클러스터 수를 포함하는 튜플.\n",
    "    \"\"\"\n",
    "    n_clusters = get_optimal_clusters(embeddings)  # 최적의 클러스터 수를 구합니다.\n",
    "    # 가우시안 혼합 모델을 초기화합니다.\n",
    "    gm = GaussianMixture(n_components=n_clusters, random_state=random_state)\n",
    "    gm.fit(embeddings)  # 임베딩에 대해 모델을 학습합니다.\n",
    "    probs = gm.predict_proba(\n",
    "        embeddings\n",
    "    )  # 임베딩이 각 클러스터에 속할 확률을 예측합니다.\n",
    "    # 임계값을 초과하는 확률을 가진 클러스터를 레이블로 선택합니다.\n",
    "    labels = [np.where(prob > threshold)[0] for prob in probs]\n",
    "    return labels, n_clusters  # 레이블과 클러스터 수를 반환합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_clustering(\n",
    "    embeddings: np.ndarray,\n",
    "    dim: int,\n",
    "    threshold: float,\n",
    ") -> List[np.ndarray]:\n",
    "    \"\"\"\n",
    "    임베딩에 대해 차원 축소, 가우시안 혼합 모델을 사용한 클러스터링, 각 글로벌 클러스터 내에서의 로컬 클러스터링을 순서대로 수행합니다.\n",
    "\n",
    "    매개변수:\n",
    "    - embeddings: numpy 배열로 된 입력 임베딩입니다.\n",
    "    - dim: UMAP 축소를 위한 목표 차원입니다.\n",
    "    - threshold: GMM에서 임베딩을 클러스터에 할당하기 위한 확률 임계값입니다.\n",
    "\n",
    "    반환값:\n",
    "    - 각 임베딩의 클러스터 ID를 포함하는 numpy 배열의 리스트입니다.\n",
    "    \"\"\"\n",
    "    if len(embeddings) <= dim + 1:\n",
    "        # 데이터가 충분하지 않을 때 클러스터링을 피합니다.\n",
    "        return [np.array([0]) for _ in range(len(embeddings))]\n",
    "\n",
    "    # 글로벌 차원 축소\n",
    "    reduced_embeddings_global = global_cluster_embeddings(embeddings, dim)\n",
    "    # 글로벌 클러스터링\n",
    "    global_clusters, n_global_clusters = GMM_cluster(\n",
    "        reduced_embeddings_global, threshold\n",
    "    )\n",
    "\n",
    "    all_local_clusters = [np.array([]) for _ in range(len(embeddings))]\n",
    "    total_clusters = 0\n",
    "\n",
    "    # 각 글로벌 클러스터를 순회하며 로컬 클러스터링 수행\n",
    "    for i in range(n_global_clusters):\n",
    "        # 현재 글로벌 클러스터에 속하는 임베딩 추출\n",
    "        global_cluster_embeddings_ = embeddings[\n",
    "            np.array([i in gc for gc in global_clusters])\n",
    "        ]\n",
    "\n",
    "        if len(global_cluster_embeddings_) == 0:\n",
    "            continue\n",
    "        if len(global_cluster_embeddings_) <= dim + 1:\n",
    "            # 작은 클러스터는 직접 할당으로 처리\n",
    "            local_clusters = [np.array([0]) for _ in global_cluster_embeddings_]\n",
    "            n_local_clusters = 1\n",
    "        else:\n",
    "            # 로컬 차원 축소 및 클러스터링\n",
    "            reduced_embeddings_local = local_cluster_embeddings(\n",
    "                global_cluster_embeddings_, dim\n",
    "            )\n",
    "            local_clusters, n_local_clusters = GMM_cluster(\n",
    "                reduced_embeddings_local, threshold\n",
    "            )\n",
    "\n",
    "        # 로컬 클러스터 ID 할당, 이미 처리된 총 클러스터 수를 조정\n",
    "        for j in range(n_local_clusters):\n",
    "            local_cluster_embeddings_ = global_cluster_embeddings_[\n",
    "                np.array([j in lc for lc in local_clusters])\n",
    "            ]\n",
    "            indices = np.where(\n",
    "                (embeddings == local_cluster_embeddings_[:, None]).all(-1)\n",
    "            )[1]\n",
    "            for idx in indices:\n",
    "                all_local_clusters[idx] = np.append(\n",
    "                    all_local_clusters[idx], j + total_clusters\n",
    "                )\n",
    "\n",
    "        total_clusters += n_local_clusters\n",
    "\n",
    "    return all_local_clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed(texts):\n",
    "    # 텍스트 문서 목록에 대한 임베딩을 생성합니다.\n",
    "    #\n",
    "    # 이 함수는 `embd` 객체가 존재한다고 가정하며, 이 객체는 텍스트 목록을 받아 그 임베딩을 반환하는 `embed_documents` 메소드를 가지고 있습니다.\n",
    "    #\n",
    "    # 매개변수:\n",
    "    # - texts: List[str], 임베딩할 텍스트 문서의 목록입니다.\n",
    "    #\n",
    "    # 반환값:\n",
    "    # - numpy.ndarray: 주어진 텍스트 문서들에 대한 임베딩 배열입니다.\n",
    "    text_embeddings = embd.embed_documents(\n",
    "        texts\n",
    "    )  # 텍스트 문서들의 임베딩을 생성합니다.\n",
    "    text_embeddings_np = np.array(text_embeddings)  # 임베딩을 numpy 배열로 변환합니다.\n",
    "    return text_embeddings_np  # 임베딩된 numpy 배열을 반환합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_cluster_texts(texts):\n",
    "    \"\"\"\n",
    "    텍스트 목록을 임베딩하고 클러스터링하여, 텍스트, 그들의 임베딩, 그리고 클러스터 라벨이 포함된 DataFrame을 반환합니다.\n",
    "\n",
    "    이 함수는 임베딩 생성과 클러스터링을 단일 단계로 결합합니다. 임베딩에 대해 클러스터링을 수행하는 `perform_clustering` 함수의 사전 정의된 존재를 가정합니다.\n",
    "\n",
    "    매개변수:\n",
    "    - texts: List[str], 처리될 텍스트 문서의 목록입니다.\n",
    "\n",
    "    반환값:\n",
    "    - pandas.DataFrame: 원본 텍스트, 그들의 임베딩, 그리고 할당된 클러스터 라벨이 포함된 DataFrame입니다.\n",
    "    \"\"\"\n",
    "    text_embeddings_np = embed(texts)  # 임베딩 생성\n",
    "    cluster_labels = perform_clustering(\n",
    "        text_embeddings_np, 10, 0.1\n",
    "    )  # 임베딩에 대해 클러스터링 수행\n",
    "    df = pd.DataFrame()  # 결과를 저장할 DataFrame 초기화\n",
    "    df[\"text\"] = texts  # 원본 텍스트 저장\n",
    "    df[\"embd\"] = list(text_embeddings_np)  # DataFrame에 리스트로 임베딩 저장\n",
    "    df[\"cluster\"] = cluster_labels  # 클러스터 라벨 저장\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fmt_txt(df: pd.DataFrame) -> str:\n",
    "    \"\"\"\n",
    "    DataFrame에 있는 텍스트 문서를 단일 문자열로 포맷합니다.\n",
    "\n",
    "    매개변수:\n",
    "    - df: 'text' 열에 포맷할 텍스트 문서가 포함된 DataFrame.\n",
    "\n",
    "    반환값:\n",
    "    - 모든 텍스트 문서가 특정 구분자로 결합된 단일 문자열.\n",
    "    \"\"\"\n",
    "    unique_txt = df[\"text\"].tolist()  # 'text' 열의 모든 텍스트를 리스트로 변환\n",
    "    return \"--- --- \\n --- --- \".join(\n",
    "        unique_txt\n",
    "    )  # 텍스트 문서들을 특정 구분자로 결합하여 반환\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_cluster_summarize_texts(\n",
    "    texts: List[str], level: int\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    텍스트 목록에 대해 임베딩, 클러스터링 및 요약을 수행합니다. 이 함수는 먼저 텍스트에 대한 임베딩을 생성하고,\n",
    "    유사성을 기반으로 클러스터링을 수행한 다음, 클러스터 할당을 확장하여 처리를 용이하게 하고 각 클러스터 내의 내용을 요약합니다.\n",
    "\n",
    "    매개변수:\n",
    "    - texts: 처리할 텍스트 문서 목록입니다.\n",
    "    - level: 처리의 깊이나 세부 사항을 정의할 수 있는 정수 매개변수입니다.\n",
    "\n",
    "    반환값:\n",
    "    - 두 개의 데이터프레임을 포함하는 튜플:\n",
    "      1. 첫 번째 데이터프레임(`df_clusters`)은 원본 텍스트, 그들의 임베딩, 그리고 클러스터 할당을 포함합니다.\n",
    "      2. 두 번째 데이터프레임(`df_summary`)은 각 클러스터에 대한 요약, 지정된 세부 수준, 그리고 클러스터 식별자를 포함합니다.\n",
    "    \"\"\"\n",
    "\n",
    "    # 텍스트를 임베딩하고 클러스터링하여 'text', 'embd', 'cluster' 열이 있는 데이터프레임을 생성합니다.\n",
    "    df_clusters = embed_cluster_texts(texts)\n",
    "\n",
    "    # 클러스터를 쉽게 조작하기 위해 데이터프레임을 확장할 준비를 합니다.\n",
    "    expanded_list = []\n",
    "\n",
    "    # 데이터프레임 항목을 문서-클러스터 쌍으로 확장하여 처리를 간단하게 합니다.\n",
    "    for index, row in df_clusters.iterrows():\n",
    "        for cluster in row[\"cluster\"]:\n",
    "            expanded_list.append(\n",
    "                {\"text\": row[\"text\"], \"embd\": row[\"embd\"], \"cluster\": cluster}\n",
    "            )\n",
    "\n",
    "    # 확장된 목록에서 새 데이터프레임을 생성합니다.\n",
    "    expanded_df = pd.DataFrame(expanded_list)\n",
    "\n",
    "    # 처리를 위해 고유한 클러스터 식별자를 검색합니다.\n",
    "    all_clusters = expanded_df[\"cluster\"].unique()\n",
    "\n",
    "    print(f\"--Generated {len(all_clusters)} clusters--\")\n",
    "\n",
    "    # 요약\n",
    "    template = \"\"\"여기 LangChain 표현 언어 문서의 하위 집합이 있습니다.\n",
    "\n",
    "    LangChain 표현 언어는 LangChain에서 체인을 구성하는 방법을 제공합니다.\n",
    "\n",
    "    제공된 문서의 자세한 요약을 제공하십시오.\n",
    "\n",
    "    문서:\n",
    "    {context}\n",
    "    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    chain = prompt | model | StrOutputParser()\n",
    "\n",
    "    # 각 클러스터 내의 텍스트를 요약을 위해 포맷팅합니다.\n",
    "    summaries = []\n",
    "    for i in all_clusters:\n",
    "        df_cluster = expanded_df[expanded_df[\"cluster\"] == i]\n",
    "        formatted_txt = fmt_txt(df_cluster)\n",
    "        summaries.append(chain.invoke({\"context\": formatted_txt}))\n",
    "\n",
    "    # 요약, 해당 클러스터 및 레벨을 저장할 데이터프레임을 생성합니다.\n",
    "    df_summary = pd.DataFrame(\n",
    "        {\n",
    "            \"summaries\": summaries,\n",
    "            \"level\": [level] * len(summaries),\n",
    "            \"cluster\": list(all_clusters),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return df_clusters, df_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_embed_cluster_summarize(\n",
    "    texts: List[str], level: int = 1, n_levels: int = 3\n",
    ") -> Dict[int, Tuple[pd.DataFrame, pd.DataFrame]]:\n",
    "    \"\"\"\n",
    "    지정된 레벨까지 또는 고유 클러스터의 수가 1이 될 때까지 텍스트를 재귀적으로 임베딩, 클러스터링, 요약하여\n",
    "    각 레벨에서의 결과를 저장합니다.\n",
    "\n",
    "    매개변수:\n",
    "    - texts: List[str], 처리할 텍스트들.\n",
    "    - level: int, 현재 재귀 레벨 (1에서 시작).\n",
    "    - n_levels: int, 재귀의 최대 깊이.\n",
    "\n",
    "    반환값:\n",
    "    - Dict[int, Tuple[pd.DataFrame, pd.DataFrame]], 재귀 레벨을 키로 하고 해당 레벨에서의 클러스터 DataFrame과 요약 DataFrame을 포함하는 튜플을 값으로 하는 사전.\n",
    "    \"\"\"\n",
    "    results = {}  # 각 레벨에서의 결과를 저장할 사전\n",
    "\n",
    "    # 현재 레벨에 대해 임베딩, 클러스터링, 요약 수행\n",
    "    df_clusters, df_summary = embed_cluster_summarize_texts(texts, level)\n",
    "\n",
    "    # 현재 레벨의 결과 저장\n",
    "    results[level] = (df_clusters, df_summary)\n",
    "\n",
    "    # 추가 재귀가 가능하고 의미가 있는지 결정\n",
    "    unique_clusters = df_summary[\"cluster\"].nunique()\n",
    "    if level < n_levels and unique_clusters > 1:\n",
    "        # 다음 레벨의 재귀 입력 텍스트로 요약 사용\n",
    "        new_texts = df_summary[\"summaries\"].tolist()\n",
    "        next_level_results = recursive_embed_cluster_summarize(\n",
    "            new_texts, level + 1, n_levels\n",
    "        )\n",
    "\n",
    "        # 다음 레벨의 결과를 현재 결과 사전에 병합\n",
    "        results.update(next_level_results)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 전체 문서의 개수\n",
    "len(docs_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Generated 1 clusters--\n",
      "아래는 제공된 LangChain Expression Language(LCEL) 관련 문서 하위 집합의 자세한 요약입니다.\n",
      "\n",
      "---\n",
      "\n",
      "## 1. LangChain Expression Language (LCEL) 개요\n",
      "\n",
      "**LCEL이란?**\n",
      "- LCEL은 LangChain에서 기존 Runnable 객체들을 조합하여 새로운 체인(Chain)을 선언적으로 구성할 수 있게 해주는 표현 언어입니다.\n",
      "- \"선언적\"이란, 어떻게 실행할지(How)가 아니라 무엇을 할지(What)를 기술하는 방식입니다.\n",
      "- LCEL로 만든 체인은 Runnable 인터페이스를 구현하며, LangChain의 표준 실행/비동기/스트리밍 API를 모두 지원합니다.\n",
      "\n",
      "**주요 장점**\n",
      "- **병렬 실행 최적화**: RunnableParallel, Batch API 등을 통해 여러 Runnable을 병렬로 실행하여 지연시간을 줄일 수 있습니다.\n",
      "- **비동기 지원**: LCEL로 만든 모든 체인은 비동기 실행이 보장되어, 서버 환경에서 대량 요청을 효율적으로 처리할 수 있습니다.\n",
      "- **스트리밍 간소화**: 체인 실행 결과를 스트리밍 방식으로 점진적으로 받을 수 있어, LLM의 첫 토큰 응답까지의 시간을 최소화할 수 있습니다.\n",
      "- **LangSmith 트레이싱**: 모든 실행 단계가 자동으로 LangSmith에 기록되어, 복잡한 체인도 디버깅과 관찰이 용이합니다.\n",
      "- **표준화된 API**: 모든 체인이 Runnable 인터페이스를 따르므로, 일관된 방식으로 사용할 수 있습니다.\n",
      "- **LangServe 배포 지원**: LCEL 체인은 프로덕션 환경에 쉽게 배포할 수 있습니다.\n",
      "\n",
      "**LCEL 사용 권장 시나리오**\n",
      "- 단순한 LLM 호출에는 LCEL이 필요 없으며, 직접 모델을 호출하면 됩니다.\n",
      "- 프롬프트+LLM+파서, 간단한 검색 등 단순 체인에는 LCEL이 적합합니다.\n",
      "- 분기, 반복, 복수 에이전트 등 복잡한 오케스트레이션이 필요한 경우에는 LangGraph를 사용하고, 개별 노드에서 LCEL을 활용할 수 있습니다.\n",
      "\n",
      "---\n",
      "\n",
      "## 2. LCEL의 조합 프리미티브(Composition Primitives)\n",
      "\n",
      "**주요 조합 방식**\n",
      "- **RunnableSequence**: 여러 Runnable을 순차적으로 연결. 앞 단계의 출력이 다음 단계의 입력이 됨.\n",
      "    ```python\n",
      "    chain = RunnableSequence([runnable1, runnable2])\n",
      "    # 또는\n",
      "    chain = runnable1 | runnable2\n",
      "    # 또는\n",
      "    chain = runnable1.pipe(runnable2)\n",
      "    ```\n",
      "- **RunnableParallel**: 여러 Runnable에 동일한 입력을 병렬로 전달, 결과는 딕셔너리로 반환.\n",
      "    ```python\n",
      "    chain = RunnableParallel({\"key1\": runnable1, \"key2\": runnable2})\n",
      "    ```\n",
      "- **자동 변환(Coercion)**:\n",
      "    - 딕셔너리는 자동으로 RunnableParallel로 변환됨.\n",
      "    - 함수는 자동으로 RunnableLambda로 변환됨.\n",
      "\n",
      "**조합 문법**\n",
      "- `|` 연산자: 두 Runnable을 순차적으로 연결(RunnableSequence).\n",
      "- `.pipe()` 메서드: 동일한 기능, 연산자 오버로딩을 피하고 싶을 때 사용.\n",
      "\n",
      "---\n",
      "\n",
      "## 3. Output Parser를 활용한 LLM 응답 구조화\n",
      "\n",
      "**Output Parser란?**\n",
      "- LLM의 텍스트 응답을 구조화된 데이터(예: JSON, Pydantic 모델 등)로 변환하는 클래스.\n",
      "- 필수 메서드:\n",
      "    - `get_format_instructions`: LLM에 출력 포맷을 안내하는 문자열 반환.\n",
      "    - `parse`: LLM의 응답 문자열을 구조화된 객체로 파싱.\n",
      "    - (선택) `parse_with_prompt`: 프롬프트 정보까지 활용해 파싱.\n",
      "\n",
      "**예시: PydanticOutputParser**\n",
      "- Pydantic 모델을 정의하고, 해당 모델에 맞는 OutputParser를 생성.\n",
      "- 프롬프트에 `format_instructions`를 삽입하여 LLM이 올바른 포맷으로 응답하도록 유도.\n",
      "- 체인: `prompt | model | parser` 형태로 연결하여, 입력 → LLM → 파싱까지 일관된 흐름 제공.\n",
      "- 스트리밍 지원: 일부 파서는 부분적으로 파싱된 객체를 스트림으로 반환 가능.\n",
      "\n",
      "---\n",
      "\n",
      "## 4. Self-Querying Retrieval\n",
      "\n",
      "**Self-Querying Retriever란?**\n",
      "- 자연어 쿼리를 받아, LLM 체인을 통해 구조화된 쿼리(Structured Query)로 변환한 뒤, 벡터스토어에 적용하여 문서를 검색하는 고급 리트리버.\n",
      "- 사용자는 자연어로 필터 조건(예: \"8.5점 이상 영화\", \"감독이 Greta Gerwig인 영화\")을 입력할 수 있고, 시스템이 이를 자동으로 필터로 변환.\n",
      "\n",
      "**구성 요소**\n",
      "- **문서와 메타데이터**: 예시에서는 영화 요약과 연도, 감독, 장르, 평점 등 메타데이터를 가진 문서 사용.\n",
      "- **AttributeInfo**: 각 메타데이터 필드의 이름, 설명, 타입을 정의.\n",
      "- **Query Constructor Chain**: 프롬프트와 OutputParser를 조합하여, 자연어 쿼리를 StructuredQuery 객체로 변환.\n",
      "- **Structured Query Translator**: StructuredQuery를 벡터스토어의 필터 문법으로 변환.\n",
      "- **SelfQueryRetriever**: 위 요소들을 조합하여, 자연어 쿼리 → 구조화 쿼리 → 필터링된 검색 결과 반환.\n",
      "\n",
      "**LCEL로 직접 구성**\n",
      "- 프롬프트, LLM, OutputParser를 LCEL 체인(`prompt | llm | output_parser`)으로 연결하여, 쿼리 생성 과정을 커스터마이즈할 수 있음.\n",
      "\n",
      "---\n",
      "\n",
      "## 5. 기타 참고사항\n",
      "\n",
      "- LCEL은 기존의 LLMChain, ConversationalRetrievalChain 등 레거시 체인보다 더 투명하고 유연한 체인 구성을 지원합니다.\n",
      "- LCEL의 모든 체인은 LangSmith 트레이싱, LangServe 배포, 표준 Runnable API 등과 자연스럽게 통합됩니다.\n",
      "- 복잡한 오케스트레이션이 필요할 때는 LangGraph를, 단순한 체인에는 LCEL을 사용하는 것이 권장됩니다.\n",
      "\n",
      "---\n",
      "\n",
      "### 요약\n",
      "\n",
      "**LangChain Expression Language(LCEL)는 LangChain에서 체인을 선언적으로, 효율적으로, 표준화된 방식으로 구성할 수 있게 해주는 표현 언어입니다.**  \n",
      "LCEL은 병렬/비동기 실행, 스트리밍, 트레이싱, 표준 API, 배포 등 다양한 이점을 제공하며, RunnableSequence와 RunnableParallel 등 조합 프리미티브를 통해 체인을 쉽게 구성할 수 있습니다.  \n",
      "Output Parser를 활용하면 LLM의 응답을 구조화된 데이터로 변환할 수 있고, Self-Querying Retriever를 통해 자연어 쿼리를 구조화 쿼리로 변환하여 고급 검색이 가능합니다.  \n",
      "복잡한 오케스트레이션에는 LangGraph를, 단순 체인에는 LCEL을 사용하는 것이 권장됩니다."
     ]
    }
   ],
   "source": [
    "# 트리 구축\n",
    "leaf_texts = docs_texts  # 문서 텍스트를 리프 텍스트로 설정\n",
    "results = recursive_embed_cluster_summarize(\n",
    "    leaf_texts, level=1, n_levels=3\n",
    ")  # 재귀적으로 임베딩, 클러스터링 및 요약을 수행하여 결과를 얻음\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# leaf_texts를 복사하여 all_texts를 초기화합니다.\n",
    "all_texts = leaf_texts.copy()\n",
    "\n",
    "# 각 레벨의 요약을 추출하여 all_texts에 추가하기 위해 결과를 순회합니다.\n",
    "for level in sorted(results.keys()):\n",
    "    # 현재 레벨의 DataFrame에서 요약을 추출합니다.\n",
    "    summaries = results[level][1][\"summaries\"].tolist()\n",
    "    # 현재 레벨의 요약을 all_texts에 추가합니다.\n",
    "    all_texts.extend(summaries)\n",
    "\n",
    "# 이제 all_texts를 사용하여 FAISS vectorstore를 구축합니다.\n",
    "vectorstore = FAISS.from_texts(texts=all_texts, embedding=embd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "DB_INDEX = \"RAPTOR\"\n",
    "\n",
    "# 로컬에 FAISS DB 인덱스가 이미 존재하는지 확인하고, 그렇다면 로드하여 vectorstore와 병합한 후 저장합니다.\n",
    "if os.path.exists(DB_INDEX):\n",
    "    local_index = FAISS.load_local(DB_INDEX, embd)\n",
    "    local_index.merge_from(vectorstore)\n",
    "    local_index.save_local(DB_INDEX)\n",
    "else:\n",
    "    vectorstore.save_local(folder_path=DB_INDEX)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retriever 생성\n",
    "retriever = vectorstore.as_retriever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# 프롬프트 생성\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# 문서 포스트 프로세싱\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    # 문서의 페이지 내용을 이어붙여 반환합니다.\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "# RAG 체인 정의\n",
    "rag_chain = (\n",
    "    # 검색 결과를 포맷팅하고 질문을 처리합니다.\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt  # 프롬프트를 적용합니다.\n",
    "    | model  # 모델을 적용합니다.\n",
    "    | StrOutputParser()  # 문자열 출력 파서를 적용합니다.\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이 문서의 핵심 주제는 LangChain Expression Language(LCEL)에 대한 설명입니다. LCEL은 LangChain에서 다양한 Runnable 객체들을 선언적으로 조합하여 체인을 효율적이고 표준화된 방식으로 구성할 수 있게 해주는 표현 언어로, 병렬/비동기 실행, 스트리밍, 트레이싱, 표준 API, 배포 등 다양한 이점을 제공합니다. 또한 Output Parser를 통한 LLM 응답 구조화, Self-Querying Retriever를 통한 고급 검색 등 실제 활용 시나리오와, 복잡한 오케스트레이션에는 LangGraph를, 단순 체인에는 LCEL을 권장하는 사용 지침도 포함되어 있습니다."
     ]
    }
   ],
   "source": [
    "# 추상적인 질문 실행\n",
    "_ = rag_chain.invoke(\"전체 문서의 핵심 주제에 대해 설명해주세요.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아래는 PydanticOutputParser를 활용한 LangChain 예시 코드입니다.\n",
      "\n",
      "```python\n",
      "from langchain_core.output_parsers import PydanticOutputParser\n",
      "from langchain_core.prompts import PromptTemplate\n",
      "from langchain_openai import OpenAI\n",
      "from pydantic import BaseModel, Field, model_validator\n",
      "\n",
      "# 1. 원하는 데이터 구조 정의\n",
      "class Joke(BaseModel):\n",
      "    setup: str = Field(description=\"question to set up a joke\")\n",
      "    punchline: str = Field(description=\"answer to resolve the joke\")\n",
      "    @model_validator(mode=\"before\")\n",
      "    @classmethod\n",
      "    def question_ends_with_question_mark(cls, values: dict) -> dict:\n",
      "        setup = values.get(\"setup\")\n",
      "        if setup and setup[-1] != \"?\":\n",
      "            raise ValueError(\"Badly formed question!\")\n",
      "        return values\n",
      "\n",
      "# 2. 파서 및 프롬프트 준비\n",
      "parser = PydanticOutputParser(pydantic_object=Joke)\n",
      "prompt = PromptTemplate(\n",
      "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
      "    input_variables=[\"query\"],\n",
      "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
      ")\n",
      "model = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0.0)\n",
      "\n",
      "# 3. 체인 구성 및 실행\n",
      "chain = prompt | model | parser\n",
      "result = chain.invoke({\"query\": \"Tell me a joke.\"})\n",
      "print(result)\n",
      "```\n",
      "\n",
      "이 코드는 LLM의 응답을 Pydantic 모델(Joke)로 구조화하여 반환합니다."
     ]
    }
   ],
   "source": [
    "# Low Level 질문 실행\n",
    "_ = rag_chain.invoke(\"PydanticOutputParser 을 활용한 예시 코드를 작성해 주세요.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self-querying 방법은 사용자의 자연어 쿼리를 LLM 체인(프롬프트+모델+파서)으로 구조화된 쿼리(Structured Query)로 변환한 뒤, 해당 쿼리를 벡터스토어에 적용하여 문서를 검색하는 고급 검색 방식입니다. 예를 들어, \"8.5점 이상 영화\"처럼 자연어로 필터 조건을 입력하면, 시스템이 이를 자동으로 필터로 변환해 검색합니다.\n",
      "\n",
      "아래는 LangChain에서 self-querying retriever를 사용하는 예시 코드입니다.\n",
      "\n",
      "```python\n",
      "from langchain_chroma import Chroma\n",
      "from langchain_core.documents import Document\n",
      "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
      "from langchain.chains.query_constructor.schema import AttributeInfo\n",
      "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
      "\n",
      "# 문서 및 벡터스토어 준비\n",
      "docs = [\n",
      "    Document(page_content=\"A bunch of scientists bring back dinosaurs and mayhem breaks loose\", metadata={\"year\": 1993, \"rating\": 7.7, \"genre\": \"science fiction\"}),\n",
      "    Document(page_content=\"A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea\", metadata={\"year\": 2006, \"director\": \"Satoshi Kon\", \"rating\": 8.6}),\n",
      "]\n",
      "vectorstore = Chroma.from_documents(docs, OpenAIEmbeddings())\n",
      "\n",
      "# 메타데이터 필드 정보 정의\n",
      "metadata_field_info = [\n",
      "    AttributeInfo(name=\"genre\", description=\"The genre of the movie.\", type=\"string\"),\n",
      "    AttributeInfo(name=\"year\", description=\"The year the movie was released\", type=\"integer\"),\n",
      "    AttributeInfo(name=\"director\", description=\"The name of the movie director\", type=\"string\"),\n",
      "    AttributeInfo(name=\"rating\", description=\"A 1-10 rating for the movie\", type=\"float\"),\n",
      "]\n",
      "\n",
      "# Self-Querying Retriever 생성\n",
      "llm = ChatOpenAI(temperature=0)\n",
      "document_content_description = \"Brief summary of a movie\"\n",
      "retriever = SelfQueryRetriever.from_llm(\n",
      "    llm,\n",
      "    vectorstore,\n",
      "    document_content_description,\n",
      "    metadata_field_info,\n",
      ")\n",
      "\n",
      "# 자연어 쿼리로 검색\n",
      "results = retriever.invoke(\"8.5점 이상 영화 보여줘\")\n",
      "print(results)\n",
      "```\n",
      "\n",
      "이 코드는 자연어 쿼리를 구조화 쿼리로 변환해, 조건에 맞는 문서만 필터링하여 반환합니다."
     ]
    }
   ],
   "source": [
    "# Low Level 질문 실행\n",
    "_ = rag_chain.invoke(\"self-querying 방법과 예시 코드를 작성해 주세요.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "대화내용을 기억하는 RAG 체인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "# 프롬프트 정의\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"당신은 Question-Answering 챗봇입니다. 주어진 질문에 대한 답변을 제공해주세요.\",\n",
    "        ),\n",
    "        # 대화기록용 key 인 chat_history 는 가급적 변경 없이 사용하세요!\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"#Question:\\n{question}\"),  # 사용자 입력을 변수로 사용\n",
    "    ]\n",
    ")\n",
    "\n",
    "# llm 생성\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "# 일반 Chain 생성\n",
    "chain = prompt | llm | StrOutputParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 세션 기록을 저장할 딕셔너리\n",
    "store = {}\n",
    "\n",
    "\n",
    "# 세션 ID를 기반으로 세션 기록을 가져오는 함수\n",
    "def get_session_history(session_ids):\n",
    "    print(f\"[대화 세션ID]: {session_ids}\")\n",
    "    if session_ids not in store:  # 세션 ID가 store에 없는 경우\n",
    "        # 새로운 ChatMessageHistory 객체를 생성하여 store에 저장\n",
    "        store[session_ids] = ChatMessageHistory()\n",
    "    return store[session_ids]  # 해당 세션 ID에 대한 세션 기록 반환\n",
    "\n",
    "\n",
    "chain_with_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,  # 세션 기록을 가져오는 함수\n",
    "    input_messages_key=\"question\",  # 사용자의 질문이 템플릿 변수에 들어갈 key\n",
    "    history_messages_key=\"chat_history\",  # 기록 메시지의 키\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[대화 세션ID]: abc123\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'안녕하세요, Charlie님! 만나서 반가워요. 어떤 도움을 드릴까요?'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_with_history.invoke(\n",
    "    # 질문 입력\n",
    "    {\"question\": \"나의 이름은 Charlie입니다.\"},\n",
    "    # 세션 ID 기준으로 대화를 기록합니다.\n",
    "    config={\"configurable\": {\"session_id\": \"abc123\"}},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[대화 세션ID]: abc123\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'당신의 이름은 Charlie입니다.'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_with_history.invoke(\n",
    "    # 질문 입력\n",
    "    {\"question\": \"내 이름이 뭐라고?\"},\n",
    "    # 세션 ID 기준으로 대화를 기록합니다.\n",
    "    config={\"configurable\": {\"session_id\": \"abc123\"}},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PDFPlumberLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from operator import itemgetter\n",
    "\n",
    "# 단계 1: 문서 로드(Load Documents)\n",
    "loader = PDFPlumberLoader(\"./SPRI_AI_Brief_2023년12월호_F.pdf\")\n",
    "docs = loader.load()\n",
    "\n",
    "# 단계 2: 문서 분할(Split Documents)\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "split_documents = text_splitter.split_documents(docs)\n",
    "\n",
    "# 단계 3: 임베딩(Embedding) 생성\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# 단계 4: DB 생성(Create DB) 및 저장\n",
    "# 벡터스토어를 생성합니다.\n",
    "vectorstore = FAISS.from_documents(documents=split_documents, embedding=embeddings)\n",
    "\n",
    "# 단계 5: 검색기(Retriever) 생성\n",
    "# 문서에 포함되어 있는 정보를 검색하고 생성합니다.\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# 단계 6: 프롬프트 생성(Create Prompt)\n",
    "# 프롬프트를 생성합니다.\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the question. \n",
    "If you don't know the answer, just say that you don't know. \n",
    "Answer in Korean.\n",
    "\n",
    "#Previous Chat History:\n",
    "{chat_history}\n",
    "\n",
    "#Question: \n",
    "{question} \n",
    "\n",
    "#Context: \n",
    "{context} \n",
    "\n",
    "#Answer:\"\"\"\n",
    ")\n",
    "\n",
    "# 단계 7: 언어모델(LLM) 생성\n",
    "# 모델(LLM) 을 생성합니다.\n",
    "llm = ChatOpenAI(model_name=\"gpt-4.1\", temperature=0)\n",
    "\n",
    "# 단계 8: 체인(Chain) 생성\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": itemgetter(\"question\") | retriever,\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "        \"chat_history\": itemgetter(\"chat_history\"),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 세션 기록을 저장할 딕셔너리\n",
    "store = {}\n",
    "\n",
    "\n",
    "# 세션 ID를 기반으로 세션 기록을 가져오는 함수\n",
    "def get_session_history(session_ids):\n",
    "    print(f\"[대화 세션ID]: {session_ids}\")\n",
    "    if session_ids not in store:  # 세션 ID가 store에 없는 경우\n",
    "        # 새로운 ChatMessageHistory 객체를 생성하여 store에 저장\n",
    "        store[session_ids] = ChatMessageHistory()\n",
    "    return store[session_ids]  # 해당 세션 ID에 대한 세션 기록 반환\n",
    "\n",
    "\n",
    "# 대화를 기록하는 RAG 체인 생성\n",
    "rag_with_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,  # 세션 기록을 가져오는 함수\n",
    "    input_messages_key=\"question\",  # 사용자의 질문이 템플릿 변수에 들어갈 key\n",
    "    history_messages_key=\"chat_history\",  # 기록 메시지의 키\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[대화 세션ID]: rag123\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"삼성전자가 만든 생성형 AI의 이름은 '삼성 가우스(Samsung Gauss)'입니다.\""
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_with_history.invoke(\n",
    "    # 질문 입력\n",
    "    {\"question\": \"삼성전자가 만든 생성형 AI 이름은?\"},\n",
    "    # 세션 ID 기준으로 대화를 기록합니다.\n",
    "    config={\"configurable\": {\"session_id\": \"rag123\"}},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[대화 세션ID]: rag123\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'이전 답변의 영어 번역은 다음과 같습니다.\\n\\n\"삼성전자가 만든 생성형 AI의 이름은 \\'삼성 가우스(Samsung Gauss)\\'입니다.\"  \\n→ \"The name of the generative AI developed by Samsung Electronics is \\'Samsung Gauss\\'.\"'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_with_history.invoke(\n",
    "    # 질문 입력\n",
    "    {\"question\": \"이전 답변을 영어로 번역해주세요.\"},\n",
    "    # 세션 ID 기준으로 대화를 기록합니다.\n",
    "    config={\"configurable\": {\"session_id\": \"rag123\"}},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMs2OSEsEhI+eP+P38efuYH",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
