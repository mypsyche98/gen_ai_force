{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"dc338ac8-931c-4b20-9057-e987e8227b9d","outputId":"45450be9-7db5-4c4d-f37d-f12b9d285523"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting torch==2.4.0\n","  Downloading torch-2.4.0-cp311-cp311-manylinux1_x86_64.whl.metadata (26 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0) (3.13.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0) (4.9.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0) (3.1.3)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0) (2024.2.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.4.0)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.4.0)\n","  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.4.0)\n","  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0) (9.1.0.70)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.4.0)\n","  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.4.0)\n","  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.4.0)\n","  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.4.0)\n","  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.4.0)\n","  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0) (2.20.5)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.4.0)\n","  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n","Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0) (3.0.0)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.4.0) (12.4.99)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.4.0) (2.1.5)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.4.0) (1.3.0)\n","Downloading torch-2.4.0-cp311-cp311-manylinux1_x86_64.whl (797.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.3/797.3 MB\u001b[0m \u001b[31m199.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m227.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m269.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m219.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m239.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m219.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m220.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m231.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m218.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n","\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","Installing collected packages: nvidia-nvtx-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, torch\n","  Attempting uninstall: nvidia-nvtx-cu12\n","    Found existing installation: nvidia-nvtx-cu12 12.4.99\n","    Uninstalling nvidia-nvtx-cu12-12.4.99:\n","      Successfully uninstalled nvidia-nvtx-cu12-12.4.99\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.3.0.142\n","    Uninstalling nvidia-cusparse-cu12-12.3.0.142:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.3.0.142\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.5.119\n","    Uninstalling nvidia-curand-cu12-10.3.5.119:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.5.119\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.2.0.44\n","    Uninstalling nvidia-cufft-cu12-11.2.0.44:\n","      Successfully uninstalled nvidia-cufft-cu12-11.2.0.44\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.4.99\n","    Uninstalling nvidia-cuda-runtime-cu12-12.4.99:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.4.99\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.4.99\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.4.99:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.4.99\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.4.99\n","    Uninstalling nvidia-cuda-cupti-cu12-12.4.99:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.4.99\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.4.2.65\n","    Uninstalling nvidia-cublas-cu12-12.4.2.65:\n","      Successfully uninstalled nvidia-cublas-cu12-12.4.2.65\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.0.99\n","    Uninstalling nvidia-cusolver-cu12-11.6.0.99:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.0.99\n","  Attempting uninstall: torch\n","    Found existing installation: torch 2.4.1+cu124\n","    Uninstalling torch-2.4.1+cu124:\n","      Successfully uninstalled torch-2.4.1+cu124\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchaudio 2.4.1+cu124 requires torch==2.4.1, but you have torch 2.4.0 which is incompatible.\n","torchvision 0.19.1+cu124 requires torch==2.4.1, but you have torch 2.4.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nvtx-cu12-12.1.105 torch-2.4.0\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n","\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n","Note: you may need to restart the kernel to use updated packages.\n","Collecting transformers==4.45.1\n","  Downloading transformers-4.45.1-py3-none-any.whl.metadata (44 kB)\n","Collecting datasets==3.0.1\n","  Downloading datasets-3.0.1-py3-none-any.whl.metadata (20 kB)\n","Collecting accelerate==0.34.2\n","  Downloading accelerate-0.34.2-py3-none-any.whl.metadata (19 kB)\n","Collecting trl==0.11.1\n","  Downloading trl-0.11.1-py3-none-any.whl.metadata (12 kB)\n","Collecting peft==0.13.0\n","  Downloading peft-0.13.0-py3-none-any.whl.metadata (13 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.45.1) (3.13.1)\n","Collecting huggingface-hub<1.0,>=0.23.2 (from transformers==4.45.1)\n","  Downloading huggingface_hub-0.34.3-py3-none-any.whl.metadata (14 kB)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.45.1) (1.26.3)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.45.1) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.45.1) (6.0.2)\n","Collecting regex!=2019.12.17 (from transformers==4.45.1)\n","  Downloading regex-2025.7.34-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.45.1) (2.32.3)\n","Collecting safetensors>=0.4.1 (from transformers==4.45.1)\n","  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n","Collecting tokenizers<0.21,>=0.20 (from transformers==4.45.1)\n","  Downloading tokenizers-0.20.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n","Collecting tqdm>=4.27 (from transformers==4.45.1)\n","  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n","Collecting pyarrow>=15.0.0 (from datasets==3.0.1)\n","  Downloading pyarrow-21.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets==3.0.1)\n","  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n","Collecting pandas (from datasets==3.0.1)\n","  Downloading pandas-2.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (91 kB)\n","Collecting xxhash (from datasets==3.0.1)\n","  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n","Collecting multiprocess (from datasets==3.0.1)\n","  Downloading multiprocess-0.70.18-py311-none-any.whl.metadata (7.5 kB)\n","Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets==3.0.1) (2024.2.0)\n","Collecting aiohttp (from datasets==3.0.1)\n","  Downloading aiohttp-3.12.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate==0.34.2) (6.0.0)\n","Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from accelerate==0.34.2) (2.4.0)\n","Collecting tyro>=0.5.11 (from trl==0.11.1)\n","  Downloading tyro-0.9.27-py3-none-any.whl.metadata (11 kB)\n","Collecting aiohappyeyeballs>=2.5.0 (from aiohttp->datasets==3.0.1)\n","  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n","Collecting aiosignal>=1.4.0 (from aiohttp->datasets==3.0.1)\n","  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.0.1) (24.2.0)\n","Collecting frozenlist>=1.1.1 (from aiohttp->datasets==3.0.1)\n","  Downloading frozenlist-1.7.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n","Collecting multidict<7.0,>=4.5 (from aiohttp->datasets==3.0.1)\n","  Downloading multidict-6.6.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n","Collecting propcache>=0.2.0 (from aiohttp->datasets==3.0.1)\n","  Downloading propcache-0.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n","Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets==3.0.1)\n","  Downloading yarl-1.20.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (73 kB)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.45.1) (4.9.0)\n","Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<1.0,>=0.23.2->transformers==4.45.1)\n","  Downloading hf_xet-1.1.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (879 bytes)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.45.1) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.45.1) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.45.1) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.45.1) (2024.8.30)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (3.1.3)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (2.20.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (12.1.105)\n","Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (3.0.0)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate==0.34.2) (12.4.99)\n","Collecting docstring-parser>=0.15 (from tyro>=0.5.11->trl==0.11.1)\n","  Downloading docstring_parser-0.17.0-py3-none-any.whl.metadata (3.5 kB)\n","Collecting rich>=11.1.0 (from tyro>=0.5.11->trl==0.11.1)\n","  Downloading rich-14.1.0-py3-none-any.whl.metadata (18 kB)\n","Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl==0.11.1)\n","  Downloading shtab-1.7.2-py3-none-any.whl.metadata (7.4 kB)\n","Collecting typeguard>=4.0.0 (from tyro>=0.5.11->trl==0.11.1)\n","  Downloading typeguard-4.4.4-py3-none-any.whl.metadata (3.3 kB)\n","Collecting typing-extensions>=3.7.4.3 (from huggingface-hub<1.0,>=0.23.2->transformers==4.45.1)\n","  Downloading typing_extensions-4.14.1-py3-none-any.whl.metadata (3.0 kB)\n","INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n","Collecting multiprocess (from datasets==3.0.1)\n","  Downloading multiprocess-0.70.17-py311-none-any.whl.metadata (7.2 kB)\n","  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==3.0.1) (2.9.0.post0)\n","Collecting pytz>=2020.1 (from pandas->datasets==3.0.1)\n","  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n","Collecting tzdata>=2022.7 (from pandas->datasets==3.0.1)\n","  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n","Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==3.0.1) (1.16.0)\n","Collecting markdown-it-py>=2.2.0 (from rich>=11.1.0->tyro>=0.5.11->trl==0.11.1)\n","  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.11.1) (2.18.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.34.2) (2.1.5)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.11/dist-packages (from sympy->torch>=1.10.0->accelerate==0.34.2) (1.3.0)\n","Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl==0.11.1)\n","  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n","Downloading transformers-4.45.1-py3-none-any.whl (9.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m179.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading datasets-3.0.1-py3-none-any.whl (471 kB)\n","Downloading accelerate-0.34.2-py3-none-any.whl (324 kB)\n","Downloading trl-0.11.1-py3-none-any.whl (318 kB)\n","Downloading peft-0.13.0-py3-none-any.whl (322 kB)\n","Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n","Downloading aiohttp-3.12.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m108.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading huggingface_hub-0.34.3-py3-none-any.whl (558 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m558.8/558.8 kB\u001b[0m \u001b[31m153.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pyarrow-21.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (42.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m282.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading regex-2025.7.34-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (798 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m798.9/798.9 kB\u001b[0m \u001b[31m228.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n","Downloading tokenizers-0.20.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m315.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n","Downloading tyro-0.9.27-py3-none-any.whl (129 kB)\n","Downloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n","Downloading pandas-2.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m484.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n","Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n","Downloading docstring_parser-0.17.0-py3-none-any.whl (36 kB)\n","Downloading frozenlist-1.7.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (235 kB)\n","Downloading hf_xet-1.1.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m388.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading multidict-6.6.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (246 kB)\n","Downloading propcache-0.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n","Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n","Downloading rich-14.1.0-py3-none-any.whl (243 kB)\n","Downloading shtab-1.7.2-py3-none-any.whl (14 kB)\n","Downloading typeguard-4.4.4-py3-none-any.whl (34 kB)\n","Downloading typing_extensions-4.14.1-py3-none-any.whl (43 kB)\n","Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n","Downloading yarl-1.20.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (348 kB)\n","Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n","Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n","Installing collected packages: pytz, xxhash, tzdata, typing-extensions, tqdm, shtab, safetensors, regex, pyarrow, propcache, multidict, mdurl, hf-xet, frozenlist, docstring-parser, dill, aiohappyeyeballs, yarl, typeguard, pandas, multiprocess, markdown-it-py, huggingface-hub, aiosignal, tokenizers, rich, aiohttp, tyro, transformers, accelerate, peft, datasets, trl\n","  Attempting uninstall: typing-extensions\n","    Found existing installation: typing_extensions 4.9.0\n","    Uninstalling typing_extensions-4.9.0:\n","      Successfully uninstalled typing_extensions-4.9.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchaudio 2.4.1+cu124 requires torch==2.4.1, but you have torch 2.4.0 which is incompatible.\n","torchvision 0.19.1+cu124 requires torch==2.4.1, but you have torch 2.4.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed accelerate-0.34.2 aiohappyeyeballs-2.6.1 aiohttp-3.12.15 aiosignal-1.4.0 datasets-3.0.1 dill-0.3.8 docstring-parser-0.17.0 frozenlist-1.7.0 hf-xet-1.1.5 huggingface-hub-0.34.3 markdown-it-py-3.0.0 mdurl-0.1.2 multidict-6.6.3 multiprocess-0.70.16 pandas-2.3.1 peft-0.13.0 propcache-0.3.2 pyarrow-21.0.0 pytz-2025.2 regex-2025.7.34 rich-14.1.0 safetensors-0.5.3 shtab-1.7.2 tokenizers-0.20.3 tqdm-4.67.1 transformers-4.45.1 trl-0.11.1 typeguard-4.4.4 typing-extensions-4.14.1 tyro-0.9.27 tzdata-2025.2 xxhash-3.5.0 yarl-1.20.1\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n","\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["%pip install \"torch==2.4.0\"\n","%pip install \"transformers==4.45.1\" \"datasets==3.0.1\" \"accelerate==0.34.2\" \"trl==0.11.1\" \"peft==0.13.0\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2ab0492f-1159-4562-bc34-9c0967b950a0"},"outputs":[],"source":["from datasets import load_dataset, Dataset\n","import torch\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","from peft import LoraConfig\n","from trl import SFTConfig, SFTTrainer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5183067f-0a44-4c1d-b5a0-b574c1238771","colab":{"referenced_widgets":["94b1e09683994f58a2f054a9eea827e3","6061b9950d2a47b7a4216a4735e56e2a","4cba3e8582284a02b61cf604b26936fa"]},"outputId":"00698f6d-e90b-4268-d157-695707e67d3b"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"94b1e09683994f58a2f054a9eea827e3","version_major":2,"version_minor":0},"text/plain":["README.md:   0%|          | 0.00/909 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6061b9950d2a47b7a4216a4735e56e2a","version_major":2,"version_minor":0},"text/plain":["train-00000-of-00001.parquet:   0%|          | 0.00/13.0M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4cba3e8582284a02b61cf604b26936fa","version_major":2,"version_minor":0},"text/plain":["Generating train split:   0%|          | 0/1884 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["원본 데이터의 type 분포:\n","paraphrased_question: 196\n","mrc_question: 491\n","no_answer: 404\n","mrc_question_with_1_to_4_negative: 296\n","synthetic_question: 497\n","\n","전체 데이터 분할 결과: Train 380개, Test 1504개\n","\n","학습 데이터의 type 분포:\n","paraphrased_question: 40\n","mrc_question: 99\n","no_answer: 81\n","mrc_question_with_1_to_4_negative: 60\n","synthetic_question: 100\n","\n","테스트 데이터의 type 분포:\n","paraphrased_question: 156\n","mrc_question: 392\n","no_answer: 323\n","mrc_question_with_1_to_4_negative: 236\n","synthetic_question: 397\n"]}],"source":["dataset = load_dataset(\"iamjoon/klue-mrc-ko-rag-dataset\", split=\"train\")\n","\n","system_message = \"\"\"당신은 검색 결과를 바탕으로 질문에 답변해야 합니다.\n","\n","다음의 지시사항을 따르십시오.\n","1. 질문과 검색 결과를 바탕으로 답변하십시오.\n","2. 검색 결과에 없는 내용을 답변하려고 하지 마십시오.\n","3. 질문에 대한 답이 검색 결과에 없다면 검색 결과에는 \"해당 질문~에 대한 내용이 없습니다.\" 라고 답변하십시오.\n","4. 답변할 때 특정 문서를 참고하여 문장 또는 문단을 작성했다면 뒤에 출처는 이중 리스트로 해당 문서 번호를 남기십시오. 예를 들어서 특정 문장이나 문단을 1번 문서에서 인용했다면 뒤에 [[ref1]]이라고 기재하십시오.\n","5. 예를 들어서 특정 문장이나 문단을 1번 문서와 5번 문서에서 동시에 인용했다면 뒤에 [[ref1]], [[ref5]]이라고 기재하십시오.\n","6. 최대한 다수의 문서를 인용하여 답변하십시오.\n","\n","검색 결과:\n","-----\n","{search_result}\"\"\"\n","\n","print(\"원본 데이터의 type 분포:\")\n","for type_name in set(dataset['type']):\n","    print(f\"{type_name}: {dataset['type'].count(type_name)}\")\n","\n","test_ratio = 0.8\n","\n","train_data = []\n","test_data = []\n","\n","for type_name in set(dataset['type']):\n","    curr_type_data = [i for i in range(len(dataset)) if dataset[i]['type'] == type_name]\n","\n","    test_size = int(len(curr_type_data) * test_ratio)\n","\n","    test_data.extend(curr_type_data[:test_size])\n","    train_data.extend(curr_type_data[test_size:])\n","\n","def format_data(sample):\n","    search_result = \"\\n-----\\n\".join([f\"문서{idx + 1}: {result}\" for idx, result in enumerate(sample[\"search_result\"])])\n","\n","    return {\n","        \"messages\": [\n","            {\n","                \"role\": \"system\",\n","                \"content\": system_message.format(search_result=search_result),\n","            },\n","            {\n","                \"role\": \"user\",\n","                \"content\": sample[\"question\"],\n","            },\n","            {\n","                \"role\": \"assistant\",\n","                \"content\": sample[\"answer\"]\n","            },\n","        ],\n","    }\n","\n","train_dataset = [format_data(dataset[i]) for i in train_data]\n","test_dataset = [format_data(dataset[i]) for i in test_data]\n","\n","print(f\"\\n전체 데이터 분할 결과: Train {len(train_dataset)}개, Test {len(test_dataset)}개\")\n","\n","print(\"\\n학습 데이터의 type 분포:\")\n","for type_name in set(dataset['type']):\n","    count = sum(1 for i in train_data if dataset[i]['type'] == type_name)\n","    print(f\"{type_name}: {count}\")\n","\n","print(\"\\n테스트 데이터의 type 분포:\")\n","for type_name in set(dataset['type']):\n","    count = sum(1 for i in test_data if dataset[i]['type'] == type_name)\n","    print(f\"{type_name}: {count}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5e561d8f-7a5c-4d50-a030-a31ecbe7853a","outputId":"375ad7a0-5849-4781-84eb-b02d3207af97"},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'list'>\n","<class 'list'>\n","<class 'datasets.arrow_dataset.Dataset'>\n","<class 'datasets.arrow_dataset.Dataset'>\n"]}],"source":["print(type(train_dataset))\n","print(type(test_dataset))\n","train_dataset = Dataset.from_list(train_dataset)\n","test_dataset = Dataset.from_list(test_dataset)\n","print(type(train_dataset))\n","print(type(test_dataset))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bb5f299a-e0ca-407f-ae21-0f0e5fadeac4","colab":{"referenced_widgets":["315cb1fc22ab4bf78994825413f357c5","c65fcc8e8d024fd89aad35a1f7859082","db35a3753c454e23bbc209a8b499f7d4","9c154d15310d441ebdfb5047b1918883","95fccabd5e4848d9b0e13ba1cdeebdf9","4dfd28e44139424c913a8035e18e73cb","28307b920aee496d9eb9864a75e0cba0","cef66dcf26304650a4470582361b21fa","8c0c87c560844ede836701c8c319c294","642196a90e3845e581df50188cac083c","c8ca040e70a747ff9f686b6e7461d59f","f6b90e9631be441a8df9996867263b47","9eec725d09d64a7a980dfdaf9649c17b"]},"outputId":"c1f13da3-76e8-4236-f11a-8ae17d5f54d1"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"315cb1fc22ab4bf78994825413f357c5","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/663 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c65fcc8e8d024fd89aad35a1f7859082","version_major":2,"version_minor":0},"text/plain":["model.safetensors.index.json: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"db35a3753c454e23bbc209a8b499f7d4","version_major":2,"version_minor":0},"text/plain":["Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9c154d15310d441ebdfb5047b1918883","version_major":2,"version_minor":0},"text/plain":["model-00001-of-00004.safetensors:   0%|          | 0.00/3.95G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"95fccabd5e4848d9b0e13ba1cdeebdf9","version_major":2,"version_minor":0},"text/plain":["model-00002-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4dfd28e44139424c913a8035e18e73cb","version_major":2,"version_minor":0},"text/plain":["model-00003-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"28307b920aee496d9eb9864a75e0cba0","version_major":2,"version_minor":0},"text/plain":["model-00004-of-00004.safetensors:   0%|          | 0.00/3.56G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cef66dcf26304650a4470582361b21fa","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8c0c87c560844ede836701c8c319c294","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/243 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"642196a90e3845e581df50188cac083c","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c8ca040e70a747ff9f686b6e7461d59f","version_major":2,"version_minor":0},"text/plain":["vocab.json: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f6b90e9631be441a8df9996867263b47","version_major":2,"version_minor":0},"text/plain":["merges.txt: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9eec725d09d64a7a980dfdaf9649c17b","version_major":2,"version_minor":0},"text/plain":["tokenizer.json: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["model_id = \"Qwen/Qwen2-7B-Instruct\"\n","\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_id,\n","    device_map=\"auto\",\n","    torch_dtype=torch.bfloat16,\n",")\n","tokenizer = AutoTokenizer.from_pretrained(model_id)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"adbcf4b1-ab29-451d-b628-21466d16519e"},"outputs":[],"source":["def collate_fn(batch):\n","    new_batch = {\n","        \"input_ids\": [],\n","        \"attention_mask\": [],\n","        \"labels\": []\n","    }\n","\n","    for example in batch:\n","        clean_messages = []\n","        for message in example[\"messages\"]:\n","            clean_message = {\n","                \"role\": message[\"role\"],\n","                \"content\": message[\"content\"]\n","            }\n","            clean_messages.append(clean_message)\n","\n","        text = tokenizer.apply_chat_template(\n","            clean_messages,\n","            tokenize=False,\n","            add_generation_prompt=False\n","        ).strip()\n","\n","        tokenized = tokenizer(\n","            text,\n","            truncation=True,\n","            max_length=max_seq_length,\n","            padding=False,\n","            return_tensors=None,\n","        )\n","\n","        input_ids = tokenized[\"input_ids\"]\n","        attention_mask = tokenized[\"attention_mask\"]\n","\n","        labels = [-100] * len(input_ids)\n","\n","        im_start = \"<|im_start|>\"\n","        im_end = \"<|im_end|>\"\n","        assistant = \"assistant\"\n","\n","        im_start_tokens = tokenizer.encode(im_start, add_special_tokens=False)\n","        im_end_tokens = tokenizer.encode(im_end, add_special_tokens=False)\n","        assistant_tokens = tokenizer.encode(assistant, add_special_tokens=False)\n","\n","        i = 0\n","        while i < len(input_ids):\n","            if (i + len(im_start_tokens) <= len(input_ids) and\n","                input_ids[i:i+len(im_start_tokens)] == im_start_tokens):\n","\n","                assistant_pos = i + len(im_start_tokens)\n","                if (assistant_pos + len(assistant_tokens) <= len(input_ids) and\n","                    input_ids[assistant_pos:assistant_pos+len(assistant_tokens)] == assistant_tokens):\n","\n","                    current_pos = assistant_pos + len(assistant_tokens)\n","\n","                    while current_pos < len(input_ids):\n","                        if (current_pos + len(im_end_tokens) <= len(input_ids) and\n","                            input_ids[current_pos:current_pos+len(im_end_tokens)] == im_end_tokens):\n","\n","                            for j in range(len(im_end_tokens)):\n","                                labels[current_pos + j] = input_ids[current_pos + j]\n","                            break\n","                        labels[current_pos] = input_ids[current_pos]\n","                        current_pos += 1\n","\n","                    i = current_pos\n","\n","            i += 1\n","\n","        new_batch[\"input_ids\"].append(input_ids)\n","        new_batch[\"attention_mask\"].append(attention_mask)\n","        new_batch[\"labels\"].append(labels)\n","\n","    max_length = max(len(ids) for ids in new_batch[\"input_ids\"])\n","\n","    for i in range(len(new_batch[\"input_ids\"])):\n","        padding_length = max_length - len(new_batch[\"input_ids\"][i])\n","\n","        new_batch[\"input_ids\"][i].extend([tokenizer.pad_token_id] * padding_length)\n","        new_batch[\"attention_mask\"][i].extend([0] * padding_length)\n","        new_batch[\"labels\"][i].extend([-100] * padding_length)\n","\n","    for k, v in new_batch.items():\n","        new_batch[k] = torch.tensor(v)\n","\n","    return new_batch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"82c8810f-2613-4d6b-8dcb-985ba50991f4"},"outputs":[],"source":["max_seq_length=8192\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b58447a2-f506-4529-ac01-a8c280cb76e9"},"outputs":[],"source":["prompt_lst = []\n","label_lst = []\n","\n","for prompt in test_dataset[\"messages\"]:\n","    text = tokenizer.apply_chat_template(\n","        prompt, tokenize=False, add_generation_prompt=False\n","    )\n","    input = text.split('<|im_start|>assistant')[0] + '<|im_start|>assistant'\n","    label = text.split('<|im_start|>assistant')[1]\n","    prompt_lst.append(input)\n","    label_lst.append(label)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"15a4262b-186b-4abf-b6cf-65d22f3c10b0"},"outputs":[],"source":["import torch\n","from peft import AutoPeftModelForCausalLM\n","from transformers import  AutoTokenizer, pipeline"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"27d9d395-a3c2-47b7-b135-34f4f78f2ec2","colab":{"referenced_widgets":["160a45ba592f4b5989f3a98d35567bcf"]},"outputId":"6e789579-2175-43a0-e6f6-8c1aa90a696e"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"160a45ba592f4b5989f3a98d35567bcf","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"]}],"source":["peft_model_id = \"qwen2-7b-rag-ko/checkpoint-285\"\n","fine_tuned_model = AutoPeftModelForCausalLM.from_pretrained(peft_model_id, device_map=\"auto\", torch_dtype=torch.float16)\n","pipe = pipeline(\"text-generation\", model=fine_tuned_model, tokenizer=tokenizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2b6fbdd3-a8db-48dc-92fa-fb51b20f2c3d"},"outputs":[],"source":["eos_token = tokenizer(\"<|im_end|>\",add_special_tokens=False)[\"input_ids\"][0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fad932e6-1c1a-4bd8-b1b5-c877a575cd51"},"outputs":[],"source":["def test_inference(pipe, prompt):\n","    outputs = pipe(prompt, max_new_tokens=1024, eos_token_id=eos_token, do_sample=False)\n","    return outputs[0]['generated_text'][len(prompt):].strip()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c41ebf47-6905-45ad-b685-a1751613caa8","outputId":"a1918ee5-4a8d-4730-9262-ee014c9f139a"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:623: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n","  warnings.warn(\n","Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"]},{"name":"stdout","output_type":"stream","text":["    response:\n","도서전에서 관람객의 관심을 받을 것으로 예상되는 프로그램은 다음과 같습니다:\n","\n","1. **'북 멘토 프로그램'**: 이 프로그램은 책과 관련된 일을 하고 싶은 사람들에게 각 분야 전문가들이 경험과 노하우를 전수해주는 멘토링 프로그램입니다. 시 창작, 번역, 북 디자인 등 다양한 분야에서 멘토링이 이루어지므로, 독자들이 직접적으로 책과 출판에 관한 실질적인 정보를 얻을 수 있습니다. 이 프로그램은 책과 관련된 직업을 찾는 사람들에게 큰 관심을 끌 것으로 예상됩니다 [[ref4]].\n","\n","2. **'인문학 아카데미'**: 이 프로그램은 유시민 전 의원, 광고인 박웅현 씨 등 다양한 분야의 전문가들이 문화, 역사, 미학 등 다양한 주제에 대해 강의하는 곳입니다. 이는 독자들이 다양한 분야의 지식을 확장하고, 문화적 이해를 높이는 데 큰 도움이 될 것으로 예상됩니다. 특히, 다양한 분야의 전문가들이 참여하는 점이 관람객들에게 큰 관심을 끌 것으로 보입니다 [[ref4]].\n","\n","3. **'저자와의 대화'**: 매년 진행되는 이 프로그램은 소설가, 시인 등 여러 분야의 작가들이 참여하여 독자들과 만날 수 있는 기회입니다. 작가들이 직접 책을 소개하고, 독자들과의 대화를 나누는 것은 독자들에게 큰 관심을 끌 것입니다. 특히, 이번 도서전 홍보대사로 나선 소설가 박범신 씨와 다른 작가들이 참여할 예정이므로, 작가들과의 만남을 기대할 수 있습니다 [[ref4]].\n","\n","4. **'인도의 영혼들' 전시**: 인도는 역대 최대 규모의 주빈국 행사를 마련하여, 노벨문학상 수상 100주년을 맞는 대문호 라빈드라나트 타고르를 비롯해 여러 인도 출신 노벨상 수상자와 관련 도서를 소개하는 전시가 예정되어 있습니다. 이는 독자들이 다양한 문화와 역사에 대한 이해를 높이는 데 큰 도움이 될 것으로 예상됩니다 [[ref4]].\n","\n","이와 같은 프로그램들은 각기 독자들에게 다양한 경험과 지식을 제공할 것이며, 관람객들의 관심을 끌 것으로 예상됩니다.\n","    label:\n","\n","도서전에서 관람객의 관심을 받을 것으로 예상되는 프로그램은 다음과 같습니다:\n","\n","1. **저자와의 대화**: 소설가 박범신, 정유정, 이인화, 최민석, 김혜나, 신달자, 함민복 등 21명의 작가들이 참여하여 독자들과 직접 만나는 프로그램입니다. 이는 독자들이 좋아하는 작가와 직접 소통할 수 있는 기회를 제공하여 큰 관심을 받을 것으로 예상됩니다 [[ref3]], [[ref5]].\n","\n","2. **인문학 아카데미**: 유시민 전 의원, 광고인 박웅현 씨 등이 문화, 역사, 미학 등 다양한 분야에 대해 강의하는 프로그램입니다. 다양한 주제의 강연은 관람객의 지적 호기심을 자극할 것입니다 [[ref3]], [[ref5]].\n","\n","3. **북 멘토 프로그램**: 시 창작, 번역, 북 디자인 등 책과 관련된 일을 하고 싶은 사람들을 위해 각 분야 전문가들이 경험과 노하우를 전수해 주는 프로그램입니다. 이는 출판업계에 관심 있는 사람들에게 유익한 기회를 제공할 것입니다 [[ref3]], [[ref5]].\n","\n","4. **특별 전시 및 영화 상영**: 인도 출신 노벨상 수상자들과 관련된 도서를 소개하는 ‘인도의 영혼들’ 전시와 인도 영화산업 100년사를 다루는 ‘인도 영화 100년사’ 전에서 영화 ‘데브다스’ 상영 등이 포함됩니다. 이는 인도의 문화와 역사를 접할 수 있는 기회를 제공하여 관람객의 관심을 끌 것입니다 [[ref3]], [[ref5]].\n","\n","5. **아트 마트**: 중견 및 신진 작가들의 뮤라섹 판화 작품을 빵가게에서 빵을 고르듯 구입할 수 있는 아트 마트가 열립니다. 이는 미술품을 소장하고 즐기는 것이 일부 특권층의 전유물이 아님을 알리기 위해 기획된 프로그램으로, 예술에 관심 있는 관람객들에게 큰 호응을 얻을 것으로 보입니다 [[ref1]].\n","\n","이와 같은 다양한 프로그램들은 도서전에서 관람객의 관심을 끌고, 풍부한 경험을 제공할 것으로 기대됩니다.<|im_end|>\n","\n","--------------------------------------------------\n","    response:\n","김광진이 과거에 연습을 구경하곤 했던 농구 선수의 이름은 문경은입니다. 문경은은 현재 SK나이츠의 감독으로, 김광진이 연습을 구경하곤 했던 농구 선수입니다 [[ref5]].\n","    label:\n","\n","김광진이 과거에 연습을 구경하곤 했던 농구 선수의 이름은 문경은입니다. 김광진은 1990년에 연세대에 새내기로 입학한 문경은이 잘한다는 소리를 듣고 그의 연습경기를 보러 연세대 체육관에 갔다고 합니다 [[ref5]].<|im_end|>\n","\n","--------------------------------------------------\n","    response:\n","문정왕후가 친형제들의 싸움에서 편을 들어주지 않은 인물은 윤원로입니다. 윤원형과 윤원로 사이의 분쟁이 발생했을 때, 문정왕후는 윤원형의 손을 들어주었지만, 윤원로는 그의 손을 들어주지 않았습니다. 윤원로는 윤원형의 사촌인 윤춘년의 상소를 받아 윤원형을 탄핵하는 상소를 올렸습니다. [[ref1]]\n","    label:\n","\n","문정왕후가 친형제들의 싸움에서 편을 들어주지 않은 인물은 윤원로입니다. 문정왕후는 윤원형과 윤원로가 싸우게 되자 윤원형의 손을 들어주었고, 결국 윤원로를 죽이는 결정을 내렸습니다 [[ref1]].<|im_end|>\n","\n","--------------------------------------------------\n","    response:\n","동궁전 화재에서 인종을 구한 사람은 숙의 정씨입니다. 동궁전 화재 당시 인종은 깊게 잠들어 화재를 피하지 못했습니다. 그러나 화재 현장을 목격한 인종의 후궁인 숙의 정씨는 직접 불길 속으로 뛰어 들어가 인종을 구했습니다. 이로 인해 숙의 정씨는 그 공로를 인정받아 귀인으로 오르게 되었습니다 [[ref4]].\n","    label:\n","\n","동궁전 화재에서 인종을 구한 사람은 인종의 후궁인 숙의 정씨입니다. 숙의 정씨는 화재 현장을 목격하고 직접 불길 속으로 뛰어 들어가 인종을 구했으며, 이 공로로 귀인으로 승격되었습니다 [[ref4]].<|im_end|>\n","\n","--------------------------------------------------\n","    response:\n","봉은사 승려 보우를 문정왕후에게 주선해준 인물은 정난정입니다. 정난정은 봉은사의 승려 보우를 문정왕후에게 소개하여 병조판서직에 오르게 하였습니다 [[ref5]].\n","    label:\n","\n","봉은사 승려 보우를 문정왕후에게 주선해준 인물은 정난정입니다. 정난정은 보우를 문정왕후에게 소개시켜 그를 총애받게 했고, 이로 인해 보우는 병조판서직에 오르게 되었습니다 [[ref5]].<|im_end|>\n","\n","--------------------------------------------------\n"]}],"source":["for prompt, label in zip(prompt_lst[300:305], label_lst[300:305]):\n","    print(f\"    response:\\n{test_inference(pipe, prompt)}\")\n","    print(f\"    label:\\n{label}\")\n","    print(\"-\"*50)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"918de7bf-b690-460e-95af-46d74c189cc6"},"outputs":[],"source":["\n","pipe_base = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fa95b03c-71e8-4453-8d97-31255c38b6ef","outputId":"72b5523a-afcb-47f4-a884-23461bed5e7d"},"outputs":[{"name":"stdout","output_type":"stream","text":["    response:\n","도서전에서 관람객의 관심을 받을 것으로 예상되는 프로그램은 여러 가지가 있을 수 있습니다. \n","\n","첫째로, '저자와의 대화' 프로그램이 주목받을 수 있습니다. 이 프로그램은 소설가 박범신 씨를 비롯한 21명의 작가들이 참여하여 독자들과 만날 예정입니다. 이는 작가와 독자 간의 직접적인 소통을 제공하며, 독자들이 작가의 창작 과정과 아이디어를 더 잘 이해하는 데 도움이 될 것입니다.\n","\n","둘째로, '인문학 아카데미'가 관람객의 관심을 끌 것으로 예상됩니다. 이 프로그램은 유시민 전 의원, 광고인 박웅현 씨 등이 문화 역사 미학 등 다양한 분야에 대해 강의하는 것을 통해 관람객들에게 새로운 지식과 통찰력을 제공할 것입니다.\n","\n","셋째로, '북 멘토 프로그램'도 관람객의 관심을 끌 것으로 예상됩니다. 이 프로그램은 책과 관련된 일을 하고 싶은 사람들에게 각 분야 전문가들이 경험과 노하우를 전수해주는 프로그램입니다. 시 창작, 번역, 북 디자인 등의 분야에서 멘토링이 이뤄지므로, 이에 관심이 있는 관람객들에게 큰 호응을 얻을 수 있습니다.\n","\n","마지막으로, 주빈국인 인도의 '인도의 영혼들' 전시와 '인도 영화 100년사' 전시도 관람객의 관심을 끌 것으로 예상됩니다. 이 전시는 인도의 문화와 역사에 대한 깊은 이해를 제공하며, 특히 노벨상 수상자들의 작품을 통해 인도 문학의 풍부함을 보여줄 것입니다. 또한 인도 영화 '데브다스'의 상영 역시 관람객들에게 인도의 영화 문화에 대한 이해를 높일 수 있을 것입니다.\n","    label:\n","\n","도서전에서 관람객의 관심을 받을 것으로 예상되는 프로그램은 다음과 같습니다:\n","\n","1. **저자와의 대화**: 소설가 박범신, 정유정, 이인화, 최민석, 김혜나, 신달자, 함민복 등 21명의 작가들이 참여하여 독자들과 직접 만나는 프로그램입니다. 이는 독자들이 좋아하는 작가와 직접 소통할 수 있는 기회를 제공하여 큰 관심을 받을 것으로 예상됩니다 [[ref3]], [[ref5]].\n","\n","2. **인문학 아카데미**: 유시민 전 의원, 광고인 박웅현 씨 등이 문화, 역사, 미학 등 다양한 분야에 대해 강의하는 프로그램입니다. 다양한 주제의 강연은 관람객의 지적 호기심을 자극할 것입니다 [[ref3]], [[ref5]].\n","\n","3. **북 멘토 프로그램**: 시 창작, 번역, 북 디자인 등 책과 관련된 일을 하고 싶은 사람들을 위해 각 분야 전문가들이 경험과 노하우를 전수해 주는 프로그램입니다. 이는 출판업계에 관심 있는 사람들에게 유익한 기회를 제공할 것입니다 [[ref3]], [[ref5]].\n","\n","4. **특별 전시 및 영화 상영**: 인도 출신 노벨상 수상자들과 관련된 도서를 소개하는 ‘인도의 영혼들’ 전시와 인도 영화산업 100년사를 다루는 ‘인도 영화 100년사’ 전에서 영화 ‘데브다스’ 상영 등이 포함됩니다. 이는 인도의 문화와 역사를 접할 수 있는 기회를 제공하여 관람객의 관심을 끌 것입니다 [[ref3]], [[ref5]].\n","\n","5. **아트 마트**: 중견 및 신진 작가들의 뮤라섹 판화 작품을 빵가게에서 빵을 고르듯 구입할 수 있는 아트 마트가 열립니다. 이는 미술품을 소장하고 즐기는 것이 일부 특권층의 전유물이 아님을 알리기 위해 기획된 프로그램으로, 예술에 관심 있는 관람객들에게 큰 호응을 얻을 것으로 보입니다 [[ref1]].\n","\n","이와 같은 다양한 프로그램들은 도서전에서 관람객의 관심을 끌고, 풍부한 경험을 제공할 것으로 기대됩니다.<|im_end|>\n","\n","--------------------------------------------------\n","    response:\n","김광진이 과거에 연습을 구경하곤 했던 농구 선수의 이름은 '문경은'입니다. 문경은 현재 SK나이츠의 감독으로 활동하고 있습니다.\n","    label:\n","\n","김광진이 과거에 연습을 구경하곤 했던 농구 선수의 이름은 문경은입니다. 김광진은 1990년에 연세대에 새내기로 입학한 문경은이 잘한다는 소리를 듣고 그의 연습경기를 보러 연세대 체육관에 갔다고 합니다 [[ref5]].<|im_end|>\n","\n","--------------------------------------------------\n","    response:\n","문서1을 참조하면, 문정왕후는 친형제들 사이의 싸움에서 윤원형의 편을 들어주었다. 따라서, 문정왕후가 편을 들어주지 않은 인물은 윤원로입니다.\n","    label:\n","\n","문정왕후가 친형제들의 싸움에서 편을 들어주지 않은 인물은 윤원로입니다. 문정왕후는 윤원형과 윤원로가 싸우게 되자 윤원형의 손을 들어주었고, 결국 윤원로를 죽이는 결정을 내렸습니다 [[ref1]].<|im_end|>\n","\n","--------------------------------------------------\n","    response:\n","동궁전 화재에서 인종을 구한 사람은 숙의 정씨입니다.\n","    label:\n","\n","동궁전 화재에서 인종을 구한 사람은 인종의 후궁인 숙의 정씨입니다. 숙의 정씨는 화재 현장을 목격하고 직접 불길 속으로 뛰어 들어가 인종을 구했으며, 이 공로로 귀인으로 승격되었습니다 [[ref4]].<|im_end|>\n","\n","--------------------------------------------------\n","    response:\n","봉은사 승려 보우를 문정왕후에게 주선해준 인물은 정난정입니다.\n","    label:\n","\n","봉은사 승려 보우를 문정왕후에게 주선해준 인물은 정난정입니다. 정난정은 보우를 문정왕후에게 소개시켜 그를 총애받게 했고, 이로 인해 보우는 병조판서직에 오르게 되었습니다 [[ref5]].<|im_end|>\n","\n","--------------------------------------------------\n"]}],"source":["for prompt, label in zip(prompt_lst[300:305], label_lst[300:305]):\n","    print(f\"    response:\\n{test_inference(pipe_base, prompt)}\")\n","    print(f\"    label:\\n{label}\")\n","    print(\"-\"*50)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7Ekt-76QmaxW"},"outputs":[],"source":["import torch\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","from peft import PeftModel\n","\n","# 경로 설정\n","base_model_path = \"Qwen/Qwen2-7B-Instruct\"\n","adapter_path = \"./qwen2-7b-rag-ko/checkpoint-285\"\n","merged_model_path = \"./Qwen2-7B-Instruct_RAG_KO_custom_20250804\"\n","\n","# 디바이스 설정\n","device_arg = {\"device_map\": \"auto\"}\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EfO3bpMDmaxX","outputId":"4ac6e8df-16c6-4ef3-b8e6-ca77f45f02be"},"outputs":[{"name":"stdout","output_type":"stream","text":["Loading and merging PEFT from: ./qwen2-7b-rag-ko/checkpoint-285\n"]}],"source":["# LoRA 어댑터 로드 및 병합\n","print(f\"Loading and merging PEFT from: {adapter_path}\")\n","peft_model = PeftModel.from_pretrained(model, adapter_path)\n","merged_model = peft_model.merge_and_unload()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NRi_ZO5zmaxX","outputId":"2e0aac6f-a281-454e-de9e-4560f1888378"},"outputs":[{"name":"stdout","output_type":"stream","text":["Saving merged model to: ./Qwen2-7B-Instruct_RAG_KO_custom_20250804\n","✅ 모델과 토크나이저 저장 완료\n"]}],"source":["# 저장\n","print(f\"Saving merged model to: {merged_model_path}\")\n","model.save_pretrained(merged_model_path)\n","tokenizer.save_pretrained(merged_model_path)\n","print(\"✅ 모델과 토크나이저 저장 완료\")\n"]},{"cell_type":"code","source":["def get_token():\n","  with open('20250801_Huggingfacie_Token.key', 'r', encoding='utf-8') as file:\n","    return file.readline().rstrip()"],"metadata":{"id":"cbrKetQ_mrGN"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XdZ8ZaKMmaxX"},"outputs":[],"source":["from huggingface_hub import HfApi\n","api = HfApi()\n","\n","username = \"mypsyche98\"\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eMC2T-DumaxX","outputId":"c5db72f9-1047-4f46-b1a2-357120c6bc58"},"outputs":[{"data":{"text/plain":["RepoUrl('https://huggingface.co/mypsyche98/Qwen2-7B-Instruct_RAG_KO_custom_20250804', endpoint='https://huggingface.co', repo_type='model', repo_id='mypsyche98/Qwen2-7B-Instruct_RAG_KO_custom_20250804')"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["api.create_repo(\n","    token=get_token(),\n","    repo_id=f\"{username}/Qwen2-7B-Instruct_RAG_KO_custom_20250804\",\n","    repo_type=\"model\"\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"referenced_widgets":["5909a64c991e45a4a99a2a0760afafd4","2c4dfe5a1761405c99891b817977152e","7014ee86bdcb4fde9882ad483629b63a","ac553ef020e54581a290053df0c181b8","b71092fae1d6425db3c9aa5cde4f3572","e019338e6f13436ebe08ceb77fcece2d","b5a04a917b2642eab2f02e757b0347c9"]},"id":"rvmaiUrMmaxX","outputId":"41781b5d-4807-4a28-a34e-298a4f267cc4"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5909a64c991e45a4a99a2a0760afafd4","version_major":2,"version_minor":0},"text/plain":["Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2c4dfe5a1761405c99891b817977152e","version_major":2,"version_minor":0},"text/plain":["New Data Upload                         : |          |  0.00B /  0.00B            "]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7014ee86bdcb4fde9882ad483629b63a","version_major":2,"version_minor":0},"text/plain":["  ...G_KO_custom_20250804/tokenizer.json:   0%|          | 24.0kB / 11.4MB            "]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ac553ef020e54581a290053df0c181b8","version_major":2,"version_minor":0},"text/plain":["  ...04/model-00004-of-00004.safetensors:   5%|4         | 50.3MB / 1.09GB            "]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b71092fae1d6425db3c9aa5cde4f3572","version_major":2,"version_minor":0},"text/plain":["  ...04/model-00003-of-00004.safetensors:   1%|1         | 58.7MB / 4.33GB            "]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e019338e6f13436ebe08ceb77fcece2d","version_major":2,"version_minor":0},"text/plain":["  ...04/model-00001-of-00004.safetensors:   2%|1         | 92.3MB / 4.88GB            "]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b5a04a917b2642eab2f02e757b0347c9","version_major":2,"version_minor":0},"text/plain":["  ...04/model-00002-of-00004.safetensors:   1%|1         | 58.7MB / 4.93GB            "]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["CommitInfo(commit_url='https://huggingface.co/mypsyche98/Qwen2-7B-Instruct_RAG_KO_custom_20250804/commit/bb8c1098bd8ed496d9c6b2db898c0d77c7905d50', commit_message='Upload folder using huggingface_hub', commit_description='', oid='bb8c1098bd8ed496d9c6b2db898c0d77c7905d50', pr_url=None, repo_url=RepoUrl('https://huggingface.co/mypsyche98/Qwen2-7B-Instruct_RAG_KO_custom_20250804', endpoint='https://huggingface.co', repo_type='model', repo_id='mypsyche98/Qwen2-7B-Instruct_RAG_KO_custom_20250804'), pr_revision=None, pr_num=None)"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["api.upload_folder(\n","    token=get_token(),\n","    repo_id=f\"{username}/Qwen2-7B-Instruct_RAG_KO_custom_20250804\",\n","    folder_path=merged_model_path,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-1eJPY0cmaxX"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.10"}},"nbformat":4,"nbformat_minor":0}