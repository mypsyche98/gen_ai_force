{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "dc338ac8-931c-4b20-9057-e987e8227b9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch==2.4.0\n",
      "  Downloading torch-2.4.0-cp311-cp311-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0) (4.9.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0) (2024.2.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.4.0)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.4.0)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.4.0)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0) (9.1.0.70)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.4.0)\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.4.0)\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.4.0)\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.4.0)\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.4.0)\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0) (2.20.5)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.4.0)\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.4.0) (12.4.99)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.4.0) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.4.0) (1.3.0)\n",
      "Downloading torch-2.4.0-cp311-cp311-manylinux1_x86_64.whl (797.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.3/797.3 MB\u001b[0m \u001b[31m245.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m596.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m653.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m540.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m243.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m649.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m598.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m654.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m653.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "Installing collected packages: nvidia-nvtx-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, torch\n",
      "  Attempting uninstall: nvidia-nvtx-cu12\n",
      "    Found existing installation: nvidia-nvtx-cu12 12.4.99\n",
      "    Uninstalling nvidia-nvtx-cu12-12.4.99:\n",
      "      Successfully uninstalled nvidia-nvtx-cu12-12.4.99\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.3.0.142\n",
      "    Uninstalling nvidia-cusparse-cu12-12.3.0.142:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.3.0.142\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.5.119\n",
      "    Uninstalling nvidia-curand-cu12-10.3.5.119:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.5.119\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.0.44\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.0.44:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.0.44\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.4.99\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.4.99:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.4.99\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.4.99\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.4.99:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.4.99\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.4.99\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.4.99:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.4.99\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.4.2.65\n",
      "    Uninstalling nvidia-cublas-cu12-12.4.2.65:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.4.2.65\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.0.99\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.0.99:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.0.99\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.4.1+cu124\n",
      "    Uninstalling torch-2.4.1+cu124:\n",
      "      Successfully uninstalled torch-2.4.1+cu124\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.4.1+cu124 requires torch==2.4.1, but you have torch 2.4.0 which is incompatible.\n",
      "torchvision 0.19.1+cu124 requires torch==2.4.1, but you have torch 2.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nvtx-cu12-12.1.105 torch-2.4.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting transformers==4.45.1\n",
      "  Downloading transformers-4.45.1-py3-none-any.whl.metadata (44 kB)\n",
      "Collecting datasets==3.0.1\n",
      "  Downloading datasets-3.0.1-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting accelerate==0.34.2\n",
      "  Downloading accelerate-0.34.2-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting trl==0.11.1\n",
      "  Downloading trl-0.11.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting peft==0.13.0\n",
      "  Downloading peft-0.13.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.45.1) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers==4.45.1)\n",
      "  Downloading huggingface_hub-0.34.3-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.45.1) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.45.1) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.45.1) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers==4.45.1)\n",
      "  Downloading regex-2025.7.34-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.45.1) (2.32.3)\n",
      "Collecting safetensors>=0.4.1 (from transformers==4.45.1)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tokenizers<0.21,>=0.20 (from transformers==4.45.1)\n",
      "  Downloading tokenizers-0.20.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting tqdm>=4.27 (from transformers==4.45.1)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting pyarrow>=15.0.0 (from datasets==3.0.1)\n",
      "  Downloading pyarrow-21.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets==3.0.1)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets==3.0.1)\n",
      "  Downloading pandas-2.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (91 kB)\n",
      "Collecting xxhash (from datasets==3.0.1)\n",
      "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets==3.0.1)\n",
      "  Downloading multiprocess-0.70.18-py311-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets==3.0.1) (2024.2.0)\n",
      "Collecting aiohttp (from datasets==3.0.1)\n",
      "  Downloading aiohttp-3.12.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate==0.34.2) (6.0.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from accelerate==0.34.2) (2.4.0)\n",
      "Collecting tyro>=0.5.11 (from trl==0.11.1)\n",
      "  Downloading tyro-0.9.27-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp->datasets==3.0.1)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp->datasets==3.0.1)\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.0.1) (24.2.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets==3.0.1)\n",
      "  Downloading frozenlist-1.7.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets==3.0.1)\n",
      "  Downloading multidict-6.6.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->datasets==3.0.1)\n",
      "  Downloading propcache-0.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets==3.0.1)\n",
      "  Downloading yarl-1.20.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (73 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.45.1) (4.9.0)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<1.0,>=0.23.2->transformers==4.45.1)\n",
      "  Downloading hf_xet-1.1.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (879 bytes)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.45.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.45.1) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.45.1) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.45.1) (2024.8.30)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate==0.34.2) (12.4.99)\n",
      "Collecting docstring-parser>=0.15 (from tyro>=0.5.11->trl==0.11.1)\n",
      "  Downloading docstring_parser-0.17.0-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rich>=11.1.0 (from tyro>=0.5.11->trl==0.11.1)\n",
      "  Downloading rich-14.1.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl==0.11.1)\n",
      "  Downloading shtab-1.7.2-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting typeguard>=4.0.0 (from tyro>=0.5.11->trl==0.11.1)\n",
      "  Downloading typeguard-4.4.4-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting typing-extensions>=3.7.4.3 (from huggingface-hub<1.0,>=0.23.2->transformers==4.45.1)\n",
      "  Downloading typing_extensions-4.14.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting multiprocess (from datasets==3.0.1)\n",
      "  Downloading multiprocess-0.70.17-py311-none-any.whl.metadata (7.2 kB)\n",
      "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==3.0.1) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets==3.0.1)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets==3.0.1)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==3.0.1) (1.16.0)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=11.1.0->tyro>=0.5.11->trl==0.11.1)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.11.1) (2.18.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.34.2) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.11/dist-packages (from sympy->torch>=1.10.0->accelerate==0.34.2) (1.3.0)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl==0.11.1)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading transformers-4.45.1-py3-none-any.whl (9.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m218.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading datasets-3.0.1-py3-none-any.whl (471 kB)\n",
      "Downloading accelerate-0.34.2-py3-none-any.whl (324 kB)\n",
      "Downloading trl-0.11.1-py3-none-any.whl (318 kB)\n",
      "Downloading peft-0.13.0-py3-none-any.whl (322 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading aiohttp-3.12.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m354.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.34.3-py3-none-any.whl (558 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m558.8/558.8 kB\u001b[0m \u001b[31m155.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-21.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (42.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m484.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2025.7.34-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (798 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m798.9/798.9 kB\u001b[0m \u001b[31m228.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "Downloading tokenizers-0.20.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m306.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading tyro-0.9.27-py3-none-any.whl (129 kB)\n",
      "Downloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "Downloading pandas-2.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m510.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Downloading docstring_parser-0.17.0-py3-none-any.whl (36 kB)\n",
      "Downloading frozenlist-1.7.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (235 kB)\n",
      "Downloading hf_xet-1.1.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m411.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multidict-6.6.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (246 kB)\n",
      "Downloading propcache-0.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
      "Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading rich-14.1.0-py3-none-any.whl (243 kB)\n",
      "Downloading shtab-1.7.2-py3-none-any.whl (14 kB)\n",
      "Downloading typeguard-4.4.4-py3-none-any.whl (34 kB)\n",
      "Downloading typing_extensions-4.14.1-py3-none-any.whl (43 kB)\n",
      "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Downloading yarl-1.20.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (348 kB)\n",
      "Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: pytz, xxhash, tzdata, typing-extensions, tqdm, shtab, safetensors, regex, pyarrow, propcache, multidict, mdurl, hf-xet, frozenlist, docstring-parser, dill, aiohappyeyeballs, yarl, typeguard, pandas, multiprocess, markdown-it-py, huggingface-hub, aiosignal, tokenizers, rich, aiohttp, tyro, transformers, accelerate, peft, datasets, trl\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.9.0\n",
      "    Uninstalling typing_extensions-4.9.0:\n",
      "      Successfully uninstalled typing_extensions-4.9.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.4.1+cu124 requires torch==2.4.1, but you have torch 2.4.0 which is incompatible.\n",
      "torchvision 0.19.1+cu124 requires torch==2.4.1, but you have torch 2.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed accelerate-0.34.2 aiohappyeyeballs-2.6.1 aiohttp-3.12.15 aiosignal-1.4.0 datasets-3.0.1 dill-0.3.8 docstring-parser-0.17.0 frozenlist-1.7.0 hf-xet-1.1.5 huggingface-hub-0.34.3 markdown-it-py-3.0.0 mdurl-0.1.2 multidict-6.6.3 multiprocess-0.70.16 pandas-2.3.1 peft-0.13.0 propcache-0.3.2 pyarrow-21.0.0 pytz-2025.2 regex-2025.7.34 rich-14.1.0 safetensors-0.5.3 shtab-1.7.2 tokenizers-0.20.3 tqdm-4.67.1 transformers-4.45.1 trl-0.11.1 typeguard-4.4.4 typing-extensions-4.14.1 tyro-0.9.27 tzdata-2025.2 xxhash-3.5.0 yarl-1.20.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install \"torch==2.4.0\"\n",
    "%pip install \"transformers==4.45.1\" \"datasets==3.0.1\" \"accelerate==0.34.2\" \"trl==0.11.1\" \"peft==0.13.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "2ab0492f-1159-4562-bc34-9c0967b950a0"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig\n",
    "from trl import SFTConfig, SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "5183067f-0a44-4c1d-b5a0-b574c1238771"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f15e7aa272845588238d6172e04c9a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/909 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27a3aac67ef94b9782264af1d91dcb0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/13.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "273712cfa35e4972b72b74b6782ba625",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1884 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 데이터의 type 분포:\n",
      "synthetic_question: 497\n",
      "paraphrased_question: 196\n",
      "no_answer: 404\n",
      "mrc_question_with_1_to_4_negative: 296\n",
      "mrc_question: 491\n",
      "\n",
      "전체 데이터 분할 결과: Train 380개, Test 1504개\n",
      "\n",
      "학습 데이터의 type 분포:\n",
      "synthetic_question: 100\n",
      "paraphrased_question: 40\n",
      "no_answer: 81\n",
      "mrc_question_with_1_to_4_negative: 60\n",
      "mrc_question: 99\n",
      "\n",
      "테스트 데이터의 type 분포:\n",
      "synthetic_question: 397\n",
      "paraphrased_question: 156\n",
      "no_answer: 323\n",
      "mrc_question_with_1_to_4_negative: 236\n",
      "mrc_question: 392\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"iamjoon/klue-mrc-ko-rag-dataset\", split=\"train\")\n",
    "\n",
    "system_message = \"\"\"당신은 검색 결과를 바탕으로 질문에 답변해야 합니다.\n",
    "\n",
    "다음의 지시사항을 따르십시오.\n",
    "1. 질문과 검색 결과를 바탕으로 답변하십시오.\n",
    "2. 검색 결과에 없는 내용을 답변하려고 하지 마십시오.\n",
    "3. 질문에 대한 답이 검색 결과에 없다면 검색 결과에는 \"해당 질문~에 대한 내용이 없습니다.\" 라고 답변하십시오.\n",
    "4. 답변할 때 특정 문서를 참고하여 문장 또는 문단을 작성했다면 뒤에 출처는 이중 리스트로 해당 문서 번호를 남기십시오. 예를 들어서 특정 문장이나 문단을 1번 문서에서 인용했다면 뒤에 [[ref1]]이라고 기재하십시오.\n",
    "5. 예를 들어서 특정 문장이나 문단을 1번 문서와 5번 문서에서 동시에 인용했다면 뒤에 [[ref1]], [[ref5]]이라고 기재하십시오.\n",
    "6. 최대한 다수의 문서를 인용하여 답변하십시오.\n",
    "\n",
    "검색 결과:\n",
    "-----\n",
    "{search_result}\"\"\"\n",
    "\n",
    "print(\"원본 데이터의 type 분포:\")\n",
    "for type_name in set(dataset['type']):\n",
    "    print(f\"{type_name}: {dataset['type'].count(type_name)}\")\n",
    "\n",
    "test_ratio = 0.8\n",
    "\n",
    "train_data = []\n",
    "test_data = []\n",
    "\n",
    "for type_name in set(dataset['type']):\n",
    "    curr_type_data = [i for i in range(len(dataset)) if dataset[i]['type'] == type_name]\n",
    "\n",
    "    test_size = int(len(curr_type_data) * test_ratio)\n",
    "\n",
    "    test_data.extend(curr_type_data[:test_size])\n",
    "    train_data.extend(curr_type_data[test_size:])\n",
    "\n",
    "def format_data(sample):\n",
    "    search_result = \"\\n-----\\n\".join([f\"문서{idx + 1}: {result}\" for idx, result in enumerate(sample[\"search_result\"])])\n",
    "\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_message.format(search_result=search_result),\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": sample[\"question\"],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": sample[\"answer\"]\n",
    "            },\n",
    "        ],\n",
    "    }\n",
    "\n",
    "train_dataset = [format_data(dataset[i]) for i in train_data]\n",
    "test_dataset = [format_data(dataset[i]) for i in test_data]\n",
    "\n",
    "print(f\"\\n전체 데이터 분할 결과: Train {len(train_dataset)}개, Test {len(test_dataset)}개\")\n",
    "\n",
    "print(\"\\n학습 데이터의 type 분포:\")\n",
    "for type_name in set(dataset['type']):\n",
    "    count = sum(1 for i in train_data if dataset[i]['type'] == type_name)\n",
    "    print(f\"{type_name}: {count}\")\n",
    "\n",
    "print(\"\\n테스트 데이터의 type 분포:\")\n",
    "for type_name in set(dataset['type']):\n",
    "    count = sum(1 for i in test_data if dataset[i]['type'] == type_name)\n",
    "    print(f\"{type_name}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "30e17067-01c0-45e9-9aa0-b352e82a4901"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': '당신은 검색 결과를 바탕으로 질문에 답변해야 합니다.\\n\\n다음의 지시사항을 따르십시오.\\n1. 질문과 검색 결과를 바탕으로 답변하십시오.\\n2. 검색 결과에 없는 내용을 답변하려고 하지 마십시오.\\n3. 질문에 대한 답이 검색 결과에 없다면 검색 결과에는 \"해당 질문~에 대한 내용이 없습니다.\" 라고 답변하십시오.\\n4. 답변할 때 특정 문서를 참고하여 문장 또는 문단을 작성했다면 뒤에 출처는 이중 리스트로 해당 문서 번호를 남기십시오. 예를 들어서 특정 문장이나 문단을 1번 문서에서 인용했다면 뒤에 [[ref1]]이라고 기재하십시오.\\n5. 예를 들어서 특정 문장이나 문단을 1번 문서와 5번 문서에서 동시에 인용했다면 뒤에 [[ref1]], [[ref5]]이라고 기재하십시오.\\n6. 최대한 다수의 문서를 인용하여 답변하십시오.\\n\\n검색 결과:\\n-----\\n문서1: 폭스바겐에 이어 계열 브랜드인 아우디와 스코다도 디젤차 배출가스를 고의로 조작한 것으로 나타났다. 폭스바겐 그룹 산하에 있는 다른 브랜드로 ‘폭스바겐 스캔들’의 여파가 확산되는 모양새다.사건이 번지면서 세 가지 의혹이 꼬리를 물고 있다. 아우디는 210만대의 디젤차에 배출가스를 속이는 소프트웨어를 장착한 사실을 확인했다고 28일(현지시간) 발표했다. 이날 스코다도 아우디와 같은 눈속임 소프트웨어를 120만대의 디젤차에 적용했다고 밝혔다. 상용차 180만대와 그룹 내 나머지 5개 브랜드의 90만대 차량도 이번 사건에 연루된 것으로 폭스바겐그룹 측은 파악하고 있다.이번 사건을 폭스바겐 최고경영진이 사전에 알고 있었다는 의혹이 제기되고 있다. 마르틴 빈터코른 전 폭스바겐그룹 최고경영자(CEO)는 지난 23일 사퇴하면서 “부정행위에 놀랐지만 조작 사실을 전혀 몰랐다”고 부인했다. 하지만 27일 독일 일간지 FAZ는 “2011년 폭스바겐 내 기술자가 ‘배출가스 조작 행위가 이뤄지고 있다’고 보고했지만 묵살당했다”고 보도했다.이어 “8년간 폭스바겐의 지배자로 군림한 빈터코른이 조작 사실을 몰랐을 리 없다”고 지적했다. 독일 검찰은 빈터코른 전 CEO를 상대로 수사에 착수했다. 일부에서는 폭스바겐 스캔들에는 어떤 ‘배경’이 있는 것 아니냐는 추측도 내놓고 있다. 미국의 ‘독일차 죽이기’가 아니냐는 의혹의 시선이다. 미국 정부는 지난 21일 폭스바겐의 배출가스 조작 사실을 발표했다. 공교롭게 애플은 이날 전기차 사업 진출을 공식 선언했다. 미국은 테슬라와 구글을 중심으로 세계 전기차 시장을 선도하고 있다. 그러나 독일을 중심으로 디젤차 판매량이 늘면서 전기차 시장의 성장 속도가 더뎠다. 디젤차의 확산은 폭스바겐의 고속 성장으로 이어졌다. 폭스바겐은 올 상반기에 504만대를 팔아 처음으로 도요타를 제치고 세계 자동차 판매량 1위에 올랐다. 미국 제너럴모터스(GM)는 3위로 내려앉았다. 과거 도요타도 비슷한 경험을 한 적이 있다. 도요타는 2008년 GM을 누르고 처음으로 세계 1위에 올랐다. 공교롭게도 이듬해 미국 정부로부터 가속페달 결함으로 대규모 리콜을 명령받아 몇 년 동안 어려움을 겪었다. 폭스바겐 주가는 21일 이후 40% 가까이 떨어졌다. 미국 도로교통안전국(NHTSA)은 신속하게 결함을 시정하지 않은 BMW의 미니도 조사 중이다. ‘미국 배후론’이 나오는 근거다. 배출가스 조작이 독일 차에만 해당하는 게 아니라 자동차업계에 퍼져 있는 관행이라는 지적도 나온다. 미국 포드는 1998년 밴 차량에 배출가스 조작장치를 달았다가 780만달러의 벌금을 물었다. 같은 해 일본 혼다도 비슷한 사례로 1710만달러의 벌금을 냈다. 1995년에 GM이 캐딜락 차량에 배출가스를 임의로 조절하는 장치를 달았다가 1100만달러의 벌금을 부담했다. 1998년엔 볼보와 르노, 캐터필러 등에 같은 이유로 8340만달러의 벌금이 부과됐다. 폭스바겐은 이번 사건으로 미국에서만 최소 180억달러(약 21조원)의 벌금을 물 것이란 전망이 나온다.영국 파이낸셜타임스는 “대부분 기업 스캔들은 경영진의 무지가 원인이지만 폭스바겐 사태는 고의성이 있다는 점에서 엔론의 회계부정 사건만큼 심각하다”며 “폭스바겐은 재정적으로 문제가 없는 상태지만 기업 신뢰도 추락으로 회사가 사라질 수 있다”고 내다봤다.\\n-----\\n문서2: 지난 12일(현지시간) 독일 동북부에 있는 인구 13만명의 소도시 볼프스부르크. 프랑크푸르트에서 기차로 3시간 걸리는 이곳엔 폭스바겐그룹 본사 외에 이 회사가 만든 자동차 테마파크 아우토슈타트가 있다. 아우토슈타트 입구에 들어서니 폭스바겐 로고나 자동차를 홍보하는 광고 선전물은 전혀 찾아볼 수 없었다. 안내를 맡고 있는 도미니크 몬은 “남녀노소 누구나 마음껏 즐길 수 있도록 다양한 시설을 갖추고 있다”며 “드러내놓고 브랜드를 알리지 않아도 놀다 보면 어느새 폭스바겐그룹의 충성고객이 된다”고 설명했다.2000년 문을 연 아우토슈타트는 매년 200만명 이상의 관광객이 다녀가 독일 관광청이 선정한 10대 관광 명소에도 이름을 올렸다. 자동차 테마파크지만 자동차에 관심이 없는 사람들도 즐길 수 있도록 꾸민 덕분이다. 최고급 호텔(리츠칼튼)은 물론 스위스의 유명 외식업체인 뫼벤픽이 운영하는 레스토랑만 9개가 있다. 여성들은 전 세계에서 유행하는 고급 소품을 모아놓은 메트로폴에서 쇼핑을 즐길 수 있다. 어린이들을 위한 면허증 취득 프로그램과 놀이공간도 마련돼 있다.폭스바겐, 부가티, 람보르기니, 아우디, 세아트, 스코다 등 브랜드별로 마련된 전시관도 있지만 반드시 폭스바겐그룹의 차량만을 고집하지는 않는다. 자동차 박물관에선 메르세데스벤츠와 BMW, 포드 등 자동차 역사에서 의미가 있는 모델들도 소개하고 있다. 아우토슈타트의 가장 큰 특징은 고객이 직접 차량을 출고해 집으로 가져갈 수 있다는 것. 테마파크 안에 두 개의 카타워와 차량 출고장인 쿤덴센터가 있다. 20층 높이의 카타워 2개에는 400대씩 총 800대의 차량이 주인을 기다리고 있다. 고객이 이곳을 찾으면 해당 차량은 카타워에서 컨베이어 터널을 통해 연결된 쿤덴센터로 옮겨져 주행거리가 ‘0㎞’인 채로 전달된다. 아우토슈타트 바로 옆에 있는 볼프스부르크 공장의 생산라인을 둘러보며 차량 제작 과정을 눈으로 확인할 수도 있다. 방실 폭스바겐코리아 홍보부장은 “아우토슈타트에서는 하루 평균 500대의 차량이 출고된다”며 “매년 독일에서 폭스바겐을 구매하는 사람의 30%가량이 이곳에서 차량을 인도받는다”고 말했다. 볼프스부르크=최진석 기자\\n-----\\n문서3: ㈜불스원의 차량용 방향제 브랜드 ‘폴라프레쉬’가 디즈니의 대표 캐릭터를 적용한 차량용 통풍구 방향제 ‘폴라프레쉬 디즈니’를 출시한다고 밝혔다. 신제품 ‘폴라프레쉬 디즈니’ 통풍구 방향제는 기존 멤브레인 리필 타입의 제품에 전 연령층에서 사랑받는 디즈니의 대표 캐릭터인 ‘미키 마우스’, ‘곰돌이 푸’, ‘도날드 덕’을 적용해 키덜트족이나 아이가 있는 가정의 차량 인테리어 소품으로 손색없는 귀여운 이미지를 더했다. 특히, 각 캐릭터를 대표하는 컬러를 입힌 부드러운 촉감의 실리콘 소재를 차용해 입체감을 더했으며, 운전자들의 선호도가 높은 ‘싱그러운 라임모히또향’, ‘상쾌한 아쿠아향’, ‘사랑스러운 프레쉬베리향’, ‘상큼한 허니레몬향’, ‘달콤한 버블껌향’, ‘시원한 오션아쿠아향’ 등 6가지로 구성했다. 에센셜 오일을 함유한 멤브레인 2개입을 기본 구성하여 약 90일 동안 은은하면서도 기분 좋은 향기를 지속적으로 즐길 수 있으며, 리필 타입으로 간편하게 멤브레인을 교체해가며 사용할 수 있기 때문에 본체는 영구적으로 사용 가능하다. 불스원은 이번 ‘폴라프레쉬 디즈니’ 통풍구 방향제 출시를 기념해 불스원 공식 온라인몰 ‘불스원몰’과 전국 주요 대형마트에서 제품 구매 고객에게 디즈니 캐릭터가 그려진 핸드폰 스마트톡(8천원 상당)을 함께 증정할 예정이다. 이와 함께 디즈니의 대표 캐릭터를 적용한 ‘폴라프레쉬 섬유탈취 스프레이’ 제품도 함께 출시했다. 차량 실내는 물론 개인 생활 공간 어디서나 사용할 수 있는 스프레이 타입의 탈취제로 구석구석 배어있는 냄새의 원인을 제거해 상쾌한 공간 분위기를 연출한다. 또한 향수를 뿌린 듯한 풍부한 향기를 즐길 수 있도록 ‘프레쉬 그린가든’, ‘스윗한 자몽향’, ‘시원한 쿨아쿠아향’ 등 3종으로 구성됐다. 신제품 ‘폴라프레쉬 디즈니’ 통풍구 방향제와 ‘폴라프레쉬 섬유탈취 스프레이’는 불스원 공식 온라인 쇼핑몰 ‘불스원몰’과 전국 주요 대형마트에서 구매 가능하며, 판매 가격은 각각 ‘폴라프레쉬 디즈니’ 12,900원(용량 4ml), ‘폴라프레쉬 섬유탈취 스프레이’ 6,900원(용량 400ml)이다. 불스원 에어케어 서현정 브랜드 담당은 “이번 신제품은 전세계적으로 사랑받는 디즈니의 대표 캐릭터를 적용해 차량용 방향제의 소장 가치를 더한 것이 특징”이라며, “앞으로도 소비자들의 취향을 만족시킬 수 있는 다양한 콜라보 제품을 선보여 폴라프레쉬만의 매력을 알릴 계획”이라고 말했다.\\n-----\\n문서4: 곤충은 부정적 또는 긍정적이든 다양한 분위기와 이미지를 설정하는데 유용하므로 문학의 소재로서 흔히 사용된다. 우리의 시와 소설 등에 대한 곤충 문화에 대한 연구는 없었으나, 나비야 청산가자(청구영언), 반딧불이(정약용), 금롱속의 귀뚜라미, 방우선(이규보), 구더기와 개미(이광수), 귀뚜라미(김소월), 나비춤(정을병), 장수하늘소(이외수) 등의 당대의 유명 작가들의 시조, 시 및 소설에서 그 흔적을 찾아보는 것이 어렵지 않다. 구비문학에서는 더 많은 관련성을 볼 수 있는데 그 대표로서 속담을 꼽을 수 있다. \\'송충이는 솔잎을 먹어야 산다.\\' 또는 \\'쇠똥벌레 떠밀 듯 한다.\\' 등과 같이 곤충이 관련된 속담을 551구 정도 찾을 수 있고 여기에 등장한 곤충 무리만도 43 무리가 구분된다.\\n\\n카프카의 변신에서는 어느 날 문득 주인공이 커다란 해충으로 변하게 되는데 카프카는 주인공 그레고르 잠자가 변한 벌레가 어떤 곤충이었는지 정확히 밝히지 않는다. 단지 혐오감을 전할뿐이다. ‘변신’에서의 벌레는 한 인간이 사회의 경제적이고 기능적 존재로서의 의무를 이행할 수 없거나 그것을 원치 않을 경우, 산업사회 속에서 무용하고 열등하며 부정적인 존재인 ‘해충’의 이미지를 비유하며 자유를 갈망하는 한 개인의 실존형식과 그것을 추구하는 삶의 태도를 형상화했다. 경제적 능력을 상실한 그레고르가 가족이라는 사회적 울타리 안에서 추방되어야하는 무용한 존재, 해충으로 그려지는 ‘변신’은 결국 인간이 인간으로서의 자유의지를 갖고 행하는 존재에서 자본주의 사회의 이익을 창출하기 위한 수단, 도구로 전락하는 ‘인간소외’를 주제로 한다.\\n\\n고대 이집트, 마야 그리고 중국의 그림문자와 상형문자에는 곤충의 형태를 차용한 문자들이 많으며, 대부분 나라의 언어에는 곤충이름이나 그의 생산물에서 유래한 것이 많다. 한 예로 폭스바겐의 \\'Beetles(딱정벌레)\\'란 자동차 제품명과 어린이용 출판사인‘ladybeetles(무당벌레)’를 들 수 있는데 전자는 차의 생김새에서 그리고 후자는 어린이들이 무당벌레를 좋아하므로 상징으로서 이용하게 된 것이다.\\n-----\\n문서5: 닛산과 도요타, GM, 폭스바겐 등 글로벌 자동차 업체들이 중국의 저가 자동차 시장을 잡기 위해 치열한 경쟁을 벌이고 있다.일본 닛산은 중국 현지에서 생산하는 소형차 마치(베누치아 R30·사진)를 4만위안(약 663만원)에 판매한다고 29일 발표했다. 지금까지 중국에서 가장 값싼 차는 미국 GM이 2010년 내놓은 6만위안(약 994만원)대 소형차 사일(sail)이었으나, 닛산이 300만원 이상 싼 모델을 선보이기로 한 것이다.독일 폭스바겐도 당초 2016년 이후에 내놓을 계획이던 5만~6만위안대의 저가 소형차를 이르면 연내 선보일 것으로 알려졌다. 폭스바겐이 현재 중국에서 판매하는 가장 낮은 가격대의 차는 9만위안(약 1491만원) 수준인 싼타나다.앞서 작년 11월 도요타는 신형 비오스를 구형 모델보다 28% 낮은 6만9800위안(약 1200만원)에 선보인 데 이어 지난 3월엔 7만위안대 저가 모델을 추가했다. 현대·기아자동차도 최저 가격이 7만3000위안 수준인 베르나(엑센트)와 K2(프라이드)로 중국 소형차 시장을 공략하고 있다. 베르나는 중국 소형차 시장에서 1, 2위를 다투고 있다.완성차 업체들은 저가 자동차 생산을 늘리기 위해 공장도 새로 짓고 있다. 폭스바겐은 중국 우한에 신공장을 건설 중이며 톈진과 칭다오에도 생산라인을 새로 지을 방침이다. 이렇게 되면 폭스바겐의 중국 공장은 13개로 늘어난다.신정관 KB투자증권 기업분석팀장은 “글로벌 완성차 업체들이 저가 차량 생산을 늘리는 것은 점유율을 확대할 수 있는 가장 쉬운 방법이기 때문”이라며 “앞으로 저가 자동차 시장을 장악하고 있는 중국 토종 브랜드의 점유율이 줄어들 것”으로 내다봤다. 지난 3월 말 기준 중국 토종 브랜드의 점유율은 36.2%였다.'},\n",
       " {'role': 'user', 'content': '폭스바겐이 곤충명을 이름으로 지은 차량은?'},\n",
       " {'role': 'assistant',\n",
       "  'content': '폭스바겐이 곤충명을 이름으로 지은 차량은 \"비틀(Beetle)\"입니다. 이 차량의 이름은 딱정벌레를 의미하는 영어 단어에서 유래했습니다 [[ref4]].'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[345][\"messages\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "5e561d8f-7a5c-4d50-a030-a31ecbe7853a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'datasets.arrow_dataset.Dataset'>\n",
      "<class 'datasets.arrow_dataset.Dataset'>\n"
     ]
    }
   ],
   "source": [
    "print(type(train_dataset))\n",
    "print(type(test_dataset))\n",
    "train_dataset = Dataset.from_list(train_dataset)\n",
    "test_dataset = Dataset.from_list(test_dataset)\n",
    "print(type(train_dataset))\n",
    "print(type(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "b2a392a5"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8415473c08af41adaed2341cea9859c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1504 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_dataset.save_to_disk(\"test_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "205c38e8-e863-446d-b1f6-e9c587138276"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [{'content': '당신은 검색 결과를 바탕으로 질문에 답변해야 합니다.\\n\\n다음의 지시사항을 따르십시오.\\n1. 질문과 검색 결과를 바탕으로 답변하십시오.\\n2. 검색 결과에 없는 내용을 답변하려고 하지 마십시오.\\n3. 질문에 대한 답이 검색 결과에 없다면 검색 결과에는 \"해당 질문~에 대한 내용이 없습니다.\" 라고 답변하십시오.\\n4. 답변할 때 특정 문서를 참고하여 문장 또는 문단을 작성했다면 뒤에 출처는 이중 리스트로 해당 문서 번호를 남기십시오. 예를 들어서 특정 문장이나 문단을 1번 문서에서 인용했다면 뒤에 [[ref1]]이라고 기재하십시오.\\n5. 예를 들어서 특정 문장이나 문단을 1번 문서와 5번 문서에서 동시에 인용했다면 뒤에 [[ref1]], [[ref5]]이라고 기재하십시오.\\n6. 최대한 다수의 문서를 인용하여 답변하십시오.\\n\\n검색 결과:\\n-----\\n문서1: 도시 농업의 경우, 세계 각지에서 지구 온난화 문제, 대기의 건조 및 미중 무역 분쟁, 한일 경제 전쟁 등의 여파에 따라 계속 발생되는 아마존 우림 산불 등과 같은 각종 자연재해 등이 지속적으로 일으키게 되면서, 기후 변화로 인한 생태계 파괴 등을 막을 수 있는 조건도 있고 자발적인 생활 패턴이 생기게 되는 대안을 제시할 수 있게 되는 이점이 있다. 그러나 도시 농업을 활성화하게 되면 숲과 같은 삼림 보호 정책은 물론 바다나 개펄 등 해양 생태계 보호 정책까지 대폭적으로 강화할 수 있는 등 다양한 니즈를 충족시킬 수 있게 된다. 기존에는 삼림 벌채 후 농경지 조성을 하였지만, 건물 최상층, 중간 피난층, 주상복합아파트 같은 주거 및 상업시설 복합 건축물의 상업 지역 부분 옥상 등지에 농지를 가꾸어져서 건물에 상주하고 있는 분들과 주거 시설 거주민들이 1년 동안 안정적으로 식량 자원을 확보할 수 있게 되는 좋은 점이 있다. 이미 EBS TV의 하나뿐인 지구라는 교양 프로그램에도 이와 같은 배경을 소재로 둔 이야기를 토대로 보면, 기후 변화 대응 체계 해소 취지를 앞장서는 여건이 마련된다. 그렇게 되면 훌륭한 조건을 가질 수 있어, 물 부족 국가 또는 강수량이 낮은 지역에게는 새로운 희망이자 기회를 얻게 되는 셈이다.\\n-----\\n문서2: 지속가능성과 안보 관점에서 봤을 때, 여러 장비나 기기에 사용되는 에너지원은 일원화될수록 좋다. 따라서 난방, 운송 부문의 에너지원을 전기로 통합한다면 재생에너지로 생산된 전력을 여러 부문에 효율적으로 사용함으로써 전체 에너지 사용량을 줄일 수 있을 것이다. 이러한 ‘섹터간 융합’은 다양한 기기의 에너지원을 전력으로 바꾸는 ‘전기화’를 통해 달성할 수 있다. 이에 더해 전기를 사용하는 기기는 작동 과정에서 온실가스를 배출하지 않으므로 발전 섹터만 탈탄소화할 수 있다면 에너지 시스템 전체의 탄소배출량을 크게 줄일 수 있다.\\n\\n열펌프가 대표적인 사례다. 열펌프는 전기를 이용해 온도가 낮은 곳에서 높은 곳으로 열을 이동시키는 장치로, 현재 상용화된 기술 중 단일 기술로는 온실가스 배출에 가장 크게 기여할 것으로 기대된다. 냉온장고나 냉난방기를 건물 단위로 적용하는 개념의 기술이라고 볼 수 있다. 열펌프 기술이 적용된 국제에너지기구(IEA)는 화석연료를 사용하는 건물 난방시스템 중 30%만 열펌프로 교체해도 연간 8% 가량의 탄소배출량을 줄일 수 있을 것으로 내다봤다. 화석연료를 이용한 보일러처럼 배출가스가 생성되지도 않으므로 대기오염물질을 배출하지 않는다는 것도 장점이다. 열펌프와 마찬가지로 전기자동차 역시 에너지 이용 효율을 높이는 동시에 대기오염물질 배출을 줄이는 데 유용하다.\\n\\n한편, 에너지를 소모하는 과정에서는 필연적으로 열에너지 형태의 손실이 발생한다. 손실열은 별도의 일을 하지 않고 버려지는 에너지다. 따라서 이러한 에너지를 회수해 다시 사용할 수 있다면 에너지시스템 전반의 효율을 크게 높일 수 있다. 완전한 탈탄소 에너지체계로 이행하는 데 과도기적인 형태인 복합화력이 대표적인 사례다. 화력발전소는 최대 40% 남짓한 효율을 낸다. 투입한 연료의 에너지 중 40% 정도만 전력으로 변환할 수 있다는 뜻이다. 나머지 60%의 에너지는 열에너지 형태로 손실된다. 이렇게 손실되는 열에너지를 도시의 블록 단위로 온수를 공급하는 지역난방에 활용하면 효율을 70%대까지 끌어올릴 수 있다. 복합화력의 연료로는 폐기물을 소각하는 과정에서 발생하는 열을 주로 사용하고 있지만 태양열이나 지열을 이용한 방식을 지역난방에 적용하는 방안도 활발하게 연구되고 있다.\\n\\n운송 부문의 전기화 또한 중요한 요소다. 이미 지하철을 비롯한 철도망이 상당 부분 전기화됐으며 도로 교통에서도 전기차가 빠르게 보급되고 있다. 다만 전기차의 핵심 부품인 배터리를 생산하는 과정에서 온실가스가 적지 않게 배출된다는 점을 고려해야 한다. 전기차에 사용되는 전기를 화력발전으로 생산할 경우, 차량의 전체 생애주기에서 전기차가 내연기관차보다 더 많은 온실가스를 생성한다. 이는 차량의 전기화가 재생에너지로의 전환과 함께 추진돼야 함을 뜻한다. 2009년 기준 유럽의 전력 포트폴리오를 바탕으로 계산해보면 전기차의 탄소배출량이 내연기관에 비해 탄소 배출량이 최대 56% 적은 것으로 나타난다.\\n\\n다만 에너지전환의 다른 요소에 비해 운송 부문의 전기화에는 오랜 시간이 걸릴 것으로 보인다. 현재 시점에서 비행기와 대형 화물 트럭, 선박의 전기화는 어려운 편이다. 이들 운송수단은 대출력이 필요해서 장착해야 하는 배터리의 용량도 큰데, 배터리의 무게로 인해 기체 중량이 지나치게 증가하므로 효율성이 낮아지기 때문이다. 대출력이 필요한 운송수단에는 대안으로 전기로 생산한 바이오연료나 수소를 이용하는 방안이 연구 중이다.\\n-----\\n문서3: 미래 청정에너지 개발에 공동 협력하기 위한 ‘2014 월드그린에너지포럼’이 22~24일 경북 경주시 힐튼호텔에서 열린다.한국경제신문과 경상북도·경주시가 주최하는 이번 포럼에는 ‘그린에너지 패러다임의 대변환’이란 주제로 30여개국 정·관·학·산업계 관계자 3000여명이 참석한다. 첫째날과 둘째날에는 참석자들이 태양광 원자력 정보통신기술 에너지저장 연료전지 등 5개 분과로 나눠 세미나를 연다.2010년 노벨경제학상 수상자인 크리스토퍼 피사리데스 런던정치경제대 교수와 이스마일 엘지줄리 기후 변화에 관한 정부 간 협의체 부의장이 연사로 참석한다. 경상북도는 지방자치단체 가운데 처음으로 개발도상국 고위급 인사 및 유엔산업개발기구 등 국제기구와 함께 개도국의 에너지 협력을 증진하고 기후 변화 대응 방안을 논의한다. 마지막날에는 한국원자력환경공단 등 산업현장을 방문하고 야간에는 경주지역 관광지를 둘러보는 나이트 투어도 계획돼 있다.포럼은 폐회식에서 지구촌의 에너지 빈곤 퇴치와 미래 청정에너지 개발 노력, 세계의 공동 번영 및 지속 가능한 개발을 추구하자는 내용을 담은 경주선언문을 채택한다.\\n-----\\n문서4: 국립종자원은 이달 초 제주도에 지원을 개설했다. 아열대 작물을 개량해 특허권을 확보하는 게 설립 목적이다. 한반도 기후가 아열대성으로 변하면서 중남미에서 나는 국화과 식용열매 아티초크나 남아프리카의 채소 오크라 등의 산지가 한국으로 바뀌었다. 전남·경남 해안까지 아열대 작물의 노지재배가 가능해지면서 매년 100종이 넘는 아열대 작물 특허가 출원되고 있기도 하다. 한반도의 아열대화는 녹차밭을 전남 보성에서 강원 고성까지 북상시키는 등 농산물 지도를 바꿔 놓고 있다.○사철 수확하는 아열대 작물열대 작물인 망고, 용과 등은 제주도 비닐하우스에서 재배된다. 겨울철에만 난방을 하면 무리없이 키울 수 있다. 브로콜리같이 생긴 열매를 먹는 아티초크는 겨울철에도 수확이 가능하다.제주도뿐 아니라 전남 일대도 아열대 작물 재배 면적이 늘어나고 있다. 전남농업기술원에 따르면 2009년 38만㎡이던 전남의 블루베리 농장 규모는 지난해 131만㎡로 커졌다. 수입에만 의존하던 아열대 채소류 오크라, 인디언시금치 등은 해남·강진·장흥 일대에 작년부터 12만㎡ 규모로 재배되기 시작했다.이성주 국립종자원 제주지원장은 “2007년 이후 아열대 작물 신품종 특허출원 신청 건수가 꾸준히 늘어 최근에는 연간 30여개 작물 100여개 품종으로 증가했다”고 말했다. 김천환 농촌진흥청 온난화대응센터 연구원은 “제주 전남 경남 해안가는 이미 아열대 기후로 분류하고 있다”고 설명했다. 국내에서 재배되는 아열대 작물은 주로 고급 레스토랑에 납품되거나 인터넷몰 ‘아시아마트’ 등을 통해 외국인들에게 판매된다. 최근에는 국내 소비자들의 수요도 증가해 백화점에서도 국내산 아열대 작물을 팔기 시작했다. 신세계백화점은 올 들어 제주산 ‘패션프루트’ 판매를 시작했다. 애플망고와 용과 판매량은 전년 대비 20% 늘었다. 갤러리아백화점의 식품관 고메이494에서는 이달 들어 제주산 ‘아테모야(슈거애플)’를 선보였다. 한 박스(3㎏)에 20만원으로 가격은 좀 비싸지만 마니아층이 두터워지고 있다는 설명이다.○청주까지 올라온 한라봉사과 등 주요 작물의 산지는 북상 중이다. 제주 특산물로 유명한 한라봉과 감귤은 이미 충북 청주에서도 생산된다. 대구 등이 주 산지인 사과는 경기 포천, 강원 영월 등에서도 많이 재배되고 있다. ‘보성녹차’로 유명한 전남 보성은 녹차의 주 산지를 강원 고성에 내줄 판이다.수산물도 심상치 않다. 10월이 제철인 찬물에 사는 낙지는 바다수온이 오르면서 올 들어 수확량이 대폭 감소했다. 반대로 따뜻한 물을 좋아하는 꽃게는 올해 풍년이다. 연평도의 올해 꽃게 어획량(900t)은 작년보다 67% 늘었다. 해양수산부에 따르면 2000년대 남해안의 평균 수온은 19.2도로 1970년대에 비해 0.8도 올랐다. 제주도 일부 지역에서만 서식하던 갯가재, 홍다리얼룩새우 등 아열대 생물이 남해안 전역에서 발견되고 있다. 반면 수온이 낮아야 잘 자라는 김 미역 다시마 등은 남해안에서 수확하기가 어려워졌다는 분석이다.\\n-----\\n문서5: 서울 강서구 마곡지구에 여의도공원 2배 규모의 아시아 최대 생태공원(조감도)이 조성된다. 서울시는 마곡지구에 5000여종의 식물을 갖춘 도시형 식물원인 ‘서울 화목원(花木園)’(가칭)을 2016년 12월까지 준공할 예정이라고 21일 발표했다. 서울 서남권의 첫 대형 공원이자 서울의 ‘녹색 허브’ 역할을 할 것으로 기대된다. 산업단지(190만㎡)에 입주하는 기업 연구시설에 이어 주거단지(106만㎡) 내 아파트 일반분양이 본격화되는 마곡지구에 호재로 작용할 전망이다.서울시에 따르면 기존의 대규모 택지개발지구에 조성되는 공원이 거주자들의 여가·휴양 기능에 초점을 맞췄다면 서울화목원은 인근 마곡첨단산업지구(산업)와 경계를 허물고 옛 마곡의 풍광을 재현하는 데 치중한다. 겸재 정선의 ‘종해청조(宗海廳潮)’에 나타난 옛 양천 지역의 모습을 재현하기 위해 인공 시설물을 최소화하고 물재생센터의 재생수와 빗물을 공원 호수로 활용하는 것이 특징이다. 전체 면적은 50만3431㎡로 여의도 공원(23만㎡)의 2배가 넘고 어린이대공원(56만㎡)보다 약간 작다. 화목원에는 6만㎡ 규모 식물원과 식물문화센터가 들어선다. 식물문화센터는 3개 건물(연면적 1만㎡)에 전시온실, 식물도서관, 가드닝센터 등이 갖춰진다. 시는 다음달 중 관계기관 협의를 거쳐 기본계획안을 확정하고 설계에 들어갈 예정이다.',\n",
       "   'role': 'system'},\n",
       "  {'content': '기후 변화가 농업에 미치는 영향은 무엇인가요?', 'role': 'user'},\n",
       "  {'content': '기후 변화는 농업에 다양한 영향을 미치고 있습니다. 기후 변화로 인해 농업 환경이 변화하면서 작물 재배 지역과 방식이 달라지고 있습니다.\\n\\n첫째, 기후 변화로 인해 한반도와 같은 지역에서는 아열대 작물의 재배가 가능해지고 있습니다. 예를 들어, 한반도의 기후가 아열대성으로 변하면서 중남미에서 나는 아티초크나 남아프리카의 오크라 등의 작물이 한국에서도 재배되고 있습니다. 이는 농산물 지도를 바꾸고 있으며, 제주도와 전남 일대에서 아열대 작물 재배 면적이 늘어나고 있습니다. 또한, 제주도에서 재배되던 한라봉과 감귤이 충북 청주에서도 생산되고 있으며, 사과의 주 산지도 북상하고 있습니다[[ref4]].\\n\\n둘째, 기후 변화는 수산물에도 영향을 미치고 있습니다. 바다 수온이 오르면서 찬물에 사는 낙지의 수확량은 감소하고, 따뜻한 물을 좋아하는 꽃게의 어획량은 증가하고 있습니다. 또한, 제주도 일부 지역에서만 서식하던 갯가재, 홍다리얼룩새우 등 아열대 생물이 남해안 전역에서 발견되고 있습니다. 반면, 수온이 낮아야 잘 자라는 김, 미역, 다시마 등의 수확은 어려워지고 있습니다[[ref4]].\\n\\n이와 같이 기후 변화는 농업에 큰 영향을 미치며, 작물 재배 지역의 변화와 새로운 작물의 도입, 수산물의 어획량 변화 등을 초래하고 있습니다. 이는 농업 생산성과 식량 안보에 중요한 영향을 미칠 수 있습니다.',\n",
       "   'role': 'assistant'}]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "bb5f299a-e0ca-407f-ae21-0f0e5fadeac4"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f6b05c2c39049fc89502f64043ea151",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/663 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8263085f8ccd428987e166da1922e6a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4d3387a2abc476dbfd4393d8dbcbe80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc6f489eb0cb4755864a1c9635271a87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/3.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4679cb4d2eb4f378c01e7afb641ffb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afb3968cbea44dd4bc3526dd25a03ac1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fb3247a61c74952b8a132450971c44f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/3.56G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b62d79d15d4c4acfa23008a3053dfd22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50ce47234b0d4ada9d36c3d588f833e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/243 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a4eec9cb5164d7981e6d5cf43acc1d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "714b5d0c37f04d9f9bf7c69610d9e4f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3635d9d132a4340ae6c058a79347435",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10de9509cb3f4905a23604450a36bee7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = \"Qwen/Qwen2-7B-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "d8df8462-72af-4ad4-b1ed-06a84dd6086f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "당신은 검색 결과를 바탕으로 질문에 답변해야 합니다.\n",
      "\n",
      "다음의 지시사항을 따르십시오.\n",
      "1. 질문과 검색 결과를 바탕으로 답변하십시오.\n",
      "2. 검색 결과에 없는 내용을 답변하려고 하지 마십시오.\n",
      "3. 질문에 대한 답이 검색 결과에 없다면 검색 결과에는 \"해당 질문~에 대한 내용이 없습니다.\" 라고 답변하십시오.\n",
      "4. 답변할 때 특정 문서를 참고하여 문장 또는 문단을 작성했다면 뒤에 출처는 이중 리스트로 해당 문서 번호를 남기십시오. 예를 들어서 특정 문장이나 문단을 1번 문서에서 인용했다면 뒤에 [[ref1]]이라고 기재하십시오.\n",
      "5. 예를 들어서 특정 문장이나 문단을 1번 문서와 5번 문서에서 동시에 인용했다면 뒤에 [[ref1]], [[ref5]]이라고 기재하십시오.\n",
      "6. 최대한 다수의 문서를 인용하여 답변하십시오.\n",
      "\n",
      "검색 결과:\n",
      "-----\n",
      "문서1: 도시 농업의 경우, 세계 각지에서 지구 온난화 문제, 대기의 건조 및 미중 무역 분쟁, 한일 경제 전쟁 등의 여파에 따라 계속 발생되는 아마존 우림 산불 등과 같은 각종 자연재해 등이 지속적으로 일으키게 되면서, 기후 변화로 인한 생태계 파괴 등을 막을 수 있는 조건도 있고 자발적인 생활 패턴이 생기게 되는 대안을 제시할 수 있게 되는 이점이 있다. 그러나 도시 농업을 활성화하게 되면 숲과 같은 삼림 보호 정책은 물론 바다나 개펄 등 해양 생태계 보호 정책까지 대폭적으로 강화할 수 있는 등 다양한 니즈를 충족시킬 수 있게 된다. 기존에는 삼림 벌채 후 농경지 조성을 하였지만, 건물 최상층, 중간 피난층, 주상복합아파트 같은 주거 및 상업시설 복합 건축물의 상업 지역 부분 옥상 등지에 농지를 가꾸어져서 건물에 상주하고 있는 분들과 주거 시설 거주민들이 1년 동안 안정적으로 식량 자원을 확보할 수 있게 되는 좋은 점이 있다. 이미 EBS TV의 하나뿐인 지구라는 교양 프로그램에도 이와 같은 배경을 소재로 둔 이야기를 토대로 보면, 기후 변화 대응 체계 해소 취지를 앞장서는 여건이 마련된다. 그렇게 되면 훌륭한 조건을 가질 수 있어, 물 부족 국가 또는 강수량이 낮은 지역에게는 새로운 희망이자 기회를 얻게 되는 셈이다.\n",
      "-----\n",
      "문서2: 지속가능성과 안보 관점에서 봤을 때, 여러 장비나 기기에 사용되는 에너지원은 일원화될수록 좋다. 따라서 난방, 운송 부문의 에너지원을 전기로 통합한다면 재생에너지로 생산된 전력을 여러 부문에 효율적으로 사용함으로써 전체 에너지 사용량을 줄일 수 있을 것이다. 이러한 ‘섹터간 융합’은 다양한 기기의 에너지원을 전력으로 바꾸는 ‘전기화’를 통해 달성할 수 있다. 이에 더해 전기를 사용하는 기기는 작동 과정에서 온실가스를 배출하지 않으므로 발전 섹터만 탈탄소화할 수 있다면 에너지 시스템 전체의 탄소배출량을 크게 줄일 수 있다.\n",
      "\n",
      "열펌프가 대표적인 사례다. 열펌프는 전기를 이용해 온도가 낮은 곳에서 높은 곳으로 열을 이동시키는 장치로, 현재 상용화된 기술 중 단일 기술로는 온실가스 배출에 가장 크게 기여할 것으로 기대된다. 냉온장고나 냉난방기를 건물 단위로 적용하는 개념의 기술이라고 볼 수 있다. 열펌프 기술이 적용된 국제에너지기구(IEA)는 화석연료를 사용하는 건물 난방시스템 중 30%만 열펌프로 교체해도 연간 8% 가량의 탄소배출량을 줄일 수 있을 것으로 내다봤다. 화석연료를 이용한 보일러처럼 배출가스가 생성되지도 않으므로 대기오염물질을 배출하지 않는다는 것도 장점이다. 열펌프와 마찬가지로 전기자동차 역시 에너지 이용 효율을 높이는 동시에 대기오염물질 배출을 줄이는 데 유용하다.\n",
      "\n",
      "한편, 에너지를 소모하는 과정에서는 필연적으로 열에너지 형태의 손실이 발생한다. 손실열은 별도의 일을 하지 않고 버려지는 에너지다. 따라서 이러한 에너지를 회수해 다시 사용할 수 있다면 에너지시스템 전반의 효율을 크게 높일 수 있다. 완전한 탈탄소 에너지체계로 이행하는 데 과도기적인 형태인 복합화력이 대표적인 사례다. 화력발전소는 최대 40% 남짓한 효율을 낸다. 투입한 연료의 에너지 중 40% 정도만 전력으로 변환할 수 있다는 뜻이다. 나머지 60%의 에너지는 열에너지 형태로 손실된다. 이렇게 손실되는 열에너지를 도시의 블록 단위로 온수를 공급하는 지역난방에 활용하면 효율을 70%대까지 끌어올릴 수 있다. 복합화력의 연료로는 폐기물을 소각하는 과정에서 발생하는 열을 주로 사용하고 있지만 태양열이나 지열을 이용한 방식을 지역난방에 적용하는 방안도 활발하게 연구되고 있다.\n",
      "\n",
      "운송 부문의 전기화 또한 중요한 요소다. 이미 지하철을 비롯한 철도망이 상당 부분 전기화됐으며 도로 교통에서도 전기차가 빠르게 보급되고 있다. 다만 전기차의 핵심 부품인 배터리를 생산하는 과정에서 온실가스가 적지 않게 배출된다는 점을 고려해야 한다. 전기차에 사용되는 전기를 화력발전으로 생산할 경우, 차량의 전체 생애주기에서 전기차가 내연기관차보다 더 많은 온실가스를 생성한다. 이는 차량의 전기화가 재생에너지로의 전환과 함께 추진돼야 함을 뜻한다. 2009년 기준 유럽의 전력 포트폴리오를 바탕으로 계산해보면 전기차의 탄소배출량이 내연기관에 비해 탄소 배출량이 최대 56% 적은 것으로 나타난다.\n",
      "\n",
      "다만 에너지전환의 다른 요소에 비해 운송 부문의 전기화에는 오랜 시간이 걸릴 것으로 보인다. 현재 시점에서 비행기와 대형 화물 트럭, 선박의 전기화는 어려운 편이다. 이들 운송수단은 대출력이 필요해서 장착해야 하는 배터리의 용량도 큰데, 배터리의 무게로 인해 기체 중량이 지나치게 증가하므로 효율성이 낮아지기 때문이다. 대출력이 필요한 운송수단에는 대안으로 전기로 생산한 바이오연료나 수소를 이용하는 방안이 연구 중이다.\n",
      "-----\n",
      "문서3: 미래 청정에너지 개발에 공동 협력하기 위한 ‘2014 월드그린에너지포럼’이 22~24일 경북 경주시 힐튼호텔에서 열린다.한국경제신문과 경상북도·경주시가 주최하는 이번 포럼에는 ‘그린에너지 패러다임의 대변환’이란 주제로 30여개국 정·관·학·산업계 관계자 3000여명이 참석한다. 첫째날과 둘째날에는 참석자들이 태양광 원자력 정보통신기술 에너지저장 연료전지 등 5개 분과로 나눠 세미나를 연다.2010년 노벨경제학상 수상자인 크리스토퍼 피사리데스 런던정치경제대 교수와 이스마일 엘지줄리 기후 변화에 관한 정부 간 협의체 부의장이 연사로 참석한다. 경상북도는 지방자치단체 가운데 처음으로 개발도상국 고위급 인사 및 유엔산업개발기구 등 국제기구와 함께 개도국의 에너지 협력을 증진하고 기후 변화 대응 방안을 논의한다. 마지막날에는 한국원자력환경공단 등 산업현장을 방문하고 야간에는 경주지역 관광지를 둘러보는 나이트 투어도 계획돼 있다.포럼은 폐회식에서 지구촌의 에너지 빈곤 퇴치와 미래 청정에너지 개발 노력, 세계의 공동 번영 및 지속 가능한 개발을 추구하자는 내용을 담은 경주선언문을 채택한다.\n",
      "-----\n",
      "문서4: 국립종자원은 이달 초 제주도에 지원을 개설했다. 아열대 작물을 개량해 특허권을 확보하는 게 설립 목적이다. 한반도 기후가 아열대성으로 변하면서 중남미에서 나는 국화과 식용열매 아티초크나 남아프리카의 채소 오크라 등의 산지가 한국으로 바뀌었다. 전남·경남 해안까지 아열대 작물의 노지재배가 가능해지면서 매년 100종이 넘는 아열대 작물 특허가 출원되고 있기도 하다. 한반도의 아열대화는 녹차밭을 전남 보성에서 강원 고성까지 북상시키는 등 농산물 지도를 바꿔 놓고 있다.○사철 수확하는 아열대 작물열대 작물인 망고, 용과 등은 제주도 비닐하우스에서 재배된다. 겨울철에만 난방을 하면 무리없이 키울 수 있다. 브로콜리같이 생긴 열매를 먹는 아티초크는 겨울철에도 수확이 가능하다.제주도뿐 아니라 전남 일대도 아열대 작물 재배 면적이 늘어나고 있다. 전남농업기술원에 따르면 2009년 38만㎡이던 전남의 블루베리 농장 규모는 지난해 131만㎡로 커졌다. 수입에만 의존하던 아열대 채소류 오크라, 인디언시금치 등은 해남·강진·장흥 일대에 작년부터 12만㎡ 규모로 재배되기 시작했다.이성주 국립종자원 제주지원장은 “2007년 이후 아열대 작물 신품종 특허출원 신청 건수가 꾸준히 늘어 최근에는 연간 30여개 작물 100여개 품종으로 증가했다”고 말했다. 김천환 농촌진흥청 온난화대응센터 연구원은 “제주 전남 경남 해안가는 이미 아열대 기후로 분류하고 있다”고 설명했다. 국내에서 재배되는 아열대 작물은 주로 고급 레스토랑에 납품되거나 인터넷몰 ‘아시아마트’ 등을 통해 외국인들에게 판매된다. 최근에는 국내 소비자들의 수요도 증가해 백화점에서도 국내산 아열대 작물을 팔기 시작했다. 신세계백화점은 올 들어 제주산 ‘패션프루트’ 판매를 시작했다. 애플망고와 용과 판매량은 전년 대비 20% 늘었다. 갤러리아백화점의 식품관 고메이494에서는 이달 들어 제주산 ‘아테모야(슈거애플)’를 선보였다. 한 박스(3㎏)에 20만원으로 가격은 좀 비싸지만 마니아층이 두터워지고 있다는 설명이다.○청주까지 올라온 한라봉사과 등 주요 작물의 산지는 북상 중이다. 제주 특산물로 유명한 한라봉과 감귤은 이미 충북 청주에서도 생산된다. 대구 등이 주 산지인 사과는 경기 포천, 강원 영월 등에서도 많이 재배되고 있다. ‘보성녹차’로 유명한 전남 보성은 녹차의 주 산지를 강원 고성에 내줄 판이다.수산물도 심상치 않다. 10월이 제철인 찬물에 사는 낙지는 바다수온이 오르면서 올 들어 수확량이 대폭 감소했다. 반대로 따뜻한 물을 좋아하는 꽃게는 올해 풍년이다. 연평도의 올해 꽃게 어획량(900t)은 작년보다 67% 늘었다. 해양수산부에 따르면 2000년대 남해안의 평균 수온은 19.2도로 1970년대에 비해 0.8도 올랐다. 제주도 일부 지역에서만 서식하던 갯가재, 홍다리얼룩새우 등 아열대 생물이 남해안 전역에서 발견되고 있다. 반면 수온이 낮아야 잘 자라는 김 미역 다시마 등은 남해안에서 수확하기가 어려워졌다는 분석이다.\n",
      "-----\n",
      "문서5: 서울 강서구 마곡지구에 여의도공원 2배 규모의 아시아 최대 생태공원(조감도)이 조성된다. 서울시는 마곡지구에 5000여종의 식물을 갖춘 도시형 식물원인 ‘서울 화목원(花木園)’(가칭)을 2016년 12월까지 준공할 예정이라고 21일 발표했다. 서울 서남권의 첫 대형 공원이자 서울의 ‘녹색 허브’ 역할을 할 것으로 기대된다. 산업단지(190만㎡)에 입주하는 기업 연구시설에 이어 주거단지(106만㎡) 내 아파트 일반분양이 본격화되는 마곡지구에 호재로 작용할 전망이다.서울시에 따르면 기존의 대규모 택지개발지구에 조성되는 공원이 거주자들의 여가·휴양 기능에 초점을 맞췄다면 서울화목원은 인근 마곡첨단산업지구(산업)와 경계를 허물고 옛 마곡의 풍광을 재현하는 데 치중한다. 겸재 정선의 ‘종해청조(宗海廳潮)’에 나타난 옛 양천 지역의 모습을 재현하기 위해 인공 시설물을 최소화하고 물재생센터의 재생수와 빗물을 공원 호수로 활용하는 것이 특징이다. 전체 면적은 50만3431㎡로 여의도 공원(23만㎡)의 2배가 넘고 어린이대공원(56만㎡)보다 약간 작다. 화목원에는 6만㎡ 규모 식물원과 식물문화센터가 들어선다. 식물문화센터는 3개 건물(연면적 1만㎡)에 전시온실, 식물도서관, 가드닝센터 등이 갖춰진다. 시는 다음달 중 관계기관 협의를 거쳐 기본계획안을 확정하고 설계에 들어갈 예정이다.<|im_end|>\n",
      "<|im_start|>user\n",
      "기후 변화가 농업에 미치는 영향은 무엇인가요?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "기후 변화는 농업에 다양한 영향을 미치고 있습니다. 기후 변화로 인해 농업 환경이 변화하면서 작물 재배 지역과 방식이 달라지고 있습니다.\n",
      "\n",
      "첫째, 기후 변화로 인해 한반도와 같은 지역에서는 아열대 작물의 재배가 가능해지고 있습니다. 예를 들어, 한반도의 기후가 아열대성으로 변하면서 중남미에서 나는 아티초크나 남아프리카의 오크라 등의 작물이 한국에서도 재배되고 있습니다. 이는 농산물 지도를 바꾸고 있으며, 제주도와 전남 일대에서 아열대 작물 재배 면적이 늘어나고 있습니다. 또한, 제주도에서 재배되던 한라봉과 감귤이 충북 청주에서도 생산되고 있으며, 사과의 주 산지도 북상하고 있습니다[[ref4]].\n",
      "\n",
      "둘째, 기후 변화는 수산물에도 영향을 미치고 있습니다. 바다 수온이 오르면서 찬물에 사는 낙지의 수확량은 감소하고, 따뜻한 물을 좋아하는 꽃게의 어획량은 증가하고 있습니다. 또한, 제주도 일부 지역에서만 서식하던 갯가재, 홍다리얼룩새우 등 아열대 생물이 남해안 전역에서 발견되고 있습니다. 반면, 수온이 낮아야 잘 자라는 김, 미역, 다시마 등의 수확은 어려워지고 있습니다[[ref4]].\n",
      "\n",
      "이와 같이 기후 변화는 농업에 큰 영향을 미치며, 작물 재배 지역의 변화와 새로운 작물의 도입, 수산물의 어획량 변화 등을 초래하고 있습니다. 이는 농업 생산성과 식량 안보에 중요한 영향을 미칠 수 있습니다.<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = tokenizer.apply_chat_template(\n",
    "    train_dataset[0][\"messages\"], tokenize=False, add_generation_prompt=False\n",
    ")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "fb39bf90-042a-470f-8ded-63dba005466a"
   },
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.1,\n",
    "        r=8,\n",
    "        bias=\"none\",\n",
    "        target_modules=[\"q_proj\", \"v_proj\"],\n",
    "        task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "d57675fd-4374-4cd6-b8bb-950bf216cad3"
   },
   "outputs": [],
   "source": [
    "args = SFTConfig(\n",
    "    output_dir=\"qwen2-7b-rag-ko\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=2,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=50,\n",
    "    bf16=True,\n",
    "    learning_rate=1e-4,\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    push_to_hub=False,\n",
    "    remove_unused_columns=False,\n",
    "    dataset_kwargs={\"skip_prepare_dataset\": True},\n",
    "    report_to=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "adbcf4b1-ab29-451d-b628-21466d16519e"
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    new_batch = {\n",
    "        \"input_ids\": [],\n",
    "        \"attention_mask\": [],\n",
    "        \"labels\": []\n",
    "    }\n",
    "\n",
    "    for example in batch:\n",
    "        clean_messages = []\n",
    "        for message in example[\"messages\"]:\n",
    "            clean_message = {\n",
    "                \"role\": message[\"role\"],\n",
    "                \"content\": message[\"content\"]\n",
    "            }\n",
    "            clean_messages.append(clean_message)\n",
    "\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            clean_messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False\n",
    "        ).strip()\n",
    "\n",
    "        tokenized = tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            max_length=max_seq_length,\n",
    "            padding=False,\n",
    "            return_tensors=None,\n",
    "        )\n",
    "\n",
    "        input_ids = tokenized[\"input_ids\"]\n",
    "        attention_mask = tokenized[\"attention_mask\"]\n",
    "\n",
    "        labels = [-100] * len(input_ids)\n",
    "\n",
    "        im_start = \"<|im_start|>\"\n",
    "        im_end = \"<|im_end|>\"\n",
    "        assistant = \"assistant\"\n",
    "\n",
    "        im_start_tokens = tokenizer.encode(im_start, add_special_tokens=False)\n",
    "        im_end_tokens = tokenizer.encode(im_end, add_special_tokens=False)\n",
    "        assistant_tokens = tokenizer.encode(assistant, add_special_tokens=False)\n",
    "\n",
    "        i = 0\n",
    "        while i < len(input_ids):\n",
    "            if (i + len(im_start_tokens) <= len(input_ids) and\n",
    "                input_ids[i:i+len(im_start_tokens)] == im_start_tokens):\n",
    "\n",
    "                assistant_pos = i + len(im_start_tokens)\n",
    "                if (assistant_pos + len(assistant_tokens) <= len(input_ids) and\n",
    "                    input_ids[assistant_pos:assistant_pos+len(assistant_tokens)] == assistant_tokens):\n",
    "\n",
    "                    current_pos = assistant_pos + len(assistant_tokens)\n",
    "\n",
    "                    while current_pos < len(input_ids):\n",
    "                        if (current_pos + len(im_end_tokens) <= len(input_ids) and\n",
    "                            input_ids[current_pos:current_pos+len(im_end_tokens)] == im_end_tokens):\n",
    "\n",
    "                            for j in range(len(im_end_tokens)):\n",
    "                                labels[current_pos + j] = input_ids[current_pos + j]\n",
    "                            break\n",
    "                        labels[current_pos] = input_ids[current_pos]\n",
    "                        current_pos += 1\n",
    "\n",
    "                    i = current_pos\n",
    "\n",
    "            i += 1\n",
    "\n",
    "        new_batch[\"input_ids\"].append(input_ids)\n",
    "        new_batch[\"attention_mask\"].append(attention_mask)\n",
    "        new_batch[\"labels\"].append(labels)\n",
    "\n",
    "    max_length = max(len(ids) for ids in new_batch[\"input_ids\"])\n",
    "\n",
    "    for i in range(len(new_batch[\"input_ids\"])):\n",
    "        padding_length = max_length - len(new_batch[\"input_ids\"][i])\n",
    "\n",
    "        new_batch[\"input_ids\"][i].extend([tokenizer.pad_token_id] * padding_length)\n",
    "        new_batch[\"attention_mask\"][i].extend([0] * padding_length)\n",
    "        new_batch[\"labels\"][i].extend([-100] * padding_length)\n",
    "\n",
    "    for k, v in new_batch.items():\n",
    "        new_batch[k] = torch.tensor(v)\n",
    "\n",
    "    return new_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "82c8810f-2613-4d6b-8dcb-985ba50991f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "처리된 배치 데이터:\n",
      "입력 ID 형태: torch.Size([1, 4328])\n",
      "어텐션 마스크 형태: torch.Size([1, 4328])\n",
      "레이블 형태: torch.Size([1, 4328])\n"
     ]
    }
   ],
   "source": [
    "max_seq_length=8192\n",
    "\n",
    "example = train_dataset[0]\n",
    "batch = collate_fn([example])\n",
    "\n",
    "print(\"\\n처리된 배치 데이터:\")\n",
    "print(\"입력 ID 형태:\", batch[\"input_ids\"].shape)\n",
    "print(\"어텐션 마스크 형태:\", batch[\"attention_mask\"].shape)\n",
    "print(\"레이블 형태:\", batch[\"labels\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "d50fbb8d-1265-4c9a-b56e-2803f09bd810"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력에 대한 정수 인코딩 결과:\n",
      "[151644, 8948, 198, 64795, 82528, 33704, 85322, 77226, 98801, 18411, 81718, 144059, 42039, 138520, 19391, 143604, 129264, 130650, 382, 13146, 48431, 20401, 66790, 29326, 131193, 17877, 125686, 125548, 139713, 624, 16, 13, 138520, 53680, 85322, 77226, 98801, 18411, 81718, 144059, 42039, 143604, 16186, 139713, 624, 17, 13, 85322, 77226, 98801, 19391, 130768, 130213, 17877, 143604, 16186, 125476, 34395, 53900, 21329, 95577, 139713, 624, 18, 13, 138520, 19391, 128605, 143603, 12802, 85322, 77226, 98801, 19391, 130671, 32290, 85322, 77226, 98801, 126377, 330, 33883, 64795, 138520, 93, 19391, 128605, 130213, 12802, 136673, 1189, 5140, 45881, 34395, 143604, 16186, 139713, 624, 19, 13, 143604, 47836, 53618, 142976, 139236, 18411, 142616, 82190, 53435, 40853, 129549, 53435, 125068, 17877, 140174, 128836, 32290, 5140, 240, 97, 19391, 36330, 250, 125746, 16560, 23084, 126402, 83634, 17380, 94613, 139236, 84621, 47324, 18411, 129624, 20487, 139713, 13, 95617, 18411, 129901, 26698, 142976, 53435, 40853, 129835, 53435, 125068, 17877, 220, 16, 42044, 139236, 56475, 58677, 26699, 128836, 32290, 5140, 240, 97, 19391, 4318, 1097, 16, 5053, 130939, 54116, 57132, 16186, 139713, 624, 20, 13, 95617, 18411, 129901, 26698, 142976, 53435, 40853, 129835, 53435, 125068, 17877, 220, 16, 42044, 139236, 80573, 220, 20, 42044, 139236, 56475, 143409, 58677, 26699, 128836, 32290, 5140, 240, 97, 19391, 4318, 1097, 16, 20492, 4318, 1097, 20, 5053, 130939, 54116, 57132, 16186, 139713, 624, 21, 13, 81173, 66845, 23573, 49367, 23259, 20401, 139236, 18411, 58677, 26699, 82190, 143604, 16186, 139713, 382, 129845, 77226, 98801, 510, 34764, 51588, 26698, 16, 25, 129392, 29326, 5140, 228, 235, 124517, 20401, 49052, 11, 133196, 126804, 21329, 56475, 66790, 88259, 38523, 101, 126588, 56290, 126674, 11, 60960, 20487, 20401, 130270, 92817, 128355, 125714, 126402, 125149, 126346, 128618, 131833, 11, 61298, 32077, 43115, 37087, 56419, 131833, 136357, 83518, 126793, 19391, 126629, 133788, 133396, 128841, 140749, 130999, 124657, 131293, 127165, 108, 134836, 77002, 53680, 129381, 142684, 140577, 57132, 33883, 77002, 12802, 66790, 126299, 128552, 83556, 33509, 126896, 57801, 97143, 131611, 11, 54116, 127033, 137525, 17380, 58677, 23573, 47818, 86372, 124781, 54969, 142194, 134454, 32985, 231, 17877, 28733, 64521, 65510, 124873, 47985, 131891, 64577, 126835, 128533, 47818, 125991, 45104, 101, 94152, 12802, 47818, 20487, 57801, 97143, 16560, 60960, 126246, 17877, 62071, 29326, 47836, 28733, 142161, 97143, 16560, 23084, 126333, 12802, 90686, 13, 130549, 129392, 29326, 5140, 228, 235, 124517, 17877, 140968, 32831, 56290, 128555, 97143, 32290, 69192, 110, 53680, 129381, 127165, 120, 131293, 63332, 47324, 36055, 126712, 33704, 133554, 81718, 13146, 60315, 73523, 144950, 77002, 60716, 126345, 47818, 86372, 124781, 63332, 47324, 36055, 126712, 128878, 60960, 135257, 128552, 129413, 56290, 47836, 28733, 64521, 77002, 135392, 34143, 230, 134133, 18411, 36330, 102, 129704, 29326, 143651, 28733, 142161, 130723, 13, 54116, 130999, 126377, 127165, 120, 131293, 47665, 234, 130752, 94315, 5140, 228, 235, 65306, 21329, 65510, 132818, 53900, 124982, 125590, 11, 130270, 126251, 81173, 55902, 137812, 11, 70943, 62275, 142510, 126588, 137812, 11, 55673, 55902, 97834, 128747, 52959, 143083, 129381, 55673, 92192, 128355, 58034, 124517, 132555, 30520, 113, 128747, 130270, 126970, 126251, 20401, 58034, 124517, 131937, 129881, 38523, 98, 55902, 77002, 21329, 19391, 5140, 228, 235, 133120, 35509, 136892, 31079, 83277, 26698, 130270, 126251, 19391, 58034, 54330, 126204, 64521, 128618, 134771, 55673, 92192, 44518, 125624, 126352, 138533, 126253, 220, 16, 126216, 126322, 126246, 95170, 29281, 128552, 28927, 251, 131837, 64577, 54321, 17877, 130729, 41671, 47836, 28733, 142161, 97143, 16560, 132968, 18585, 238, 12802, 90686, 13, 90667, 468, 7347, 5883, 20401, 125703, 127871, 31328, 66790, 88259, 129615, 127048, 126345, 84255, 81650, 127483, 130612, 23084, 80573, 129381, 73669, 65306, 17877, 126291, 57132, 17380, 5140, 239, 242, 130861, 18411, 10764, 228, 254, 129923, 132513, 11, 54116, 127033, 137525, 60960, 131518, 48364, 112, 124781, 60716, 43590, 131565, 133120, 139274, 40853, 26698, 16560, 83518, 124873, 12802, 140175, 132306, 13, 132330, 97143, 32290, 10764, 249, 234, 144295, 23573, 65510, 124873, 17877, 35509, 128732, 28733, 127353, 11, 133553, 85403, 129704, 133152, 129549, 129413, 23259, 131837, 12802, 37195, 106, 33704, 131937, 126327, 16560, 134585, 10764, 251, 105, 130472, 12802, 25715, 54116, 61741, 18411, 79302, 119, 57801, 97143, 16560, 3315, 227, 230, 125489, 624, 34764, 51588, 26698, 17, 25, 66790, 126299, 19969, 66019, 32831, 53680, 95170, 41671, 92751, 126333, 56475, 5140, 112, 97, 17877, 53618, 11, 127296, 129359, 70582, 60315, 54116, 132264, 40720, 128841, 90486, 127085, 137709, 33704, 83556, 54321, 56290, 131396, 23259, 49664, 125953, 13146, 13, 134084, 37195, 250, 126321, 11, 132028, 127105, 85403, 51588, 20401, 90486, 127085, 137709, 17877, 56419, 20487, 17380, 125206, 128747, 140295, 129242, 76435, 142653, 17380, 141091, 52300, 56419, 135818, 127296, 85403, 51588, 19391, 141540, 132841, 128552, 40720, 77953, 42039, 135660, 137138, 90486, 127085, 21329, 40720, 131837, 17877, 53989, 226, 32077, 28733, 130689, 128900, 13, 131367, 3369, 144272, 33861, 62275, 37064, 113, 128747, 527, 33704, 135392, 54116, 20487, 20401, 90486, 127085, 137709, 17877, 56419, 28754, 42039, 81718, 136892, 16560, 3369, 65865, 20487, 56290, 527, 18411, 131582, 34143, 105, 32831, 47836, 28733, 90686, 13, 23084, 19391, 126366, 33883, 56419, 131777, 40720, 42905, 54116, 135854, 68232, 57089, 45130, 120, 29281, 56475, 38523, 101, 125086, 19969, 24897, 18411, 73669, 69923, 87425, 50696, 33509, 137621, 95996, 65865, 27767, 117, 33861, 72553, 74361, 230, 135356, 43590, 56290, 47836, 28733, 90686, 32290, 90486, 127085, 21329, 44518, 24897, 127649, 137138, 20401, 74361, 226, 43590, 130609, 69923, 131837, 17877, 143231, 53989, 226, 32077, 28733, 90686, 382, 53955, 144732, 126445, 19969, 60960, 126414, 128533, 32129, 131000, 13146, 13, 130847, 144732, 126445, 16560, 56419, 131777, 126563, 33883, 38523, 101, 47985, 19969, 37195, 106, 33704, 45130, 111, 56475, 5140, 228, 240, 33704, 45130, 111, 42039, 130847, 17877, 23084, 57089, 135298, 16560, 129359, 59698, 17380, 11, 132270, 58034, 26699, 56290, 52300, 54116, 125880, 70943, 129400, 32077, 54116, 125880, 17380, 16560, 38523, 101, 125086, 19969, 24897, 73669, 69923, 19391, 130887, 143231, 54116, 57026, 47836, 132091, 54116, 66845, 132306, 13, 5140, 225, 231, 130000, 40853, 34395, 60315, 5140, 225, 231, 126588, 126321, 131777, 130270, 126251, 129400, 80901, 17380, 135969, 42905, 142497, 20401, 54116, 125880, 130939, 30520, 120, 28733, 90686, 13, 130847, 144732, 126445, 54116, 125880, 12802, 135969, 52300, 138249, 142653, 20487, 88259, 7, 5371, 32, 8, 16560, 46832, 242, 129150, 125568, 63256, 18411, 40720, 42905, 130270, 126251, 37195, 250, 126321, 134620, 70943, 220, 18, 15, 4, 72553, 130847, 144732, 126445, 17380, 127048, 49543, 33883, 47985, 77353, 62275, 220, 23, 4, 35509, 131837, 20401, 74361, 226, 43590, 130609, 69923, 131837, 17877, 53989, 226, 32077, 28733, 130689, 132091, 66136, 13146, 141551, 13146, 13, 46832, 242, 129150, 125568, 63256, 18411, 126563, 23573, 63332, 32077, 60294, 131137, 73669, 69923, 19969, 24897, 19969, 51917, 132553, 47985, 50696, 33509, 137621, 60960, 20487, 57268, 137284, 126251, 128732, 17877, 73669, 69923, 87425, 127728, 130822, 137638, 129359, 126333, 125489, 13, 130847, 144732, 126445, 80573, 95577, 138143, 19969, 21329, 17380, 56419, 20487, 143995, 139512, 90486, 127085, 21329, 126563, 141540, 132841, 17877, 5140, 228, 240, 132044, 143409, 60960, 20487, 57268, 137284, 126251, 128732, 73669, 69923, 17877, 53989, 226, 132044, 5140, 41902, 126310, 26699, 129330, 382, 23573, 129027, 11, 90486, 127085, 133120, 126291, 129439, 42905, 45130, 120, 29281, 129889, 75528, 125568, 128552, 130847, 142653, 141966, 20401, 77596, 238, 125086, 12802, 133396, 51876, 13, 77596, 238, 125086, 53955, 33704, 30520, 226, 47985, 20401, 138507, 53900, 21329, 135201, 86831, 125476, 130974, 90486, 127085, 21329, 13146, 13, 134084, 131367, 90486, 127085, 133120, 98005, 23259, 33883, 131170, 40720, 47836, 28733, 90686, 32290, 90486, 127085, 21329, 134620, 56419, 126641, 20401, 141540, 132841, 17877, 143231, 5140, 228, 240, 32077, 28733, 90686, 13, 74884, 226, 65865, 23573, 74361, 230, 135356, 43590, 90486, 127085, 21329, 49543, 124781, 17380, 23084, 124528, 42905, 5140, 41902, 45130, 120, 47985, 20487, 128533, 141966, 31328, 30520, 113, 128747, 56290, 28754, 12802, 60960, 126414, 128533, 32129, 131000, 13146, 13, 46832, 242, 28754, 126835, 65865, 43590, 16560, 81173, 66845, 220, 19, 15, 4, 129624, 140847, 23573, 141540, 132841, 17877, 37195, 116, 13146, 13, 10764, 230, 105, 43866, 23573, 77353, 63256, 20401, 90486, 127085, 21329, 70943, 220, 19, 15, 4, 131219, 72553, 56419, 28754, 42039, 46319, 65238, 47836, 28733, 135511, 5140, 250, 119, 125489, 13, 73518, 132125, 21329, 220, 21, 15, 4, 20401, 90486, 127085, 130974, 130847, 142653, 141966, 17380, 77596, 238, 125086, 132306, 13, 130653, 77596, 238, 125086, 128841, 130847, 142652, 133120, 129392, 29326, 20401, 5140, 116, 242, 49664, 129400, 80901, 17380, 38523, 101, 23259, 18411, 125466, 128911, 42905, 131937, 126588, 126321, 19391, 140969, 126559, 141540, 132841, 17877, 220, 22, 15, 4, 66845, 128878, 5140, 223, 234, 31079, 130137, 135379, 28733, 90686, 13, 30520, 113, 128747, 56290, 28754, 20401, 77353, 63256, 17380, 16560, 69441, 238, 20487, 137075, 126291, 126317, 42905, 45130, 120, 29281, 56475, 133396, 42905, 130847, 17877, 55673, 17380, 40720, 126204, 141258, 74361, 250, 126345, 53955, 129835, 66790, 53955, 17877, 126563, 23573, 74808, 76337, 17877, 131937, 126588, 126321, 19391, 135969, 42905, 74808, 126246, 47985, 140968, 126835, 128555, 131698, 130357, 90686, 382, 93672, 127105, 85403, 51588, 20401, 56419, 20487, 56290, 130005, 136361, 85997, 43590, 13146, 13, 90667, 66790, 16186, 131573, 17877, 73986, 143719, 23573, 48364, 254, 47985, 130472, 12802, 58034, 64795, 129881, 56419, 20487, 56290, 134521, 127378, 129392, 17380, 127048, 125160, 136448, 56419, 20487, 125625, 19969, 5140, 117, 254, 125548, 57801, 63332, 128911, 130357, 90686, 13, 139293, 56419, 20487, 125625, 20401, 20136, 113, 125512, 85403, 125678, 31328, 73669, 33861, 133886, 141091, 42905, 45130, 120, 29281, 56475, 38523, 101, 125086, 19969, 24897, 19969, 135968, 21329, 50696, 57801, 73669, 69923, 52300, 130822, 18585, 238, 17877, 126429, 125476, 129264, 129112, 13, 56419, 20487, 125625, 19391, 40720, 128841, 56419, 131777, 46832, 242, 28754, 126835, 65865, 42039, 141091, 47836, 49052, 11, 129882, 131837, 20401, 137138, 47818, 126898, 54330, 20487, 56475, 56419, 20487, 125625, 19969, 66136, 125568, 131854, 125625, 129885, 126366, 129875, 38523, 101, 125086, 19969, 24897, 18411, 51917, 51876, 13, 23084, 16560, 129882, 131837, 20401, 56419, 20487, 56290, 19969, 129242, 76435, 142653, 17380, 20401, 56419, 65238, 53680, 129676, 141210, 136398, 89659, 50972, 17877, 5140, 250, 119, 51876, 13, 220, 17, 15, 15, 24, 126216, 54116, 129044, 126310, 134798, 20401, 56419, 28754, 98869, 28626, 144087, 28002, 57268, 18411, 81718, 144059, 42039, 94203, 85057, 33883, 41671, 32290, 56419, 20487, 125625, 20401, 74361, 226, 43590, 130609, 69923, 131837, 12802, 66136, 125568, 131854, 19391, 73986, 33883, 74361, 226, 43590, 73669, 69923, 131837, 12802, 81173, 66845, 220, 20, 21, 4, 135968, 33704, 132091, 135513, 126588, 13146, 382, 13146, 72553, 90486, 127085, 21329, 65865, 65238, 20401, 128772, 85997, 43590, 19391, 73986, 33883, 132028, 127105, 85403, 51588, 20401, 56419, 20487, 56290, 126377, 73077, 136499, 130217, 12802, 131961, 135379, 132091, 63332, 31328, 13146, 13, 132270, 44518, 126333, 56475, 73986, 124528, 20487, 80573, 60960, 128909, 46832, 242, 126251, 127819, 116, 144329, 11, 129296, 129471, 20401, 56419, 20487, 56290, 16560, 124685, 125476, 93672, 10764, 236, 116, 125489, 13, 23084, 64850, 132028, 127105, 23259, 125068, 33704, 60960, 53496, 57133, 12802, 126871, 96137, 129359, 135375, 129264, 128956, 73669, 33861, 28002, 20401, 65722, 102, 131837, 47985, 132182, 124419, 11, 73669, 33861, 28002, 20401, 125149, 57801, 17380, 58677, 33883, 54116, 49543, 70943, 131837, 12802, 66790, 60315, 59698, 57801, 132376, 251, 19969, 16186, 137621, 141540, 132841, 137032, 37195, 106, 52959, 21329, 20487, 137141, 13, 60960, 53496, 57133, 12802, 134028, 132028, 127105, 23259, 125068, 126377, 60960, 126246, 42039, 56419, 20487, 17380, 141091, 23573, 81718, 12802, 57268, 125568, 63256, 60315, 28733, 43590, 18411, 126563, 42905, 74808, 126246, 12802, 131698, 70943, 125489, 624, 34764, 51588, 26698, 18, 25, 143005, 48364, 255, 29281, 142653, 73523, 126835, 19391, 140429, 47455, 239, 28754, 66425, 130679, 3369, 17, 15, 16, 19, 85413, 242, 29346, 48606, 129807, 142653, 128808, 125894, 527, 12802, 220, 17, 17, 93, 17, 19, 32077, 43115, 131226, 43115, 136559, 10764, 252, 238, 98933, 47324, 128100, 56475, 130847, 129807, 13146, 13, 23573, 124785, 132249, 82528, 51588, 53680, 43115, 55902, 131226, 47985, 13935, 65306, 136559, 19969, 55673, 128215, 42905, 132183, 98869, 125894, 126377, 3369, 48606, 129807, 142653, 45104, 101, 60294, 13146, 93701, 20401, 60960, 126667, 65238, 527, 12802, 129804, 55673, 37087, 17380, 220, 18, 15, 57026, 59761, 124785, 36055, 13935, 124780, 13935, 124632, 13935, 134066, 124781, 92751, 124781, 25715, 220, 18, 15, 15, 15, 57026, 79632, 12802, 127969, 129150, 51876, 13, 48364, 104, 83666, 129378, 53680, 5140, 239, 246, 83666, 129378, 126377, 127969, 129150, 25715, 126253, 74361, 250, 126345, 126861, 129093, 25715, 28754, 60039, 139269, 131040, 90486, 127085, 21329, 14467, 54470, 77353, 63256, 65865, 21329, 77002, 220, 20, 59761, 128618, 53680, 17380, 73518, 144478, 125674, 56039, 60315, 18411, 77353, 13146, 13, 17, 15, 16, 15, 126216, 127042, 144094, 132249, 124632, 55902, 28733, 55902, 25715, 31328, 143230, 28002, 24897, 124154, 57160, 235, 120, 142510, 55054, 28002, 124419, 24897, 5140, 253, 108, 125615, 29281, 59698, 132249, 66845, 142220, 80573, 23084, 24897, 125544, 32077, 24485, 246, 21329, 131303, 28002, 54116, 127033, 137525, 19391, 130207, 36055, 63089, 16778, 226, 47455, 239, 20401, 49543, 85403, 20401, 40853, 12802, 77353, 55054, 17380, 127969, 129150, 51876, 13, 43115, 55902, 131226, 47985, 16560, 66790, 126321, 25715, 59698, 137438, 142180, 136065, 42039, 73523, 126835, 47985, 55902, 124785, 126429, 80901, 128911, 58677, 55054, 128355, 126310, 136233, 134066, 131570, 20487, 88259, 77002, 138249, 20487, 88259, 80573, 129676, 73523, 47985, 124785, 20401, 90486, 127085, 21329, 47455, 239, 135818, 132376, 251, 85251, 126204, 54116, 127033, 137525, 60960, 131518, 74808, 126246, 17877, 127041, 120, 20401, 51876, 13, 140887, 129378, 126377, 130092, 54321, 25715, 28754, 132892, 78125, 125068, 77002, 127165, 108, 124517, 126407, 137471, 141875, 126204, 23872, 120, 62275, 126377, 43115, 54330, 21329, 126346, 92751, 126861, 133120, 5140, 239, 246, 60294, 41671, 16560, 37195, 62618, 28626, 10764, 230, 105, 31079, 47985, 94203, 127324, 136398, 90686, 13, 128808, 125894, 33704, 69441, 238, 61741, 76337, 56475, 66790, 88259, 144089, 20401, 90486, 127085, 21329, 5140, 117, 230, 142561, 10764, 229, 112, 59698, 80573, 143005, 48364, 255, 29281, 142653, 73523, 126835, 136111, 11, 133196, 20401, 140429, 84621, 125144, 128355, 66790, 126299, 143964, 73523, 126835, 17877, 57835, 88259, 16186, 132343, 130213, 17877, 34143, 112, 33704, 43115, 54330, 125519, 129709, 51588, 17877, 3315, 109, 226, 127919, 51876, 624, 34764, 51588, 26698, 19, 25, 124973, 126702, 126337, 25715, 54321, 33704, 23084, 129062, 83315, 62071, 54330, 47985, 19391, 132185, 17877, 73523, 14559, 44680, 244, 19946, 13, 48408, 53955, 66845, 68232, 137075, 73523, 131837, 33883, 127820, 131976, 128739, 17877, 130729, 41671, 42905, 98927, 57852, 126702, 134953, 125489, 13, 61298, 126641, 47985, 54116, 127033, 19969, 48408, 53955, 66845, 32831, 42039, 46319, 132537, 70943, 131793, 56039, 56475, 132311, 124973, 56290, 53680, 28927, 251, 26699, 53955, 129865, 48408, 131131, 132618, 81133, 60315, 129624, 52959, 126445, 28002, 129616, 20401, 3315, 109, 226, 43590, 142209, 223, 105, 50340, 136357, 127165, 108, 21329, 19969, 130092, 42039, 81718, 144194, 125761, 13, 56419, 131793, 13935, 65306, 131793, 60716, 126246, 128878, 48408, 53955, 66845, 68232, 126251, 20401, 127042, 21329, 57132, 130609, 19969, 95351, 33883, 21329, 131611, 126932, 126216, 220, 16, 15, 15, 126337, 12802, 143835, 16560, 48408, 53955, 66845, 68232, 126251, 127820, 131976, 19969, 36330, 250, 54321, 130357, 35339, 130898, 53900, 13146, 13, 61298, 126641, 47985, 20401, 48408, 53955, 66845, 56290, 16560, 127041, 117, 125625, 144533, 17877, 56419, 131793, 63332, 32831, 56475, 129413, 54321, 126429, 32831, 128878, 139963, 55902, 135298, 16560, 77002, 5140, 228, 235, 85057, 126251, 66790, 47985, 18411, 81718, 144379, 5140, 228, 241, 34395, 90686, 13, 136277, 55054, 131573, 28733, 133085, 42905, 48408, 53955, 66845, 68232, 126251, 53955, 66845, 68232, 126251, 31328, 32985, 251, 34395, 11, 65722, 102, 53680, 77002, 33704, 62071, 54330, 47985, 73986, 144039, 16186, 40281, 24897, 56475, 129242, 130609, 132306, 13, 23894, 101, 126893, 131573, 19391, 72553, 37195, 250, 126321, 17877, 53900, 32290, 125149, 28002, 131702, 10764, 92120, 126893, 28733, 90686, 13, 5140, 116, 234, 17380, 144063, 28002, 131380, 12802, 47818, 133507, 130847, 129865, 18411, 137767, 117, 16560, 48408, 131131, 132618, 81133, 16560, 23894, 101, 126893, 131573, 130612, 28733, 133085, 12802, 95351, 129330, 13, 37087, 54330, 47985, 127871, 130651, 56419, 131793, 83556, 66845, 47985, 48408, 53955, 66845, 68232, 126251, 129242, 130609, 48108, 112, 133864, 143861, 246, 31079, 60315, 34395, 90686, 13, 56419, 131793, 133084, 124517, 131040, 54321, 19391, 125686, 125548, 32290, 220, 17, 15, 15, 24, 126216, 220, 18, 23, 72553, 144562, 12802, 125615, 56419, 131793, 20401, 5140, 116, 242, 126746, 132947, 28002, 5140, 228, 235, 40853, 134313, 129439, 16560, 133146, 33883, 220, 16, 18, 16, 72553, 144562, 17380, 89095, 97, 141798, 13, 28733, 43866, 19391, 72553, 124970, 130999, 16186, 125615, 48408, 53955, 66845, 3315, 109, 226, 43590, 97929, 142209, 223, 105, 50340, 11, 58677, 89235, 129709, 29326, 125052, 59698, 77002, 33704, 60716, 131793, 13935, 130262, 85251, 13935, 40853, 138609, 83556, 66845, 19391, 68232, 126216, 126558, 220, 16, 17, 72553, 144562, 134313, 129439, 17380, 129242, 130609, 141874, 93721, 128836, 13, 12802, 32831, 54330, 124973, 126702, 126337, 25715, 54321, 62071, 54330, 137709, 40853, 33704, 1036, 17, 15, 15, 22, 126216, 136331, 48408, 53955, 66845, 68232, 126251, 28927, 57160, 240, 230, 126337, 127820, 131976, 69923, 54321, 128753, 125118, 130270, 135444, 8620, 122, 116, 129044, 125511, 143861, 246, 31079, 139465, 126377, 77353, 62275, 220, 18, 15, 57026, 59761, 68232, 126251, 220, 16, 15, 15, 57026, 59761, 10764, 240, 230, 126337, 42039, 132376, 251, 19969, 128836, 854, 34395, 126254, 128836, 13, 130508, 129034, 65238, 5140, 228, 235, 144089, 85251, 138609, 125118, 38523, 101, 126588, 56290, 66845, 131518, 134310, 131698, 54321, 33704, 1036, 37087, 54330, 56419, 131793, 43115, 131793, 60716, 126246, 132184, 90667, 48408, 53955, 66845, 54116, 127033, 17380, 128618, 97929, 126204, 90686, 854, 34395, 133828, 128836, 13, 141185, 56475, 129242, 130609, 128841, 48408, 53955, 66845, 68232, 126251, 33704, 55673, 17380, 126429, 128911, 5140, 254, 230, 24897, 129283, 133738, 19391, 37195, 102, 125678, 64119, 127451, 58677, 33861, 128013, 136671, 3369, 52959, 29326, 52959, 125544, 28626, 527, 134454, 131582, 74884, 116, 124785, 31328, 132812, 140568, 129865, 132306, 13, 139465, 126377, 141185, 126291, 70582, 25715, 129360, 28733, 35711, 47985, 132376, 251, 19969, 33883, 22042, 109, 56290, 126333, 136448, 141185, 85057, 48408, 53955, 66845, 68232, 137075, 45104, 242, 20487, 93721, 128836, 13, 128753, 41429, 124781, 130728, 56290, 126333, 33704, 38523, 105, 129901, 62071, 54330, 85057, 3369, 133087, 92031, 126445, 126746, 28626, 527, 140568, 129865, 18411, 93721, 128836, 13, 23872, 57160, 242, 234, 130472, 34395, 80573, 65722, 102, 53680, 140568, 129865, 131837, 33704, 56419, 126216, 60960, 70582, 220, 17, 15, 4, 143861, 246, 125761, 13, 16778, 97, 60294, 28002, 52959, 130728, 56290, 126333, 20401, 28927, 251, 125678, 124780, 126429, 84667, 12802, 19, 24, 19, 129889, 23084, 129062, 129901, 62071, 54330, 85057, 3369, 52959, 130229, 129439, 89659, 7, 144018, 92192, 30395, 57160, 242, 234, 8, 527, 18411, 129296, 41671, 139836, 13, 61298, 22042, 243, 24897, 7, 18, 145189, 8, 19391, 220, 17, 15, 72553, 54321, 42039, 35509, 126614, 33704, 130572, 73986, 140611, 125590, 95577, 83036, 52959, 137812, 12802, 129419, 33861, 130109, 134497, 135511, 133828, 125489, 13, 136277, 125118, 54330, 128878, 38523, 105, 50340, 130000, 61298, 50340, 135402, 55054, 53680, 77002, 55673, 35711, 68232, 126251, 20401, 127165, 108, 130974, 139963, 55902, 70943, 125489, 13, 62071, 54330, 127820, 85057, 126251, 17380, 126310, 79632, 23573, 61298, 50340, 135402, 53680, 129423, 145237, 33704, 90667, 36330, 102, 131226, 48364, 255, 54330, 136448, 141091, 132306, 13, 60960, 88259, 77002, 12802, 55673, 127165, 108, 21329, 31328, 32129, 53680, 16560, 43115, 20487, 98869, 129034, 11, 129413, 54321, 126440, 128514, 77002, 136448, 130966, 129242, 130609, 130357, 90686, 13, 3369, 41671, 32831, 144067, 125625, 527, 17380, 126310, 79632, 23573, 56419, 131793, 63332, 32831, 33704, 127041, 117, 125625, 20401, 55673, 127165, 108, 133120, 129413, 54321, 126429, 32831, 19391, 66136, 131303, 140568, 125489, 13, 23259, 85057, 126251, 47985, 28927, 105, 55902, 59698, 50696, 13146, 13, 220, 16, 15, 128514, 12802, 62071, 131573, 31328, 62099, 105, 126251, 19391, 32129, 16560, 37195, 247, 130974, 81718, 13146, 23259, 130000, 12802, 73077, 125548, 131611, 38523, 105, 129901, 28733, 133085, 131837, 12802, 60960, 135257, 129423, 43590, 128836, 13, 63757, 129923, 125686, 136196, 23573, 133553, 17877, 138779, 42905, 8620, 121, 225, 57801, 16560, 38523, 105, 33883, 10764, 240, 235, 126216, 125489, 13, 77353, 126742, 47985, 20401, 38523, 105, 33883, 8620, 121, 225, 57801, 124685, 127324, 131837, 7, 24, 15, 15, 83, 8, 33704, 68232, 126216, 129885, 220, 21, 22, 4, 143861, 246, 125761, 13, 60716, 126345, 23259, 85057, 63089, 19391, 125686, 125548, 32290, 220, 17, 15, 15, 15, 126216, 66845, 129624, 33883, 126246, 20401, 69441, 231, 139746, 28733, 130000, 33704, 220, 16, 24, 13, 17, 47985, 17380, 220, 16, 24, 22, 15, 126216, 66845, 19391, 73986, 33883, 220, 15, 13, 23, 47985, 38523, 105, 144110, 13146, 13, 62071, 54330, 47985, 136605, 131937, 56475, 72553, 89860, 76337, 16186, 125615, 16778, 107, 19969, 57132, 11, 46832, 235, 13146, 28002, 143279, 144171, 131097, 40281, 77002, 48408, 53955, 66845, 47818, 126251, 12802, 129624, 33883, 126246, 56419, 126346, 56475, 142300, 130357, 90686, 13, 63757, 32290, 28733, 130000, 12802, 37195, 106, 52959, 89659, 126720, 64577, 129615, 130508, 125714, 126346, 131170, 125544, 77002, 33704, 129624, 33883, 126246, 56475, 28733, 133085, 66425, 19969, 124685, 125476, 130109, 128036, 130822, 128618, 129150, 125489, 624, 34764, 51588, 26698, 20, 25, 136905, 129413, 26698, 88259, 95577, 139822, 21329, 88259, 19391, 83518, 20401, 47985, 78125, 54321, 220, 17, 130609, 134313, 129439, 20401, 48408, 29326, 52959, 81173, 66845, 47818, 86372, 78125, 54321, 7, 92817, 129567, 47985, 8, 12802, 65510, 32831, 132306, 13, 136905, 29326, 16560, 95577, 139822, 21329, 88259, 19391, 220, 20, 15, 15, 15, 57026, 126337, 20401, 28927, 251, 137075, 143143, 144079, 129392, 29326, 128909, 28927, 251, 126251, 54321, 31328, 3369, 26698, 126893, 46832, 242, 87608, 54321, 7, 99232, 75405, 101620, 8, 527, 7, 19969, 141676, 8, 17877, 220, 17, 15, 16, 21, 126216, 220, 16, 17, 128514, 128878, 138267, 78125, 47836, 95617, 29281, 130939, 220, 17, 16, 32077, 142234, 128836, 13, 136905, 89860, 131793, 128739, 20401, 48364, 104, 60960, 128909, 125466, 54321, 12802, 25715, 136905, 20401, 3369, 144067, 77226, 10764, 245, 230, 131196, 527, 127864, 47836, 17877, 95002, 132091, 54116, 66845, 132306, 13, 127165, 108, 124517, 125068, 21329, 7, 16, 24, 15, 72553, 144562, 8, 19391, 38150, 54330, 42905, 54116, 124517, 131698, 132555, 19391, 23084, 31079, 55673, 92192, 125068, 21329, 7, 16, 15, 21, 72553, 144562, 8, 66136, 48408, 143083, 134664, 79716, 126345, 12802, 129238, 126614, 56290, 128841, 95577, 139822, 21329, 88259, 19391, 91043, 57132, 17380, 68232, 26699, 47836, 56419, 130472, 125489, 13, 26698, 126893, 29326, 19391, 125686, 125548, 32290, 54116, 130999, 20401, 60960, 131005, 129439, 10764, 29389, 21329, 131570, 21329, 88259, 19391, 65510, 32831, 128841, 125466, 54321, 12802, 126352, 54330, 25715, 129360, 83518, 19969, 13935, 137266, 126345, 54116, 66019, 19391, 83315, 138913, 131417, 145063, 133099, 136905, 56290, 87608, 54321, 33704, 58677, 125722, 95577, 139822, 144078, 125068, 134066, 21329, 88259, 7, 134066, 8, 80573, 43115, 124781, 18411, 10764, 245, 230, 126251, 34395, 38523, 249, 95577, 139822, 20401, 10764, 240, 235, 126861, 17877, 129242, 126407, 42905, 5140, 41902, 90711, 246, 126402, 51876, 13, 23894, 116, 57132, 36055, 125519, 20401, 3369, 126337, 33883, 125118, 92817, 7, 100401, 55135, 103637, 100227, 8, 527, 19391, 135513, 126588, 38523, 249, 79302, 239, 129034, 131937, 20401, 136665, 17877, 129242, 126407, 66425, 130039, 58677, 78125, 44518, 125624, 137075, 81173, 43590, 56290, 126204, 133553, 57132, 76435, 134310, 20401, 129242, 76435, 23259, 80573, 5140, 117, 245, 137075, 125466, 54321, 91043, 23259, 17380, 140969, 42905, 128584, 127820, 135946, 125489, 13, 137138, 48108, 112, 80968, 33704, 220, 20, 15, 72553, 18, 19, 18, 16, 144562, 17380, 83518, 20401, 47985, 125466, 54321, 7, 17, 18, 72553, 144562, 8, 20401, 220, 17, 130609, 19969, 143835, 34395, 124685, 129807, 12802, 66845, 78125, 54321, 7, 20, 21, 72553, 144562, 8, 129885, 23872, 121, 62275, 68232, 13146, 13, 46832, 242, 87608, 54321, 126377, 220, 21, 72553, 144562, 134313, 129439, 28927, 251, 126251, 54321, 53680, 28927, 251, 126251, 131655, 134310, 19969, 129901, 125519, 13146, 13, 28927, 251, 126251, 131655, 134310, 16560, 220, 18, 59761, 130270, 126251, 7, 125568, 32290, 80968, 220, 16, 72553, 144562, 8, 19391, 56419, 29326, 130000, 125086, 11, 28927, 251, 126251, 47985, 26698, 124780, 11, 35509, 29346, 144262, 134310, 77002, 12802, 143143, 144162, 85251, 13146, 13, 44518, 16560, 126844, 129062, 70943, 92751, 124781, 131854, 47455, 239, 20401, 18411, 126352, 133847, 136089, 132597, 126246, 17877, 130729, 29281, 126204, 57852, 124781, 19391, 129901, 131498, 95617, 29281, 125489, 13, 151645, 198, 151644, 872, 198, 20487, 127033, 137525, 19969, 5140, 228, 235, 124517, 19391, 125714, 59698, 16560, 126440, 129321, 33704, 132779, 31328, 19969, 35711, 30, 151645, 198, 151644, 77091, 198, 20487, 127033, 137525, 16560, 5140, 228, 235, 124517, 19391, 135392, 126440, 129321, 17877, 125714, 59698, 34395, 128472, 13, 54116, 127033, 137525, 17380, 58677, 33883, 5140, 228, 235, 124517, 46832, 246, 65306, 12802, 137525, 132537, 68232, 126251, 129242, 130609, 131937, 53680, 74808, 76337, 12802, 34143, 105, 50340, 134497, 128472, 382, 136689, 83666, 11, 54116, 127033, 137525, 17380, 58677, 33883, 61298, 126641, 47985, 80573, 129381, 131937, 129889, 48408, 53955, 66845, 68232, 126251, 20401, 129242, 130609, 19969, 95351, 33883, 134497, 128472, 13, 95617, 18411, 129901, 11, 61298, 126641, 47985, 20401, 54116, 127033, 19969, 48408, 53955, 66845, 32831, 42039, 46319, 132537, 70943, 131793, 56039, 56475, 132311, 48408, 131131, 132618, 81133, 60315, 129624, 52959, 126445, 28002, 129616, 20401, 142209, 223, 105, 50340, 136357, 68232, 126251, 12802, 130092, 136448, 129242, 130609, 130357, 128472, 13, 23084, 16560, 5140, 228, 235, 85057, 126251, 66790, 47985, 18411, 81718, 136892, 34395, 132931, 11, 62071, 54330, 47985, 80573, 56419, 131793, 83556, 66845, 56475, 48408, 53955, 66845, 68232, 126251, 129242, 130609, 48108, 112, 133864, 143861, 246, 31079, 60315, 34395, 128472, 13, 130005, 11, 62071, 54330, 47985, 56475, 129242, 130609, 64119, 125615, 61298, 50340, 135402, 53680, 129423, 145237, 12802, 36330, 102, 131226, 48364, 255, 54330, 136448, 141091, 130357, 132931, 11, 32129, 53680, 20401, 55673, 127165, 108, 133692, 139963, 55902, 126204, 128472, 15505, 1097, 19, 5053, 382, 136904, 83666, 11, 54116, 127033, 137525, 16560, 28733, 85057, 126251, 130612, 126440, 129321, 17877, 125714, 59698, 34395, 128472, 13, 81718, 13146, 28733, 130000, 12802, 73077, 125548, 131611, 62099, 105, 126251, 19391, 32129, 16560, 37195, 247, 21329, 20401, 28733, 133085, 131837, 33704, 129423, 43590, 126204, 11, 125686, 136196, 23573, 133553, 17877, 138779, 42905, 8620, 121, 225, 57801, 20401, 124685, 127324, 131837, 33704, 132376, 251, 19969, 126204, 128472, 13, 130005, 11, 62071, 54330, 47985, 136605, 131937, 56475, 72553, 89860, 76337, 16186, 125615, 16778, 107, 19969, 57132, 11, 46832, 235, 13146, 28002, 143279, 144171, 131097, 40281, 77002, 48408, 53955, 66845, 47818, 126251, 12802, 129624, 33883, 126246, 56419, 126346, 56475, 142300, 130357, 128472, 13, 63757, 32290, 11, 28733, 130000, 12802, 37195, 106, 52959, 89659, 126720, 64577, 129615, 130508, 11, 125714, 126346, 11, 131170, 125544, 136357, 28733, 133085, 33704, 124685, 125476, 130109, 134497, 128472, 15505, 1097, 19, 5053, 382, 12802, 80573, 131050, 54116, 127033, 137525, 16560, 5140, 228, 235, 124517, 19391, 132182, 126440, 129321, 17877, 125714, 59698, 124905, 11, 68232, 126251, 129242, 130609, 131937, 20401, 137525, 80573, 134585, 68232, 126251, 20401, 129392, 43866, 11, 28733, 85057, 126251, 20401, 124685, 127324, 131837, 137525, 134454, 83315, 53442, 126204, 128472, 13, 23084, 16560, 5140, 228, 235, 124517, 141091, 32831, 53680, 28927, 251, 131837, 95170, 41671, 19391, 136361, 126440, 129321, 17877, 125714, 142588, 28733, 128472, 13, 151645]\n"
     ]
    }
   ],
   "source": [
    "print('입력에 대한 정수 인코딩 결과:')\n",
    "print(batch[\"input_ids\"][0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "0c10d8b0-3604-45b8-95c1-c91c02599db2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "레이블에 대한 정수 인코딩 결과:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 198, 20487, 127033, 137525, 16560, 5140, 228, 235, 124517, 19391, 135392, 126440, 129321, 17877, 125714, 59698, 34395, 128472, 13, 54116, 127033, 137525, 17380, 58677, 33883, 5140, 228, 235, 124517, 46832, 246, 65306, 12802, 137525, 132537, 68232, 126251, 129242, 130609, 131937, 53680, 74808, 76337, 12802, 34143, 105, 50340, 134497, 128472, 382, 136689, 83666, 11, 54116, 127033, 137525, 17380, 58677, 33883, 61298, 126641, 47985, 80573, 129381, 131937, 129889, 48408, 53955, 66845, 68232, 126251, 20401, 129242, 130609, 19969, 95351, 33883, 134497, 128472, 13, 95617, 18411, 129901, 11, 61298, 126641, 47985, 20401, 54116, 127033, 19969, 48408, 53955, 66845, 32831, 42039, 46319, 132537, 70943, 131793, 56039, 56475, 132311, 48408, 131131, 132618, 81133, 60315, 129624, 52959, 126445, 28002, 129616, 20401, 142209, 223, 105, 50340, 136357, 68232, 126251, 12802, 130092, 136448, 129242, 130609, 130357, 128472, 13, 23084, 16560, 5140, 228, 235, 85057, 126251, 66790, 47985, 18411, 81718, 136892, 34395, 132931, 11, 62071, 54330, 47985, 80573, 56419, 131793, 83556, 66845, 56475, 48408, 53955, 66845, 68232, 126251, 129242, 130609, 48108, 112, 133864, 143861, 246, 31079, 60315, 34395, 128472, 13, 130005, 11, 62071, 54330, 47985, 56475, 129242, 130609, 64119, 125615, 61298, 50340, 135402, 53680, 129423, 145237, 12802, 36330, 102, 131226, 48364, 255, 54330, 136448, 141091, 130357, 132931, 11, 32129, 53680, 20401, 55673, 127165, 108, 133692, 139963, 55902, 126204, 128472, 15505, 1097, 19, 5053, 382, 136904, 83666, 11, 54116, 127033, 137525, 16560, 28733, 85057, 126251, 130612, 126440, 129321, 17877, 125714, 59698, 34395, 128472, 13, 81718, 13146, 28733, 130000, 12802, 73077, 125548, 131611, 62099, 105, 126251, 19391, 32129, 16560, 37195, 247, 21329, 20401, 28733, 133085, 131837, 33704, 129423, 43590, 126204, 11, 125686, 136196, 23573, 133553, 17877, 138779, 42905, 8620, 121, 225, 57801, 20401, 124685, 127324, 131837, 33704, 132376, 251, 19969, 126204, 128472, 13, 130005, 11, 62071, 54330, 47985, 136605, 131937, 56475, 72553, 89860, 76337, 16186, 125615, 16778, 107, 19969, 57132, 11, 46832, 235, 13146, 28002, 143279, 144171, 131097, 40281, 77002, 48408, 53955, 66845, 47818, 126251, 12802, 129624, 33883, 126246, 56419, 126346, 56475, 142300, 130357, 128472, 13, 63757, 32290, 11, 28733, 130000, 12802, 37195, 106, 52959, 89659, 126720, 64577, 129615, 130508, 11, 125714, 126346, 11, 131170, 125544, 136357, 28733, 133085, 33704, 124685, 125476, 130109, 134497, 128472, 15505, 1097, 19, 5053, 382, 12802, 80573, 131050, 54116, 127033, 137525, 16560, 5140, 228, 235, 124517, 19391, 132182, 126440, 129321, 17877, 125714, 59698, 124905, 11, 68232, 126251, 129242, 130609, 131937, 20401, 137525, 80573, 134585, 68232, 126251, 20401, 129392, 43866, 11, 28733, 85057, 126251, 20401, 124685, 127324, 131837, 137525, 134454, 83315, 53442, 126204, 128472, 13, 23084, 16560, 5140, 228, 235, 124517, 141091, 32831, 53680, 28927, 251, 131837, 95170, 41671, 19391, 136361, 126440, 129321, 17877, 125714, 142588, 28733, 128472, 13, 151645]\n"
     ]
    }
   ],
   "source": [
    "print('레이블에 대한 정수 인코딩 결과:')\n",
    "print(batch[\"labels\"][0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "a511a4b7-d2f8-4942-a501-7d7a4e4cfb34"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/usr/local/lib/python3.11/dist-packages/trl/trainer/sft_trainer.py:283: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    max_seq_length=max_seq_length,\n",
    "    train_dataset=train_dataset,\n",
    "    data_collator=collate_fn,\n",
    "    peft_config=peft_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "edb9fbff-19ef-4b4b-8381-11df04aa96a1",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='285' max='285' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [285/285 30:16, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.568200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.566200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.426400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.453700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.407500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.419500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.384400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.459700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.455200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.379700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.436000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.399800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.342100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.319500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.359500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.347900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.476600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.399100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.351000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.351500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.271500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.332000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.326900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.347200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.298000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.362200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.403100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.266100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "b58447a2-f506-4529-ac01-a8c280cb76e9"
   },
   "outputs": [],
   "source": [
    "prompt_lst = []\n",
    "label_lst = []\n",
    "\n",
    "for prompt in test_dataset[\"messages\"]:\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        prompt, tokenize=False, add_generation_prompt=False\n",
    "    )\n",
    "    input = text.split('<|im_start|>assistant')[0] + '<|im_start|>assistant'\n",
    "    label = text.split('<|im_start|>assistant')[1]\n",
    "    prompt_lst.append(input)\n",
    "    label_lst.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "ed095c91-c0ab-4b6c-b60c-cf8c8314db3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "당신은 검색 결과를 바탕으로 질문에 답변해야 합니다.\n",
      "\n",
      "다음의 지시사항을 따르십시오.\n",
      "1. 질문과 검색 결과를 바탕으로 답변하십시오.\n",
      "2. 검색 결과에 없는 내용을 답변하려고 하지 마십시오.\n",
      "3. 질문에 대한 답이 검색 결과에 없다면 검색 결과에는 \"해당 질문~에 대한 내용이 없습니다.\" 라고 답변하십시오.\n",
      "4. 답변할 때 특정 문서를 참고하여 문장 또는 문단을 작성했다면 뒤에 출처는 이중 리스트로 해당 문서 번호를 남기십시오. 예를 들어서 특정 문장이나 문단을 1번 문서에서 인용했다면 뒤에 [[ref1]]이라고 기재하십시오.\n",
      "5. 예를 들어서 특정 문장이나 문단을 1번 문서와 5번 문서에서 동시에 인용했다면 뒤에 [[ref1]], [[ref5]]이라고 기재하십시오.\n",
      "6. 최대한 다수의 문서를 인용하여 답변하십시오.\n",
      "\n",
      "검색 결과:\n",
      "-----\n",
      "문서1: ‘관피아 논란’에도 불구하고 일찌감치 민간으로 옮겨 전문성과 능력을 인정받는 관료 출신들도 적지 않다. 대표적인 인물이 권용원 키움증권 사장이다. 권 사장은 1999년까지 산업자원부 산업기술개발과장(기술고시 21회)을 지내다가 이듬해 다우그룹 전략경영실장으로 이직했다. 다우그룹에서 신사업을 전담한 그는 신생 온라인 증권사였던 키움증권을 대형 증권사로 키워냈다.금융투자업계에선 전병조 KB투자증권 부사장이 눈에 띈다. 6년 전 기획재정부 관료(행정고시 29회)에서 기업금융(IB) 부문 전문가로 변신했다. 재정경제부 출신 이형승 케이스템셀 회장(행시 29회)은 삼성증권(이사) CJ그룹(소장) IBK투자증권(사장)을 거쳐 바이오회사로 갔다. 재정경제부 출신 이현승 전 SK증권 사장(행시 32회)도 AT커니(이사) 메릴린치(이사) GE에너지(사장) 등을 거쳤다.대기업 쪽에도 많다. 박영춘 SK 전무(행시 31회)는 금융위원회 금융정책과장을 거쳐 청와대 비상경제상황실 금융팀장을 지낸 금융 전문가로 SK그룹의 전략업무를 맡고 있다. 박 전무와 행시 동기인 문홍성 두산 부사장은 기획재정부 출신으로 국제금융 전문가로 손꼽힌다.\n",
      "-----\n",
      "문서2: 대구지역 섬유업체인 (주)이주의 이창석 사장은 건축학도의 길을 걷던 1996년 고심 끝에 가업을 이어받기로 했다. 부친의 뜻을 따르기로 했지만 무엇부터 해야 할지 막막했다. 이 사장은 “섬유업은 이미 1990년대 후반부터 선진국과 후발국 사이에 낀 샌드위치 신세라 사업을 어떻게 유지할지 고민이 많았다”고 회상했다.그는 해외로 공장을 옮기는 대신 국내 기술 수준을 끌어올리기로 했다. 생산원가를 낮춰 중국에 대응하는 것보다 선진국처럼 고부가가치 산업으로 탈바꿈하는 게 낫다는 생각에서다.2005년 대표이사로 취임한 뒤 사명을 이주염공에서 (주)이주로 바꿨다. 또 아웃도어 의류에 많이 쓰이는 기능성 소재를 염색하고 가공하는 염색가공업에 집중하기로 했다. 이듬해 8월 기술연구소를 설립했다. 직원이 32명에 불과한 점을 고려하면 파격적 조치였다. 직원 중 20%가 넘는 7명을 연구원으로 충원했다.연구개발(R&D)을 강화한 뒤 길이 열리기 시작했다. 2006년 정부로부터 부품소재전문기업으로 인정받았고 2008년엔 기술혁신형 중소기업으로 선정됐다. 좋은 조건으로 쓸 수 있는 자금도 많아졌다. 2009년과 2011년엔 대구시장 표창과 교육과학기술부 장관 표창을 받았다.(주)이주의 경우 소품종 대량 생산으로 생산원가를 낮추는 방법 대신 다품종 소량 생산을 선택하며 성공을 일군 드문 경우다. 거래 업체에서 주문받는 원단 수를 줄이더라도 고급 염색과 가공에 초점을 맞췄다. 이 사장은 “염색가공업은 의류 원단을 직접 만들어 부가가치를 창출하는 게 아니라 원단을 생산하는 업체에 염색가공 컨설팅을 하는 것이나 마찬가지”라고 설명했다.그래도 30여명의 인력으로 염색가공업을 하는 것은 벅찬 일이었다. 이 사장은 외부 연구소와의 협력으로 인력 부족 문제를 해결했다. 정부 과제를 수주한 뒤 대구·경북 지역의 전문 연구소와 제휴를 맺어 연구 과제를 수행했다.안성익 영남대 경영학과 교수는 “(주)이주의 성공 사례는 경영자의 노력에 따라 사양산업인 섬유업도 지식산업으로 탈바꿈할 수 있다는 점을 보여준다”고 말했다.\n",
      "-----\n",
      "문서3: “물건이 없어서 못 팔 정도로 인기가 많았다.”조창섭 영신물산 대표(사진)가 섬유염색업에 뛰어든 1971년은 업황이 좋았다. 당시 염색기술을 가진 공장이 몇 군데 없었다. 물건을 만들면 바로 동이 났다. 고등학교를 졸업하고 처음 입사한 서울 성수동의 한 염색공장에서 꼬박 19년을 일했다. 이 경험을 바탕으로 1990년 1월5일 경기 양주에 영신물산을 설립했다. 국내업체 간 경쟁이 격화되면서 2003년 남미 과테말라 공장을 세우고 해외로 눈을 돌렸다. 과테말라에서 생산되는 모든 물량은 미국으로 수출된다. 지난해 매출 586억원 가운데 70%에 이른다. 조 대표는 11일 섬유패션업계 발전에 이바지한 공로를 인정받아 산업통상자원부로부터 금탑산업훈장을 받았다.영신물산은 25년간 섬유염색업 한우물만 팠다. 남들은 사양산업이라고 사업을 축소했지만 조 대표는 지속적으로 투자를 늘렸다. 외환위기 당시에도 이탈리아에서 설비를 들여왔다. “이 위기만 넘기면 살아날 수 있다. 여기서 투자를 멈춰선 안 된다”는 신념 때문이었다. 2003년 과테말라로 생산설비를 옮길 때 회사 내부에서 반대가 있었다. “회사가 잘되고 있는데 굳이 모험을 할 필요가 있느냐”는 것이었다. 그는 안주하기 싫었다. 과감히 5000만달러를 투자했다. 전략이 먹혔다. 현재 과테말라에 공장 3개가 있다. 지난해 매출 586억원 가운데 400억원 이상이 과테말라에서 나왔다.산업부는 조 대표의 이 같은 노력이 한국 기업들이 중남미 섬유시장 점유율의 90%를 차지하는 데 기여한 것으로 평가하고 있다. 조 대표는 “지속적인 설비투자와 연구개발을 통해 경쟁력을 유지하겠다”고 말했다.이날 행사에는 정우영 제원화섬 대표와 안희정 한아인터내셔날 대표가 각각 은탑산업훈장과 동탑산업훈장을 받았다.\n",
      "-----\n",
      "문서4: 삼성그룹이 26일 제일모직과 삼성물산의 합병으로 탄생할 통합 법인의 이름을 삼성물산으로 정하면서 ‘제일모직’이라는 사명은 역사 속으로 사라지게 된다. 삼성 창업주인 고(故) 이병철 선대회장이 1954년 9월 당시 자본금 1억환을 들여 제일모직공업주식회사를 설립한 지 61년 만이다.제일모직은 이병철 선대회장이 삼성물산과 제일제당에 이어 세 번째로 세운 회사다. 섬유 원단 대부분을 해외에서 수입하다 보니 양복 한 벌 가격이 직장인 월급 3개월치와 맞먹는 상황을 해결하기 위해 섬유를 국산화하겠다는 목표를 내걸면서다. 제일모직은 1956년 대구에 국내 최초의 모직 공장을 세웠고, 독일 전문기술자를 초빙해 민간기업 최초로 해외 기술을 도입했다. 이 선대회장은 1987년 별세할 때까지 제일모직 등기이사를 맡을 만큼 애정을 가진 것으로 전해졌다.삼성그룹은 2013년 12월 삼성에버랜드와 제일모직 패션부문을 합병할 때도 제일모직이라는 이름을 고수했다. 당시 매출 비중을 놓고 보면 6 대 4 정도로 삼성에버랜드의 사업 규모가 더 컸다. 통합 법인이 부동산 및 레저사업 구조까지 아우르고 있어 ‘에버랜드’라는 사명이 더 적합하다는 평가가 지배적이었지만 제일모직을 사명으로 채택했다. 그만큼 제일모직이라는 이름에 애착이 많았다는 의미다.삼성물산은 이 선대회장이 삼성그룹 계열사 가운데 가장 먼저 창업한 회사다. 1938년 3월 대구시 수동(현 인교동)에서 청과물과 건어물 수출업으로 창업한 ‘삼성상회’가 그 뿌리다. 당시 지상 4층, 지하 1층 건물에 지은 250평 규모의 창고가 전부였다. 삼성상회는 광복 이후 서울로 자리를 옮겨 1948년 삼성물산공사로 이름을 바꿨다. 1951년에는 삼성물산주식회사로 개명했다. 1975년에는 한국 종합상사 1호로 지정됐다. 삼성물산은 1995년 삼성건설을 합병한 이후 건설부문과 상사부문으로 나뉘어 세계 50여개국에서 사업을 전개하고 있다.\n",
      "-----\n",
      "문서5: 이병철 삼성그룹 창업주는 1983년 2월 반도체사업에 뛰어들기로 결정했다. 당시 이 회장은 “삼성의 이익만을 위해서가 아니라 국가적 견지에서 삼성이 먼저 반도체사업을 한다”는 어록을 남겼다. 임원 회의 때마다 “국가가 부흥하면 삼성 같은 건 망해도 또 생길 수 있지만 국가가 망하면 삼성은 영원히 없어진다”고 말한 것과 같은 맥락이었다.전국경제인연합회는 5일 지금은 고인이 된 1세대 창업자 특유의 기업가 정신을 소개한 책을 내놨다. 제목은 ‘한강의 기적과 기업가 정신’이다. 월간조선 편집장 출신인 김용삼 미래한국 편집장이 썼다. 고(故) 이병철 창업주 등 타계한 1세대 기업인들을 탐구했다. 김 편집장은 사업을 통해 국가에 보답한다는 사업보국(事業報國) 정신을 한국 기업가 정신의 전형으로 정의했다. 이 때문에 정부와 기업 간 협업이 많았고 신뢰도 두터웠다는 게 저자의 판단이다. 정주영 현대그룹 명예회장과 박정희 대통령을 대표적 예로 들었다. 정 회장은 1960년대 초 현대건설이 시공하는 소양강댐 건설 방식을 두고 댐 설계 전문가인 구보다 유타카 일본공영 회장과 설전을 벌였다. 구보다 회장은 소양강댐도 일본 댐들처럼 콘크리트와 철근을 이용해 만들어야 한다고 주장했다. 하지만 정 회장은 “소양강 주변에 흙과 모래, 자갈이 지천으로 널려 있으니 암석을 이용한 사력(砂礫)댐으로 건설해야 한다”고 맞섰다.정 회장은 댐에 대해 문외한이나 마찬가지였지만 박 대통령은 정 회장의 손을 들어줬다. 그 결과 공사비를 30% 절감했고 더 안전하다는 평가를 받았다. 코오롱 창업주인 이원만 회장은 1963년 박정희 대통령을 설득해 서울 구로동 수출공업단지 건설을 이끌어냈다. 당시 이 회장은 “일본은 별 기술 없이 포크와 나이프를 수출해 연간 3억달러 이상 벌고 있다”며 “한국 근로자들의 손재주는 일본에 뒤지지 않으니 스테인리스를 잘 닦아 광택을 내면 된다”고 주장했다. 구인회 LG그룹 창업주는 “우리도 머리 싸매고 땀 흘리면 남처럼 못 살 게 없다”고 강조했다. 기아자동차 창업주인 김철호 회장이 보여준 기업 간 신뢰도 한국 기업가 정신의 특이한 점으로 소개됐다. 김 회장은 1973년 국내 최초로 2.0L 가솔린 자동차 엔진을 국산화한 뒤 임원진의 반대를 무릅쓰고 정주영 회장이 기아차 공장을 둘러보도록 허락했다. 김 회장은 “기업이란 주변의 모든 기업이 함께 발전해야 더 클 수 있다”고 강조했다.<|im_end|>\n",
      "<|im_start|>user\n",
      "정부와 기업 간 협업의 사례와 그 결과는 무엇인가요?<|im_end|>\n",
      "<|im_start|>assistant\n"
     ]
    }
   ],
   "source": [
    "print(prompt_lst[42])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "2bc52e99-eb23-46d9-a85f-85ef08d3e587"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "정부와 기업 간 협업의 사례와 그 결과는 다음과 같습니다.\n",
      "\n",
      "1. **(주)이주와 정부의 협업**:\n",
      "   - **사례**: 대구지역 섬유업체인 (주)이주는 2006년 정부로부터 부품소재전문기업으로 인정받고, 2008년에는 기술혁신형 중소기업으로 선정되었습니다. 이 회사는 정부 과제를 수주한 뒤 대구·경북 지역의 전문 연구소와 제휴를 맺어 연구 과제를 수행했습니다.\n",
      "   - **결과**: 이러한 협업을 통해 (주)이주는 좋은 조건으로 자금을 사용할 수 있게 되었고, 대구시장 표창과 교육과학기술부 장관 표창을 받는 등 여러 성과를 거두었습니다. 이는 섬유업이 지식산업으로 탈바꿈할 수 있다는 가능성을 보여주는 사례로 평가받고 있습니다[[ref2]].\n",
      "\n",
      "2. **정주영 현대그룹 명예회장과 박정희 대통령의 협업**:\n",
      "   - **사례**: 1960년대 초 현대건설이 시공하는 소양강댐 건설 방식을 두고 정주영 회장과 박정희 대통령이 협업했습니다. 정 회장은 일본의 댐 설계 전문가와의 설전에서 소양강 주변의 자원을 이용한 사력댐 건설을 주장했고, 박 대통령은 정 회장의 손을 들어주었습니다.\n",
      "   - **결과**: 이 협업의 결과로 공사비를 30% 절감할 수 있었고, 더 안전한 댐을 건설할 수 있었습니다[[ref5]].\n",
      "\n",
      "3. **코오롱 창업주 이원만 회장과 박정희 대통령의 협업**:\n",
      "   - **사례**: 이원만 회장은 1963년 박정희 대통령을 설득해 서울 구로동 수출공업단지 건설을 이끌어냈습니다. 그는 한국 근로자들의 손재주를 활용해 스테인리스 제품을 수출할 수 있다는 점을 강조했습니다.\n",
      "   - **결과**: 이 협업을 통해 구로동 수출공업단지가 건설되었고, 이는 한국의 수출 산업 발전에 기여했습니다[[ref5]].\n",
      "\n",
      "이와 같은 사례들은 정부와 기업 간의 협업이 기업의 성장과 국가 경제 발전에 중요한 역할을 할 수 있음을 보여줍니다.<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(label_lst[42])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "15a4262b-186b-4abf-b6cf-65d22f3c10b0"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import  AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"Qwen/Qwen2-7B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "27d9d395-a3c2-47b7-b135-34f4f78f2ec2"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dcae3e1ab064037a0c0c8efcbc0785d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    }
   ],
   "source": [
    "peft_model_id = \"qwen2-7b-rag-ko/checkpoint-285\"\n",
    "fine_tuned_model = AutoPeftModelForCausalLM.from_pretrained(peft_model_id, device_map=\"auto\", torch_dtype=torch.float16)\n",
    "pipe = pipeline(\"text-generation\", model=fine_tuned_model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "2b6fbdd3-a8db-48dc-92fa-fb51b20f2c3d"
   },
   "outputs": [],
   "source": [
    "eos_token = tokenizer(\"<|im_end|>\",add_special_tokens=False)[\"input_ids\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "fad932e6-1c1a-4bd8-b1b5-c877a575cd51"
   },
   "outputs": [],
   "source": [
    "def test_inference(pipe, prompt):\n",
    "    outputs = pipe(prompt, max_new_tokens=1024, eos_token_id=eos_token, do_sample=False)\n",
    "    return outputs[0]['generated_text'][len(prompt):].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "c41ebf47-6905-45ad-b685-a1751613caa8"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prompt_lst' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt, label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[43mprompt_lst\u001b[49m[\u001b[38;5;241m300\u001b[39m:\u001b[38;5;241m305\u001b[39m], label_lst[\u001b[38;5;241m300\u001b[39m:\u001b[38;5;241m305\u001b[39m]):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    response:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtest_inference(pipe,\u001b[38;5;250m \u001b[39mprompt)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    label:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'prompt_lst' is not defined"
     ]
    }
   ],
   "source": [
    "for prompt, label in zip(prompt_lst[300:305], label_lst[300:305]):\n",
    "    print(f\"    response:\\n{test_inference(pipe, prompt)}\")\n",
    "    print(f\"    label:\\n{label}\")\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "918de7bf-b690-460e-95af-46d74c189cc6"
   },
   "outputs": [],
   "source": [
    "base_model_id = \"Qwen/Qwen2-7B-Instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_id, device_map=\"auto\", torch_dtype=torch.float16)\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fa95b03c-71e8-4453-8d97-31255c38b6ef"
   },
   "outputs": [],
   "source": [
    "for prompt, label in zip(prompt_lst[300:305], label_lst[300:305]):\n",
    "    print(f\"    response:\\n{test_inference(pipe, prompt)}\")\n",
    "    print(f\"    label:\\n{label}\")\n",
    "    print(\"-\"*50)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyN99JBREvMaVdFQYHZDmcky",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
