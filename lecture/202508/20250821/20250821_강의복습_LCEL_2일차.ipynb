{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install PyDrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아래 링크를 복사하여 웹 브라우저에 붙여넣으세요.\n",
      "https://accounts.google.com/o/oauth2/auth?client_id=35726703810-4v13dfqmilhgv6shlc3cv9i3ktuh73j1.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&access_type=offline&response_type=code\n",
      "Authentication successful.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import mykeys\n",
    "\n",
    "project_name = 'CH13_LCEL'\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = project_name\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = mykeys.get_key('LANG')\n",
    "os.environ[\"LANGCHAIN_HUB_API_KEY\"] = mykeys.get_key('LANG')\n",
    "os.environ[\"OPENAI_API_KEY\"] = mykeys.get_key('GPT')\n",
    "os.environ[\"GOOGLE_API_KEY\"] = mykeys.get_key('GOO')\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = mykeys.get_key('HF')\n",
    "os.environ[\"UPSTAGE_API_KEY\"] = mykeys.get_key('UP')\n",
    "os.environ[\"COHERE_API_KEY\"] = mykeys.get_key('COH')\n",
    "os.environ[\"JINA_API_KEY\"] = mykeys.get_key('JINA')\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = mykeys.get_key('ANT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith 추적을 시작합니다.\n",
      "[프로젝트명]\n",
      "CH13_LCEL\n"
     ]
    }
   ],
   "source": [
    "from langchain_teddynote import logging\n",
    "\n",
    "# 프로젝트 이름을 입력합니다.\n",
    "logging.langsmith(project_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote import logging\n",
    "\n",
    "# set_enable=False 로 지정하면 추적을 하지 않습니다.\n",
    "logging.langsmith(project_name, set_enable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CH13 LangChain Expression Language(LCEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@chain 데코레이터로 Runnable 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import chain\n",
    "from langchain_openai import ChatOpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 프롬프트 템플릿을 정의합니다.\n",
    "prompt1 = ChatPromptTemplate.from_template(\"{topic} 에 대해 짧게 한글로 설명해주세요.\")\n",
    "prompt2 = ChatPromptTemplate.from_template(\n",
    "    \"{sentence} 를 emoji를 활용한 인스타그램 게시글로 만들어주세요.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@chain\n",
    "def custom_chain(text):\n",
    "    # 첫 번째 프롬프트, ChatOpenAI, 문자열 출력 파서를 연결하여 체인을 생성합니다.\n",
    "    chain1 = prompt1 | ChatOpenAI(model=\"gpt-4o-mini\") | StrOutputParser()\n",
    "    output1 = chain1.invoke({\"topic\": text})\n",
    "\n",
    "    # 두 번째 프롬프트, ChatOpenAI, 문자열 출력 파서를 연결하여 체인을 생성합니다.\n",
    "    chain2 = prompt2 | ChatOpenAI(model=\"gpt-4o-mini\") | StrOutputParser()\n",
    "    # 두 번째 체인을 호출하여 파싱된 첫 번째 결과를 전달하고 최종 결과를 반환합니다.\n",
    "    return chain2.invoke({\"sentence\": output1})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌌✨ **양자역학의 매력!** ✨🌌\n",
      "\n",
      "양자역학은 물리학의 한 분야로, 미시 세계에서 입자와 에너지의 행동을 설명하는 이론이에요! 🔍✨\n",
      "\n",
      "고전 물리학과는 다르게, 양자역학에서는 입자가 특정한 위치나 속도를 가지지 않고, 🌀 확률로 설명된답니다! 또, 파동-입자 이중성을 통해 미세 입자들이 어떻게 행동하는지 이해할 수 있어요. 🌊➡️🔹\n",
      "\n",
      "주요 개념들:\n",
      "- 양자 상태 🌀\n",
      "- 불확정성 원리 ❓🔄\n",
      "- 양자 얽힘 🤝✨\n",
      "\n",
      "이 이론들은 원자, 전자, 광자와 같은 미세 입자의 행동을 이해하는 데 필수적이에요. 💡🔬 그리고 양자역학은 현대 물리학, 화학, 전자공학 등 다양한 분야에 응용되고 있답니다! 🌐📈\n",
      "\n",
      "양자역학의 세계에 함께 빠져보세요! 🚀💖 #양자역학 #물리학 #과학 #미세세계 #파동입자이중성 #양자상태 #불확정성원리 #과학의아름다움\n"
     ]
    }
   ],
   "source": [
    "# custom_chain을 호출\n",
    "print(custom_chain.invoke(\"양자역학\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RunnableWithMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI()\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"당신은 {ability} 에 능숙한 어시스턴트입니다. 20자 이내로 응답하세요\",\n",
    "        ),\n",
    "        # 대화 기록을 변수로 사용, history 가 MessageHistory 의 key 가 됨\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{input}\"),  # 사용자 입력을 변수로 사용\n",
    "    ]\n",
    ")\n",
    "runnable = prompt | model  # 프롬프트와 모델을 연결하여 runnable 객체 생성\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "store = {}  # 세션 기록을 저장할 딕셔너리\n",
    "\n",
    "\n",
    "# 세션 ID를 기반으로 세션 기록을 가져오는 함수\n",
    "def get_session_history(session_ids: str) -> BaseChatMessageHistory:\n",
    "    print(session_ids)\n",
    "    if session_ids not in store:  # 세션 ID가 store에 없는 경우\n",
    "        # 새로운 ChatMessageHistory 객체를 생성하여 store에 저장\n",
    "        store[session_ids] = ChatMessageHistory()\n",
    "    return store[session_ids]  # 해당 세션 ID에 대한 세션 기록 반환\n",
    "\n",
    "\n",
    "with_message_history = (\n",
    "    RunnableWithMessageHistory(  # RunnableWithMessageHistory 객체 생성\n",
    "        runnable,  # 실행할 Runnable 객체\n",
    "        get_session_history,  # 세션 기록을 가져오는 함수\n",
    "        input_messages_key=\"input\",  # 입력 메시지의 키\n",
    "        history_messages_key=\"history\",  # 기록 메시지의 키\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abc123\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Cosine represents the ratio of the adjacent side to the hypotenuse in a right triangle.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 47, 'total_tokens': 66, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-C6nPziFq81VdfVphmYS818MYvLLfS', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--3aefd77b-2b9f-4ae8-8f91-e645382741c7-0', usage_metadata={'input_tokens': 47, 'output_tokens': 19, 'total_tokens': 66, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke(\n",
    "    # 수학 관련 질문 \"코사인의 의미는 무엇인가요?\"를 입력으로 전달합니다.\n",
    "    {\"ability\": \"math\", \"input\": \"What does cosine mean?\"},\n",
    "    # 설정 정보로 세션 ID \"abc123\"을 전달합니다.\n",
    "    config={\"configurable\": {\"session_id\": \"abc123\"}},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abc123\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='코사인은 직각삼각형에서 인접변과 빗변의 비율을 나타냅니다.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 40, 'prompt_tokens': 91, 'total_tokens': 131, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-C6nQEwlV2px9xUQJqmwXZUw949Xfm', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--598668bc-75bf-4dd9-8e1c-aaffdb1b20b1-0', usage_metadata={'input_tokens': 91, 'output_tokens': 40, 'total_tokens': 131, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 메시지 기록을 포함하여 호출합니다.\n",
    "with_message_history.invoke(\n",
    "    # 능력과 입력을 설정합니다.\n",
    "    {\"ability\": \"math\", \"input\": \"이전의 내용을 한글로 답변해 주세요.\"},\n",
    "    # 설정 옵션을 지정합니다.\n",
    "    config={\"configurable\": {\"session_id\": \"abc123\"}},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def234\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='수학에 능숙한 어시스턴트입니다.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 58, 'total_tokens': 77, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-C6nQQQl4wRvPNWlJcuFX2haue7bLv', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--76ad5d4e-7298-4392-a9ae-2981ee9e2461-0', usage_metadata={'input_tokens': 58, 'output_tokens': 19, 'total_tokens': 77, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 새로운 session_id로 인해 이전 대화 내용을 기억하지 않습니다.\n",
    "with_message_history.invoke(\n",
    "    # 수학 능력과 입력 메시지를 전달합니다.\n",
    "    {\"ability\": \"math\", \"input\": \"이전의 내용을 한글로 답변해 주세요\"},\n",
    "    # 새로운 session_id를 설정합니다.\n",
    "    config={\"configurable\": {\"session_id\": \"def234\"}},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import ConfigurableFieldSpec\n",
    "\n",
    "store = {}  # 빈 딕셔너리를 초기화합니다.\n",
    "\n",
    "\n",
    "def get_session_history(user_id: str, conversation_id: str) -> BaseChatMessageHistory:\n",
    "    # 주어진 user_id와 conversation_id에 해당하는 세션 기록을 반환합니다.\n",
    "    if (user_id, conversation_id) not in store:\n",
    "        # 해당 키가 store에 없으면 새로운 ChatMessageHistory를 생성하여 저장합니다.\n",
    "        store[(user_id, conversation_id)] = ChatMessageHistory()\n",
    "    return store[(user_id, conversation_id)]\n",
    "\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    runnable,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\",\n",
    "    history_factory_config=[  # 기존의 \"session_id\" 설정을 대체하게 됩니다.\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"user_id\",  # get_session_history 함수의 첫 번째 인자로 사용됩니다.\n",
    "            annotation=str,\n",
    "            name=\"User ID\",\n",
    "            description=\"사용자의 고유 식별자입니다.\",\n",
    "            default=\"\",\n",
    "            is_shared=True,\n",
    "        ),\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"conversation_id\",  # get_session_history 함수의 두 번째 인자로 사용됩니다.\n",
    "            annotation=str,\n",
    "            name=\"Conversation ID\",\n",
    "            description=\"대화의 고유 식별자입니다.\",\n",
    "            default=\"\",\n",
    "            is_shared=True,\n",
    "        ),\n",
    "    ],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output_message': AIMessage(content='Cosine is a mathematical function that, given an angle in a right-angled triangle, is equal to the ratio of the length of the side adjacent to the given angle to the length of the hypotenuse of the triangle. It is typically denoted as cos.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 54, 'prompt_tokens': 14, 'total_tokens': 68, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-C6nRSyajajZN0dypzZMJAES38PQnv', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--3c72babe-4b0b-4d74-b132-0dc4d9df5578-0', usage_metadata={'input_tokens': 14, 'output_tokens': 54, 'total_tokens': 68, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "# chain 생성\n",
    "chain = RunnableParallel({\"output_message\": ChatOpenAI()})\n",
    "\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    # 세션 ID에 해당하는 대화 기록이 저장소에 없으면 새로운 ChatMessageHistory를 생성합니다.\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    # 세션 ID에 해당하는 대화 기록을 반환합니다.\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "# 체인에 대화 기록 기능을 추가한 RunnableWithMessageHistory 객체를 생성합니다.\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    # 입력 메시지의 키를 \"input\"으로 설정합니다.(생략시 Message 객체로 입력)\n",
    "    # input_messages_key=\"input\",\n",
    "    # 출력 메시지의 키를 \"output_message\"로 설정합니다. (생략시 Message 객체로 출력)\n",
    "    output_messages_key=\"output_message\",\n",
    ")\n",
    "\n",
    "# 주어진 메시지와 설정으로 체인을 실행합니다.\n",
    "with_message_history.invoke(\n",
    "    # 혹은 \"what is the definition of cosine?\" 도 가능\n",
    "    [HumanMessage(content=\"what is the definition of cosine?\")],\n",
    "    config={\"configurable\": {\"session_id\": \"abc123\"}},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output_message': AIMessage(content='코사인은 직각 삼각형에서 주어진 각에 대해 인접한 변의 길이와 빗변의 길이의 비율과 같은 수학적인 함수입니다. 일반적으로 cos로 표시됩니다.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 72, 'prompt_tokens': 93, 'total_tokens': 165, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-C6nRflEL3mPvwPepfbrVc1iP7Rfk0', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--342eb67a-80e1-4a66-b66c-1134e2b400a3-0', usage_metadata={'input_tokens': 93, 'output_tokens': 72, 'total_tokens': 165, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke(\n",
    "    # 이전의 답변에 대하여 한글로 답변을 재요청합니다.\n",
    "    [HumanMessage(content=\"이전의 내용을 한글로 답변해 주세요!\")],\n",
    "    # 설정 옵션을 딕셔너리 형태로 전달합니다.\n",
    "    config={\"configurable\": {\"session_id\": \"abc123\"}},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history = RunnableWithMessageHistory(\n",
    "    ChatOpenAI(),  # ChatOpenAI 언어 모델을 사용합니다.\n",
    "    get_session_history,  # 대화 세션 기록을 가져오는 함수를 지정합니다.\n",
    "    # 입력 메시지의 키를 \"input\"으로 설정합니다.(생략시 Message 객체로 입력)\n",
    "    # input_messages_key=\"input\",\n",
    "    # 출력 메시지의 키를 \"output_message\"로 설정합니다. (생략시 Message 객체로 출력)\n",
    "    # output_messages_key=\"output_message\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='코사인은 삼각함수 중 하나로, 한 각의 코사인 값을 구하는 함수입니다. 삼각형의 빗변과 인접한 변의 길이의 비율을 나타냅니다. 각도가 바뀔 때 코사인 값도 바뀌게 됩니다. 코사인 함수는 삼각형이나 원을 다루는 수학에서 자주 사용되며, 주로 삼각비나 삼각함수와 관련된 계산에 이용됩니다.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 152, 'prompt_tokens': 24, 'total_tokens': 176, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-C6nSLPDEkp0XdkofaHLearHGyKdOI', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--dd81509e-9a1e-426e-818f-5d6ac69bb0fb-0', usage_metadata={'input_tokens': 24, 'output_tokens': 152, 'total_tokens': 176, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke(\n",
    "    # 이전의 답변에 대하여 한글로 답변을 재요청합니다.\n",
    "    [HumanMessage(content=\"코사인의 의미는 무엇인가요?\")],\n",
    "    # 설정 옵션을 딕셔너리 형태로 전달합니다.\n",
    "    config={\"configurable\": {\"session_id\": \"def123\"}},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    # \"input_messages\" 키를 사용하여 입력 메시지를 가져와 ChatOpenAI()에 전달합니다.\n",
    "    itemgetter(\"input_messages\") | ChatOpenAI(),\n",
    "    get_session_history,  # 세션 기록을 가져오는 함수입니다.\n",
    "    input_messages_key=\"input_messages\",  # 입력 메시지의 키를 지정합니다.\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='코사인은 삼각함수 중 하나로, 직각삼각형의 두 변의 길이를 이용하여 만들어지는 각도의 코사인 값을 나타냅니다. 간단히 말해, 코사인은 \"인접변의 길이 나빗변의 길이\"로 정의되며, 삼각형의 각도와 각의 변을 사용하여 계산됩니다. 코사인은 삼각형의 각도를 이용하여 변의 길이나 각의 크기를 구하는 데 사용되며, 수학적으로 많은 응용 분야에서 사용됩니다.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 190, 'prompt_tokens': 24, 'total_tokens': 214, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-C6nSeISTnX9uVlRok7GbAM8spkWMO', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--da35c02a-b897-4f79-992f-711600ca49dd-0', usage_metadata={'input_tokens': 24, 'output_tokens': 190, 'total_tokens': 214, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke(\n",
    "    {\"input_messages\": \"코사인의 의미는 무엇인가요?\"},\n",
    "    # 설정 옵션을 딕셔너리 형태로 전달합니다.\n",
    "    config={\"configurable\": {\"session_id\": \"xyz123\"}},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: redis in /opt/anaconda3/envs/LangChain/lib/python3.11/site-packages (5.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run -d -p 6379:6379 -p 8001:8001 redis/redis-stack:latest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redis 서버의 URL을 지정합니다.\n",
    "REDIS_URL = \"redis://localhost:6379/0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import RedisChatMessageHistory\n",
    "\n",
    "\n",
    "def get_message_history(session_id: str) -> RedisChatMessageHistory:\n",
    "    # 세션 ID를 기반으로 RedisChatMessageHistory 객체를 반환합니다.\n",
    "    return RedisChatMessageHistory(session_id, url=REDIS_URL)\n",
    "\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    runnable,  # 실행 가능한 객체\n",
    "    get_message_history,  # 메시지 기록을 가져오는 함수\n",
    "    input_messages_key=\"input\",  # 입력 메시지의 키\n",
    "    history_messages_key=\"history\",  # 기록 메시지의 키\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='cosine 는 삼각함수 중 하나로, 직각 삼각형에서 밑변과 빗변의 비율을 의미합니다.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 49, 'prompt_tokens': 47, 'total_tokens': 96, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-C6nWdGhqXP73jTTC9RzF73FASdeoF', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--fcbc0df2-60a8-4775-9b9a-47b3616161b9-0', usage_metadata={'input_tokens': 47, 'output_tokens': 49, 'total_tokens': 96, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke(\n",
    "    # 수학 관련 질문 \"코사인의 의미는 무엇인가요?\"를 입력으로 전달합니다.\n",
    "    {\"ability\": \"math\", \"input\": \"What does cosine mean?\"},\n",
    "    # 설정 옵션으로 세션 ID를 \"redis123\" 로 지정합니다.\n",
    "    config={\"configurable\": {\"session_id\": \"redis123\"}},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='cosine은 삼각함수 중 하나로, 직각 삼각형에서 밑변과 빗변의 비율을 의미합니다.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 47, 'prompt_tokens': 122, 'total_tokens': 169, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-C6nWkOvbp0oOHoIFp9VFeXtXbcFQe', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--aedd2ecd-3b7f-47d6-8c38-650c0b80115e-0', usage_metadata={'input_tokens': 122, 'output_tokens': 47, 'total_tokens': 169, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke(\n",
    "    # 이전 답변에 대한 한글 번역을 요청합니다.\n",
    "    {\"ability\": \"math\", \"input\": \"이전의 답변을 한글로 번역해 주세요.\"},\n",
    "    # 설정 값으로 세션 ID를 \"foobar\"로 지정합니다.\n",
    "    config={\"configurable\": {\"session_id\": \"redis123\"}},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='수학에 능하신 비서입니다.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 60, 'total_tokens': 73, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-C6nX8MfEZrT3XTLOZjGwUxoRdWOh0', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--373918c4-f05f-47fb-95ca-59beebbb185b-0', usage_metadata={'input_tokens': 60, 'output_tokens': 13, 'total_tokens': 73, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke(\n",
    "    # 이전 답변에 대한 한글 번역을 요청합니다.\n",
    "    {\"ability\": \"math\", \"input\": \"이전의 답변을 한글로 번역해 주세요.\"},\n",
    "    # 설정 값으로 세션 ID를 \"redis456\"로 지정합니다.\n",
    "    config={\"configurable\": {\"session_id\": \"redis456\"}},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "사용자 정의 제네레이터(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterator, List\n",
    "\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    # 주어진 회사와 유사한 5개의 회사를 쉼표로 구분된 목록으로 작성하세요.\n",
    "    \"Write a comma-separated list of 5 companies similar to: {company}\"\n",
    ")\n",
    "# 온도를 0.0으로 설정하여 ChatOpenAI 모델을 초기화합니다.\n",
    "model = ChatOpenAI(temperature=0.0, model=\"gpt-4.1\")\n",
    "\n",
    "# 프롬프트와 모델을 연결하고 문자열 출력 파서를 적용하여 체인을 생성합니다.\n",
    "str_chain = prompt | model | StrOutputParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microsoft, Apple, Amazon, Meta, IBM"
     ]
    }
   ],
   "source": [
    "# 데이터를 스트리밍합니다.\n",
    "for chunk in str_chain.stream({\"company\": \"Google\"}):\n",
    "    # 각 청크를 출력하고, 줄 바꿈 없이 버퍼를 즉시 플러시합니다.\n",
    "    print(chunk, end=\"\", flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력으로 llm 토큰의 반복자를 받아 쉼표로 구분된 문자열 리스트로 분할하는 사용자 정의 파서입니다.\n",
    "def split_into_list(input: Iterator[str]) -> Iterator[List[str]]:\n",
    "    # 쉼표가 나올 때까지 부분 입력을 보관합니다.\n",
    "    buffer = \"\"\n",
    "    for chunk in input:\n",
    "        # 현재 청크를 버퍼에 추가합니다.\n",
    "        buffer += chunk\n",
    "        # 버퍼에 쉼표가 있는 동안 반복합니다.\n",
    "        while \",\" in buffer:\n",
    "            # 버퍼를 쉼표로 분할합니다.\n",
    "            comma_index = buffer.index(\",\")\n",
    "            # 쉼표 이전의 모든 내용을 반환합니다.\n",
    "            yield [buffer[:comma_index].strip()]\n",
    "            # 나머지는 다음 반복을 위해 저장합니다.\n",
    "            buffer = buffer[comma_index + 1 :]\n",
    "    # 마지막 청크를 반환합니다.\n",
    "    yield [buffer.strip()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_chain = str_chain | split_into_list  # 문자열 체인을 리스트로 분할합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Microsoft']\n",
      "['Apple']\n",
      "['Amazon']\n",
      "['Meta']\n",
      "['IBM']\n"
     ]
    }
   ],
   "source": [
    "# 생성한 list_chain 이 문제없이 스트리밍되는지 확인합니다.\n",
    "for chunk in list_chain.stream({\"company\": \"Google\"}):\n",
    "    print(chunk, flush=True)  # 각 청크를 출력하고, 버퍼를 즉시 플러시합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Microsoft', 'Apple', 'Amazon', 'Meta', 'IBM']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list_chain 에 데이터를 invoke 합니다.\n",
    "list_chain.invoke({\"company\": \"Google\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import AsyncIterator\n",
    "\n",
    "\n",
    "# 비동기 함수 정의\n",
    "async def asplit_into_list(input: AsyncIterator[str]) -> AsyncIterator[List[str]]:\n",
    "    buffer = \"\"\n",
    "    # `input`은 `async_generator` 객체이므로 `async for`를 사용\n",
    "    async for chunk in input:\n",
    "        buffer += chunk\n",
    "        while \",\" in buffer:\n",
    "            comma_index = buffer.index(\",\")\n",
    "            yield [\n",
    "                buffer[:comma_index].strip()\n",
    "            ]  # 쉼표를 기준으로 분할하여 리스트로 반환\n",
    "            buffer = buffer[comma_index + 1:]\n",
    "    yield [buffer.strip()]  # 남은 버퍼 내용을 리스트로 반환\n",
    "\n",
    "\n",
    "# alist_chain 과 asplit_into_list 를 파이프라인으로 연결\n",
    "alist_chain = str_chain | asplit_into_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Microsoft']\n",
      "['Apple']\n",
      "['Amazon']\n",
      "['Meta']\n",
      "['IBM']\n"
     ]
    }
   ],
   "source": [
    "# async for 루프를 사용하여 데이터를 스트리밍합니다.\n",
    "async for chunk in alist_chain.astream({\"company\": \"Google\"}):\n",
    "    # 각 청크를 출력하고 버퍼를 비웁니다.\n",
    "    print(chunk, flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Microsoft', 'Apple', 'Amazon', 'Meta', 'IBM']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 리스트 체인을 비동기적으로 호출합니다.\n",
    "await alist_chain.ainvoke({\"company\": \"Google\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Runtime Arguments 바인딩\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EQUATION:  \n",
      "\\( x^3 + 7 = 12 \\)\n",
      "\n",
      "SOLUTION:  \n",
      "Subtract 7 from both sides:  \n",
      "\\( x^3 = 12 - 7 \\)  \n",
      "\\( x^3 = 5 \\)\n",
      "\n",
      "Take the cube root of both sides:  \n",
      "\\( x = \\sqrt[3]{5} \\)\n",
      "\n",
      "So,  \n",
      "\\( x = \\sqrt[3]{5} \\)\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            # 대수 기호를 사용하여 다음 방정식을 작성한 다음 풀이하세요.\n",
    "            \"Write out the following equation using algebraic symbols then solve it. \"\n",
    "            \"Use the format\\n\\nEQUATION:...\\nSOLUTION:...\\n\\n\",\n",
    "        ),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"{equation_statement}\",  # 사용자가 입력한 방정식 문장을 변수로 받습니다.\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "# ChatOpenAI 모델을 초기화하고 temperature를 0으로 설정합니다.\n",
    "model = ChatOpenAI(model=\"gpt-4.1\", temperature=0)\n",
    "\n",
    "# 방정식 문장을 입력받아 프롬프트에 전달하고, 모델에서 생성된 결과를 문자열로 파싱합니다.\n",
    "runnable = (\n",
    "    {\"equation_statement\": RunnablePassthrough()} | prompt | model | StrOutputParser()\n",
    ")\n",
    "\n",
    "# 예시 방정식 문장을 입력하여 결과를 출력합니다.\n",
    "print(runnable.invoke(\"x raised to the third plus seven equals 12\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EQUATION:  \n",
      "\\( x^3 + 7 = 12 \\)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "runnable = (\n",
    "    # 실행 가능한 패스스루 객체를 생성하여 \"equation_statement\" 키에 할당합니다.\n",
    "    {\"equation_statement\": RunnablePassthrough()}\n",
    "    | prompt  # 프롬프트를 파이프라인에 추가합니다.\n",
    "    | model.bind(\n",
    "        stop=\"SOLUTION\"\n",
    "    )  # 모델을 바인딩하고 \"SOLUTION\" 토큰에서 생성을 중지하도록 설정합니다.\n",
    "    | StrOutputParser()  # 문자열 출력 파서를 파이프라인에 추가합니다.\n",
    ")\n",
    "# \"x raised to the third plus seven equals 12\"라는 입력으로 파이프라인을 실행하고 결과를 출력합니다.\n",
    "print(runnable.invoke(\"x raised to the third plus seven equals 12\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_function = {\n",
    "    \"name\": \"solver\",  # 함수의 이름\n",
    "    # 함수의 설명: 방정식을 수립하고 해결합니다.\n",
    "    \"description\": \"Formulates and solves an equation\",\n",
    "    \"parameters\": {  # 함수의 매개변수\n",
    "        \"type\": \"object\",  # 매개변수의 타입: 객체\n",
    "        \"properties\": {  # 매개변수의 속성\n",
    "            \"equation\": {  # 방정식 속성\n",
    "                \"type\": \"string\",  # 방정식의 타입: 문자열\n",
    "                \"description\": \"The algebraic expression of the equation\",  # 방정식의 대수식 표현\n",
    "            },\n",
    "            \"solution\": {  # 해답 속성\n",
    "                \"type\": \"string\",  # 해답의 타입: 문자열\n",
    "                \"description\": \"The solution to the equation\",  # 방정식의 해답\n",
    "            },\n",
    "        },\n",
    "        \"required\": [\"equation\", \"solution\"],  # 필수 매개변수: 방정식과 해답\n",
    "    },\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"equation\":\"x^3 + 7 = 12\",\"solution\":\"x^3 = 12 - 7 = 5; x = 5^(1/3)\"}', 'name': 'solver'}, 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 39, 'prompt_tokens': 95, 'total_tokens': 134, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-2025-04-14', 'system_fingerprint': 'fp_3502f4eb73', 'id': 'chatcmpl-C6ncYrFNd6qOBDmpeZAybrRdZv3GG', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--580a6dc6-f2a3-483d-ad8b-cf7637a54592-0', usage_metadata={'input_tokens': 95, 'output_tokens': 39, 'total_tokens': 134, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 다음 방정식을 대수 기호를 사용하여 작성한 다음 해결하세요.\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Write out the following equation using algebraic symbols then solve it.\",\n",
    "        ),\n",
    "        (\"human\", \"{equation_statement}\"),\n",
    "    ]\n",
    ")\n",
    "model = ChatOpenAI(model=\"gpt-4.1\", temperature=0).bind(\n",
    "    function_call={\"name\": \"solver\"},  # openai_function schema 를 바인딩합니다.\n",
    "    functions=[openai_function],\n",
    ")\n",
    "runnable = {\"equation_statement\": RunnablePassthrough()} | prompt | model\n",
    "# x의 세제곱에 7을 더하면 12와 같다\n",
    "runnable.invoke(\"x raised to the third plus seven equals 12\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_current_weather\",  # 현재 날씨를 가져오는 함수의 이름\n",
    "            \"description\": \"주어진 위치의 현재 날씨를 가져옵니다\",  # 함수에 대한 설명\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"도시와 주, 예: San Francisco, CA\",  # 위치 매개변수에 대한 설명\n",
    "                    },\n",
    "                    # 온도 단위 매개변수 (섭씨 또는 화씨)\n",
    "                    \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n",
    "                },\n",
    "                \"required\": [\"location\"],  # 필수 매개변수 지정\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_kD6Qsr4eCLXiqFfLNgHn4QQ8', 'function': {'arguments': '{\"location\": \"San Francisco, CA\"}', 'name': 'get_current_weather'}, 'type': 'function'}, {'id': 'call_sWMMP2PVbbnEvwpQMCPevXod', 'function': {'arguments': '{\"location\": \"New York, NY\"}', 'name': 'get_current_weather'}, 'type': 'function'}, {'id': 'call_XSm8r03yuNkB4s8tBeh6WfGM', 'function': {'arguments': '{\"location\": \"Los Angeles, CA\"}', 'name': 'get_current_weather'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 70, 'prompt_tokens': 99, 'total_tokens': 169, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-2025-04-14', 'system_fingerprint': 'fp_daf5fcc80a', 'id': 'chatcmpl-C6nczCX1kiRWE39M6f8IsovtcQl8c', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--6effe5a1-3dcf-4614-94ed-042db292e3ef-0', tool_calls=[{'name': 'get_current_weather', 'args': {'location': 'San Francisco, CA'}, 'id': 'call_kD6Qsr4eCLXiqFfLNgHn4QQ8', 'type': 'tool_call'}, {'name': 'get_current_weather', 'args': {'location': 'New York, NY'}, 'id': 'call_sWMMP2PVbbnEvwpQMCPevXod', 'type': 'tool_call'}, {'name': 'get_current_weather', 'args': {'location': 'Los Angeles, CA'}, 'id': 'call_XSm8r03yuNkB4s8tBeh6WfGM', 'type': 'tool_call'}], usage_metadata={'input_tokens': 99, 'output_tokens': 70, 'total_tokens': 169, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ChatOpenAI 모델을 초기화하고 도구를 바인딩합니다.\n",
    "model = ChatOpenAI(model=\"gpt-4.1\").bind(tools=tools)\n",
    "# 모델을 호출하여 샌프란시스코, 뉴욕, 로스앤젤레스의 날씨에 대해 질문합니다.\n",
    "model.invoke(\"샌프란시스코, 뉴욕, 로스앤젤레스의 현재 날씨에 대해 알려줘?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "폴백(fallback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_openai import ChatOpenAI\n",
    "from unittest.mock import patch\n",
    "\n",
    "import httpx\n",
    "from openai import RateLimitError\n",
    "\n",
    "request = httpx.Request(\"GET\", \"/\")  # GET 요청을 생성합니다.\n",
    "response = httpx.Response(\n",
    "    200, request=request\n",
    ")  # 200 상태 코드와 함께 응답을 생성합니다.\n",
    "# \"rate limit\" 메시지와 응답 및 빈 본문을 포함하는 RateLimitError를 생성합니다.\n",
    "error = RateLimitError(\"rate limit\", response=response, body=\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI의 ChatOpenAI 모델을 사용하여 openai_llm 객체를 생성합니다.\n",
    "# max_retries를 0으로 설정하여 속도 제한 등으로 인한 재시도를 방지합니다.\n",
    "openai_llm = ChatOpenAI(max_retries=0)\n",
    "\n",
    "# Anthropic의 ChatAnthropic 모델을 사용하여 anthropic_llm 객체를 생성합니다.\n",
    "anthropic_llm = ChatAnthropic(model=\"claude-3-7-sonnet-latest\")\n",
    "\n",
    "# openai_llm을 기본으로 사용하고, 실패 시 anthropic_llm을 대체로 사용하도록 설정합니다.\n",
    "llm = openai_llm.with_fallbacks([anthropic_llm])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "에러 발생\n"
     ]
    }
   ],
   "source": [
    "# OpenAI LLM을 먼저 사용하여 오류가 발생하는 것을 보여줍니다.\n",
    "with patch(\"openai.resources.chat.completions.Completions.create\", side_effect=error):\n",
    "    try:\n",
    "        # \"닭이 길을 건넌 이유는 무엇일까요?\"라는 질문을 OpenAI LLM에 전달합니다.\n",
    "        print(openai_llm.invoke(\"Why did the chicken cross the road?\"))\n",
    "    except RateLimitError:\n",
    "        # 오류가 발생하면 오류를 출력합니다.\n",
    "        print(\"에러 발생\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='대한민국의 수도는 서울입니다. 서울은 한국의 정치, 경제, 문화의 중심지로, 한강을 따라 위치해 있으며 약 1,000만 명의 인구가 거주하고 있습니다.' additional_kwargs={} response_metadata={'id': 'msg_01RkfQnAV95JN64eStzQ5sEQ', 'model': 'claude-3-7-sonnet-20250219', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'cache_creation': {'ephemeral_1h_input_tokens': 0, 'ephemeral_5m_input_tokens': 0}, 'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 22, 'output_tokens': 86, 'server_tool_use': None, 'service_tier': 'standard'}, 'model_name': 'claude-3-7-sonnet-20250219'} id='run--fa7bd93e-fc18-4eb3-a6e2-400561dd368a-0' usage_metadata={'input_tokens': 22, 'output_tokens': 86, 'total_tokens': 108, 'input_token_details': {'cache_read': 0, 'cache_creation': 0, 'ephemeral_5m_input_tokens': 0, 'ephemeral_1h_input_tokens': 0}}\n"
     ]
    }
   ],
   "source": [
    "# OpenAI API 호출 시 에러가 발생하는 경우 Anthropic 으로 대체하는 코드\n",
    "with patch(\"openai.resources.chat.completions.Completions.create\", side_effect=error):\n",
    "    try:\n",
    "        # \"대한민국의 수도는 어디야?\"라는 질문을 언어 모델에 전달하여 응답을 출력합니다.\n",
    "        print(llm.invoke(\"대한민국의 수도는 어디야?\"))\n",
    "    except RateLimitError:\n",
    "        # RateLimitError가 발생하면 \"에러 발생\"를 출력합니다.\n",
    "        print(\"에러 발생\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='대한민국의 수도는 서울입니다.' additional_kwargs={} response_metadata={'id': 'msg_01QVVeJa9NfUSXFfHmWvVsZy', 'model': 'claude-3-7-sonnet-20250219', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'cache_creation': {'ephemeral_1h_input_tokens': 0, 'ephemeral_5m_input_tokens': 0}, 'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 46, 'output_tokens': 20, 'server_tool_use': None, 'service_tier': 'standard'}, 'model_name': 'claude-3-7-sonnet-20250219'} id='run--783503ed-a00e-45f5-b2b6-773c577611e3-0' usage_metadata={'input_tokens': 46, 'output_tokens': 20, 'total_tokens': 66, 'input_token_details': {'cache_read': 0, 'cache_creation': 0, 'ephemeral_5m_input_tokens': 0, 'ephemeral_1h_input_tokens': 0}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"질문에 짧고 간결하게 답변해 주세요.\",  # 시스템 역할 설명\n",
    "        ),\n",
    "        (\"human\", \"{country} 의 수도는 어디입니까?\"),  # 사용자 질문 템플릿\n",
    "    ]\n",
    ")\n",
    "chain = prompt | llm  # 프롬프트와 언어 모델을 연결하여 체인 생성\n",
    "# chain = prompt | ChatOpenAI() # 이 코드이 주석을 풀고 실행하면 \"오류 발생\" 문구가 출력됩니다.\n",
    "with patch(\"openai.resources.chat.completions.Completions.create\", side_effect=error):\n",
    "    try:\n",
    "        print(chain.invoke({\"country\": \"대한민국\"}))  # 체인을 호출하여 결과 출력\n",
    "    except RateLimitError:  # API 비용 제한 오류 처리\n",
    "        print(\"오류 발생\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "오류 발생\n"
     ]
    }
   ],
   "source": [
    "llm = openai_llm.with_fallbacks(\n",
    "    # 대체 LLM으로 anthropic_llm을 사용하고, 예외 처리할 대상으로 KeyboardInterrupt를 지정합니다.\n",
    "    [anthropic_llm],\n",
    "    exceptions_to_handle=(KeyboardInterrupt,),  # 예외 처리 대상을 지정합니다.\n",
    ")\n",
    "\n",
    "# 프롬프트와 LLM을 연결하여 체인을 생성합니다.\n",
    "chain = prompt | llm\n",
    "with patch(\"openai.resources.chat.completions.Completions.create\", side_effect=error):\n",
    "    try:\n",
    "        # 체인을 호출하여 결과를 출력합니다.\n",
    "        print(chain.invoke({\"country\": \"대한민국\"}))\n",
    "    except RateLimitError:\n",
    "        # RateLimitError 예외가 발생하면 \"오류 발생\"를 출력합니다.\n",
    "        print(\"오류 발생\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 프롬프트 생성\n",
    "prompt_template = (\n",
    "    \"질문에 짧고 간결하게 답변해 주세요.\\n\\nQuestion:\\n{question}\\n\\nAnswer:\"\n",
    ")\n",
    "prompt = PromptTemplate.from_template(prompt_template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기서는 쉽게 오류를 발생시킬 수 있는 잘못된 모델 이름을 사용하여 체인을 생성할 것입니다.\n",
    "chat_model = ChatOpenAI(model_name=\"gpt-fake\")\n",
    "bad_chain = prompt | chat_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fallback 체인을 생성합니다.\n",
    "fallback_chain1 = prompt | ChatOpenAI(model=\"gpt-3.6-turbo\") # 오류\n",
    "fallback_chain2 = prompt | ChatOpenAI(model=\"gpt-3.5-turbo\") # 정상\n",
    "fallback_chain3 = prompt | ChatOpenAI(model=\"gpt-4-turbo-preview\") # 정상\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='서울입니다.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 46, 'total_tokens': 51, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-C6npD5369HoKl4K1gJpslfyxyy0sr', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--6cec6dd5-6978-4e3f-b101-aa45acf5950f-0', usage_metadata={'input_tokens': 46, 'output_tokens': 5, 'total_tokens': 51, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 두 개의 체인을 결합하여 최종 체인을 생성합니다.\n",
    "chain = bad_chain.with_fallbacks(\n",
    "    [fallback_chain1, fallback_chain2, fallback_chain3])\n",
    "# 생성된 체인을 호출하여 입력값을 전달합니다.\n",
    "chain.invoke({\"question\": \"대한민국의 수도는 어디야?\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMs2OSEsEhI+eP+P38efuYH",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
