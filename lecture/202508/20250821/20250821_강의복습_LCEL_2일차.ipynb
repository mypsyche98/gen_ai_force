{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install PyDrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì•„ë˜ ë§í¬ë¥¼ ë³µì‚¬í•˜ì—¬ ì›¹ ë¸Œë¼ìš°ì €ì— ë¶™ì—¬ë„£ìœ¼ì„¸ìš”.\n",
      "https://accounts.google.com/o/oauth2/auth?client_id=35726703810-4v13dfqmilhgv6shlc3cv9i3ktuh73j1.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&access_type=offline&response_type=code\n",
      "Authentication successful.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import mykeys\n",
    "\n",
    "project_name = 'CH13_LCEL'\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = project_name\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = mykeys.get_key('LANG')\n",
    "os.environ[\"LANGCHAIN_HUB_API_KEY\"] = mykeys.get_key('LANG')\n",
    "os.environ[\"OPENAI_API_KEY\"] = mykeys.get_key('GPT')\n",
    "os.environ[\"GOOGLE_API_KEY\"] = mykeys.get_key('GOO')\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = mykeys.get_key('HF')\n",
    "os.environ[\"UPSTAGE_API_KEY\"] = mykeys.get_key('UP')\n",
    "os.environ[\"COHERE_API_KEY\"] = mykeys.get_key('COH')\n",
    "os.environ[\"JINA_API_KEY\"] = mykeys.get_key('JINA')\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = mykeys.get_key('ANT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith ì¶”ì ì„ ì‹œì‘í•©ë‹ˆë‹¤.\n",
      "[í”„ë¡œì íŠ¸ëª…]\n",
      "CH13_LCEL\n"
     ]
    }
   ],
   "source": [
    "from langchain_teddynote import logging\n",
    "\n",
    "# í”„ë¡œì íŠ¸ ì´ë¦„ì„ ì…ë ¥í•©ë‹ˆë‹¤.\n",
    "logging.langsmith(project_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote import logging\n",
    "\n",
    "# set_enable=False ë¡œ ì§€ì •í•˜ë©´ ì¶”ì ì„ í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n",
    "logging.langsmith(project_name, set_enable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CH13 LangChain Expression Language(LCEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@chain ë°ì½”ë ˆì´í„°ë¡œ Runnable êµ¬ì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import chain\n",
    "from langchain_openai import ChatOpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ ì •ì˜í•©ë‹ˆë‹¤.\n",
    "prompt1 = ChatPromptTemplate.from_template(\"{topic} ì— ëŒ€í•´ ì§§ê²Œ í•œê¸€ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”.\")\n",
    "prompt2 = ChatPromptTemplate.from_template(\n",
    "    \"{sentence} ë¥¼ emojië¥¼ í™œìš©í•œ ì¸ìŠ¤íƒ€ê·¸ë¨ ê²Œì‹œê¸€ë¡œ ë§Œë“¤ì–´ì£¼ì„¸ìš”.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@chain\n",
    "def custom_chain(text):\n",
    "    # ì²« ë²ˆì§¸ í”„ë¡¬í”„íŠ¸, ChatOpenAI, ë¬¸ìì—´ ì¶œë ¥ íŒŒì„œë¥¼ ì—°ê²°í•˜ì—¬ ì²´ì¸ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "    chain1 = prompt1 | ChatOpenAI(model=\"gpt-4o-mini\") | StrOutputParser()\n",
    "    output1 = chain1.invoke({\"topic\": text})\n",
    "\n",
    "    # ë‘ ë²ˆì§¸ í”„ë¡¬í”„íŠ¸, ChatOpenAI, ë¬¸ìì—´ ì¶œë ¥ íŒŒì„œë¥¼ ì—°ê²°í•˜ì—¬ ì²´ì¸ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "    chain2 = prompt2 | ChatOpenAI(model=\"gpt-4o-mini\") | StrOutputParser()\n",
    "    # ë‘ ë²ˆì§¸ ì²´ì¸ì„ í˜¸ì¶œí•˜ì—¬ íŒŒì‹±ëœ ì²« ë²ˆì§¸ ê²°ê³¼ë¥¼ ì „ë‹¬í•˜ê³  ìµœì¢… ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "    return chain2.invoke({\"sentence\": output1})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸŒŒâœ¨ **ì–‘ìì—­í•™ì˜ ë§¤ë ¥!** âœ¨ğŸŒŒ\n",
      "\n",
      "ì–‘ìì—­í•™ì€ ë¬¼ë¦¬í•™ì˜ í•œ ë¶„ì•¼ë¡œ, ë¯¸ì‹œ ì„¸ê³„ì—ì„œ ì…ìì™€ ì—ë„ˆì§€ì˜ í–‰ë™ì„ ì„¤ëª…í•˜ëŠ” ì´ë¡ ì´ì—ìš”! ğŸ”âœ¨\n",
      "\n",
      "ê³ ì „ ë¬¼ë¦¬í•™ê³¼ëŠ” ë‹¤ë¥´ê²Œ, ì–‘ìì—­í•™ì—ì„œëŠ” ì…ìê°€ íŠ¹ì •í•œ ìœ„ì¹˜ë‚˜ ì†ë„ë¥¼ ê°€ì§€ì§€ ì•Šê³ , ğŸŒ€ í™•ë¥ ë¡œ ì„¤ëª…ëœë‹µë‹ˆë‹¤! ë˜, íŒŒë™-ì…ì ì´ì¤‘ì„±ì„ í†µí•´ ë¯¸ì„¸ ì…ìë“¤ì´ ì–´ë–»ê²Œ í–‰ë™í•˜ëŠ”ì§€ ì´í•´í•  ìˆ˜ ìˆì–´ìš”. ğŸŒŠâ¡ï¸ğŸ”¹\n",
      "\n",
      "ì£¼ìš” ê°œë…ë“¤:\n",
      "- ì–‘ì ìƒíƒœ ğŸŒ€\n",
      "- ë¶ˆí™•ì •ì„± ì›ë¦¬ â“ğŸ”„\n",
      "- ì–‘ì ì–½í˜ ğŸ¤âœ¨\n",
      "\n",
      "ì´ ì´ë¡ ë“¤ì€ ì›ì, ì „ì, ê´‘ìì™€ ê°™ì€ ë¯¸ì„¸ ì…ìì˜ í–‰ë™ì„ ì´í•´í•˜ëŠ” ë° í•„ìˆ˜ì ì´ì—ìš”. ğŸ’¡ğŸ”¬ ê·¸ë¦¬ê³  ì–‘ìì—­í•™ì€ í˜„ëŒ€ ë¬¼ë¦¬í•™, í™”í•™, ì „ìê³µí•™ ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì— ì‘ìš©ë˜ê³  ìˆë‹µë‹ˆë‹¤! ğŸŒğŸ“ˆ\n",
      "\n",
      "ì–‘ìì—­í•™ì˜ ì„¸ê³„ì— í•¨ê»˜ ë¹ ì ¸ë³´ì„¸ìš”! ğŸš€ğŸ’– #ì–‘ìì—­í•™ #ë¬¼ë¦¬í•™ #ê³¼í•™ #ë¯¸ì„¸ì„¸ê³„ #íŒŒë™ì…ìì´ì¤‘ì„± #ì–‘ììƒíƒœ #ë¶ˆí™•ì •ì„±ì›ë¦¬ #ê³¼í•™ì˜ì•„ë¦„ë‹¤ì›€\n"
     ]
    }
   ],
   "source": [
    "# custom_chainì„ í˜¸ì¶œ\n",
    "print(custom_chain.invoke(\"ì–‘ìì—­í•™\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RunnableWithMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI()\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"ë‹¹ì‹ ì€ {ability} ì— ëŠ¥ìˆ™í•œ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. 20ì ì´ë‚´ë¡œ ì‘ë‹µí•˜ì„¸ìš”\",\n",
    "        ),\n",
    "        # ëŒ€í™” ê¸°ë¡ì„ ë³€ìˆ˜ë¡œ ì‚¬ìš©, history ê°€ MessageHistory ì˜ key ê°€ ë¨\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{input}\"),  # ì‚¬ìš©ì ì…ë ¥ì„ ë³€ìˆ˜ë¡œ ì‚¬ìš©\n",
    "    ]\n",
    ")\n",
    "runnable = prompt | model  # í”„ë¡¬í”„íŠ¸ì™€ ëª¨ë¸ì„ ì—°ê²°í•˜ì—¬ runnable ê°ì²´ ìƒì„±\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "store = {}  # ì„¸ì…˜ ê¸°ë¡ì„ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬\n",
    "\n",
    "\n",
    "# ì„¸ì…˜ IDë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì„¸ì…˜ ê¸°ë¡ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜\n",
    "def get_session_history(session_ids: str) -> BaseChatMessageHistory:\n",
    "    print(session_ids)\n",
    "    if session_ids not in store:  # ì„¸ì…˜ IDê°€ storeì— ì—†ëŠ” ê²½ìš°\n",
    "        # ìƒˆë¡œìš´ ChatMessageHistory ê°ì²´ë¥¼ ìƒì„±í•˜ì—¬ storeì— ì €ì¥\n",
    "        store[session_ids] = ChatMessageHistory()\n",
    "    return store[session_ids]  # í•´ë‹¹ ì„¸ì…˜ IDì— ëŒ€í•œ ì„¸ì…˜ ê¸°ë¡ ë°˜í™˜\n",
    "\n",
    "\n",
    "with_message_history = (\n",
    "    RunnableWithMessageHistory(  # RunnableWithMessageHistory ê°ì²´ ìƒì„±\n",
    "        runnable,  # ì‹¤í–‰í•  Runnable ê°ì²´\n",
    "        get_session_history,  # ì„¸ì…˜ ê¸°ë¡ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜\n",
    "        input_messages_key=\"input\",  # ì…ë ¥ ë©”ì‹œì§€ì˜ í‚¤\n",
    "        history_messages_key=\"history\",  # ê¸°ë¡ ë©”ì‹œì§€ì˜ í‚¤\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abc123\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Cosine represents the ratio of the adjacent side to the hypotenuse in a right triangle.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 47, 'total_tokens': 66, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-C6nPziFq81VdfVphmYS818MYvLLfS', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--3aefd77b-2b9f-4ae8-8f91-e645382741c7-0', usage_metadata={'input_tokens': 47, 'output_tokens': 19, 'total_tokens': 66, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke(\n",
    "    # ìˆ˜í•™ ê´€ë ¨ ì§ˆë¬¸ \"ì½”ì‚¬ì¸ì˜ ì˜ë¯¸ëŠ” ë¬´ì—‡ì¸ê°€ìš”?\"ë¥¼ ì…ë ¥ìœ¼ë¡œ ì „ë‹¬í•©ë‹ˆë‹¤.\n",
    "    {\"ability\": \"math\", \"input\": \"What does cosine mean?\"},\n",
    "    # ì„¤ì • ì •ë³´ë¡œ ì„¸ì…˜ ID \"abc123\"ì„ ì „ë‹¬í•©ë‹ˆë‹¤.\n",
    "    config={\"configurable\": {\"session_id\": \"abc123\"}},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abc123\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='ì½”ì‚¬ì¸ì€ ì§ê°ì‚¼ê°í˜•ì—ì„œ ì¸ì ‘ë³€ê³¼ ë¹—ë³€ì˜ ë¹„ìœ¨ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 40, 'prompt_tokens': 91, 'total_tokens': 131, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-C6nQEwlV2px9xUQJqmwXZUw949Xfm', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--598668bc-75bf-4dd9-8e1c-aaffdb1b20b1-0', usage_metadata={'input_tokens': 91, 'output_tokens': 40, 'total_tokens': 131, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ë©”ì‹œì§€ ê¸°ë¡ì„ í¬í•¨í•˜ì—¬ í˜¸ì¶œí•©ë‹ˆë‹¤.\n",
    "with_message_history.invoke(\n",
    "    # ëŠ¥ë ¥ê³¼ ì…ë ¥ì„ ì„¤ì •í•©ë‹ˆë‹¤.\n",
    "    {\"ability\": \"math\", \"input\": \"ì´ì „ì˜ ë‚´ìš©ì„ í•œê¸€ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”.\"},\n",
    "    # ì„¤ì • ì˜µì…˜ì„ ì§€ì •í•©ë‹ˆë‹¤.\n",
    "    config={\"configurable\": {\"session_id\": \"abc123\"}},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def234\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='ìˆ˜í•™ì— ëŠ¥ìˆ™í•œ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 58, 'total_tokens': 77, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-C6nQQQl4wRvPNWlJcuFX2haue7bLv', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--76ad5d4e-7298-4392-a9ae-2981ee9e2461-0', usage_metadata={'input_tokens': 58, 'output_tokens': 19, 'total_tokens': 77, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ìƒˆë¡œìš´ session_idë¡œ ì¸í•´ ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ê¸°ì–µí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n",
    "with_message_history.invoke(\n",
    "    # ìˆ˜í•™ ëŠ¥ë ¥ê³¼ ì…ë ¥ ë©”ì‹œì§€ë¥¼ ì „ë‹¬í•©ë‹ˆë‹¤.\n",
    "    {\"ability\": \"math\", \"input\": \"ì´ì „ì˜ ë‚´ìš©ì„ í•œê¸€ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”\"},\n",
    "    # ìƒˆë¡œìš´ session_idë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.\n",
    "    config={\"configurable\": {\"session_id\": \"def234\"}},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import ConfigurableFieldSpec\n",
    "\n",
    "store = {}  # ë¹ˆ ë”•ì…”ë„ˆë¦¬ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.\n",
    "\n",
    "\n",
    "def get_session_history(user_id: str, conversation_id: str) -> BaseChatMessageHistory:\n",
    "    # ì£¼ì–´ì§„ user_idì™€ conversation_idì— í•´ë‹¹í•˜ëŠ” ì„¸ì…˜ ê¸°ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "    if (user_id, conversation_id) not in store:\n",
    "        # í•´ë‹¹ í‚¤ê°€ storeì— ì—†ìœ¼ë©´ ìƒˆë¡œìš´ ChatMessageHistoryë¥¼ ìƒì„±í•˜ì—¬ ì €ì¥í•©ë‹ˆë‹¤.\n",
    "        store[(user_id, conversation_id)] = ChatMessageHistory()\n",
    "    return store[(user_id, conversation_id)]\n",
    "\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    runnable,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\",\n",
    "    history_factory_config=[  # ê¸°ì¡´ì˜ \"session_id\" ì„¤ì •ì„ ëŒ€ì²´í•˜ê²Œ ë©ë‹ˆë‹¤.\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"user_id\",  # get_session_history í•¨ìˆ˜ì˜ ì²« ë²ˆì§¸ ì¸ìë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.\n",
    "            annotation=str,\n",
    "            name=\"User ID\",\n",
    "            description=\"ì‚¬ìš©ìì˜ ê³ ìœ  ì‹ë³„ìì…ë‹ˆë‹¤.\",\n",
    "            default=\"\",\n",
    "            is_shared=True,\n",
    "        ),\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"conversation_id\",  # get_session_history í•¨ìˆ˜ì˜ ë‘ ë²ˆì§¸ ì¸ìë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.\n",
    "            annotation=str,\n",
    "            name=\"Conversation ID\",\n",
    "            description=\"ëŒ€í™”ì˜ ê³ ìœ  ì‹ë³„ìì…ë‹ˆë‹¤.\",\n",
    "            default=\"\",\n",
    "            is_shared=True,\n",
    "        ),\n",
    "    ],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output_message': AIMessage(content='Cosine is a mathematical function that, given an angle in a right-angled triangle, is equal to the ratio of the length of the side adjacent to the given angle to the length of the hypotenuse of the triangle. It is typically denoted as cos.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 54, 'prompt_tokens': 14, 'total_tokens': 68, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-C6nRSyajajZN0dypzZMJAES38PQnv', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--3c72babe-4b0b-4d74-b132-0dc4d9df5578-0', usage_metadata={'input_tokens': 14, 'output_tokens': 54, 'total_tokens': 68, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "# chain ìƒì„±\n",
    "chain = RunnableParallel({\"output_message\": ChatOpenAI()})\n",
    "\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    # ì„¸ì…˜ IDì— í•´ë‹¹í•˜ëŠ” ëŒ€í™” ê¸°ë¡ì´ ì €ì¥ì†Œì— ì—†ìœ¼ë©´ ìƒˆë¡œìš´ ChatMessageHistoryë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    # ì„¸ì…˜ IDì— í•´ë‹¹í•˜ëŠ” ëŒ€í™” ê¸°ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "# ì²´ì¸ì— ëŒ€í™” ê¸°ë¡ ê¸°ëŠ¥ì„ ì¶”ê°€í•œ RunnableWithMessageHistory ê°ì²´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    # ì…ë ¥ ë©”ì‹œì§€ì˜ í‚¤ë¥¼ \"input\"ìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.(ìƒëµì‹œ Message ê°ì²´ë¡œ ì…ë ¥)\n",
    "    # input_messages_key=\"input\",\n",
    "    # ì¶œë ¥ ë©”ì‹œì§€ì˜ í‚¤ë¥¼ \"output_message\"ë¡œ ì„¤ì •í•©ë‹ˆë‹¤. (ìƒëµì‹œ Message ê°ì²´ë¡œ ì¶œë ¥)\n",
    "    output_messages_key=\"output_message\",\n",
    ")\n",
    "\n",
    "# ì£¼ì–´ì§„ ë©”ì‹œì§€ì™€ ì„¤ì •ìœ¼ë¡œ ì²´ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.\n",
    "with_message_history.invoke(\n",
    "    # í˜¹ì€ \"what is the definition of cosine?\" ë„ ê°€ëŠ¥\n",
    "    [HumanMessage(content=\"what is the definition of cosine?\")],\n",
    "    config={\"configurable\": {\"session_id\": \"abc123\"}},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output_message': AIMessage(content='ì½”ì‚¬ì¸ì€ ì§ê° ì‚¼ê°í˜•ì—ì„œ ì£¼ì–´ì§„ ê°ì— ëŒ€í•´ ì¸ì ‘í•œ ë³€ì˜ ê¸¸ì´ì™€ ë¹—ë³€ì˜ ê¸¸ì´ì˜ ë¹„ìœ¨ê³¼ ê°™ì€ ìˆ˜í•™ì ì¸ í•¨ìˆ˜ì…ë‹ˆë‹¤. ì¼ë°˜ì ìœ¼ë¡œ cosë¡œ í‘œì‹œë©ë‹ˆë‹¤.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 72, 'prompt_tokens': 93, 'total_tokens': 165, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-C6nRflEL3mPvwPepfbrVc1iP7Rfk0', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--342eb67a-80e1-4a66-b66c-1134e2b400a3-0', usage_metadata={'input_tokens': 93, 'output_tokens': 72, 'total_tokens': 165, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke(\n",
    "    # ì´ì „ì˜ ë‹µë³€ì— ëŒ€í•˜ì—¬ í•œê¸€ë¡œ ë‹µë³€ì„ ì¬ìš”ì²­í•©ë‹ˆë‹¤.\n",
    "    [HumanMessage(content=\"ì´ì „ì˜ ë‚´ìš©ì„ í•œê¸€ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”!\")],\n",
    "    # ì„¤ì • ì˜µì…˜ì„ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ì „ë‹¬í•©ë‹ˆë‹¤.\n",
    "    config={\"configurable\": {\"session_id\": \"abc123\"}},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history = RunnableWithMessageHistory(\n",
    "    ChatOpenAI(),  # ChatOpenAI ì–¸ì–´ ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "    get_session_history,  # ëŒ€í™” ì„¸ì…˜ ê¸°ë¡ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜ë¥¼ ì§€ì •í•©ë‹ˆë‹¤.\n",
    "    # ì…ë ¥ ë©”ì‹œì§€ì˜ í‚¤ë¥¼ \"input\"ìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.(ìƒëµì‹œ Message ê°ì²´ë¡œ ì…ë ¥)\n",
    "    # input_messages_key=\"input\",\n",
    "    # ì¶œë ¥ ë©”ì‹œì§€ì˜ í‚¤ë¥¼ \"output_message\"ë¡œ ì„¤ì •í•©ë‹ˆë‹¤. (ìƒëµì‹œ Message ê°ì²´ë¡œ ì¶œë ¥)\n",
    "    # output_messages_key=\"output_message\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='ì½”ì‚¬ì¸ì€ ì‚¼ê°í•¨ìˆ˜ ì¤‘ í•˜ë‚˜ë¡œ, í•œ ê°ì˜ ì½”ì‚¬ì¸ ê°’ì„ êµ¬í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤. ì‚¼ê°í˜•ì˜ ë¹—ë³€ê³¼ ì¸ì ‘í•œ ë³€ì˜ ê¸¸ì´ì˜ ë¹„ìœ¨ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ê°ë„ê°€ ë°”ë€” ë•Œ ì½”ì‚¬ì¸ ê°’ë„ ë°”ë€Œê²Œ ë©ë‹ˆë‹¤. ì½”ì‚¬ì¸ í•¨ìˆ˜ëŠ” ì‚¼ê°í˜•ì´ë‚˜ ì›ì„ ë‹¤ë£¨ëŠ” ìˆ˜í•™ì—ì„œ ìì£¼ ì‚¬ìš©ë˜ë©°, ì£¼ë¡œ ì‚¼ê°ë¹„ë‚˜ ì‚¼ê°í•¨ìˆ˜ì™€ ê´€ë ¨ëœ ê³„ì‚°ì— ì´ìš©ë©ë‹ˆë‹¤.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 152, 'prompt_tokens': 24, 'total_tokens': 176, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-C6nSLPDEkp0XdkofaHLearHGyKdOI', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--dd81509e-9a1e-426e-818f-5d6ac69bb0fb-0', usage_metadata={'input_tokens': 24, 'output_tokens': 152, 'total_tokens': 176, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke(\n",
    "    # ì´ì „ì˜ ë‹µë³€ì— ëŒ€í•˜ì—¬ í•œê¸€ë¡œ ë‹µë³€ì„ ì¬ìš”ì²­í•©ë‹ˆë‹¤.\n",
    "    [HumanMessage(content=\"ì½”ì‚¬ì¸ì˜ ì˜ë¯¸ëŠ” ë¬´ì—‡ì¸ê°€ìš”?\")],\n",
    "    # ì„¤ì • ì˜µì…˜ì„ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ì „ë‹¬í•©ë‹ˆë‹¤.\n",
    "    config={\"configurable\": {\"session_id\": \"def123\"}},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    # \"input_messages\" í‚¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ì…ë ¥ ë©”ì‹œì§€ë¥¼ ê°€ì ¸ì™€ ChatOpenAI()ì— ì „ë‹¬í•©ë‹ˆë‹¤.\n",
    "    itemgetter(\"input_messages\") | ChatOpenAI(),\n",
    "    get_session_history,  # ì„¸ì…˜ ê¸°ë¡ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.\n",
    "    input_messages_key=\"input_messages\",  # ì…ë ¥ ë©”ì‹œì§€ì˜ í‚¤ë¥¼ ì§€ì •í•©ë‹ˆë‹¤.\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='ì½”ì‚¬ì¸ì€ ì‚¼ê°í•¨ìˆ˜ ì¤‘ í•˜ë‚˜ë¡œ, ì§ê°ì‚¼ê°í˜•ì˜ ë‘ ë³€ì˜ ê¸¸ì´ë¥¼ ì´ìš©í•˜ì—¬ ë§Œë“¤ì–´ì§€ëŠ” ê°ë„ì˜ ì½”ì‚¬ì¸ ê°’ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ê°„ë‹¨íˆ ë§í•´, ì½”ì‚¬ì¸ì€ \"ì¸ì ‘ë³€ì˜ ê¸¸ì´ ë‚˜ë¹—ë³€ì˜ ê¸¸ì´\"ë¡œ ì •ì˜ë˜ë©°, ì‚¼ê°í˜•ì˜ ê°ë„ì™€ ê°ì˜ ë³€ì„ ì‚¬ìš©í•˜ì—¬ ê³„ì‚°ë©ë‹ˆë‹¤. ì½”ì‚¬ì¸ì€ ì‚¼ê°í˜•ì˜ ê°ë„ë¥¼ ì´ìš©í•˜ì—¬ ë³€ì˜ ê¸¸ì´ë‚˜ ê°ì˜ í¬ê¸°ë¥¼ êµ¬í•˜ëŠ” ë° ì‚¬ìš©ë˜ë©°, ìˆ˜í•™ì ìœ¼ë¡œ ë§ì€ ì‘ìš© ë¶„ì•¼ì—ì„œ ì‚¬ìš©ë©ë‹ˆë‹¤.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 190, 'prompt_tokens': 24, 'total_tokens': 214, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-C6nSeISTnX9uVlRok7GbAM8spkWMO', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--da35c02a-b897-4f79-992f-711600ca49dd-0', usage_metadata={'input_tokens': 24, 'output_tokens': 190, 'total_tokens': 214, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke(\n",
    "    {\"input_messages\": \"ì½”ì‚¬ì¸ì˜ ì˜ë¯¸ëŠ” ë¬´ì—‡ì¸ê°€ìš”?\"},\n",
    "    # ì„¤ì • ì˜µì…˜ì„ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ì „ë‹¬í•©ë‹ˆë‹¤.\n",
    "    config={\"configurable\": {\"session_id\": \"xyz123\"}},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: redis in /opt/anaconda3/envs/LangChain/lib/python3.11/site-packages (5.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run -d -p 6379:6379 -p 8001:8001 redis/redis-stack:latest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redis ì„œë²„ì˜ URLì„ ì§€ì •í•©ë‹ˆë‹¤.\n",
    "REDIS_URL = \"redis://localhost:6379/0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import RedisChatMessageHistory\n",
    "\n",
    "\n",
    "def get_message_history(session_id: str) -> RedisChatMessageHistory:\n",
    "    # ì„¸ì…˜ IDë¥¼ ê¸°ë°˜ìœ¼ë¡œ RedisChatMessageHistory ê°ì²´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "    return RedisChatMessageHistory(session_id, url=REDIS_URL)\n",
    "\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    runnable,  # ì‹¤í–‰ ê°€ëŠ¥í•œ ê°ì²´\n",
    "    get_message_history,  # ë©”ì‹œì§€ ê¸°ë¡ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜\n",
    "    input_messages_key=\"input\",  # ì…ë ¥ ë©”ì‹œì§€ì˜ í‚¤\n",
    "    history_messages_key=\"history\",  # ê¸°ë¡ ë©”ì‹œì§€ì˜ í‚¤\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='cosine ëŠ” ì‚¼ê°í•¨ìˆ˜ ì¤‘ í•˜ë‚˜ë¡œ, ì§ê° ì‚¼ê°í˜•ì—ì„œ ë°‘ë³€ê³¼ ë¹—ë³€ì˜ ë¹„ìœ¨ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 49, 'prompt_tokens': 47, 'total_tokens': 96, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-C6nWdGhqXP73jTTC9RzF73FASdeoF', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--fcbc0df2-60a8-4775-9b9a-47b3616161b9-0', usage_metadata={'input_tokens': 47, 'output_tokens': 49, 'total_tokens': 96, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke(\n",
    "    # ìˆ˜í•™ ê´€ë ¨ ì§ˆë¬¸ \"ì½”ì‚¬ì¸ì˜ ì˜ë¯¸ëŠ” ë¬´ì—‡ì¸ê°€ìš”?\"ë¥¼ ì…ë ¥ìœ¼ë¡œ ì „ë‹¬í•©ë‹ˆë‹¤.\n",
    "    {\"ability\": \"math\", \"input\": \"What does cosine mean?\"},\n",
    "    # ì„¤ì • ì˜µì…˜ìœ¼ë¡œ ì„¸ì…˜ IDë¥¼ \"redis123\" ë¡œ ì§€ì •í•©ë‹ˆë‹¤.\n",
    "    config={\"configurable\": {\"session_id\": \"redis123\"}},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='cosineì€ ì‚¼ê°í•¨ìˆ˜ ì¤‘ í•˜ë‚˜ë¡œ, ì§ê° ì‚¼ê°í˜•ì—ì„œ ë°‘ë³€ê³¼ ë¹—ë³€ì˜ ë¹„ìœ¨ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 47, 'prompt_tokens': 122, 'total_tokens': 169, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-C6nWkOvbp0oOHoIFp9VFeXtXbcFQe', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--aedd2ecd-3b7f-47d6-8c38-650c0b80115e-0', usage_metadata={'input_tokens': 122, 'output_tokens': 47, 'total_tokens': 169, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke(\n",
    "    # ì´ì „ ë‹µë³€ì— ëŒ€í•œ í•œê¸€ ë²ˆì—­ì„ ìš”ì²­í•©ë‹ˆë‹¤.\n",
    "    {\"ability\": \"math\", \"input\": \"ì´ì „ì˜ ë‹µë³€ì„ í•œê¸€ë¡œ ë²ˆì—­í•´ ì£¼ì„¸ìš”.\"},\n",
    "    # ì„¤ì • ê°’ìœ¼ë¡œ ì„¸ì…˜ IDë¥¼ \"foobar\"ë¡œ ì§€ì •í•©ë‹ˆë‹¤.\n",
    "    config={\"configurable\": {\"session_id\": \"redis123\"}},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='ìˆ˜í•™ì— ëŠ¥í•˜ì‹  ë¹„ì„œì…ë‹ˆë‹¤.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 60, 'total_tokens': 73, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-C6nX8MfEZrT3XTLOZjGwUxoRdWOh0', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--373918c4-f05f-47fb-95ca-59beebbb185b-0', usage_metadata={'input_tokens': 60, 'output_tokens': 13, 'total_tokens': 73, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke(\n",
    "    # ì´ì „ ë‹µë³€ì— ëŒ€í•œ í•œê¸€ ë²ˆì—­ì„ ìš”ì²­í•©ë‹ˆë‹¤.\n",
    "    {\"ability\": \"math\", \"input\": \"ì´ì „ì˜ ë‹µë³€ì„ í•œê¸€ë¡œ ë²ˆì—­í•´ ì£¼ì„¸ìš”.\"},\n",
    "    # ì„¤ì • ê°’ìœ¼ë¡œ ì„¸ì…˜ IDë¥¼ \"redis456\"ë¡œ ì§€ì •í•©ë‹ˆë‹¤.\n",
    "    config={\"configurable\": {\"session_id\": \"redis456\"}},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì‚¬ìš©ì ì •ì˜ ì œë„¤ë ˆì´í„°(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterator, List\n",
    "\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    # ì£¼ì–´ì§„ íšŒì‚¬ì™€ ìœ ì‚¬í•œ 5ê°œì˜ íšŒì‚¬ë¥¼ ì‰¼í‘œë¡œ êµ¬ë¶„ëœ ëª©ë¡ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.\n",
    "    \"Write a comma-separated list of 5 companies similar to: {company}\"\n",
    ")\n",
    "# ì˜¨ë„ë¥¼ 0.0ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ChatOpenAI ëª¨ë¸ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.\n",
    "model = ChatOpenAI(temperature=0.0, model=\"gpt-4.1\")\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸ì™€ ëª¨ë¸ì„ ì—°ê²°í•˜ê³  ë¬¸ìì—´ ì¶œë ¥ íŒŒì„œë¥¼ ì ìš©í•˜ì—¬ ì²´ì¸ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "str_chain = prompt | model | StrOutputParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microsoft, Apple, Amazon, Meta, IBM"
     ]
    }
   ],
   "source": [
    "# ë°ì´í„°ë¥¼ ìŠ¤íŠ¸ë¦¬ë°í•©ë‹ˆë‹¤.\n",
    "for chunk in str_chain.stream({\"company\": \"Google\"}):\n",
    "    # ê° ì²­í¬ë¥¼ ì¶œë ¥í•˜ê³ , ì¤„ ë°”ê¿ˆ ì—†ì´ ë²„í¼ë¥¼ ì¦‰ì‹œ í”ŒëŸ¬ì‹œí•©ë‹ˆë‹¤.\n",
    "    print(chunk, end=\"\", flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì…ë ¥ìœ¼ë¡œ llm í† í°ì˜ ë°˜ë³µìë¥¼ ë°›ì•„ ì‰¼í‘œë¡œ êµ¬ë¶„ëœ ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸ë¡œ ë¶„í• í•˜ëŠ” ì‚¬ìš©ì ì •ì˜ íŒŒì„œì…ë‹ˆë‹¤.\n",
    "def split_into_list(input: Iterator[str]) -> Iterator[List[str]]:\n",
    "    # ì‰¼í‘œê°€ ë‚˜ì˜¬ ë•Œê¹Œì§€ ë¶€ë¶„ ì…ë ¥ì„ ë³´ê´€í•©ë‹ˆë‹¤.\n",
    "    buffer = \"\"\n",
    "    for chunk in input:\n",
    "        # í˜„ì¬ ì²­í¬ë¥¼ ë²„í¼ì— ì¶”ê°€í•©ë‹ˆë‹¤.\n",
    "        buffer += chunk\n",
    "        # ë²„í¼ì— ì‰¼í‘œê°€ ìˆëŠ” ë™ì•ˆ ë°˜ë³µí•©ë‹ˆë‹¤.\n",
    "        while \",\" in buffer:\n",
    "            # ë²„í¼ë¥¼ ì‰¼í‘œë¡œ ë¶„í• í•©ë‹ˆë‹¤.\n",
    "            comma_index = buffer.index(\",\")\n",
    "            # ì‰¼í‘œ ì´ì „ì˜ ëª¨ë“  ë‚´ìš©ì„ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "            yield [buffer[:comma_index].strip()]\n",
    "            # ë‚˜ë¨¸ì§€ëŠ” ë‹¤ìŒ ë°˜ë³µì„ ìœ„í•´ ì €ì¥í•©ë‹ˆë‹¤.\n",
    "            buffer = buffer[comma_index + 1 :]\n",
    "    # ë§ˆì§€ë§‰ ì²­í¬ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "    yield [buffer.strip()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_chain = str_chain | split_into_list  # ë¬¸ìì—´ ì²´ì¸ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë¶„í• í•©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Microsoft']\n",
      "['Apple']\n",
      "['Amazon']\n",
      "['Meta']\n",
      "['IBM']\n"
     ]
    }
   ],
   "source": [
    "# ìƒì„±í•œ list_chain ì´ ë¬¸ì œì—†ì´ ìŠ¤íŠ¸ë¦¬ë°ë˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.\n",
    "for chunk in list_chain.stream({\"company\": \"Google\"}):\n",
    "    print(chunk, flush=True)  # ê° ì²­í¬ë¥¼ ì¶œë ¥í•˜ê³ , ë²„í¼ë¥¼ ì¦‰ì‹œ í”ŒëŸ¬ì‹œí•©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Microsoft', 'Apple', 'Amazon', 'Meta', 'IBM']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list_chain ì— ë°ì´í„°ë¥¼ invoke í•©ë‹ˆë‹¤.\n",
    "list_chain.invoke({\"company\": \"Google\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import AsyncIterator\n",
    "\n",
    "\n",
    "# ë¹„ë™ê¸° í•¨ìˆ˜ ì •ì˜\n",
    "async def asplit_into_list(input: AsyncIterator[str]) -> AsyncIterator[List[str]]:\n",
    "    buffer = \"\"\n",
    "    # `input`ì€ `async_generator` ê°ì²´ì´ë¯€ë¡œ `async for`ë¥¼ ì‚¬ìš©\n",
    "    async for chunk in input:\n",
    "        buffer += chunk\n",
    "        while \",\" in buffer:\n",
    "            comma_index = buffer.index(\",\")\n",
    "            yield [\n",
    "                buffer[:comma_index].strip()\n",
    "            ]  # ì‰¼í‘œë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë¶„í• í•˜ì—¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜\n",
    "            buffer = buffer[comma_index + 1:]\n",
    "    yield [buffer.strip()]  # ë‚¨ì€ ë²„í¼ ë‚´ìš©ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜\n",
    "\n",
    "\n",
    "# alist_chain ê³¼ asplit_into_list ë¥¼ íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ì—°ê²°\n",
    "alist_chain = str_chain | asplit_into_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Microsoft']\n",
      "['Apple']\n",
      "['Amazon']\n",
      "['Meta']\n",
      "['IBM']\n"
     ]
    }
   ],
   "source": [
    "# async for ë£¨í”„ë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ë¥¼ ìŠ¤íŠ¸ë¦¬ë°í•©ë‹ˆë‹¤.\n",
    "async for chunk in alist_chain.astream({\"company\": \"Google\"}):\n",
    "    # ê° ì²­í¬ë¥¼ ì¶œë ¥í•˜ê³  ë²„í¼ë¥¼ ë¹„ì›ë‹ˆë‹¤.\n",
    "    print(chunk, flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Microsoft', 'Apple', 'Amazon', 'Meta', 'IBM']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ë¦¬ìŠ¤íŠ¸ ì²´ì¸ì„ ë¹„ë™ê¸°ì ìœ¼ë¡œ í˜¸ì¶œí•©ë‹ˆë‹¤.\n",
    "await alist_chain.ainvoke({\"company\": \"Google\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Runtime Arguments ë°”ì¸ë”©\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EQUATION:  \n",
      "\\( x^3 + 7 = 12 \\)\n",
      "\n",
      "SOLUTION:  \n",
      "Subtract 7 from both sides:  \n",
      "\\( x^3 = 12 - 7 \\)  \n",
      "\\( x^3 = 5 \\)\n",
      "\n",
      "Take the cube root of both sides:  \n",
      "\\( x = \\sqrt[3]{5} \\)\n",
      "\n",
      "So,  \n",
      "\\( x = \\sqrt[3]{5} \\)\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            # ëŒ€ìˆ˜ ê¸°í˜¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¤ìŒ ë°©ì •ì‹ì„ ì‘ì„±í•œ ë‹¤ìŒ í’€ì´í•˜ì„¸ìš”.\n",
    "            \"Write out the following equation using algebraic symbols then solve it. \"\n",
    "            \"Use the format\\n\\nEQUATION:...\\nSOLUTION:...\\n\\n\",\n",
    "        ),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"{equation_statement}\",  # ì‚¬ìš©ìê°€ ì…ë ¥í•œ ë°©ì •ì‹ ë¬¸ì¥ì„ ë³€ìˆ˜ë¡œ ë°›ìŠµë‹ˆë‹¤.\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "# ChatOpenAI ëª¨ë¸ì„ ì´ˆê¸°í™”í•˜ê³  temperatureë¥¼ 0ìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.\n",
    "model = ChatOpenAI(model=\"gpt-4.1\", temperature=0)\n",
    "\n",
    "# ë°©ì •ì‹ ë¬¸ì¥ì„ ì…ë ¥ë°›ì•„ í”„ë¡¬í”„íŠ¸ì— ì „ë‹¬í•˜ê³ , ëª¨ë¸ì—ì„œ ìƒì„±ëœ ê²°ê³¼ë¥¼ ë¬¸ìì—´ë¡œ íŒŒì‹±í•©ë‹ˆë‹¤.\n",
    "runnable = (\n",
    "    {\"equation_statement\": RunnablePassthrough()} | prompt | model | StrOutputParser()\n",
    ")\n",
    "\n",
    "# ì˜ˆì‹œ ë°©ì •ì‹ ë¬¸ì¥ì„ ì…ë ¥í•˜ì—¬ ê²°ê³¼ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.\n",
    "print(runnable.invoke(\"x raised to the third plus seven equals 12\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EQUATION:  \n",
      "\\( x^3 + 7 = 12 \\)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "runnable = (\n",
    "    # ì‹¤í–‰ ê°€ëŠ¥í•œ íŒ¨ìŠ¤ìŠ¤ë£¨ ê°ì²´ë¥¼ ìƒì„±í•˜ì—¬ \"equation_statement\" í‚¤ì— í• ë‹¹í•©ë‹ˆë‹¤.\n",
    "    {\"equation_statement\": RunnablePassthrough()}\n",
    "    | prompt  # í”„ë¡¬í”„íŠ¸ë¥¼ íŒŒì´í”„ë¼ì¸ì— ì¶”ê°€í•©ë‹ˆë‹¤.\n",
    "    | model.bind(\n",
    "        stop=\"SOLUTION\"\n",
    "    )  # ëª¨ë¸ì„ ë°”ì¸ë”©í•˜ê³  \"SOLUTION\" í† í°ì—ì„œ ìƒì„±ì„ ì¤‘ì§€í•˜ë„ë¡ ì„¤ì •í•©ë‹ˆë‹¤.\n",
    "    | StrOutputParser()  # ë¬¸ìì—´ ì¶œë ¥ íŒŒì„œë¥¼ íŒŒì´í”„ë¼ì¸ì— ì¶”ê°€í•©ë‹ˆë‹¤.\n",
    ")\n",
    "# \"x raised to the third plus seven equals 12\"ë¼ëŠ” ì…ë ¥ìœ¼ë¡œ íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.\n",
    "print(runnable.invoke(\"x raised to the third plus seven equals 12\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_function = {\n",
    "    \"name\": \"solver\",  # í•¨ìˆ˜ì˜ ì´ë¦„\n",
    "    # í•¨ìˆ˜ì˜ ì„¤ëª…: ë°©ì •ì‹ì„ ìˆ˜ë¦½í•˜ê³  í•´ê²°í•©ë‹ˆë‹¤.\n",
    "    \"description\": \"Formulates and solves an equation\",\n",
    "    \"parameters\": {  # í•¨ìˆ˜ì˜ ë§¤ê°œë³€ìˆ˜\n",
    "        \"type\": \"object\",  # ë§¤ê°œë³€ìˆ˜ì˜ íƒ€ì…: ê°ì²´\n",
    "        \"properties\": {  # ë§¤ê°œë³€ìˆ˜ì˜ ì†ì„±\n",
    "            \"equation\": {  # ë°©ì •ì‹ ì†ì„±\n",
    "                \"type\": \"string\",  # ë°©ì •ì‹ì˜ íƒ€ì…: ë¬¸ìì—´\n",
    "                \"description\": \"The algebraic expression of the equation\",  # ë°©ì •ì‹ì˜ ëŒ€ìˆ˜ì‹ í‘œí˜„\n",
    "            },\n",
    "            \"solution\": {  # í•´ë‹µ ì†ì„±\n",
    "                \"type\": \"string\",  # í•´ë‹µì˜ íƒ€ì…: ë¬¸ìì—´\n",
    "                \"description\": \"The solution to the equation\",  # ë°©ì •ì‹ì˜ í•´ë‹µ\n",
    "            },\n",
    "        },\n",
    "        \"required\": [\"equation\", \"solution\"],  # í•„ìˆ˜ ë§¤ê°œë³€ìˆ˜: ë°©ì •ì‹ê³¼ í•´ë‹µ\n",
    "    },\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"equation\":\"x^3 + 7 = 12\",\"solution\":\"x^3 = 12 - 7 = 5; x = 5^(1/3)\"}', 'name': 'solver'}, 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 39, 'prompt_tokens': 95, 'total_tokens': 134, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-2025-04-14', 'system_fingerprint': 'fp_3502f4eb73', 'id': 'chatcmpl-C6ncYrFNd6qOBDmpeZAybrRdZv3GG', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--580a6dc6-f2a3-483d-ad8b-cf7637a54592-0', usage_metadata={'input_tokens': 95, 'output_tokens': 39, 'total_tokens': 134, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ë‹¤ìŒ ë°©ì •ì‹ì„ ëŒ€ìˆ˜ ê¸°í˜¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‘ì„±í•œ ë‹¤ìŒ í•´ê²°í•˜ì„¸ìš”.\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Write out the following equation using algebraic symbols then solve it.\",\n",
    "        ),\n",
    "        (\"human\", \"{equation_statement}\"),\n",
    "    ]\n",
    ")\n",
    "model = ChatOpenAI(model=\"gpt-4.1\", temperature=0).bind(\n",
    "    function_call={\"name\": \"solver\"},  # openai_function schema ë¥¼ ë°”ì¸ë”©í•©ë‹ˆë‹¤.\n",
    "    functions=[openai_function],\n",
    ")\n",
    "runnable = {\"equation_statement\": RunnablePassthrough()} | prompt | model\n",
    "# xì˜ ì„¸ì œê³±ì— 7ì„ ë”í•˜ë©´ 12ì™€ ê°™ë‹¤\n",
    "runnable.invoke(\"x raised to the third plus seven equals 12\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_current_weather\",  # í˜„ì¬ ë‚ ì”¨ë¥¼ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜ì˜ ì´ë¦„\n",
    "            \"description\": \"ì£¼ì–´ì§„ ìœ„ì¹˜ì˜ í˜„ì¬ ë‚ ì”¨ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤\",  # í•¨ìˆ˜ì— ëŒ€í•œ ì„¤ëª…\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"ë„ì‹œì™€ ì£¼, ì˜ˆ: San Francisco, CA\",  # ìœ„ì¹˜ ë§¤ê°œë³€ìˆ˜ì— ëŒ€í•œ ì„¤ëª…\n",
    "                    },\n",
    "                    # ì˜¨ë„ ë‹¨ìœ„ ë§¤ê°œë³€ìˆ˜ (ì„­ì”¨ ë˜ëŠ” í™”ì”¨)\n",
    "                    \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n",
    "                },\n",
    "                \"required\": [\"location\"],  # í•„ìˆ˜ ë§¤ê°œë³€ìˆ˜ ì§€ì •\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_kD6Qsr4eCLXiqFfLNgHn4QQ8', 'function': {'arguments': '{\"location\": \"San Francisco, CA\"}', 'name': 'get_current_weather'}, 'type': 'function'}, {'id': 'call_sWMMP2PVbbnEvwpQMCPevXod', 'function': {'arguments': '{\"location\": \"New York, NY\"}', 'name': 'get_current_weather'}, 'type': 'function'}, {'id': 'call_XSm8r03yuNkB4s8tBeh6WfGM', 'function': {'arguments': '{\"location\": \"Los Angeles, CA\"}', 'name': 'get_current_weather'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 70, 'prompt_tokens': 99, 'total_tokens': 169, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-2025-04-14', 'system_fingerprint': 'fp_daf5fcc80a', 'id': 'chatcmpl-C6nczCX1kiRWE39M6f8IsovtcQl8c', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--6effe5a1-3dcf-4614-94ed-042db292e3ef-0', tool_calls=[{'name': 'get_current_weather', 'args': {'location': 'San Francisco, CA'}, 'id': 'call_kD6Qsr4eCLXiqFfLNgHn4QQ8', 'type': 'tool_call'}, {'name': 'get_current_weather', 'args': {'location': 'New York, NY'}, 'id': 'call_sWMMP2PVbbnEvwpQMCPevXod', 'type': 'tool_call'}, {'name': 'get_current_weather', 'args': {'location': 'Los Angeles, CA'}, 'id': 'call_XSm8r03yuNkB4s8tBeh6WfGM', 'type': 'tool_call'}], usage_metadata={'input_tokens': 99, 'output_tokens': 70, 'total_tokens': 169, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ChatOpenAI ëª¨ë¸ì„ ì´ˆê¸°í™”í•˜ê³  ë„êµ¬ë¥¼ ë°”ì¸ë”©í•©ë‹ˆë‹¤.\n",
    "model = ChatOpenAI(model=\"gpt-4.1\").bind(tools=tools)\n",
    "# ëª¨ë¸ì„ í˜¸ì¶œí•˜ì—¬ ìƒŒí”„ë€ì‹œìŠ¤ì½”, ë‰´ìš•, ë¡œìŠ¤ì•¤ì ¤ë ˆìŠ¤ì˜ ë‚ ì”¨ì— ëŒ€í•´ ì§ˆë¬¸í•©ë‹ˆë‹¤.\n",
    "model.invoke(\"ìƒŒí”„ë€ì‹œìŠ¤ì½”, ë‰´ìš•, ë¡œìŠ¤ì•¤ì ¤ë ˆìŠ¤ì˜ í˜„ì¬ ë‚ ì”¨ì— ëŒ€í•´ ì•Œë ¤ì¤˜?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "í´ë°±(fallback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_openai import ChatOpenAI\n",
    "from unittest.mock import patch\n",
    "\n",
    "import httpx\n",
    "from openai import RateLimitError\n",
    "\n",
    "request = httpx.Request(\"GET\", \"/\")  # GET ìš”ì²­ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "response = httpx.Response(\n",
    "    200, request=request\n",
    ")  # 200 ìƒíƒœ ì½”ë“œì™€ í•¨ê»˜ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "# \"rate limit\" ë©”ì‹œì§€ì™€ ì‘ë‹µ ë° ë¹ˆ ë³¸ë¬¸ì„ í¬í•¨í•˜ëŠ” RateLimitErrorë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "error = RateLimitError(\"rate limit\", response=response, body=\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAIì˜ ChatOpenAI ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ openai_llm ê°ì²´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "# max_retriesë¥¼ 0ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ì†ë„ ì œí•œ ë“±ìœ¼ë¡œ ì¸í•œ ì¬ì‹œë„ë¥¼ ë°©ì§€í•©ë‹ˆë‹¤.\n",
    "openai_llm = ChatOpenAI(max_retries=0)\n",
    "\n",
    "# Anthropicì˜ ChatAnthropic ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ anthropic_llm ê°ì²´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "anthropic_llm = ChatAnthropic(model=\"claude-3-7-sonnet-latest\")\n",
    "\n",
    "# openai_llmì„ ê¸°ë³¸ìœ¼ë¡œ ì‚¬ìš©í•˜ê³ , ì‹¤íŒ¨ ì‹œ anthropic_llmì„ ëŒ€ì²´ë¡œ ì‚¬ìš©í•˜ë„ë¡ ì„¤ì •í•©ë‹ˆë‹¤.\n",
    "llm = openai_llm.with_fallbacks([anthropic_llm])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì—ëŸ¬ ë°œìƒ\n"
     ]
    }
   ],
   "source": [
    "# OpenAI LLMì„ ë¨¼ì € ì‚¬ìš©í•˜ì—¬ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ëŠ” ê²ƒì„ ë³´ì—¬ì¤ë‹ˆë‹¤.\n",
    "with patch(\"openai.resources.chat.completions.Completions.create\", side_effect=error):\n",
    "    try:\n",
    "        # \"ë‹­ì´ ê¸¸ì„ ê±´ë„Œ ì´ìœ ëŠ” ë¬´ì—‡ì¼ê¹Œìš”?\"ë¼ëŠ” ì§ˆë¬¸ì„ OpenAI LLMì— ì „ë‹¬í•©ë‹ˆë‹¤.\n",
    "        print(openai_llm.invoke(\"Why did the chicken cross the road?\"))\n",
    "    except RateLimitError:\n",
    "        # ì˜¤ë¥˜ê°€ ë°œìƒí•˜ë©´ ì˜¤ë¥˜ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.\n",
    "        print(\"ì—ëŸ¬ ë°œìƒ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ëŠ” ì„œìš¸ì…ë‹ˆë‹¤. ì„œìš¸ì€ í•œêµ­ì˜ ì •ì¹˜, ê²½ì œ, ë¬¸í™”ì˜ ì¤‘ì‹¬ì§€ë¡œ, í•œê°•ì„ ë”°ë¼ ìœ„ì¹˜í•´ ìˆìœ¼ë©° ì•½ 1,000ë§Œ ëª…ì˜ ì¸êµ¬ê°€ ê±°ì£¼í•˜ê³  ìˆìŠµë‹ˆë‹¤.' additional_kwargs={} response_metadata={'id': 'msg_01RkfQnAV95JN64eStzQ5sEQ', 'model': 'claude-3-7-sonnet-20250219', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'cache_creation': {'ephemeral_1h_input_tokens': 0, 'ephemeral_5m_input_tokens': 0}, 'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 22, 'output_tokens': 86, 'server_tool_use': None, 'service_tier': 'standard'}, 'model_name': 'claude-3-7-sonnet-20250219'} id='run--fa7bd93e-fc18-4eb3-a6e2-400561dd368a-0' usage_metadata={'input_tokens': 22, 'output_tokens': 86, 'total_tokens': 108, 'input_token_details': {'cache_read': 0, 'cache_creation': 0, 'ephemeral_5m_input_tokens': 0, 'ephemeral_1h_input_tokens': 0}}\n"
     ]
    }
   ],
   "source": [
    "# OpenAI API í˜¸ì¶œ ì‹œ ì—ëŸ¬ê°€ ë°œìƒí•˜ëŠ” ê²½ìš° Anthropic ìœ¼ë¡œ ëŒ€ì²´í•˜ëŠ” ì½”ë“œ\n",
    "with patch(\"openai.resources.chat.completions.Completions.create\", side_effect=error):\n",
    "    try:\n",
    "        # \"ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì•¼?\"ë¼ëŠ” ì§ˆë¬¸ì„ ì–¸ì–´ ëª¨ë¸ì— ì „ë‹¬í•˜ì—¬ ì‘ë‹µì„ ì¶œë ¥í•©ë‹ˆë‹¤.\n",
    "        print(llm.invoke(\"ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì•¼?\"))\n",
    "    except RateLimitError:\n",
    "        # RateLimitErrorê°€ ë°œìƒí•˜ë©´ \"ì—ëŸ¬ ë°œìƒ\"ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.\n",
    "        print(\"ì—ëŸ¬ ë°œìƒ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ëŠ” ì„œìš¸ì…ë‹ˆë‹¤.' additional_kwargs={} response_metadata={'id': 'msg_01QVVeJa9NfUSXFfHmWvVsZy', 'model': 'claude-3-7-sonnet-20250219', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'cache_creation': {'ephemeral_1h_input_tokens': 0, 'ephemeral_5m_input_tokens': 0}, 'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 46, 'output_tokens': 20, 'server_tool_use': None, 'service_tier': 'standard'}, 'model_name': 'claude-3-7-sonnet-20250219'} id='run--783503ed-a00e-45f5-b2b6-773c577611e3-0' usage_metadata={'input_tokens': 46, 'output_tokens': 20, 'total_tokens': 66, 'input_token_details': {'cache_read': 0, 'cache_creation': 0, 'ephemeral_5m_input_tokens': 0, 'ephemeral_1h_input_tokens': 0}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"ì§ˆë¬¸ì— ì§§ê³  ê°„ê²°í•˜ê²Œ ë‹µë³€í•´ ì£¼ì„¸ìš”.\",  # ì‹œìŠ¤í…œ ì—­í•  ì„¤ëª…\n",
    "        ),\n",
    "        (\"human\", \"{country} ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì…ë‹ˆê¹Œ?\"),  # ì‚¬ìš©ì ì§ˆë¬¸ í…œí”Œë¦¿\n",
    "    ]\n",
    ")\n",
    "chain = prompt | llm  # í”„ë¡¬í”„íŠ¸ì™€ ì–¸ì–´ ëª¨ë¸ì„ ì—°ê²°í•˜ì—¬ ì²´ì¸ ìƒì„±\n",
    "# chain = prompt | ChatOpenAI() # ì´ ì½”ë“œì´ ì£¼ì„ì„ í’€ê³  ì‹¤í–‰í•˜ë©´ \"ì˜¤ë¥˜ ë°œìƒ\" ë¬¸êµ¬ê°€ ì¶œë ¥ë©ë‹ˆë‹¤.\n",
    "with patch(\"openai.resources.chat.completions.Completions.create\", side_effect=error):\n",
    "    try:\n",
    "        print(chain.invoke({\"country\": \"ëŒ€í•œë¯¼êµ­\"}))  # ì²´ì¸ì„ í˜¸ì¶œí•˜ì—¬ ê²°ê³¼ ì¶œë ¥\n",
    "    except RateLimitError:  # API ë¹„ìš© ì œí•œ ì˜¤ë¥˜ ì²˜ë¦¬\n",
    "        print(\"ì˜¤ë¥˜ ë°œìƒ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì˜¤ë¥˜ ë°œìƒ\n"
     ]
    }
   ],
   "source": [
    "llm = openai_llm.with_fallbacks(\n",
    "    # ëŒ€ì²´ LLMìœ¼ë¡œ anthropic_llmì„ ì‚¬ìš©í•˜ê³ , ì˜ˆì™¸ ì²˜ë¦¬í•  ëŒ€ìƒìœ¼ë¡œ KeyboardInterruptë¥¼ ì§€ì •í•©ë‹ˆë‹¤.\n",
    "    [anthropic_llm],\n",
    "    exceptions_to_handle=(KeyboardInterrupt,),  # ì˜ˆì™¸ ì²˜ë¦¬ ëŒ€ìƒì„ ì§€ì •í•©ë‹ˆë‹¤.\n",
    ")\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸ì™€ LLMì„ ì—°ê²°í•˜ì—¬ ì²´ì¸ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "chain = prompt | llm\n",
    "with patch(\"openai.resources.chat.completions.Completions.create\", side_effect=error):\n",
    "    try:\n",
    "        # ì²´ì¸ì„ í˜¸ì¶œí•˜ì—¬ ê²°ê³¼ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.\n",
    "        print(chain.invoke({\"country\": \"ëŒ€í•œë¯¼êµ­\"}))\n",
    "    except RateLimitError:\n",
    "        # RateLimitError ì˜ˆì™¸ê°€ ë°œìƒí•˜ë©´ \"ì˜¤ë¥˜ ë°œìƒ\"ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.\n",
    "        print(\"ì˜¤ë¥˜ ë°œìƒ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸ ìƒì„±\n",
    "prompt_template = (\n",
    "    \"ì§ˆë¬¸ì— ì§§ê³  ê°„ê²°í•˜ê²Œ ë‹µë³€í•´ ì£¼ì„¸ìš”.\\n\\nQuestion:\\n{question}\\n\\nAnswer:\"\n",
    ")\n",
    "prompt = PromptTemplate.from_template(prompt_template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì—¬ê¸°ì„œëŠ” ì‰½ê²Œ ì˜¤ë¥˜ë¥¼ ë°œìƒì‹œí‚¬ ìˆ˜ ìˆëŠ” ì˜ëª»ëœ ëª¨ë¸ ì´ë¦„ì„ ì‚¬ìš©í•˜ì—¬ ì²´ì¸ì„ ìƒì„±í•  ê²ƒì…ë‹ˆë‹¤.\n",
    "chat_model = ChatOpenAI(model_name=\"gpt-fake\")\n",
    "bad_chain = prompt | chat_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fallback ì²´ì¸ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "fallback_chain1 = prompt | ChatOpenAI(model=\"gpt-3.6-turbo\") # ì˜¤ë¥˜\n",
    "fallback_chain2 = prompt | ChatOpenAI(model=\"gpt-3.5-turbo\") # ì •ìƒ\n",
    "fallback_chain3 = prompt | ChatOpenAI(model=\"gpt-4-turbo-preview\") # ì •ìƒ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='ì„œìš¸ì…ë‹ˆë‹¤.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 46, 'total_tokens': 51, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-C6npD5369HoKl4K1gJpslfyxyy0sr', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--6cec6dd5-6978-4e3f-b101-aa45acf5950f-0', usage_metadata={'input_tokens': 46, 'output_tokens': 5, 'total_tokens': 51, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ë‘ ê°œì˜ ì²´ì¸ì„ ê²°í•©í•˜ì—¬ ìµœì¢… ì²´ì¸ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "chain = bad_chain.with_fallbacks(\n",
    "    [fallback_chain1, fallback_chain2, fallback_chain3])\n",
    "# ìƒì„±ëœ ì²´ì¸ì„ í˜¸ì¶œí•˜ì—¬ ì…ë ¥ê°’ì„ ì „ë‹¬í•©ë‹ˆë‹¤.\n",
    "chain.invoke({\"question\": \"ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì•¼?\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMs2OSEsEhI+eP+P38efuYH",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
